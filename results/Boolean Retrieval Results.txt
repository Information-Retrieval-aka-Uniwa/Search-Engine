============ Ερώτημα αναζήτησης: protein or language ============
--------------------------------------------------
#1
--------------------------------------------------
Document ID: 0
Title: Monadic transductions and definable classes of matroids
Authors: ['Susan Jowett', 'Dillon Mayhew', 'Songbao Mo', 'Christopher Tuffley']
Subjects: ['Combinatorics']
Abstract: A transduction provides us with a way of using the monadic second-order language of a structure to make statements about a derived structure. Any transduction induces a relation on the set of these structures. This article presents a self-contained presentation of the theory of transductions for the monadic second-order language of matroids. This includes a proof of the matroid version of the Backwards Translation Theorem, which lifts any formula applied to the images of the transduction into a formula which we can apply to the pre-images. Applications include proofs that the class of lattice-path matroids and the class of spike-minors can be defined by sentences in monadic second-order logic.
Comments:  
Date: 23 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.12969

--------------------------------------------------
#2
--------------------------------------------------
Document ID: 6
Title: Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion
Authors: ['Dylan Zhang', 'Curt Tigges', 'Zory Zhang', 'Stella Biderman', 'Maxim Raginsky', 'Talia Ringer']
Subjects: ['Computation and Language', 'Artificial Intelligence', 'Formal Languages and Automata Theory', 'Logic in Computer Science', 'Programming Languages']
Abstract: This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture.
  With our framework as a powerful conceptual tool, we identify different issues under various set-ups. The models trained to emulate recursive computations cannot fully capture the recursion yet instead fit short-cut algorithms and thus cannot solve certain edge cases that are under-represented in the training distribution. In addition, it is difficult for state-of-the-art large language models (LLMs) to mine recursive rules from in-context demonstrations. Meanwhile, these LLMs fail in interesting ways when emulating reduction (step-wise computation) of the recursive function.
Comments: arXiv admin note: text overlap with arXiv:2305.14699
Date: 23 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.12947

--------------------------------------------------
#3
--------------------------------------------------
Document ID: 58
Title: Controlling the C3 super class linearization algorithm
Authors: ['Florent Hivert', 'Nicolas M. Thiéry']
Subjects: ['Combinatorics']
Abstract: C3 is an algorithm used by several widely used programming languages such as Python to support multiple inheritance in object oriented programming (OOP): for each class, C3 computes recursively a linear extension of the poset of all its super classes (the Method Resolution Order, MRO) from user-provided local information (an ordering of the direct super classes). This algorithm can fail if the local information is not consistent.
  For large hierarchies of classes, as encountered when modeling hierarchies of concepts from abstract algebra in the SageMath computational system, maintaining consistent local information by hand does not scale and leads to unpredictable C3 failures.
  This paper reports on the authors' work to analyze and circumvent this maintenance nightmare. First, we discovered through extensive computer exploration that there exists posets admitting no consistent local information; we exhibit the smallest one which has 10 elements. Then, we provide and analyze an algorithm that, given a poset and a linear extension, automatically builds local information for C3 in such a way that guarantees that it will never fail, at the price of a slight relaxation of the hypotheses. This algorithm has been used in production in SageMath since 2013.
Comments: 15 pages
Date: 23 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.12740

--------------------------------------------------
#4
--------------------------------------------------
Document ID: 111
Title: The outcomes of generative AI are exactly the Nash equilibria of a non-potential game
Authors: ['Boualem Djehiche', 'Hamidou Tembine']
Subjects: ['Computer Science and Game Theory']
Abstract: In this article we show that the asymptotic outcomes of both shallow and deep neural networks such as those used in BloombergGPT to generate economic time series are exactly the Nash equilibria of a non-potential game. We then design and analyze deep neural network algorithms that converge to these equilibria. The methodology is extended to federated deep neural networks between clusters of regional servers and on-device clients. Finally, the variational inequalities behind large language models including encoder-decoder related transformers are established.
Comments: 24 pages. Accepted and to appear in: International Econometric Conference of Vietnam
Date: 22 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.12321

--------------------------------------------------
#5
--------------------------------------------------
Document ID: 192
Title: Learning to be Homo Economicus: Can an LLM Learn Preferences from Choice
Authors: ['Jeongbin Kim', 'Matthew Kovach', 'Kyu-Min Lee', 'Euncheol Shin', 'Hector Tzavellas']
Subjects: ['General Economics']
Abstract: This paper explores the use of Large Language Models (LLMs) as decision aids, with a focus on their ability to learn preferences and provide personalized recommendations. To establish a baseline, we replicate standard economic experiments on choice under risk (Choi et al., 2007) with GPT, one of the most prominent LLMs, prompted to respond as (i) a human decision maker or (ii) a recommendation system for customers. With these baselines established, GPT is provided with a sample set of choices and prompted to make recommendations based on the provided data. From the data generated by GPT, we identify its (revealed) preferences and explore its ability to learn from data. Our analysis yields three results. First, GPT's choices are consistent with (expected) utility maximization theory. Second, GPT can align its recommendations with people's risk aversion, by recommending less risky portfolios to more risk-averse decision makers, highlighting GPT's potential as a personalized decision aid. Third, however, GPT demonstrates limited alignment when it comes to disappointment aversion.
Comments:  
Date: 14 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.07345

--------------------------------------------------
#6
--------------------------------------------------
Document ID: 209
Title: PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding
Authors: ['Ruochi Zhang', 'Haoran Wu', 'Chang Liu', 'Huaping Li', 'Yuqian Wu', 'Kewei Li', 'Yifan Wang', 'Yifan Deng', 'Jiahui Chen', 'Fengfeng Zhou', 'Xin Gao']
Subjects: ['Machine Learning', 'Artificial Intelligence', 'Computational Engineering, Finance, and Science', 'Biomolecules']
Abstract: Recent advances in protein language models have catalyzed significant progress in peptide sequence representation. Despite extensive exploration in this field, pre-trained models tailored for peptide-specific needs remain largely unaddressed due to the difficulty in capturing the complex and sometimes unstable structures of peptides. This study introduces a novel multi-view contrastive learning framework PepHarmony for the sequence-based peptide encoding task. PepHarmony innovatively combines both sequence- and structure-level information into a sequence-level encoding module through contrastive learning. We carefully select datasets from the Protein Data Bank (PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences and structures. The experimental data highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures compared with the baseline and fine-tuned models. The robustness of our model is confirmed through extensive ablation studies, which emphasize the crucial roles of contrastive loss and strategic data sorting in enhancing predictive performance. The proposed PepHarmony framework serves as a notable contribution to peptide representations, and offers valuable insights for future applications in peptide drug discovery and peptide engineering. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony or http://www.healthinformaticslab.org/supp/.
Comments: 25 pages, 5 figures, 3 tables
Date: 20 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.11360

--------------------------------------------------
#7
--------------------------------------------------
Document ID: 211
Title: BioFinBERT: Finetuning Large Language Models (LLMs) to Analyze Sentiment of Press Releases and Financial Text Around Inflection Points of Biotech Stocks
Authors: ['Valentina Aparicio', 'Daniel Gordon', 'Sebastian G. Huayamares', 'Yuhuai Luo']
Subjects: ['General Finance', 'Computational Finance', 'Trading and Market Microstructure']
Abstract: Large language models (LLMs) are deep learning algorithms being used to perform natural language processing tasks in various fields, from social sciences to finance and biomedical sciences. Developing and training a new LLM can be very computationally expensive, so it is becoming a common practice to take existing LLMs and finetune them with carefully curated datasets for desired applications in different fields. Here, we present BioFinBERT, a finetuned LLM to perform financial sentiment analysis of public text associated with stocks of companies in the biotechnology sector. The stocks of biotech companies developing highly innovative and risky therapeutic drugs tend to respond very positively or negatively upon a successful or failed clinical readout or regulatory approval of their drug, respectively. These clinical or regulatory results are disclosed by the biotech companies via press releases, which are followed by a significant stock response in many cases. In our attempt to design a LLM capable of analyzing the sentiment of these press releases,we first finetuned BioBERT, a biomedical language representation model designed for biomedical text mining, using financial textual databases. Our finetuned model, termed BioFinBERT, was then used to perform financial sentiment analysis of various biotech-related press releases and financial text around inflection points that significantly affected the price of biotech stocks.
Comments:  
Date: 19 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.11011

--------------------------------------------------
#8
--------------------------------------------------
Document ID: 239
Title: Empirical Evidence for the Fragment level Understanding on Drug Molecular Structure of LLMs
Authors: ['Xiuyuan Hu', 'Guoqing Liu', 'Yang Zhao', 'Hao Zhang']
Subjects: ['Machine Learning', 'Computational Engineering, Finance, and Science', 'Biomolecules']
Abstract: AI for drug discovery has been a research hotspot in recent years, and SMILES-based language models has been increasingly applied in drug molecular design. However, no work has explored whether and how language models understand the chemical spatial structure from 1D sequences. In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments. The results indicate that language models can understand chemical structures from the perspective of molecular fragments, and the structural knowledge learned through fine-tuning is reflected in the high-frequency SMILES substrings generated by the model.
Comments: Accepted by AAAI 2024 workshop: Large Language Models for Biological Discoveries (LLMs4Bio)
Date: 15 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.07657

--------------------------------------------------
#9
--------------------------------------------------
Document ID: 240
Title: Graph database while computationally efficient filters out quickly the ESG integrated equities in investment management
Authors: ['Partha Sen', 'Sumana Sen']
Subjects: ['Computational Finance']
Abstract: Design/methodology/approach This research evaluated the databases of SQL, No-SQL and graph databases to compare and contrast efficiency and performance. To perform this experiment the data were collected from multiple sources including stock price and financial news. Python is used as an interface to connect and query databases (to create database structures according to the feed file structure, to load data into tables, objects, to read data , to connect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM (Large language model) including RAG (Retrieval Augmented Generation) with Machine Learning, deep learning, NLP (natural language processing) or Decision Analytics are computationally expensive. Finding a better option to consume less resources and time to get the result. Findings The Graph database of ESG (Environmental, Social and Governance) is comparatively better and can be considered for extended analytics to integrate ESG in business and investment. Practical implications A graph ML with a RAG architecture model can be introduced as a new framework with less computationally expensive LLM application in the equity filtering process for portfolio management. Originality/value Filtering out selective stocks out of two thousand or more listed companies in any stock exchange for active investment, consuming less resource consumption especially memory and energy to integrate artificial intelligence and ESG in business and investment.
Comments: 10 pages, 17 figures
Date: 15 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.07483

--------------------------------------------------
#10
--------------------------------------------------
Document ID: 242
Title: DocFinQA: A Long-Context Financial Reasoning Dataset
Authors: ['Varshini Reddy', 'Rik Koncel-Kedziorski', 'Viet Dac Lai', 'Chris Tanner']
Subjects: ['Computation and Language', 'Artificial Intelligence']
Abstract: Research in quantitative reasoning within the financial domain indeed necessitates the use of realistic tasks and data, primarily because of the significant impact of decisions made in business and finance. Financial professionals often interact with documents hundreds of pages long, but most research datasets drastically reduce this context length. To address this, we introduce a long-document financial QA task. We augment 7,621 questions from the existing FinQA dataset with full-document context, extending the average context length for each question from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments of retrieval-based QA pipelines and long-context language models on the augmented data. Our results show that DocFinQA provides challenges for even the strongest, state-of-the-art systems.
Comments: 13 pages
Date: 12 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.06915

--------------------------------------------------
#11
--------------------------------------------------
Document ID: 248
Title: Multimodal Gen-AI for Fundamental Investment Research
Authors: ['Lezhi Li', 'Ting-Yu Chang', 'Hai Wang']
Subjects: ['General Finance', 'Machine Learning']
Abstract: This report outlines a transformative initiative in the financial investment industry, where the conventional decision-making process, laden with labor-intensive tasks such as sifting through voluminous documents, is being reimagined. Leveraging language models, our experiments aim to automate information summarization and investment idea generation. We seek to evaluate the effectiveness of fine-tuning methods on a base model (Llama2) to achieve specific application-level goals, including providing insights into the impact of events on companies and sectors, understanding market condition relationships, generating investor-aligned investment ideas, and formatting results with stock recommendations and detailed explanations. Through state-of-the-art generative modeling techniques, the ultimate objective is to develop an AI agent prototype, liberating human investors from repetitive tasks and allowing a focus on high-level strategic thinking. The project encompasses a diverse corpus dataset, including research reports, investment memos, market news, and extensive time-series market data. We conducted three experiments applying unsupervised and supervised LoRA fine-tuning on the llama2_7b_hf_chat as the base model, as well as instruction fine-tuning on the GPT3.5 model. Statistical and human evaluations both show that the fine-tuned versions perform better in solving text modeling, summarization, reasoning, and finance domain questions, demonstrating a pivotal step towards enhancing decision-making processes in the financial domain. Code implementation for the project can be found on GitHub: https://github.com/Firenze11/finance_lm.
Comments:  
Date: 23 December, 2023
PDF_URL: https://arxiv.org/pdf/2401.06164

--------------------------------------------------
#12
--------------------------------------------------
Document ID: 249
Title: De novo Drug Design using Reinforcement Learning with Multiple GPT Agents
Authors: ['Xiuyuan Hu', 'Guoqing Liu', 'Yang Zhao', 'Hao Zhang']
Subjects: ['Biomolecules', 'Computational Engineering, Finance, and Science', 'Machine Learning']
Abstract: De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT.
Comments: Accepted by NeurIPS 2023
Date: 21 December, 2023
PDF_URL: https://arxiv.org/pdf/2401.06155

--------------------------------------------------
#13
--------------------------------------------------
Document ID: 253
Title: Designing Heterogeneous LLM Agents for Financial Sentiment Analysis
Authors: ['Frank Xing']
Subjects: ['Computation and Language', 'Artificial Intelligence', 'Multiagent Systems', 'General Finance']
Abstract: Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed.
Comments: 15 pages
Date: 11 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.05799

--------------------------------------------------
#14
--------------------------------------------------
Document ID: 274
Title: Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection
Authors: ['Georgios Fatouros', 'Konstantinos Metaxas', 'John Soldatos', 'Dimosthenis Kyriazis']
Subjects: ['Computational Finance', 'Artificial Intelligence', 'Computational Engineering, Finance, and Science', 'Computation and Language', 'Machine Learning']
Abstract: In the dynamic and data-driven landscape of financial markets, this paper introduces MarketSenseAI, a novel AI-driven framework leveraging the advanced reasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI incorporates Chain of Thought and In-Context Learning methodologies to analyze a wide array of data sources, including market price dynamics, financial news, company fundamentals, and macroeconomic reports emulating the decision making process of prominent financial investment teams. The development, implementation, and empirical validation of MarketSenseAI are detailed, with a focus on its ability to provide actionable investment signals (buy, hold, sell) backed by cogent explanations. A notable aspect of this study is the use of GPT-4 not only as a predictive tool but also as an evaluator, revealing the significant impact of the AI-generated explanations on the reliability and acceptance of the suggested investment signals. In an extensive empirical evaluation with S&P 100 stocks, MarketSenseAI outperformed the benchmark index by 13%, achieving returns up to 40%, while maintaining a risk profile comparable to the market. These results demonstrate the efficacy of Large Language Models in complex financial decision-making and mark a significant advancement in the integration of AI into financial analysis and investment strategies. This research contributes to the financial AI field, presenting an innovative approach and underscoring the transformative potential of AI in revolutionizing traditional financial analysis investment methodologies.
Comments: 15 pages, 12 figures, 12 tables
Date: 8 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.03737

--------------------------------------------------
#15
--------------------------------------------------
Document ID: 284
Title: ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach
Authors: ['Zeynep Hilal Kilimci', 'Mustafa Yalcin']
Subjects: ['Biomolecules', 'Artificial Intelligence', 'Computational Engineering, Finance, and Science', 'Machine Learning']
Abstract: Anticancer peptides (ACPs) are a class of molecules that have gained significant attention in the field of cancer research and therapy. ACPs are short chains of amino acids, the building blocks of proteins, and they possess the ability to selectively target and kill cancer cells. One of the key advantages of ACPs is their ability to selectively target cancer cells while sparing healthy cells to a greater extent. This selectivity is often attributed to differences in the surface properties of cancer cells compared to normal cells. That is why ACPs are being investigated as potential candidates for cancer therapy. ACPs may be used alone or in combination with other treatment modalities like chemotherapy and radiation therapy. While ACPs hold promise as a novel approach to cancer treatment, there are challenges to overcome, including optimizing their stability, improving selectivity, and enhancing their delivery to cancer cells, continuous increasing in number of peptide sequences, developing a reliable and precise prediction model. In this work, we propose an efficient transformer-based framework to identify anticancer peptides for by performing accurate a reliable and precise prediction model. For this purpose, four different transformer models, namely ESM, ProtBert, BioBERT, and SciBERT are employed to detect anticancer peptides from amino acid sequences. To demonstrate the contribution of the proposed framework, extensive experiments are carried on widely-used datasets in the literature, two versions of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of proposed model enhances classification accuracy when compared to the state-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of accuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and 88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.
Comments:  
Date: 4 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.02124

--------------------------------------------------
#16
--------------------------------------------------
Document ID: 287
Title: Text mining arXiv: a look through quantitative finance papers
Authors: ['Michele Leonardo Bianchi']
Subjects: ['Digital Libraries', 'Information Retrieval', 'General Finance']
Abstract: This paper explores articles hosted on the arXiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research. Employing text mining techniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arXiv from 1997 to 2022. We extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain. Additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches.
Comments:  
Date: 3 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.01751

--------------------------------------------------
#17
--------------------------------------------------
Document ID: 302
Title: HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments
Authors: ['Qinhong Zhou', 'Sunli Chen', 'Yisong Wang', 'Haozhe Xu', 'Weihua Du', 'Hongxin Zhang', 'Yilun Du', 'Joshua B. Tenenbaum', 'Chuang Gan']
Subjects: ['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Computation and Language']
Abstract: Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.
Comments: ICLR 2024. The first two authors contributed equally to this work
Date: 23 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.12975

--------------------------------------------------
#18
--------------------------------------------------
Document ID: 306
Title: AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents
Authors: ['Michael Ahn', 'Debidatta Dwibedi', 'Chelsea Finn', 'Montse Gonzalez Arenas', 'Keerthana Gopalakrishnan', 'Karol Hausman', 'Brian Ichter', 'Alex Irpan', 'Nikhil Joshi', 'Ryan Julian', 'Sean Kirmani', 'Isabel Leal', 'Edward Lee', 'Sergey Levine', 'Yao Lu', 'Isabel Leal', 'Sharath Maddineni', 'Kanishka Rao', 'Dorsa Sadigh', 'Pannag Sanketi', 'Pierre Sermanet', 'Quan Vuong', 'Stefan Welker', 'Fei Xia', 'Ted Xiao', 'et al. (3 additional authors not shown)']
Subjects: ['Robotics', 'Artificial Intelligence', 'Computation and Language', 'Computer Vision and Pattern Recognition', 'Machine Learning']
Abstract: Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such "in-the-wild" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.
Comments: 26 pages, 9 figures
Date: 23 January, 2024
PDF_URL: https://arxiv.org/pdf/2401.12963

