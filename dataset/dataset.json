[
    {
        "doc_id": 0,
        "title": "Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning",
        "authors": [
            "Philip Amortila",
            "Tongyi Cao",
            "Akshay Krishnamurthy"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Optimization and Control"
        ],
        "abstract": "A pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ. As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. In this paper, we study the effect of distribution shift in the presence of model misspecification, specifically focusing on $L_{\\infty}$-misspecified regression and adversarial covariate shift, where the regression target remains fixed while the covariate distribution changes arbitrarily. We show that empirical risk minimization, or standard least squares regression, can result in undesirable misspecification amplification where the error due to misspecification is amplified by the density ratio between the training and testing distributions. As our main result, we develop a new algorithm -- inspired by robust optimization techniques -- that avoids this undesirable behavior, resulting in no misspecification amplification while still obtaining optimal statistical rates. As applications, we use this regression procedure to obtain new guarantees in offline and online reinforcement learning with misspecification and establish new separations between previously studied structural conditions and notions of coverage.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12216"
    },
    {
        "doc_id": 1,
        "title": "Unsupervised Machine Learning for the Classification of Astrophysical X-ray Sources",
        "authors": [
            "V\u00edctor Samuel P\u00e9rez-D\u00edaz",
            "Juan Rafael Mart\u00ednez-Galarza",
            "Alexander Caicedo",
            "Raffaele D'Abrusco"
        ],
        "subjects": [
            "Instrumentation and Methods for Astrophysics",
            "Artificial Intelligence"
        ],
        "abstract": "The automatic classification of X-ray detections is a necessary step in extracting astrophysical information from compiled catalogs of astrophysical sources. Classification is useful for the study of individual objects, statistics for population studies, as well as for anomaly detection, i.e., the identification of new unexplored phenomena, including transients and spectrally extreme sources. Despite the importance of this task, classification remains challenging in X-ray astronomy due to the lack of optical counterparts and representative training sets. We develop an alternative methodology that employs an unsupervised machine learning approach to provide probabilistic classes to Chandra Source Catalog sources with a limited number of labeled sources, and without ancillary information from optical and infrared catalogs. We provide a catalog of probabilistic classes for 8,756 sources, comprising a total of 14,507 detections, and demonstrate the success of the method at identifying emission from young stellar objects, as well as distinguishing between small-scale and large-scale compact accretors with a significant level of confidence. We investigate the consistency between the distribution of features among classified objects and well-established astrophysical hypotheses such as the unified AGN model. This provides interpretability to the probabilistic classifier. Code and tables are available publicly through GitHub. We provide a web playground for readers to explore our final classification at https://umlcaxs-playground.streamlit.app.",
        "comments": "21 pages, 11 figures. Accepted in MNRAS",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12203"
    },
    {
        "doc_id": 2,
        "title": "Using spatial extreme-value theory with machine learning to model and understand spatially compounding extremes",
        "authors": [
            "Jonathan Koh",
            "Daniel Steinfeld",
            "Olivia Martius"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "When extreme weather events affect large areas, their regional to sub-continental spatial scale is important for their impacts. We propose a novel methodology that combines spatial extreme-value theory with a machine learning (ML) algorithm to model weather extremes and quantify probabilities associated with the occurrence, intensity and spatial extent of these events. The model is here applied to Western European summertime heat extremes. Using new loss functions adapted to extreme values, we fit a theoretically-motivated spatial model to extreme positive temperature anomaly fields from 1959-2022, using the daily 500-hpa geopotential height fields across the Euro-Atlantic region and the local soil moisture as predictors. Our generative model reveals the importance of individual circulation features in determining different facets of heat extremes, thereby enriching our process understanding of them from a data-driven perspective. The occurrence, intensity, and spatial extent of heat extremes are sensitive to the relative position of individual ridges and troughs that are part of a large-scale wave pattern. Heat extremes in Europe are thus the result of a complex interplay between local and remote physical processes. Our approach is able to extrapolate beyond the range of the data to make risk-related probabilistic statements, and applies more generally to other weather extremes. It also offers an attractive alternative to physical model-based techniques, or to ML approaches that optimise scores focusing on predicting well the bulk instead of the tail of the data distribution.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12195"
    },
    {
        "doc_id": 3,
        "title": "Concentration inequalities for the sample correlation coefficient",
        "authors": [
            "Daniel Salnikov"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "The sample correlation coefficient $R$ plays an important role in many statistical analyses. We study the moments of $R$ under the bivariate Gaussian model assumption, provide a novel approximation for its finite sample mean and connect it with known results for the variance. We exploit these approximations to present non-asymptotic concentration inequalities for $R$. Finally, we illustrate our results in a simulation experiment that further validates the approximations presented in this work.",
        "comments": "10 pages, preprint",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12190"
    },
    {
        "doc_id": 4,
        "title": "Semi-supervised segmentation of land cover images using nonlinear canonical correlation analysis with multiple features and t-SNE",
        "authors": [
            "Hong Wei",
            "James Xiao",
            "Yichao Zhang",
            "Xia Hong"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ],
        "abstract": "Image segmentation is a clustering task whereby each pixel is assigned a cluster label. Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available. This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple bands and a local patch as appropriate. Similarly, multiple types of texture features based on a pixel's local patch would also be beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount of ground truth, then training a supervised model, which is sometimes impractical. In this work, by resorting to label only a small quantity of pixels, a new semi-supervised segmentation approach is proposed. Initially, over all pixels, an image data matrix is created in high dimensional feature space. Then, t-SNE projects the high dimensional data onto 3D embedding. By using radial basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix via the small labelled data set. The associated canonical variables, obtained for the full image, are applied by k-means clustering algorithm. The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral images, demonstrating excellent segmentation results.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12164"
    },
    {
        "doc_id": 5,
        "title": "The accuracy of ALMA estimates of young disk radii and masses. Predicted observations from numerical simulations",
        "authors": [
            "Ngo-Duy Tung",
            "Leonardo Testi",
            "Ugo Lebreuilly",
            "Patrick Hennebelle",
            "Ana\u00eblle Maury",
            "Ralf S. Klessen",
            "Luca Cacciapuoti",
            "Matthias Gonz\u00e1lez",
            "Giovanni Rosotti",
            "Sergio Molinari"
        ],
        "subjects": [
            "Earth and Planetary Astrophysics",
            "Solar and Stellar Astrophysics"
        ],
        "abstract": "Protoplanetary disks, which are the natural consequence of the gravitational collapse of the dense molecular cloud cores, host the formation of the planetary systems known today in our universe. Numerous efforts have been dedicated to investigate the properties of these disks in the more mature Class II stage, either by using numerical simulations of disk evolution from a limited range of initial conditions or by observations of their dust continuum and line emission from specific molecular tracers, and to compare the results from the two standpoints. Yet few studies have investigated the main limitations at work when measuring the embedded Class 0/I disk properties from observations, especially in a statistical fashion. In this study, we provide a first attempt to compare the accuracy of some critical disk parameters in Class 0/I systems, as derived on real ALMA observational data, with the corresponding physical parameters that modellers can directly define in numerical simulations. The approach we follow is to provide full post-processing of the numerical simulations and apply on the synthetic observations the same techniques used by observers to derive the physical parameters. To that end, we performed 3D Monte Carlo radiative transfer and mock interferometric observations of the disk populations formed in an MHD simulation model of disk formation through the collapse of massive clumps with the tools RADMC-3D and CASA, respectively, to obtain their synthetic observations. With these observations, we re-employ the techniques commonly used in disk modelling from their continuum emissions to infer their properties that one would likely obtain if one observed them with real interferometers. We then demonstrate how their properties vary from the gas kinematics analyses to the dust continuum modelling.",
        "comments": "Accepted for publication in Astronomy & Astrophysics, 32 pages, 28 figures",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12142"
    },
    {
        "doc_id": 6,
        "title": "Evaluation of QCNN-LSTM for Disability Forecasting in Multiple Sclerosis Using Sequential Multisequence MRI",
        "authors": [
            "John D. Mayfield",
            "Issam El Naqa"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Emerging Technologies",
            "Image and Video Processing"
        ],
        "abstract": "Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term Memory (LSTM) models were studied to provide sequential relationships for each timepoint in MRIs of patients with Multiple Sclerosis (MS). In this pilot study, we compared three QCNN-LSTM models for binary classification of MS disability benchmarked against classical neural network architectures. Our hypothesis is that quantum models will provide competitive performance. Methods Matrix Product State (MPS), reverse Multistate Entanglement Renormalization Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM layer to process near-annual MRI data of patients diagnosed with MS. These were benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision Transformer (ViViT). Predicted logits were measured against ground truth labels of each patient's Extended Disability Severity Score (EDSS) using binary cross-entropy loss. Training/validation/holdout testing was partitioned using 5-fold cross validation with a total split of 60:20:20. Levene's test of variance was used to measure statistical difference and Student's t-test for paired model differences in mean. Results The MPS-LSTM, reverse MERA-LSTM, and TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively (p-value 0.915). VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73 and 0.77, respectively (p-value 0.631). Overall variance and mean were not statistically significant (p-value 0.713), however, time to train was significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218, respectively, p-value <0.001). Conclusion QCNN-LSTM models perform competitively to their classical counterparts with greater efficiency in train time. Clinically, these can add value in terms of efficiency to time-dependent deep learning prediction of disease progression based upon medical imaging.",
        "comments": "ACM Class:          I.2.0; I.2.6",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12132"
    },
    {
        "doc_id": 7,
        "title": "Biological species delimitation based on genetic and spatial dissimilarity: a comparative study",
        "authors": [
            "Gabriele d'Angella",
            "Christian Hennig"
        ],
        "subjects": [
            "Populations and Evolution",
            "Applications",
            "Methodology"
        ],
        "abstract": "The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. This paper investigates the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Various approaches are compared (some of which already exist in the literature) based on simulated metapopulations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Approaches involve partial Mantel testing, maximum likelihood mixed-effects models with a population effect, and jackknife-based homogeneity tests. A key challenge is that most tests perform on genetic and geographical distance data, violating standard independence assumptions. Simulations showed that partial Mantel tests and mixed-effects models have larger power than jackknife-based methods, but tend to display type-I-error rates slightly above the significance level. Moreover, a multiple regression model neglecting the dependence in the dissimilarities did not show inflated type-I-error rate. An application on brassy ringlets concludes the paper.",
        "comments": "paper of 23 pages with 4 figures; appendix of 11 pages with 4 figures",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12126"
    },
    {
        "doc_id": 8,
        "title": "Temporal Aggregation for the Synthetic Control Method",
        "authors": [
            "Liyang Sun",
            "Eli Ben-Michael",
            "Avi Feller"
        ],
        "subjects": [
            "Econometrics",
            "Methodology"
        ],
        "abstract": "The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit with panel data. Two challenges arise with higher frequency data (e.g., monthly versus yearly): (1) achieving excellent pre-treatment fit is typically more challenging; and (2) overfitting to noise is more likely. Aggregating data over time can mitigate these problems but can also destroy important signal. In this paper, we bound the bias for SCM with disaggregated and aggregated outcomes and give conditions under which aggregating tightens the bounds. We then propose finding weights that balance both disaggregated and aggregated series.",
        "comments": "9 pages, 3 figures, Prepared for 2024 AEA Papers and Proceedings \"Treatment Effects: Theory and Implementation\"",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12084"
    },
    {
        "doc_id": 9,
        "title": "The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization",
        "authors": [
            "Matan Schliserman",
            "Uri Sherman",
            "Tomer Koren"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "We study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence. First, for full-batch gradient descent (GD) we give a construction of a learning problem in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance of the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\u03a9(1)$ population excess risk. Our bound translates to a lower bound of $\u03a9(\\sqrt{d})$ on the number of training examples required for standard GD to reach a non-trivial test error, answering an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b) and showing that a non-trivial dimension dependence is unavoidable. Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that an application of the same construction technique provides a similar $\u03a9(\\sqrt{d})$ lower bound for the sample complexity of SGD to reach a non-trivial empirical error, despite achieving optimal test performance. This again provides an exponential improvement in the dimension dependence compared to previous work (Koren, Livni, Mansour, and Sherman, 2022), resolving an open question left therein.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12058"
    },
    {
        "doc_id": 10,
        "title": "A Bracketing Relationship for Long-Term Policy Evaluation with Combined Experimental and Observational Data",
        "authors": [
            "Yechan Park",
            "Yuya Sasaki"
        ],
        "subjects": [
            "Econometrics"
        ],
        "abstract": "Combining short-term experimental data with observational data enables credible long-term policy evaluation. The literature offers two key but non-nested assumptions, namely the latent unconfoundedness (LU; Athey et al., 2020) and equi-confounding bias (ECB; Ghassami et al., 2022) conditions, to correct observational selection. Committing to the wrong assumption leads to biased estimation. To mitigate such risks, we provide a novel bracketing relationship (cf. Angrist and Pischke, 2009) repurposed for the setting with data combination: the LU-based estimand and the ECB-based estimand serve as the lower and upper bounds, respectively, with the true causal effect lying in between if either assumption holds. For researchers further seeking point estimates, our Lalonde-style exercise suggests the conservatively more robust LU-based lower bounds align closely with the hold-out experimental estimates for educational policy evaluation. We investigate the economic substantives of these findings through the lens of a nonparametric class of selection mechanisms and sensitivity analysis. We uncover as key the sub-martingale property and sufficient-statistics role (Chetty, 2009) of the potential outcomes of student test scores (Chetty et al., 2011, 2014).",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12050"
    },
    {
        "doc_id": 11,
        "title": "Multi-objective optimisation using expected quantile improvement for decision making in disease outbreaks",
        "authors": [
            "Daria Semochkina",
            "Alexander I. J. Forrester",
            "David C Woods"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Optimization under uncertainty is important in many applications, particularly to inform policy and decision making in areas such as public health. A key source of uncertainty arises from the incorporation of environmental variables as inputs into computational models or simulators. Such variables represent uncontrollable features of the optimization problem and reliable decision making must account for the uncertainty they propagate to the simulator outputs. Often, multiple, competing objectives are defined from these outputs such that the final optimal decision is a compromise between different goals.\n  Here, we present emulation-based optimization methodology for such problems that extends expected quantile improvement (EQI) to address multi-objective optimization. Focusing on the practically important case of two objectives, we use a sequential design strategy to identify the Pareto front of optimal solutions. Uncertainty from the environmental variables is integrated out using Monte Carlo samples from the simulator. Interrogation of the expected output from the simulator is facilitated by use of (Gaussian process) emulators. The methodology is demonstrated on an optimization problem from public health involving the dispersion of anthrax spores across a spatial terrain. Environmental variables include meteorological features that impact the dispersion, and the methodology identifies the Pareto front even when there is considerable input uncertainty.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12031"
    },
    {
        "doc_id": 12,
        "title": "Four Gluon Vertex from Lattice QCD",
        "authors": [
            "Manuel Cola\u00e7o",
            "Orlando Oliveira",
            "Paulo J. Silva"
        ],
        "subjects": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Theory"
        ],
        "abstract": "A lattice QCD calculation for the four gluon one-particle irreducible Green function in the Landau gauge is discussed. Results for some of the associated form factors are reported for kinematical configurations with a single momentum scale. Our results show that the computation of this Green function requires large statistical ensembles with 10K or larger number of gauge configurations. The simulations considered herein have a clear Monte Carlo signal for momenta up to $\\sim 1$ GeV. The form factors show an hierarchy, with the form factor associated with the tree level Feynman rule being dominant and essentially constant for the range of momenta accessed. The remaining form factors seem to increase as the momentum decreases, suggesting that a possible $\\log$ divergence may occur. The computed form factors are, at least, in qualitative agreement with the results obtained with continuum approaches to this vertex, when available.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12008"
    },
    {
        "doc_id": 13,
        "title": "Integrating Statistical Significance and Discriminative Power in Pattern Discovery",
        "authors": [
            "Leonardo Alexandre",
            "Rafael S. Costa",
            "Rui Henriques"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains. Actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power. Our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality. We also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria. To test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\u03b4$-Trimax and TriGen, that use various pattern quality criteria, such as Mean Squared Residual (MSR), Least Squared Lines (LSL), and Multi Slope Measure (MSL). Results from three case studies show the role of the proposed methodology in discovering patterns with pronounced improvements of discriminative power and statistical significance without quality deterioration, highlighting its importance in supervisedly guiding the search. Although the proposed methodology is motivated over multivariate time series data, it can be straightforwardly extended to pattern discovery tasks involving multivariate, N-way (N>3), transactional, and sequential data structures.\n  Availability: The code is freely available at https://github.com/JupitersMight/MOF_Triclustering under the MIT license.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12000"
    },
    {
        "doc_id": 14,
        "title": "Elasticity of self-organized frustrated disordered spring networks",
        "authors": [
            "Tommaso Pettinari",
            "Gustavo D\u00fcring",
            "Edan Lerner"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Statistical Mechanics"
        ],
        "abstract": "There have been some interesting recent advances in understanding the notion of mechanical disorder in structural glasses and the statistical mechanics of these systems' low-energy excitations. Here we contribute to these advances by studying a minimal model for structural glasses' elasticity in which the degree of mechanical disorder -- as characterized by recently introduced dimensionless quantifiers -- is readily tunable over a very large range. We comprehensively investigate a number of scaling laws observed for various macro-, meso- and microscopic elastic properties, and rationalize them using scaling arguments. Interestingly, we demonstrate that the model features the universal quartic glassy vibrational density of states as seen in many atomistic and molecular models of structural glasses formed by cooling a melt. The emergence of this universal glassy spectrum highlights the role of self-organization (towards mechanical equilibrium) in its formation, and elucidates why models featuring structural frustration alone do not feature the same universal glassy spectrum. Finally, we discuss relations to existing work in the context of strain-stiffening of elastic networks and of low-energy excitations in structural glasses, in addition to future research directions.",
        "comments": "9 pages, 7 figures",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11996"
    },
    {
        "doc_id": 15,
        "title": "1/f noise in quantum nanoscience",
        "authors": [
            "Giuseppe Falci",
            "Pertti J. Hakonen",
            "Elisabetta Paladino"
        ],
        "subjects": [
            "Mesoscale and Nanoscale Physics",
            "Superconductivity"
        ],
        "abstract": "Fundamental issues of 1/f noise in quantum nanoscience are reviewed starting from basic statistical noise processes. Fundamental noise models based on two-level systems (TLS) are described. We emphasize the importance of TLSs in materials parameter fluctuations, such as dielectric constant. The present understanding of 1/f noise in superconducting quantum interferometers and in single electron devices is summarized. For coherent quantum nanoscience, we introduce superconducting qubits and the relation between decoherence and 1/f noise using the filter function formulation. We also clarify the qubit noise spectroscopy and emphasize the importance of materials with reduced 1/f noise for future quantum coherent nanodevices.",
        "comments": "15 pages, 4 figures",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11989"
    },
    {
        "doc_id": 16,
        "title": "Cross-Validation Conformal Risk Control",
        "authors": [
            "Kfir M. Cohen",
            "Sangwoo Park",
            "Osvaldo Simeone",
            "Shlomo Shamai"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate. The original CRC requires the available data set to be split between training and validation data sets. This can be problematic when data availability is limited, resulting in inefficient set predictors. In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC. The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions. CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor. Furthermore, numerical experiments show that CV-CRC can reduce the average set size with respect to CRC when the available data are limited.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11974"
    },
    {
        "doc_id": 17,
        "title": "RUMBoost: Gradient Boosted Random Utility Models",
        "authors": [
            "Nicolas Salvad\u00e9",
            "Tim Hillel"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "This paper introduces the RUMBoost model, a novel discrete choice modelling approach that combines the interpretability and behavioural robustness of Random Utility Models (RUMs) with the generalisation and predictive ability of deep learning methods. We obtain the full functional form of non-linear utility specifications by replacing each linear parameter in the utility functions of a RUM with an ensemble of gradient boosted regression trees. This enables piece-wise constant utility values to be imputed for all alternatives directly from the data for any possible combination of input variables. We introduce additional constraints on the ensembles to ensure three crucial features of the utility specifications: (i) dependency of the utilities of each alternative on only the attributes of that alternative, (ii) monotonicity of marginal utilities, and (iii) an intrinsically interpretable functional form, where the exact response of the model is known throughout the entire input space. Furthermore, we introduce an optimisation-based smoothing technique that replaces the piece-wise constant utility values of alternative attributes with monotonic piece-wise cubic splines to identify non-linear parameters with defined gradient. We demonstrate the potential of the RUMBoost model compared to various ML and Random Utility benchmark models for revealed preference mode choice data from London. The results highlight the great predictive performance and the direct interpretability of our proposed approach. Furthermore, the smoothed attribute utility functions allow for the calculation of various behavioural indicators and marginal utilities. Finally, we demonstrate the flexibility of our methodology by showing how the RUMBoost model can be extended to complex model specifications, including attribute interactions, correlation within alternative error terms and heterogeneity within the population.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11954"
    },
    {
        "doc_id": 18,
        "title": "The Ensemble Kalman Filter for Dynamic Inverse Problems",
        "authors": [
            "Simon Weissmann",
            "Neil K. Chada",
            "Xin T. Tong"
        ],
        "subjects": [
            "Numerical Analysis",
            "Methodology"
        ],
        "abstract": "In inverse problems, the goal is to estimate unknown model parameters from noisy observational data. Traditionally, inverse problems are solved under the assumption of a fixed forward operator describing the observation model. In this article, we consider the extension of this approach to situations where we have a dynamic forward model, motivated by applications in scientific computation and engineering. We specifically consider this extension for a derivative-free optimizer, the ensemble Kalman inversion (EKI). We introduce and justify a new methodology called dynamic-EKI, which is a particle-based method with a changing forward operator. We analyze our new method, presenting results related to the control of our particle system through its covariance structure. This analysis includes moment bounds and an ensemble collapse, which are essential for demonstrating a convergence result. We establish convergence in expectation and validate our theoretical findings through experiments with dynamic-EKI applied to a 2D Darcy flow partial differential equation.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11948"
    },
    {
        "doc_id": 19,
        "title": "Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent",
        "authors": [
            "Zhiyu Liu",
            "Zhi Han",
            "Yandong Tang",
            "Xi-Le Zhao",
            "Yao Wang"
        ],
        "subjects": [
            "Machine Learning",
            "Optimization and Control",
            "Machine Learning"
        ],
        "abstract": "This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it is worth noting that our method does not require the precise estimation of the tensor tubal-rank. Even in cases where the tubal-rank is slightly overestimated, our approach continues to demonstrate robust performance. A series of experiments have been carried out to demonstrate that, as compared to other popular ones, our approach exhibits superior performance in multiple scenarios, in terms of the faster computational speed and the smaller convergence error.",
        "comments": "13 pages, 4 figures",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11940"
    },
    {
        "doc_id": 20,
        "title": "Large deviation full counting statistics in adiabatic open quantum dynamics",
        "authors": [
            "Paulo J. Paulino",
            "Igor Lesanovsky",
            "Federico Carollo"
        ],
        "subjects": [
            "Statistical Mechanics",
            "Quantum Physics"
        ],
        "abstract": "The state of an open quantum system undergoing an adiabatic process evolves by following the instantaneous stationary state of its time-dependent generator. This observation allows one to characterize, for a generic adiabatic evolution, the average dynamics of the open system. However, information about fluctuations of dynamical observables, such as the number of photons emitted or the time-integrated stochastic entropy production in single experimental runs, requires controlling the whole spectrum of the generator and not only the stationary state. Here, we show how such information can be obtained in adiabatic open quantum dynamics by exploiting tools from large deviation theory. We prove an adiabatic theorem for deformed generators, which allows us to encode, in a biased quantum state, the full counting statistics of generic time-integrated dynamical observables. We further compute the probability associated with an arbitrary \"rare\" time-history of the observable and derive a dynamics which realizes it in its typical behavior. Our results provide a way to characterize and engineer adiabatic open quantum dynamics and to control their fluctuations.",
        "comments": "7 + 8 pages, 3 figures",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11933"
    },
    {
        "doc_id": 21,
        "title": "Combination of searches for pair-produced leptoquarks at $\\sqrt{s} = 13$ TeV with the ATLAS detector",
        "authors": [
            "ATLAS Collaboration"
        ],
        "subjects": [
            "High Energy Physics - Experiment"
        ],
        "abstract": "A statistical combination of various searches for pair-produced leptoquarks is presented, using the full LHC Run 2 (2015-2018) data set of $139$ fb$^{-1}$ collected with the ATLAS detector from proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}=13$ TeV. All possible decays of the leptoquarks into quarks of the third generation and charged or neutral leptons of any generation are investigated. Since no significant deviations from the Standard Model expectation are observed in any of the individual analyses, combined exclusion limits are set on the production cross-sections for scalar and vector leptoquarks. The resulting lower bounds on leptoquark masses exceed those from the individual analyses by up to 100 GeV, depending on the signal hypothesis.",
        "comments": "36 pages in total, authorlist starting on p19, 7 figures, 2 tables submitted to Phys. Lett. B. All figures are available at http://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/EXOT-2020-27",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11928"
    },
    {
        "doc_id": 22,
        "title": "Inertia drives concentration-wave turbulence in swimmer suspensions",
        "authors": [
            "Purnima Jain",
            "Navdeep Rana",
            "Sriram Ramaswamy",
            "Prasad Perlekar"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Statistical Mechanics",
            "Fluid Dynamics"
        ],
        "abstract": "We discover an instability mechanism in suspensions of self-propelled particles that does not involve active stress. Instead, it is driven by a subtle interplay of inertia, swimmer motility, and concentration fluctuations, through a crucial time lag between the velocity and the concentration field. The resulting time-persistent state seen in our high-resolution numerical simulations consists of self-sustained waves of concentration and orientation, transiting from regular oscillations to wave turbulence. We analyze the statistical features of this active turbulence, including an intriguing connection to the Batchelor spectrum of passive scalars.",
        "comments": "11 pages and 9 figures including supplementary material",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11927"
    },
    {
        "doc_id": 23,
        "title": "Comparison of Model Output Statistics and Neural Networks to Postprocess Wind Gusts",
        "authors": [
            "Cristina Primo Ramos",
            "Benedikt Schulz",
            "Sebastian Lerch",
            "Reinhold Hess"
        ],
        "subjects": [
            "Applications",
            "Atmospheric and Oceanic Physics"
        ],
        "abstract": "Wind gust prediction plays an important role in warning strategies of national meteorological services due to the high impact of its extreme values. However, forecasting wind gusts is challenging because they are influenced by small-scale processes and local characteristics. To account for the different sources of uncertainty, meteorological centers run ensembles of forecasts and derive probabilities of wind gusts exceeding a threshold. These probabilities often exhibit systematic errors and require postprocessing. Model Output Statistics (MOS) is a common operational postprocessing technique, although more modern methods such as neural network-bases approaches have shown promising results in research studies. The transition from research to operations requires an exhaustive comparison of both techniques. Taking a first step into this direction, our study presents a comparison of a postprocessing technique based on linear and logistic regression approaches with different neural network methods proposed in the literature to improve wind gust predictions, specifically distributional regression networks and Bernstein quantile networks. We further contribute to investigating optimal design choices for neural network-based postprocessing methods regarding changes of the numerical model in the training period, the use of persistence predictors, and the temporal composition of training datasets. The performance of the different techniques is compared in terms of calibration, accuracy, reliability and resolution based on case studies of wind gust forecasts from the operational weather model of the German weather service and observations from 170 weather stations.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11896"
    },
    {
        "doc_id": 24,
        "title": "Bootstrap prediction regions for daily curves of electricity demand and price using functional data",
        "authors": [
            "Rebeca Pel\u00e1ez",
            "Germ\u00e1n Aneiros",
            "Juan Vilar"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "The aim of this paper is to compute one-day-ahead prediction regions for daily curves of electricity demand and price. Three model-based procedures to construct general prediction regions are proposed, all of them using bootstrap algorithms. The first proposed method considers any $L_p$ norm for functional data to measure the distance between curves, the second one is designed to take different variabilities along the curve into account, and the third one takes advantage of the notion of depth of a functional data. The regression model with functional response on which our proposed prediction regions are based is rather general: it allows to include both endogenous and exogenous functional variables, as well as exogenous scalar variables; in addition, the effect of such variables on the response one is modeled in a parametric, nonparametric or semi-parametric way. A comparative study is carried out to analyse the performance of these prediction regions for the electricity market of mainland Spain, in year 2012. This work extends and complements the methods and results in Aneiros et al. (2016) (focused on curve prediction) and Vilar et al. (2018) (focused on prediction intervals), which use the same database as here.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11885"
    },
    {
        "doc_id": 25,
        "title": "A theoretical framework for BL Her stars -- II. New period-luminosity relations in the Gaia passbands",
        "authors": [
            "Susmita Das",
            "L\u00e1szl\u00f3 Moln\u00e1r",
            "Shashi M. Kanbur",
            "Meridith Joyce",
            "Anupam Bhardwaj",
            "Harinder P. Singh",
            "Marcella Marconi",
            "Vincenzo Ripepi",
            "Radoslaw Smolec"
        ],
        "subjects": [
            "Solar and Stellar Astrophysics"
        ],
        "abstract": "We present new theoretical period-luminosity (PL) and period-Wesenheit (PW) relations for a fine grid of convective BL Her, the shortest period T2Cs, models computed using MESA-RSP and compare our results with the empirical relations from Gaia DR3. We use the state-of-the-art 1D non-linear radial stellar pulsation tool MESA-RSP to compute models of BL Her stars over a wide range of input parameters - metallicity (-2.0 dex $\\leq$ [Fe/H] $\\leq$ 0.0 dex), stellar mass (0.5M$_{\\odot}$-0.8M$_{\\odot}$), stellar luminosity (50L$_{\\odot}$-300L$_{\\odot}$) and effective temperature (full extent of the instability strip; in steps of 50K). The BL Her stars in the All Sky region exhibit statistically different PL slopes compared to the theoretical PL slopes computed using the four sets of convection parameters. We find the empirical PL and PW slopes from BL Her stars in the Magellanic Clouds to be statistically consistent with the theoretical relations computed using the different convection parameter sets in the Gaia passbands. There is negligible effect of metallicity on the PL relations in the individual Gaia passbands. However, there exists a small but significant negative coefficient of metallicity in the PWZ relations for the BL Her models using the four sets of convection parameters. This could be attributed to the increased sensitivity of bolometric corrections to metallicities at wavelengths shorter than the V band. Our BL Her models also suggest a dependence of the mass-luminosity relation on metallicity. We found the observed Fourier parameter space to be covered well by our models. Higher mass models (> 0.6M$_{\\odot}$) may be needed to reliably model the observed light curves of BL Her stars in the All Sky region. We also found the theoretical light curve structures (especially the Fourier amplitude parameters) to be affected by the choice of convection parameters.",
        "comments": "19 pages, 8 figures, accepted in Astronomy & Astrophysics",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11869"
    },
    {
        "doc_id": 26,
        "title": "Fast measurement of group index variation with ultimate precision using Hong-Ou-Mandel interferometry",
        "authors": [
            "Sandeep Singh",
            "Vimlesh Kumar",
            "G. K. Samanta"
        ],
        "subjects": [
            "Quantum Physics",
            "Optics"
        ],
        "abstract": "Hong-Ou-Mandel (HOM) interferometry has emerged as a valuable tool for quantum sensing applications, particularly in measuring physical parameters that influence the relative optical delay between pair photons. Unlike classical techniques, HOM-based quantum sensors offer higher resolution due to their intrinsic dispersion cancellation property. Despite this advantage, achieving precise measurements of optical delay crucial for practical applications often involves time-consuming integration and post-processing with traditional statistical methods. To address this challenge, our recent work focused on optimizing optical delay measurements in a time-efficient manner. By carefully selecting the length of a 1 mm periodically-poled KTP (PPKTP) crystal for pair photon generation, we achieved a remarkable group index measurement precision of $\\sim 6.75\\times 10^{-6}$ per centimeter of sample length, surpassing the previous maximum precision by over 400$\\%$. These current measurements maintain fast detection and high photon counts, which are essential for practical quantum sensing applications. The HOM-based method, while limiting the measurement range, can be extended by compensating for photon delay using an optical delay stage. As a proof-of-principle, we measured the group index variation of PPKTP over a temperature range up to 200$^{\\circ}$C with a precision in the range of one part per million ($\\sim$10$^{-6}$). This advancement not only contributes to quantum sensing but also holds promising implications for high-precision and long-range measurements in quantum optical coherence tomography.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11853"
    },
    {
        "doc_id": 27,
        "title": "Subgroup analysis methods for time-to-event outcomes in heterogeneous randomized controlled trials",
        "authors": [
            "Valentine Perrin",
            "Nathan Noiry",
            "Nicolas Loiseau",
            "Alex Nowak"
        ],
        "subjects": [
            "Methodology",
            "Applications",
            "Machine Learning"
        ],
        "abstract": "Non-significant randomized control trials can hide subgroups of good responders to experimental drugs, thus hindering subsequent development. Identifying such heterogeneous treatment effects is key for precision medicine and many post-hoc analysis methods have been developed for that purpose. While several benchmarks have been carried out to identify the strengths and weaknesses of these methods, notably for binary and continuous endpoints, similar systematic empirical evaluation of subgroup analysis for time-to-event endpoints are lacking. This work aims to fill this gap by evaluating several subgroup analysis algorithms in the context of time-to-event outcomes, by means of three different research questions: Is there heterogeneity? What are the biomarkers responsible for such heterogeneity? Who are the good responders to treatment? In this context, we propose a new synthetic and semi-synthetic data generation process that allows one to explore a wide range of heterogeneity scenarios with precise control on the level of heterogeneity. We provide an open source Python package, available on Github, containing our generation process and our comprehensive benchmark framework. We hope this package will be useful to the research community for future investigations of heterogeneity of treatment effects and subgroup analysis methods benchmarking.",
        "comments": "9 pages, 8 figures, 2 tables. Code available at: https://github.com/owkin/hte. Comments are welcome!",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11842"
    },
    {
        "doc_id": 28,
        "title": "The NOSTRA model: coherent estimation of infection sources in the case of possible nosocomial transmission",
        "authors": [
            "David J Pascall",
            "Chris Jackson",
            "Stephanie Evans",
            "Theodore Gouliouris",
            "Chris Illingworth",
            "Stefan Piatek",
            "Julie V Robotham",
            "Oliver Stirrup",
            "Ben Warne",
            "Judith Breuer",
            "Daniela De Angelis"
        ],
        "subjects": [
            "Applications",
            "Quantitative Methods"
        ],
        "abstract": "Nosocomial infections have important consequences for patients and hospital staff: they worsen patient outcomes and their management stresses already overburdened health systems. Accurate judgements of whether an infection is nosocomial helps staff make appropriate choices to protect other patients within the hospital. Nosocomiality cannot be properly assessed without considering whether the infected patient came into contact with high risk potential infectors within the hospital. We developed a Bayesian model that integrates epidemiological, contact and pathogen genetic data to determine how likely an infection is to be nosocomial and the probability of given infection candidates being the source of the infection.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11837"
    },
    {
        "doc_id": 29,
        "title": "NSPT for $O(N)$ non-linear sigma model: the larger $N$ the better",
        "authors": [
            "Paolo Baglioni",
            "Francesco Di Renzo"
        ],
        "subjects": [
            "High Energy Physics - Lattice"
        ],
        "abstract": "The $O(N)$ non-linear sigma model (NLSM) is an example of field theory on a target space with nontrivial geometry. One interesting feature of NLSM is asymptotic freedom, which makes perturbative calculations interesting. Given the successes in Lattice Gauge Theories, Numerical Stochastic Perturbation Theory (NSPT) is a natural candidate for performing high-order computations also in the case of NLSM. However, in low-dimensional systems NSPT is known to display statistical fluctuations substantially increasing for increasing orders. In this work, we explore how for $O(N)$ NLSM this behaviour is strongly dependent on $N$. As largely expected on general grounds, the larger is $N$, the larger is the order at which a NSPT computation can be effectively performed.",
        "comments": "Proceedings of the 40th International Symposium on Lattice Field Theory (Lattice 2023), July 31st - August 4th, 2023, Fermilab, Batavia, Illinois, USA",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11833"
    },
    {
        "doc_id": 30,
        "title": "A Fair Evaluation of Various Deep Learning-Based Document Image Binarization Approaches",
        "authors": [
            "Richin Sukesh",
            "Mathias Seuret",
            "Anguelos Nicolaou",
            "Martin Mayr",
            "Vincent Christlein"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Binarization of document images is an important pre-processing step in the field of document analysis. Traditional image binarization techniques usually rely on histograms or local statistics to identify a valid threshold to differentiate between different aspects of the image. Deep learning techniques are able to generate binarized versions of the images by learning context-dependent features that are less error-prone to degradation typically occurring in document images. In recent years, many deep learning-based methods have been developed for document binarization. But which one to choose? There have been no studies that compare these methods rigorously. Therefore, this work focuses on the evaluation of different deep learning-based methods under the same evaluation protocol. We evaluate them on different Document Image Binarization Contest (DIBCO) datasets and obtain very heterogeneous results. We show that the DE-GAN model was able to perform better compared to other models when evaluated on the DIBCO2013 dataset while DP-LinkNet performed best on the DIBCO2017 dataset. The 2-StageGAN performed best on the DIBCO2018 dataset while SauvolaNet outperformed the others on the DIBCO2019 challenge. Finally, we make the code, all models and evaluation publicly available (https://github.com/RichSu95/Document_Binarization_Collection) to ensure reproducibility and simplify future binarization evaluations.",
        "comments": "DAS 2022",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11831"
    },
    {
        "doc_id": 31,
        "title": "Flexible Models for Simple Longitudinal Data",
        "authors": [
            "Helen Ogden"
        ],
        "subjects": [
            "Methodology",
            "Applications"
        ],
        "abstract": "We propose a new method for estimating subject-specific mean functions from longitudinal data. We aim to do this in a flexible manner (without restrictive assumptions about the shape of the subject-specific mean functions), while exploiting similarities in the mean functions between different subjects. Functional principal components analysis fulfils both requirements, and methods for functional principal components analysis have been developed for longitudinal data. However, we find that these existing methods sometimes give fitted mean functions which are more complex than needed to provide a good fit to the data. We develop a new penalised likelihood approach to flexibly model longitudinal data, with a penalty term to control the balance between fit to the data and smoothness of the subject-specific mean curves. We run simulation studies to demonstrate that the new method substantially improves the quality of inference relative to existing methods across a range of examples, and apply the method to data on changes in body composition in adolescent girls.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11827"
    },
    {
        "doc_id": 32,
        "title": "On the importance of factorization for fast binned likelihood inference",
        "authors": [
            "C\u00e9sar",
            "Jes\u00fas-Valls"
        ],
        "subjects": [
            "High Energy Physics - Experiment"
        ],
        "abstract": "Likelihood-based inference, central in modern particle physics data analysis requires the extensive evaluation of a likelihood function that depends on set of parameters defined by the statistical model under consideration. If an analytical expression for the likelihood can be defined from first principles the procedure is computationally straightforward. However, most experiments require approximating the likelihood numerically using large statistical samples of synthetic events generated using Monte Carlo methods. As a result, the likelihood consists of a comparison of the expected versus the observed event rates in a collection of histogram bins, defining binned likelihood functions. When this occurs, evaluating the likelihood function involves, on each occasion, recalculating the prediction in those bins, increasing the computational load of these analysis drastically. In this text, I highlight the importance of identifying which are the unique event configurations in the binned likelihood definition and I provide an exact formula to update the event rate predictions utilizing the minimum number of necessary calculations by means of factorization. The aim of the discussion is to decrease the computational load of widespread high-energy physics analyses, leading to substantial speed improvements and reduced carbon footprints.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11806"
    },
    {
        "doc_id": 33,
        "title": "Regression Copulas for Multivariate Responses",
        "authors": [
            "Nadja Klein",
            "Michael Stanley Smith",
            "David Nott",
            "Ryan Chrisholm"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "We propose a novel distributional regression model for a multivariate response vector based on a copula process over the covariate space. It uses the implicit copula of a Gaussian multivariate regression, which we call a ``regression copula''. To allow for large covariate vectors their coefficients are regularized using a novel multivariate extension of the horseshoe prior. Bayesian inference and distributional predictions are evaluated using efficient variational inference methods, allowing application to large datasets. An advantage of the approach is that the marginal distributions of the response vector can be estimated separately and accurately, resulting in predictive distributions that are marginally-calibrated. Two substantive applications of the methodology highlight its efficacy in multivariate modeling. The first is the econometric modeling and prediction of half-hourly regional Australian electricity prices. Here, our approach produces more accurate distributional forecasts than leading benchmark methods. The second is the evaluation of multivariate posteriors in likelihood-free inference (LFI) of a model for tree species abundance data, extending a previous univariate regression copula LFI method. In both applications, we demonstrate that our new approach exhibits a desirable marginal calibration property.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11804"
    },
    {
        "doc_id": 34,
        "title": "Stein EWMA Control Charts for Count Processes",
        "authors": [
            "Christian H. Wei\u00df"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "The monitoring of serially independent or autocorrelated count processes is considered, having a Poisson or (negative) binomial marginal distribution under in-control conditions. Utilizing the corresponding Stein identities, exponentially weighted moving-average (EWMA) control charts are constructed, which can be flexibly adapted to uncover zero inflation, over- or underdispersion. The proposed Stein EWMA charts' performance is investigated by simulations, and their usefulness is demonstrated by a real-world data example from health surveillance.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11789"
    },
    {
        "doc_id": 35,
        "title": "Very High-Energy ($>$50 GeV) Gamma-ray Flux Variability of Bright Fermi Blazars",
        "authors": [
            "Vaidehi S. Paliya"
        ],
        "subjects": [
            "High Energy Astrophysical Phenomena"
        ],
        "abstract": "Understanding the high-energy emission processes and variability patterns are two of the most challenging research problems associated with relativistic jets. In particular, the long-term (months-to-years) flux variability at very high energies (VHE, $>$50 GeV) has remained an unexplored domain so far. This is possibly due to the decreased sensitivity of the Fermi Large Area Telescope (LAT) above a few GeV, hence low photon statistics, and observing constraints associated with the ground-based Cherenkov telescopes. This paper reports the results obtained from the 0.05$-$2 TeV Fermi-LAT data analysis of a sample of 29 blazars with the primary objective to explore their months-to-year long VHE flux variability behavior. This systematic search has led to, for the first time, the detection of significant flux variations in 5 blazars at $>$99\\% confidence level, whereas, 8 of them exhibit variability albeit at a lower confidence level ($\\sim$95\\%-99\\%). A comparison of the 0.05$-$2 TeV flux variations with that observed at 0.1$-$50 GeV band has revealed similar variability behavior for most of the sources. However, complex variability patterns that are not reflected contemporaneously in both energy bands were also detected, thereby providing tantalizing clues about the underlying radiative mechanisms. These results open up a new dimension to unravel the VHE emission processes operating in relativistic jets, hence sowing the seeds for their future observations with the upcoming Cherenkov Telescope Array.",
        "comments": "ApJ, in press",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11762"
    },
    {
        "doc_id": 36,
        "title": "Texture Identification in Liquid Crystal-Protein Droplets using Evaporative Drying, Generalized Additive Modeling, and K-means Clustering",
        "authors": [
            "Anusuya Pal",
            "Amalesh Gope"
        ],
        "subjects": [
            "Soft Condensed Matter"
        ],
        "abstract": "Sessile drying droplets manifest distinct morphological patterns, encompassing diverse systems viz., DNA, proteins, blood, and protein-liquid crystal (LC) complexes. This study employs an integrated methodology that combines drying droplet, image texture analysis (features from First Order Statistics, Gray Level Co-occurrence Matrix, Gray Level Run Length Matrix, Gray Level Size Zone Matrix, and Gray Level Dependence Matrix), and statistical data analysis (Generalized Additive Modeling and K-means clustering). It provides a comprehensive qualitative and quantitative exploration by examining LC-protein droplets at varying initial phosphate buffered concentrations (0x, 0.25x, 0.5x, 0.75x, and 1x) during the drying process under optical microscopy with crossed polarizing configuration. Notably, it unveils distinct LC-protein textures across three drying stages: initial, middle, and final. The Generalized Additive Modeling (GAM) reveals that all the features significantly contribute to differentiating LC-protein droplets. Integrating the K-means clustering method with GAM analysis elucidates how textures evolve through the three drying stages compared to the entire drying process. Notably, the final drying stage stands out with well-defined, non-overlapping clusters, supporting the visual observations of unique LC textures. Furthermore, this paper contributes valuable insights, showcasing the efficacy of drying droplets as a rapid and straightforward tool for characterizing and classifying dynamic LC textures.",
        "comments": "23 pages, 5 figures, and 2 tables",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11745"
    },
    {
        "doc_id": 37,
        "title": "Knowledge Navigation: Inferring the Interlocking Map of Knowledge from Research Trajectories",
        "authors": [
            "Shibing Xiang",
            "Bing Liu",
            "Yurui Huang",
            "Chaolin Tian",
            "Xin Jiang",
            "Yifang Ma"
        ],
        "subjects": [
            "Information Retrieval",
            "Digital Libraries",
            "Applications"
        ],
        "abstract": "\"If I have seen further, it is by standing on the shoulders of giants,\" Isaac Newton's renowned statement hints that new knowledge builds upon existing foundations, which means there exists an interdependent relationship between knowledge, which, yet uncovered, is implied in the historical development of scientific systems for hundreds of years. By leveraging natural language processing techniques, this study introduces an innovative embedding scheme designed to infer the \"knowledge interlocking map.\" This map, derived from the research trajectories of millions of scholars, reveals the intricate connections among knowledge. We validate that the inferred map effectively delineates disciplinary boundaries and captures the intricate relationships between diverse concepts. The utility of the interlocking map is showcased through multiple applications. Firstly, we demonstrated the multi-step analogy inferences within the knowledge space and the functional connectivity between concepts in different disciplines. Secondly, we trace the evolution of knowledge across domains, observing trends such as shifts from \"Theoretical\" to \"Applied\" or \"Chemistry\" to \"Biomedical\" along predefined functional directions. Lastly, by analyzing the high-dimensional knowledge network structure, we found that knowledge connects each other with shorter global pathways, and the interdisciplinary knowledge plays a critical role in accessibility of the global knowledge network. Our framework offers a novel approach to mining knowledge inheritance pathways in extensive scientific literature, which is of great significance for understanding scientific development patterns, tailoring scientific learning trajectories, and accelerating scientific progress.",
        "comments": "28 pages, 9 figures, 5 tables",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11742"
    },
    {
        "doc_id": 38,
        "title": "The Bayes factor surface for searches for new physics",
        "authors": [
            "Andrew Fowlie"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "Cosmology and Nongalactic Astrophysics",
            "High Energy Physics - Experiment",
            "Data Analysis, Statistics and Probability"
        ],
        "abstract": "The Bayes factor surface is a new way to present results from experimental searches for new physics. Searches are regularly expressed in terms of phenomenological parameters - such as the mass and cross-section of a weakly interacting massive particle. Bayes factor surfaces indicate the strength of evidence for or against models relative to the background only model in terms of the phenomenological parameters that they predict. They provide a clear and direct measure of evidence, may be easily reinterpreted, but do not depend on choices of prior or parameterization. We demonstrate the Bayes factor surface with examples from dark matter, cosmology, and collider physics.",
        "comments": "15 pages, 5 figures, comments welcome",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11710"
    },
    {
        "doc_id": 39,
        "title": "Simulating Nighttime Visible Satellite Imagery of Tropical Cyclones Using Conditional Generative Adversarial Networks",
        "authors": [
            "Jinghuai Yao",
            "Puyuan Du",
            "Yucheng Zhao",
            "Yubo Wang"
        ],
        "subjects": [
            "Atmospheric and Oceanic Physics",
            "Machine Learning"
        ],
        "abstract": "Visible (VIS) imagery of satellites has various important applications in meteorology, including monitoring Tropical Cyclones (TCs). However, it is unavailable at night because of the lack of sunlight. This study presents a Conditional Generative Adversarial Networks (CGAN) model that generates highly accurate nighttime visible reflectance using infrared (IR) bands and sunlight direction parameters as input. The model was trained and validated using target area observations of the Advanced Himawari Imager (AHI) in the daytime. This study also presents the first nighttime model validation using the Day/Night Band (DNB) of the Visible/Infrared Imager Radiometer Suite (VIIRS). The daytime statistical results of the Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), Root Mean Square Error (RMSE), Correlation Coefficient (CC), and Bias are 0.885, 28.3, 0.0428, 0.984, and -0.0016 respectively, completely surpassing the model performance of previous studies. The nighttime statistical results of SSIM, PSNR, RMSE, and CC are 0.821, 24.4, 0.0643, and 0.969 respectively, which are slightly negatively impacted by the parallax between satellites. We performed full-disk model validation which proves our model could also be readily applied in the tropical ocean without TCs in the northern hemisphere. This model contributes to the nighttime monitoring of meteorological phenomena by providing accurate AI-generated visible imagery with adjustable virtual sunlight directions.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11679"
    },
    {
        "doc_id": 40,
        "title": "Asymptotic distribution of spiked eigenvalues in the large signal-plus-noise models",
        "authors": [
            "Zeqin Lin",
            "Guangming Pan",
            "Peng Zhao",
            "Jia Zhou"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "Consider large signal-plus-noise data matrices of the form $S + \u03a3^{1/2} X$, where $S$ is a low-rank deterministic signal matrix and the noise covariance matrix $\u03a3$ can be anisotropic. We establish the asymptotic joint distribution of its spiked singular values when the dimensionality and sample size are comparably large and the signals are supercritical under general assumptions concerning the structure of $(S, \u03a3)$ and the distribution of the random noise $X$. It turns out that the asymptotic distributions exhibit nonuniversality in the sense of dependence on the distributions of the entries of $X$, which contrasts with what has previously been established for the spiked sample eigenvalues in the context of spiked population models. Such a result yields the asymptotic distribution of the sample spiked eigenvalues associated with mixture models. We also explore the application of these findings in detecting mean heterogeneity of data matrices.",
        "comments": "59 pages",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11672"
    },
    {
        "doc_id": 41,
        "title": "Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo",
        "authors": [
            "Haoyang Zheng",
            "Wei Deng",
            "Christian Moya",
            "Guang Lin"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\\mathcal{\\tilde O}(d)$ to $\\mathcal{\\tilde O}(\\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.",
        "comments": "50 pages, 1 figure, to appear in AISTATS 2024",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11665"
    },
    {
        "doc_id": 42,
        "title": "Nonparametric Estimation via Variance-Reduced Sketching",
        "authors": [
            "Yuehaw Khoo",
            "Yifan Peng",
            "Daren Wang"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Numerical Analysis",
            "Methodology"
        ],
        "abstract": "Nonparametric models are of great interest in various scientific and engineering disciplines. Classical kernel methods, while numerically robust and statistically sound in low-dimensional settings, become inadequate in higher-dimensional settings due to the curse of dimensionality. In this paper, we introduce a new framework called Variance-Reduced Sketching (VRS), specifically designed to estimate density functions and nonparametric regression functions in higher dimensions with a reduced curse of dimensionality. Our framework conceptualizes multivariable functions as infinite-size matrices, and facilitates a new sketching technique motivated by numerical linear algebra literature to reduce the variance in estimation problems. We demonstrate the robust numerical performance of VRS through a series of simulated experiments and real-world data applications. Notably, VRS shows remarkable improvement over existing neural network estimators and classical kernel methods in numerous density estimation and nonparametric regression models. Additionally, we offer theoretical justifications for VRS to support its ability to deliver nonparametric estimation with a reduced curse of dimensionality.",
        "comments": "64 pages, 8 figures",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11646"
    },
    {
        "doc_id": 43,
        "title": "Efficient PSF Modeling with ShOpt.jl: A PSF Benchmarking Study with JWST NIRCam Imaging",
        "authors": [
            "Edward Berman",
            "Jacqueline McCleary",
            "Anton M. Koekemoer",
            "Maximilien Franco",
            "Nicole E. Drakos",
            "Daizhong Liu",
            "James W. Nightingale",
            "Marko Shuntov",
            "Diana Scognamiglio",
            "Richard Massey",
            "Guillaume Mahler",
            "Henry Joy McCracken",
            "Brant E. Robertson",
            "Andreas L. Faisst Caitlin M. Casey",
            "Jeyhan S. Kartaltepe"
        ],
        "subjects": [
            "Instrumentation and Methods for Astrophysics"
        ],
        "abstract": "With their high angular resolutions of 30-100 mas, large fields of view, and complex optical systems, imagers on next-generation optical/near-infrared space observatories, such as the Near-Infrared Camera (NIRCam) on the James Webb Space Telescope (JWST), present both new opportunities for science and also new challenges for empirical point spread function (PSF) characterization. In this context, we introduce ShOpt, a new PSF fitting tool developed in Julia and designed to bridge the advanced features of PIFF (PSFs in the Full Field of View) with the computational efficiency of PSFEx (PSF Extractor). Along with ShOpt, we propose a suite of non-parametric statistics suitable for evaluating PSF fit quality in space-based imaging. Our study benchmarks ShOpt against the established PSF fitters PSFEx and PIFF using real and simulated COSMOS-Web Survey imaging. We assess their respective PSF model fidelity with our proposed diagnostic statistics and investigate their computational efficiencies, focusing on their processing speed relative to the complexity and size of the PSF models. Despite being in active development, we find that ShOpt can already achieve PSF model fidelity comparable to PSFEx and PIFF while maintaining competitive processing speeds, constructing PSF models for large NIRCam mosaics within minutes.",
        "comments": "53 pages, 27 figures, submitted to Astronomical Journal",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11625"
    },
    {
        "doc_id": 44,
        "title": "Radiative decays of X(3872) discriminate between the molecular and compact interpretations",
        "authors": [
            "B. Grinstein",
            "L. Maiani",
            "A. D. Polosa"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "Radiative decays X --> psi(1S) + gamma and X --> psi(2S) + gamma might be expected to have a ratio of branching fractions following the phase space volumes ratio. However data suggest the opposite, indicating a value for R=B(X --> psi^prime + gamma) / B(X --> psi +gamma) consistently larger than one. In this paper we present a calculation of R for both a compact Born-Oppenheimer cc-bar q-qbar state and a DD^* molecule. In the former case R~1 or larger is found, a value to be confronted with forthcoming high statistics data analyses. In the molecular picture, with D and D^* mesons described by the universal wave function used by Voloshin, Braaten and Kusunoki, we find that R would be of order 10^-2. A more precise experimental measure would be extremely helpful in clarifying the true nature of the X(3872).",
        "comments": "12 pages, 9 figures",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11623"
    },
    {
        "doc_id": 45,
        "title": "Efficient local linearity regularization to overcome catastrophic overfitting",
        "authors": [
            "Elias Abad Rocamora",
            "Fanghui Liu",
            "Grigorios G. Chrysos",
            "Pablo M. Olmos",
            "Volkan Cevher"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Cryptography and Security",
            "Machine Learning"
        ],
        "abstract": "Catastrophic overfitting (CO) in single-step adversarial training (AT) results in abrupt drops in the adversarial test accuracy (even down to 0%). For models trained with multi-step AT, it has been observed that the loss function behaves locally linearly with respect to the input, this is however lost in single-step AT. To address CO in single-step AT, several methods have been proposed to enforce local linearity of the loss via regularization. However, these regularization terms considerably slow down training due to Double Backpropagation. Instead, in this work, we introduce a regularization term, called ELLE, to mitigate CO effectively and efficiently in classical AT evaluations, as well as some more difficult regimes, e.g., large adversarial perturbations and long training schedules. Our regularization term can be theoretically linked to curvature of the loss function and is computationally cheaper than previous methods by avoiding Double Backpropagation. Our thorough experimental validation demonstrates that our work does not suffer from CO, even in challenging settings where previous works suffer from it. We also notice that adapting our regularization parameter during training (ELLE-A) greatly improves the performance, specially in large $\u03b5$ setups. Our implementation is available in https://github.com/LIONS-EPFL/ELLE .",
        "comments": "Accepted in ICLR 2024",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11618"
    },
    {
        "doc_id": 46,
        "title": "An Interacting Wasserstein Gradient Flow Strategy to Robust Bayesian Inference",
        "authors": [
            "Felipe Igea",
            "Alice Cicirello"
        ],
        "subjects": [
            "Computation"
        ],
        "abstract": "Model Updating is frequently used in Structural Health Monitoring to determine structures' operating conditions and whether maintenance is required. Data collected by sensors are used to update the values of some initially unknown physics-based model's parameters. Bayesian Inference techniques for model updating require the assumption of a prior distribution. This choice of prior may affect posterior predictions and subsequent decisions on maintenance requirements, specially under the typical case in engineering applications of little informative data. Therefore, understanding how the choice of prior may affect the posterior prediction is of great interest. In this paper, a Robust Bayesian Inference technique evaluates the optimal and worst-case prior in the vicinity of a chosen nominal prior, and their corresponding posteriors. This technique employs an interacting Wasserstein gradient flow formulation. Two numerical case studies are used to showcase the proposed algorithm: a double-banana-posterior and a double beam structure. Optimal and worst-case prior are modelled by specifying an ambiguity set containing any distribution at a statistical distance to the nominal prior, less or equal to the radius. Examples show how particles flow from an initial assumed Gaussian distribution to the optimal worst-case prior distribution that lies inside the defined ambiguity set, and the resulting particles from the approximation to the posterior. The resulting posteriors may be used to yield the lower and upper bounds on subsequent calculations used for decision-making. If the metric used for decision-making is not sensitive to the resulting posteriors, it may be assumed that decisions taken are robust to prior uncertainty.",
        "comments": "Preprint submitted to Data-Centric Engineering",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11607"
    },
    {
        "doc_id": 47,
        "title": "Understanding the Generalization Benefits of Late Learning Rate Decay",
        "authors": [
            "Yinuo Ren",
            "Chao Ma",
            "Lexing Ying"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Why do neural networks trained with large learning rates for a longer time often lead to better generalization? In this paper, we delve into this question by examining the relation between training and testing loss in neural networks. Through visualization of these losses, we note that the training trajectory with a large learning rate navigates through the minima manifold of the training loss, finally nearing the neighborhood of the testing loss minimum. Motivated by these findings, we introduce a nonlinear model whose loss landscapes mirror those observed for real neural networks. Upon investigating the training process using SGD on our model, we demonstrate that an extended phase with a large learning rate steers our model towards the minimum norm solution of the training loss, which may achieve near-optimal generalization, thereby affirming the empirically observed benefits of late learning rate decay.",
        "comments": "Accepted by AISTATS 2024",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11600"
    },
    {
        "doc_id": 48,
        "title": "Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis",
        "authors": [
            "Sharu Theresa Jose",
            "Shana Moothedath"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "We explore a stochastic contextual linear bandit problem where the agent observes a noisy, corrupted version of the true context through a noise channel with an unknown noise parameter. Our objective is to design an action policy that can approximate\" that of an oracle, which has access to the reward model, the channel parameter, and the predictive distribution of the true context from the observed noisy context. In a Bayesian framework, we introduce a Thompson sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting an information-theoretic analysis, we demonstrate the Bayesian regret of our algorithm concerning the oracle's action policy. We also extend this problem to a scenario where the agent observes the true context with some delay after receiving the reward and show that delayed true contexts lead to lower Bayesian regret. Finally, we empirically demonstrate the performance of the proposed algorithms against baselines.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11565"
    },
    {
        "doc_id": 49,
        "title": "Enhancing selectivity using Wasserstein distance based reweighing",
        "authors": [
            "Pratik Worah"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "Given two labeled data-sets $\\mathcal{S}$ and $\\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\\mathcal{T}$.\n  On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.\n  Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1 (a highly similar protein). We tune the algorithm's parameter so that overall change in holdout loss is negligible, but the selectivity, i.e., the fraction of top 100 MNK2 binders that are MNK1 non-binders, increases from 54\\% to 95\\%, as a result of our reweighing. Of the 43 distinct small molecules predicted to be most selective from the enamine catalog, 2 small molecules were experimentally verified to be selective, i.e., they reduced the enzyme activity of MNK2 below 50\\% but not MNK1, at 10$\u03bc$M -- a 5\\% success rate.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11562"
    },
    {
        "doc_id": 50,
        "title": "Transfer Learning under Covariate Shift: Local $k$-Nearest Neighbours Regression with Heavy-Tailed Design",
        "authors": [
            "Petr Zamolodtchikov",
            "Hanyuan Hang"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "Covariate shift is a common transfer learning scenario where the marginal distributions of input variables vary between source and target data while the conditional distribution of the output variable remains consistent. The existing notions describing differences between marginal distributions face limitations in handling scenarios with unbounded support, particularly when the target distribution has a heavier tail. To overcome these challenges, we introduce a new concept called density ratio exponent to quantify the relative decay rates of marginal distributions' tails under covariate shift. Furthermore, we propose the local k-nearest neighbour regressor for transfer learning, which adapts the number of nearest neighbours based on the marginal likelihood of each test sample. From a theoretical perspective, convergence rates with and without supervision information on the target domain are established. Those rates indicate that our estimator achieves faster convergence rates when the density ratio exponent satisfies certain conditions, highlighting the benefits of using density estimation for determining different numbers of nearest neighbours for each test sample. Our contributions enhance the understanding and applicability of transfer learning under covariate shift, especially in scenarios with unbounded support and heavy-tailed distributions.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11554"
    },
    {
        "doc_id": 51,
        "title": "Investigation of triangle counts in graphs evolved by uniform clustering attachment",
        "authors": [
            "N. M. Markovich",
            "M. Vai\u010diulis"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "The clustering attachment model introduced in the paper Bagrow and Brockmann (2013) may be used as an evolution tool of random networks. We propose a new clustering attachment model which can be considered as the limit of the former clustering attachment model as model parameter $\u03b1$ tends to zero. We focus on the study of a total triangle count that is considered in the literature as an important characteristic of the network clustering. It is proved that total triangle count tends to infinity a.s. for the proposed model. Our simulation study is used for the modeling of sequences of triangle counts. It is based on the interpretation of the clustering attachment as a generalized P\u00f3lya-Eggenberger urn model that is introduced here at first time.",
        "comments": "16 pages, 2 figures",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11548"
    },
    {
        "doc_id": 52,
        "title": "A new flexible class of kernel-based tests of independence",
        "authors": [
            "Marija Cupari\u0107",
            "Bruno Ebner",
            "Bojana Milo\u0161evi\u0107"
        ],
        "subjects": [
            "Methodology",
            "Statistics Theory"
        ],
        "abstract": "Spherical and hyperspherical data are commonly encountered in diverse applied research domains, underscoring the vital task of assessing independence within such data structures. In this context, we investigate the properties of test statistics relying on distance correlation measures originally introduced for the energy distance, and generalize the concept to strongly negative definite kernel-based distances. An important benefit of employing this method lies in its versatility across diverse forms of directional data, enabling the examination of independence among vectors of varying types. The applicability of tests is demonstrated on several real datasets.",
        "comments": "MSC Class:          62H11; 62H15; 62H20",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11540"
    },
    {
        "doc_id": 53,
        "title": "Addressing researcher degrees of freedom through minP adjustment",
        "authors": [
            "Maximilian M Mandl",
            "Andrea S Becker-Pennrich",
            "Ludwig C Hinske",
            "Sabine Hoffmann",
            "Anne-Laure Boulesteix"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "When different researchers study the same research question using the same dataset they may obtain different and potentially even conflicting results. This is because there is often substantial flexibility in researchers' analytical choices, an issue also referred to as ''researcher degrees of freedom''. Combined with selective reporting of the smallest p-value or largest effect, researcher degrees of freedom may lead to an increased rate of false positive and overoptimistic results. In this paper, we address this issue by formalizing the multiplicity of analysis strategies as a multiple testing problem. As the test statistics of different analysis strategies are usually highly dependent, a naive approach such as the Bonferroni correction is inappropriate because it leads to an unacceptable loss of power. Instead, we propose using the ''minP'' adjustment method, which takes potential test dependencies into account and approximates the underlying null distribution of the minimal p-value through a permutation-based procedure. This procedure is known to achieve more power than simpler approaches while ensuring a weak control of the family-wise error rate. We illustrate our approach for addressing researcher degrees of freedom by applying it to a study on the impact of perioperative paO2 on post-operative complications after neurosurgery. A total of 48 analysis strategies are considered and adjusted using the minP procedure. This approach allows to selectively report the result of the analysis strategy yielding the most convincing evidence, while controlling the type 1 error -- and thus the risk of publishing false positive results that may not be replicable.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11537"
    },
    {
        "doc_id": 54,
        "title": "Geometry-driven Bayesian Inference for Ultrametric Covariance Matrices",
        "authors": [
            "Tsung-Hung Yao",
            "Zhenke Wu",
            "Karthik Bharath",
            "Veerabhadran Baladandayuthapani"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Ultrametric matrices arise as covariance matrices in latent tree models for multivariate data with hierarchically correlated components. As a parameter space in a model, the set of ultrametric matrices is neither convex nor a smooth manifold, and focus in literature has hitherto mainly been restricted to estimation through projections and relaxation-based techniques. Leveraging the link between an ultrametric matrix and a rooted tree, we equip the set of ultrametric matrices with a convenient geometry based on the well-known geometry of phylogenetic trees, whose attractive properties (e.g. unique geodesics and Fr\u00e9chet means) the set of ultrametric matrices inherits. This results in a novel representation of an ultrametric matrix by coordinates of the tree space, which we then use to define a class of Markovian and consistent prior distributions on the set of ultrametric matrices in a Bayesian model, and develop an efficient algorithm to sample from the posterior distribution that generates updates by making intrinsic local moves along geodesics within the set of ultrametric matrices. In simulation studies, our proposed algorithm restores the underlying matrices with posterior samples that recover the tree topology with a high frequency of true topology and generate element-wise credible intervals with a high nominal coverage rate. We use the proposed algorithm on the pre-clinical cancer data to investigate the mechanism similarity by constructing the underlying treatment tree and identify treatments with high mechanism similarity also target correlated pathways in biological literature.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11515"
    },
    {
        "doc_id": 55,
        "title": "Topological superconductors in trapped-ion system and their Floquet engineering",
        "authors": [
            "Ming-Jian Gao",
            "Yu-Peng Ma",
            "Jun-Hong An"
        ],
        "subjects": [
            "Quantum Physics",
            "Superconductivity"
        ],
        "abstract": "Obeying non-Abelian statistics, Majorana fermion holds a promise to implement topological quantum computing. It was found that Majorana fermion can be simulated by the zero-energy excitation in a semiconducting nanowire with strong spin-orbit coupling interacting with a $s$-wave superconductor under a magnetic field. We here propose an alternative scheme to simulate the Majorana fermion in a trapped-ion system. Our dimitrized-ion configuration permits us to generate the Majorana modes not only at zero energy but also at the nonzero ones. We also investigate the controllability of the Majorana modes by Floquet engineering. It is found that a widely tunable number of Majorana modes are created on demand by applying a periodic driving on a topologically trivial trapped-ion system. Enriching the platforms for simulating Majorana fermion, our result would open another avenue for realizing topological quantum computing.",
        "comments": "8 pages and 4 figures in the main text and 3 pages in the supplentmental material",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11510"
    },
    {
        "doc_id": 56,
        "title": "Redundant multiple testing corrections: The fallacy of using family-based error rates to make inferences about individual hypotheses",
        "authors": [
            "Mark Rubin"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "During multiple testing, researchers often adjust their alpha level to control the familywise error rate for a statistical inference about a joint union alternative hypothesis (e.g., \"H1 or H2\"). However, in some cases, they do not make this inference and instead make separate inferences about each of the individual hypotheses that comprise the joint hypothesis (e.g., H1 and H2). For example, a researcher might use a Bonferroni correction to adjust their alpha level from the conventional level of 0.050 to 0.025 when testing H1 and H2, find a significant result for H1 (p < 0.025) and not for H2 (p > .0.025), and so claim support for H1 and not for H2. However, these separate individual inferences do not require an alpha adjustment. Only a statistical inference about the union alternative hypothesis \"H1 or H2\" requires an alpha adjustment because it is based on \"at least one\" significant result among the two tests, and so it depends on the familywise error rate. When a researcher corrects their alpha level during multiple testing but does not make an inference about the union alternative hypothesis, their correction is redundant. In the present article, I discuss this redundant correction problem, including its associated loss of statistical power and its potential causes vis-\u00e0-vis error rate confusions and the alpha adjustment ritual. I also provide three illustrations of redundant corrections from recent psychology studies. I conclude that redundant corrections represent a symptom of statisticism, and I call for a more nuanced and context-specific approach to multiple testing corrections.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11507"
    },
    {
        "doc_id": 57,
        "title": "Functional Limit Theorems for Hawkes Processes",
        "authors": [
            "Ulrich Horst",
            "Wei Xu"
        ],
        "subjects": [
            "Probability",
            "Statistics Theory",
            "Mathematical Finance"
        ],
        "abstract": "We prove that the long-run behavior of Hawkes processes is fully determined by the average number and the dispersion of child events. For subcritical processes we provide FLLNs and FCLTs under minimal conditions on the kernel of the process with the precise form of the limit theorems depending strongly on the dispersion of child events. For a critical Hawkes process with weakly dispersed child events, functional central limit theorems do not hold. Instead, we prove that the rescaled intensity processes and rescaled Hawkes processes behave like CIR-processes without mean-reversion, respectively integrated CIR-processes. We provide the rate of convergence by establishing an upper bound on the Wasserstein distance between the distributions of rescaled Hawkes process and the corresponding limit process. By contrast, critical Hawkes process with heavily dispersed child events share many properties of subcritical ones. In particular, functional limit theorems hold. However, unlike subcritical processes critical ones with heavily dispersed child events display long-range dependencies.",
        "comments": "59 pages; Keywords and phrases: Hawkes process, functional limit theorem, regular variation, convergence rate",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11495"
    },
    {
        "doc_id": 58,
        "title": "Local Identification in the Instrumental Variable Multivariate Quantile Regression Model",
        "authors": [
            "Haruki Kono"
        ],
        "subjects": [
            "Econometrics",
            "Statistics Theory"
        ],
        "abstract": "The instrumental variable (IV) quantile regression model introduced by Chernozhukov and Hansen (2005) is a useful tool for analyzing quantile treatment effects in the presence of endogeneity, but when outcome variables are multidimensional, it is silent on the joint distribution of different dimensions of each variable. To overcome this limitation, we propose an IV model built on the optimal-transport-based multivariate quantile that takes into account the correlation between the entries of the outcome variable. We then provide a local identification result for the model. Surprisingly, we find that the support size of the IV required for the identification is independent of the dimension of the outcome vector, as long as the IV is sufficiently informative. Our result follows from a general identification theorem that we establish, which has independent theoretical significance.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11422"
    },
    {
        "doc_id": 59,
        "title": "MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning",
        "authors": [
            "Mao Hong",
            "Zhiyue Zhang",
            "Yue Wu",
            "Yanxun Xu"
        ],
        "subjects": [
            "Machine Learning",
            "Statistics Theory",
            "Methodology",
            "Machine Learning"
        ],
        "abstract": "Model-based offline reinforcement learning methods (RL) have achieved state-of-the-art performance in many decision-making problems thanks to their sample efficiency and generalizability. Despite these advancements, existing model-based offline RL approaches either focus on theoretical studies without developing practical algorithms or rely on a restricted parametric policy space, thus not fully leveraging the advantages of an unrestricted policy space inherent to model-based methods. To address this limitation, we develop MoMA, a model-based mirror ascent algorithm with general function approximations under partial coverage of offline data. MoMA distinguishes itself from existing literature by employing an unrestricted policy class. In each iteration, MoMA conservatively estimates the value function by a minimization procedure within a confidence set of transition models in the policy evaluation step, then updates the policy with general function approximations instead of commonly-used parametric policy classes in the policy improvement step. Under some mild assumptions, we establish theoretical guarantees of MoMA by proving an upper bound on the suboptimality of the returned policy. We also provide a practically implementable, approximate version of the algorithm. The effectiveness of MoMA is demonstrated via numerical studies.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11380"
    },
    {
        "doc_id": 60,
        "title": "When exposure affects subgroup membership: Framing relevant causal questions in perinatal epidemiology and beyond",
        "authors": [
            "Shalika Gupta",
            "Laura B. Balzer",
            "Moses R. Kamya",
            "Diane V. Havlir",
            "Maya L. Petersen"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Perinatal epidemiology often aims to evaluate exposures on infant outcomes. When the exposure affects the composition of people who give birth to live infants (e.g., by affecting fertility, behavior, or birth outcomes), this \"live birth process\" mediates the exposure effect on infant outcomes. Causal estimands previously proposed for this setting include the total exposure effect on composite birth and infant outcomes, controlled direct effects (e.g., enforcing birth), and principal stratum direct effects. Using perinatal HIV transmission in the SEARCH Study as a motivating example, we present two alternative causal estimands: 1) conditional total effects; and 2) conditional stochastic direct effects, formulated under a hypothetical intervention to draw mediator values from some distribution (possibly conditional on covariates). The proposed conditional total effect includes impacts of an intervention that operate by changing the types of people who have a live birth and the timing of births. The proposed conditional stochastic direct effects isolate the effect of an exposure on infant outcomes excluding any impacts through this live birth process. In SEARCH, this approach quantifies the impact of a universal testing and treatment intervention on infant HIV-free survival absent any effect of the intervention on the live birth process, within a clearly defined target population of women of reproductive age with HIV at study baseline. Our approach has implications for the evaluation of intervention effects in perinatal epidemiology broadly, and whenever causal effects within a subgroup are of interest and exposure affects membership in the subgroup.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11368"
    },
    {
        "doc_id": 61,
        "title": "Folding Custom Gates with Verifier Input",
        "authors": [
            "Aard Vark",
            "Yan X Zhang"
        ],
        "subjects": [
            "Cryptography and Security",
            "Logic in Computer Science"
        ],
        "abstract": "In the context of interactive proofs, a \"folding scheme\" (popularized by Nova) is a way to combine multiple instances of a constraint system into a single instance, so the validity of the multiple instances can statistically be reduced to the validity of a single one. We show how Nova folding can be generalized to ``custom'' gates and extra rounds of verifier randomness. As an application of this extension, we present Origami, the first (to our knowledge) known example of a folding scheme for lookups.",
        "comments": "MSC Class:          94A60",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11364"
    },
    {
        "doc_id": 62,
        "title": "The Exact Risks of Reference Panel-based Regularized Estimators",
        "authors": [
            "Buxin Su",
            "Qiang Sun",
            "Xiaochen Yang",
            "Bingxin Zhao"
        ],
        "subjects": [
            "Methodology",
            "Statistics Theory"
        ],
        "abstract": "Reference panel-based estimators have become widely used in genetic prediction of complex traits due to their ability to address data privacy concerns and reduce computational and communication costs. These estimators estimate the covariance matrix of predictors using an external reference panel, instead of relying solely on the original training data. In this paper, we investigate the performance of reference panel-based $L_1$ and $L_2$ regularized estimators within a unified framework based on approximate message passing (AMP). We uncover several key factors that influence the accuracy of reference panel-based estimators, including the sample sizes of the training data and reference panels, the signal-to-noise ratio, the underlying sparsity of the signal, and the covariance matrix among predictors. Our findings reveal that, even when the sample size of the reference panel matches that of the training data, reference panel-based estimators tend to exhibit lower accuracy compared to traditional regularized estimators. Furthermore, we observe that this performance gap widens as the amount of training data increases, highlighting the importance of constructing large-scale reference panels to mitigate this issue. To support our theoretical analysis, we develop a novel non-separable matrix AMP framework capable of handling the complexities introduced by a general covariance matrix and the additional randomness associated with a reference panel. We validate our theoretical results through extensive simulation studies and real data analyses using the UK Biobank database.",
        "comments": "100 pages, 11 figures",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11359"
    },
    {
        "doc_id": 63,
        "title": "Squared Wasserstein-2 Distance for Efficient Reconstruction of Stochastic Differential Equations",
        "authors": [
            "Mingtao Xia",
            "Xiangting Li",
            "Qijing Shen",
            "Tom Chou"
        ],
        "subjects": [
            "Probability",
            "Machine Learning",
            "Methodology"
        ],
        "abstract": "We provide an analysis of the squared Wasserstein-2 ($W_2$) distance between two probability distributions associated with two stochastic differential equations (SDEs). Based on this analysis, we propose the use of a squared $W_2$ distance-based loss functions in the \\textit{reconstruction} of SDEs from noisy data. To demonstrate the practicality of our Wasserstein distance-based loss functions, we performed numerical experiments that demonstrate the efficiency of our method in reconstructing SDEs that arise across a number of applications.",
        "comments": "37 pages, 5 figures",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11354"
    },
    {
        "doc_id": 64,
        "title": "Geometric Insights and Empirical Observations on Covariate Adjustment and Stratified Randomization in Randomized Clinical Trials",
        "authors": [
            "Zhiwei Zhang"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "The statistical efficiency of randomized clinical trials can be improved by incorporating information from baseline covariates (i.e., pre-treatment patient characteristics). This can be done in the design stage using a covariate-adaptive randomization scheme such as stratified (permutated block) randomization, or in the analysis stage through covariate adjustment. This article provides a geometric perspective on covariate adjustment and stratified randomization in a unified framework where all regular, asymptotically linear estimators are identified as augmented estimators. From this perspective, covariate adjustment can be viewed as an effort to approximate the optimal augmentation function, and stratified randomization aims to improve a given approximation by projecting it into an affine subspace containing the optimal augmentation function. The efficiency benefit of stratified randomization is asymptotically equivalent to making full use of stratum information in covariate adjustment, which can be achieved using a simple calibration procedure. Simulation results indicate that stratified randomization is clearly beneficial to unadjusted estimators and much less so to adjusted ones and that calibration is an effective way to recover the efficiency benefit of stratified randomization without actually performing stratified randomization. These insights and observations are illustrated using real clinical trial data.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11352"
    },
    {
        "doc_id": 65,
        "title": "Quantum Machine Learning: from NISQ to Fault Tolerance",
        "authors": [
            "Yunfei Wang",
            "Junyu Liu"
        ],
        "subjects": [
            "Quantum Physics",
            "Artificial Intelligence",
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.",
        "comments": "28 pages. Invited review",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11351"
    },
    {
        "doc_id": 66,
        "title": "A study of experimental sensitivities to nucleon parton distributions with xFitter",
        "authors": [
            "Lucas Kotz"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "In collider physics, parton distribution functions (PDFs) play a crucial role in computing theoretical cross sections for scattering reactions. This study explores how different experimental data sets influence extracted PDFs in CTEQ-TEA and MSHT NNLO PDF analyses. To gauge the impact of experimental data, including the HERA and ZEUS combined charm and beauty production, LHCb 7 TeV charm and beauty production, and CMS 2.76 TeV W+c production, I utilize the $L_2$ sensitivity in the Hessian framework as a visual representation of their respective impacts. This sensitivity quantifies the statistical pulls on individual data sets against the best-fit PDFs, facilitating the identification of tensions among competing data sets. Using the QCD fitting framework xFitter, I extract the necessary values for plotting $L_2$ sensitivities for eight distinct data sets implemented in the program, employing recent PDF sets from the CTEQ-TEA and MSHT groups. The computed $L_2$ sensitivities estimate the potential impact of the examined data sets.",
        "comments": "11 pages, 11 figures",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11350"
    },
    {
        "doc_id": 67,
        "title": "Optimization of random cost functions and statistical physics",
        "authors": [
            "Andrea Montanari"
        ],
        "subjects": [
            "Disordered Systems and Neural Networks"
        ],
        "abstract": "This is the text of my report presented at the 29th Solvay Conference on Physics on `The Structure and Dynamics of Disordered Systems' held in Bruxelles from October 19 to 21, 2023. I consider the problem of minimizing a random energy function $H(\u03c3)$, where $\u03c3$ is an $N$-dimensional vector, in the high-dimensional regime $N\\gg 1$. Using as a reference point a 1986 paper by Fu and Anderson, I take stock of the progress on this question over the last 40 years. In particular, I focus on the influence and ramifications of ideas originating from statistical physics. My own conclusion is that several of the most fundamental questions in this area (which in 1986 were barely formulated) have now received mathematically rigorous answers, at least in simple -- yet highly nontrivial -- settings. Instrumental to this spectacular progress was the dialogue between different research communities: physics, computer science, mathematics.",
        "comments": "21 pages; 5 figures",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11348"
    },
    {
        "doc_id": 68,
        "title": "Estimating Default Probability and Correlation using Stan",
        "authors": [
            "Jesus A. Pinera-Esquivel"
        ],
        "subjects": [
            "Applications",
            "Methodology"
        ],
        "abstract": "This work has the objective of estimating default probabilities and correlations of credit portfolios given default rate information through a Bayesian framework using Stan. We use Vasicek's single factor credit model to establish the theoretical framework for the behavior of the default rates, and use NUTS Markov Chain Monte Carlo to estimate the parameters. We compare the Bayesian estimates with classical estimates such as moments estimators and maximum likelihood estimates. We apply the methodology both to simulated data and to corporate default rates, and perform inferences through Bayesian methods in order to exhibit the advantages of such a framework. We perform default forecasting and exhibit the importance of an adequate estimation of default correlations, and exhibit the advantage of using Stan to perform sampling regarding prior choice.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11346"
    },
    {
        "doc_id": 69,
        "title": "Mortality Modelling using Generalized Estimating Equations",
        "authors": [
            "Reza Dastranj",
            "Martin Kolar"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "This paper presents an application of Generalized Estimating Equations (GEE) for analyzing age-specific death rates (ASDRs), constituting a longitudinal dataset with repeated measurements over time. GEE models, known for their robustness in handling correlated data, offer a reliable solution when individual data records lack independence, thus violating the commonly assumed independence and identically distributed (iid) condition in traditional models. In the context of ASDRs, where correlations emerge among observations within age groups, two distinct GEE models for single and multipopulation ASDRs are introduced, providing robust estimates for regression parameters and their variances. We explore correlation structures, encompassing independence, AR(1), unstructured, and exchangeable structures, offering a comprehensive evaluation of GEE model efficiency in both single and multipopulation ASDRs. We critically examines the strengths and limitations of GEE models, shedding light on their applicability for mortality forecasting. Through detailed model specifications and empirical illustrations, the study contributes to an enhanced understanding of the nuanced capabilities of GEE models in predicting mortality rates.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11332"
    },
    {
        "doc_id": 70,
        "title": "A Hierarchical Decision-Based Maintenance for a Complex Modular System Driven by the { MoMA} Algorithm",
        "authors": [
            "M. L. Gamiz",
            "D. Montoro-Cazorla",
            "M. C. Segovia-Garcia"
        ],
        "subjects": [
            "Systems and Control",
            "Methodology"
        ],
        "abstract": "This paper presents a maintenance policy for a modular system formed by K independent modules (n-subsystems) subjected to environmental conditions (shocks). For the modeling of this complex system, the use of the Matrix-Analytical Method (MAM) is proposed under a layered approach according to its hierarchical structure. Thus, the operational state of the system (top layer) depends on the states of the modules (middle layer), which in turn depend on the states of their components (bottom layer). This allows a detailed description of the system operation to plan maintenance actions appropriately and optimally. We propose a hierarchical decision-based maintenance strategy with periodic inspections as follows: at the time of the inspection, the condition of the system is first evaluated. If intervention is necessary, the modules are then checked to make individual decisions based on their states, and so on. Replacement or repair will be carried out as appropriate. An optimization problem is formulated as a function of the length of the inspection period and the intervention cost incurred over the useful life of the system. Our method shows the advantages, providing compact and implementable expressions. The model is illustrated on a submarine Electrical Control Unit (ECU).",
        "comments": "43 pages, 6 figures",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11328"
    },
    {
        "doc_id": 71,
        "title": "Measuring hierarchically-organized interactions in dynamic networks through spectral entropy rates: theory, estimation, and illustrative application to physiological networks",
        "authors": [
            "Laura Sparacino",
            "Yuri Antonacci",
            "Gorana Mijatovic",
            "Luca Faes"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Recent advances in signal processing and information theory are boosting the development of new approaches for the data-driven modelling of complex network systems. In the fields of Network Physiology and Network Neuroscience where the signals of interest are often rich of oscillatory content, the spectral representation of network systems is essential to ascribe the analyzed interactions to specific oscillations with physiological meaning. In this context, the present work formalizes a coherent framework which integrates several information dynamics approaches to quantify node-specific, pairwise and higher-order interactions in network systems. The framework establishes a hierarchical organization of interactions of different order using measures of entropy rate, mutual information rate and O-information rate, to quantify respectively the dynamics of individual nodes, the links between pairs of nodes, and the redundant/synergistic hyperlinks between groups of nodes. All measures are formulated in the time domain, and then expanded to the spectral domain to obtain frequency-specific information. The practical computation of all measures is favored presenting a toolbox that implements their parametric and non-parametric estimation, and includes approaches to assess their statistical significance. The framework is illustrated first using theoretical examples where the properties of the measures are displayed in benchmark simulated network systems, and then applied to representative examples of multivariate time series in the context of Network Neuroscience and Network Physiology.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11327"
    },
    {
        "doc_id": 72,
        "title": "PRILoRA: Pruned and Rank-Increasing Low-Rank Adaptation",
        "authors": [
            "Nadav Benedek",
            "Lior Wolf"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence"
        ],
        "abstract": "With the proliferation of large pre-trained language models (PLMs), fine-tuning all model parameters becomes increasingly inefficient, particularly when dealing with numerous downstream tasks that entail substantial training and storage costs. Several approaches aimed at achieving parameter-efficient fine-tuning (PEFT) have been proposed. Among them, Low-Rank Adaptation (LoRA) stands out as an archetypal method, incorporating trainable rank decomposition matrices into each target module. Nevertheless, LoRA does not consider the varying importance of each layer. To address these challenges, we introduce PRILoRA, which linearly allocates a different rank for each layer, in an increasing manner, and performs pruning throughout the training process, considering both the temporary magnitude of weights and the accumulated statistics of the input to any given layer. We validate the effectiveness of PRILoRA through extensive experiments on eight GLUE benchmarks, setting a new state of the art.",
        "comments": "EACL 2024",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11316"
    },
    {
        "doc_id": 73,
        "title": "Weakly-Supervised Semantic Segmentation of Circular-Scan, Synthetic-Aperture-Sonar Imagery",
        "authors": [
            "Isaac J. Sledge",
            "Dominic M. Byrne",
            "Jonathan L. King",
            "Steven H. Ostertag",
            "Denton L. Woods",
            "James L. Prater",
            "Jermaine L. Kennedy",
            "Timothy M. Marston",
            "Jose C. Principe"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning",
            "Image and Video Processing"
        ],
        "abstract": "We propose a weakly-supervised framework for the semantic segmentation of circular-scan synthetic-aperture-sonar (CSAS) imagery. The first part of our framework is trained in a supervised manner, on image-level labels, to uncover a set of semi-sparse, spatially-discriminative regions in each image. The classification uncertainty of each region is then evaluated. Those areas with the lowest uncertainties are then chosen to be weakly labeled segmentation seeds, at the pixel level, for the second part of the framework. Each of the seed extents are progressively resized according to an unsupervised, information-theoretic loss with structured-prediction regularizers. This reshaping process uses multi-scale, adaptively-weighted features to delineate class-specific transitions in local image content. Content-addressable memories are inserted at various parts of our framework so that it can leverage features from previously seen images to improve segmentation performance for related images.\n  We evaluate our weakly-supervised framework using real-world CSAS imagery that contains over ten seafloor classes and ten target classes. We show that our framework performs comparably to nine fully-supervised deep networks. Our framework also outperforms eleven of the best weakly-supervised deep networks. We achieve state-of-the-art performance when pre-training on natural imagery. The average absolute performance gap to the next-best weakly-supervised network is well over ten percent for both natural imagery and sonar imagery. This gap is found to be statistically significant.",
        "comments": "Submitted to the IEEE Journal of Oceanic Engineering",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11313"
    },
    {
        "doc_id": 74,
        "title": "Handling incomplete outcomes and covariates in cluster-randomized trials: doubly-robust estimation, efficiency considerations, and sensitivity analysis",
        "authors": [
            "Bingkai Wang",
            "Fan Li",
            "Rui Wang"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "In cluster-randomized trials, missing data can occur in various ways, including missing values in outcomes and baseline covariates at the individual or cluster level, or completely missing information for non-participants. Among the various types of missing data in CRTs, missing outcomes have attracted the most attention. However, no existing method comprehensively addresses all the aforementioned types of missing data simultaneously due to their complexity. This gap in methodology may lead to confusion and potential pitfalls in the analysis of CRTs. In this article, we propose a doubly-robust estimator for a variety of estimands that simultaneously handles missing outcomes under a missing-at-random assumption, missing covariates with the missing-indicator method (with no constraint on missing covariate distributions), and missing cluster-population sizes via a uniform sampling framework. Furthermore, we provide three approaches to improve precision by choosing the optimal weights for intracluster correlation, leveraging machine learning, and modeling the propensity score for treatment assignment. To evaluate the impact of violated missing data assumptions, we additionally propose a sensitivity analysis that measures when missing data alter the conclusion of treatment effect estimation. Simulation studies and data applications both show that our proposed method is valid and superior to the existing methods.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11278"
    },
    {
        "doc_id": 75,
        "title": "Asymptotics for non-degenerate multivariate $U$-statistics with estimated nuisance parameters under the null and local alternative hypotheses",
        "authors": [
            "Alain Desgagn\u00e9",
            "Christian Genest",
            "Fr\u00e9d\u00e9ric Ouimet"
        ],
        "subjects": [
            "Statistics Theory",
            "Probability",
            "Applications",
            "Methodology"
        ],
        "abstract": "The large-sample behavior of non-degenerate multivariate $U$-statistics of arbitrary degree is investigated under the assumption that their kernel depends on parameters that can be estimated consistently. Mild regularity conditions are given which guarantee that once properly normalized, such statistics are asymptotically multivariate Gaussian both under the null hypothesis and sequences of local alternatives. The work of Randles (1982, Ann. Statist.) is extended in three ways: the data and the kernel values can be multivariate rather than univariate, the limiting behavior under local alternatives is studied for the first time, and the effect of knowing some of the nuisance parameters is quantified. These results can be applied to a broad range of goodness-of-fit testing contexts, as shown in one specific example.",
        "comments": "16 pages, 1 figure",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11272"
    },
    {
        "doc_id": 76,
        "title": "Assessing the Competitiveness of Matrix-Free Block Likelihood Estimation in Spatial Models",
        "authors": [
            "Alfredo Alegr\u00eda"
        ],
        "subjects": [
            "Methodology",
            "Statistics Theory"
        ],
        "abstract": "In geostatistics, block likelihood offers a balance between statistical accuracy and computational efficiency when estimating covariance functions. This balance is reached by dividing the sample into blocks and computing a weighted sum of (sub) log-likelihoods corresponding to pairs of blocks. Practitioners often choose block sizes ranging from hundreds to a few thousand observations, inherently involving matrix-based implementations. An alternative, residing at the opposite end of this methodological spectrum, treats each observation as a block, resulting in the matrix-free pairwise likelihood method. We propose an additional alternative within this broad methodological landscape, systematically constructing blocks of size two and merging pairs of blocks through conditioning. Importantly, our method strategically avoids large-sized blocks, facilitating explicit calculations that ultimately do not rely on matrix computations. Studies with both simulated and real data validate the effectiveness of our approach, on one hand demonstrating its superiority over pairwise likelihood, and on the other, challenging the intuitive notion that employing matrix-based versions universally lead to better statistical performance.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11265"
    },
    {
        "doc_id": 77,
        "title": "Adaptive Bayesian Optimization Algorithm for Unpredictable Business Environments",
        "authors": [
            "Sarit Maitra"
        ],
        "subjects": [
            "Optimization and Control"
        ],
        "abstract": "This paper presents an innovative optimization framework and algorithm based on the Bayes theorem, featuring adaptive conditioning and jitter. The adaptive conditioning function dynamically modifies the mean objective function in each iteration, enhancing its adaptability. The mean function, representing the model's best estimate of the optimal value for the true objective function, is adjusted based on observed data. The framework also incorporates an adaptive acquisition jitter function, enhancing adaptability by adjusting the jitter of the acquisition function. It also introduces a robust objective function with a penalty term, aiming to generate robust solutions under uncertainty. The evaluation of the framework includes single-objective, decoupled multi-objective, and combined multi-objective functions. Statistical analyses, including t-statistics, p-values, and effect size measures, highlight the superiority of the proposed framework over the original Bayes optimization. The adaptive nature of the conditioning function allows the algorithm to seamlessly incorporate new data, making it particularly beneficial in dynamic optimization scenarios.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11264"
    },
    {
        "doc_id": 78,
        "title": "Estimating heterogeneous treatment effect from survival outcomes via (orthogonal) censoring unbiased learning",
        "authors": [
            "Shenbo Xu",
            "Raluca Cobzaru",
            "Bang Zheng",
            "Stan N. Finkelstein",
            "Roy E. Welsch",
            "Kenney Ng",
            "Ioanna Tzoulaki",
            "Zach Shahn"
        ],
        "subjects": [
            "Methodology",
            "Machine Learning"
        ],
        "abstract": "Methods for estimating heterogeneous treatment effects (HTE) from observational data have largely focused on continuous or binary outcomes, with less attention paid to survival outcomes and almost none to settings with competing risks. In this work, we develop censoring unbiased transformations (CUTs) for survival outcomes both with and without competing risks.After converting time-to-event outcomes using these CUTs, direct application of HTE learners for continuous outcomes yields consistent estimates of heterogeneous cumulative incidence effects, total effects, and separable direct effects. Our CUTs enable application of a much larger set of state of the art HTE learners for censored outcomes than had previously been available, especially in competing risks settings. We provide generic model-free learner-specific oracle inequalities bounding the finite-sample excess risk. The oracle efficiency results depend on the oracle selector and estimated nuisance functions from all steps involved in the transformation. We demonstrate the empirical performance of the proposed methods in simulation studies.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11263"
    },
    {
        "doc_id": 79,
        "title": "Maximum Likelihood Estimators of Quantum Probabilities",
        "authors": [
            "Mirko Navara",
            "Jan \u0160evic"
        ],
        "subjects": [
            "Quantum Physics",
            "Statistics Theory"
        ],
        "abstract": "Classical probability theory is based on assumptions which are often violated in practice. Therefore quantum probability is a proposed alternative not only in quantum physics, but also in other sciences. However, so far it mostly criticizes the classical approach, but does not suggest a working alternative. Maximum likelihood estimators were given very low attention in this context. We show that they can be correctly defined and their computation in closed form is feasible at least in some cases.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11253"
    },
    {
        "doc_id": 80,
        "title": "AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking",
        "authors": [
            "Mehmet Y. Turali",
            "Mehmet E. Lorasdagi",
            "Ali T. Koc",
            "Suleyman S. Kozat"
        ],
        "subjects": [
            "Machine Learning",
            "Signal Processing",
            "Machine Learning"
        ],
        "abstract": "We study the problem of feature selection in general machine learning (ML) context, which is one of the most critical subjects in the field. Although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. To this end, we introduce the ``Adaptive Feature Selection with Binary Masking\" (AFS-BM) which remedies these problems. AFS-BM achieves this by joint optimization for simultaneous feature selection and model training. In particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. This approach leads to significant improvements in model accuracy and a reduction in computational requirements. We provide an extensive set of experiments where we compare AFS-BM with the established feature selection methods using well-known datasets from real-life competitions. Our results show that AFS-BM makes significant improvement in terms of accuracy and requires significantly less computational complexity. This is due to AFS-BM's ability to dynamically adjust to the changing importance of features during the training process, which an important contribution to the field. We openly share our code for the replicability of our results and to facilitate further research.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11250"
    },
    {
        "doc_id": 81,
        "title": "Evaluating if trust and personal information privacy concerns are barriers to using health insurance that explicitly utilizes AI",
        "authors": [
            "Alex Zarifis",
            "Peter Kawalek",
            "Aida Azadegan"
        ],
        "subjects": [
            "Computers and Society",
            "Artificial Intelligence"
        ],
        "abstract": "Trust and privacy have emerged as significant concerns in online transactions. Sharing information on health is especially sensitive but it is necessary for purchasing and utilizing health insurance. Evidence shows that consumers are increasingly comfortable with technology in place of humans, but the expanding use of AI potentially changes this. This research explores whether trust and privacy concern are barriers to the adoption of AI in health insurance. Two scenarios are compared: The first scenario has limited AI that is not in the interface and its presence is not explicitly revealed to the consumer. In the second scenario there is an AI interface and AI evaluation, and this is explicitly revealed to the consumer. The two scenarios were modeled and compared using SEM PLS-MGA. The findings show that trust is significantly lower in the second scenario where AI is visible. Privacy concerns are higher with AI but the difference is not statistically significant within the model.",
        "comments": "Journal of Internet Commerce (2021)",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11249"
    },
    {
        "doc_id": 82,
        "title": "Estimation with Pairwise Observations",
        "authors": [
            "Felix Chan",
            "Laszlo Matyas"
        ],
        "subjects": [
            "Econometrics",
            "Statistics Theory"
        ],
        "abstract": "The paper introduces a new estimation method for the standard linear regression model. The procedure is not driven by the optimisation of any objective function rather, it is a simple weighted average of slopes from observation pairs. The paper shows that such estimator is consistent for carefully selected weights. Other properties, such as asymptotic distributions, have also been derived to facilitate valid statistical inference. Unlike traditional methods, such as Least Squares and Maximum Likelihood, among others, the estimated residual of this estimator is not by construction orthogonal to the explanatory variables of the model. This property allows a wide range of practical applications, such as the testing of endogeneity, i.e.,the correlation between the explanatory variables and the disturbance terms, and potentially several others.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11229"
    },
    {
        "doc_id": 83,
        "title": "On the Information Leakage Performance of Secure Finite Blocklength Transmissions over Rayleigh Fading Channels",
        "authors": [
            "Milad Tatar Mamaghani",
            "Xiangyun Zhou",
            "Nan Yang",
            "A. Lee Swindlehurst",
            "H. Vincent Poor"
        ],
        "subjects": [
            "Information Theory"
        ],
        "abstract": "This paper presents a secrecy performance study of a wiretap communication system with finite blocklength (FBL) transmissions over Rayleigh fading channels, based on the definition of an average information leakage (AIL) metric. We evaluate the exact and closed-form approximate AIL performance, assuming that only statistical channel state information (CSI) of the eavesdropping link is available. Then, we reveal an inherent statistical relationship between the AIL metric in the FBL regime and the commonly-used secrecy outage probability in conventional infinite blocklength communications. Aiming to improve the secure communication performance of the considered system, we formulate a blocklength optimization problem and solve it via a low-complexity approach. Next, we present numerical results to verify our analytical findings and provide various important insights into the impacts of system parameters on the AIL. Specifically, our results indicate that i) compromising a small amount of AIL can lead to significant reliability improvements, and ii) the AIL experiences a secrecy floor in the high signal-to-noise ratio regime.",
        "comments": "6 pages, 5 figures. Accepted for presentation at the 2024 IEEE International Conference on Communications (CT Symposium), 9 - 13 June 2024, Denver, CO United States. Note: An extended version of this work is available as arXiv:2308.13184",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11219"
    },
    {
        "doc_id": 84,
        "title": "Bessel kernel determinants and integrable equations",
        "authors": [
            "Giulio Ruzza"
        ],
        "subjects": [
            "Exactly Solvable and Integrable Systems",
            "Mathematical Physics",
            "Probability"
        ],
        "abstract": "We derive differential equations for multiplicative statistics of the Bessel determinantal point process depending on two parameters. In particular, we prove that such statistics are solutions to an integrable nonlinear partial differential equation describing isospectral deformations of a Sturm--Liouville equation. We also derive identities relating solutions to the integrable partial differential equation and to the Sturm--Liouville equation which imply an analogue for Painlev\u00e9 V of Amir--Corwin--Quastel ``integro-differential Painlev\u00e9 II equation''. This equation reduces, in a degenerate limit, to the system of coupled Painlev\u00e9 V equations derived by Charlier and Doeraene for the generating function of the Bessel process, and to the Painlev\u00e9 V equation derived by Tracy and Widom for the gap probability of the Bessel process. Finally, we study an initial value problem for the integrable partial differential equation. The approach is based on Its--Izergin--Korepin--Slavnov theory of integrable operators and their associated Riemann--Hilbert problems.",
        "comments": "22 pages",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11213"
    },
    {
        "doc_id": 85,
        "title": "$\u03c0$- and $K$-meson properties for large $N_f$ and $N_c$",
        "authors": [
            "Aftab Ahmad",
            "Mumtaz Khan"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "Nuclear Theory"
        ],
        "abstract": "Dynamical chiral symmetry restoration for higher number of light quark flavors $N_f$ and breaking for higher number of colors $N_c$ implies the suppression and enhancement of the dynamically generated quark mass. The study of various larger values of number of colors and flavors may have greater impact on the internal structure of light hadrons. In this work, we study the properties of the pion and kaon, such as mass, condensate, and leptonic decay constant, for various $N_f$ and $N_c$. We use the symmetry-preserving vector-vector flavor-dependent contact interaction model of quark. The dynamical quark masses are calculated by using the Schwinger-Dyson equation (SDE). The masses of the pion and kaon for different values of $N_f$ and $N_c$ are determined using the homogeneous Bethe-Salpeter equation. For fixed $N_f=2$ and $N_c$ is increased, the dynamically generated quark mass ( mass of up and down quarks), strange quark mass, meson in-condensate, and decay constant, all increases. The pion mass remains approximately constant until $N_c$ reaches around 6.5, after which it grows rapidly. On the other hand, the kaon mass increases slowly with increasing $N_c$ until it reaches approximately $N_c=7.5$, beyond which it rises quickly. When $N_c=3$ is fixed at and various values of $N_f$ are considered, all the parameter values decrease as a function of $N_f$, except for the pion and kaon mass, which increase above a critical value of $N_f$ around $8$. This is the region where chiral symmetry is restored, and the pion and kaon behave as free particles, similar to their behavior in the presence of a heat bath. The results obtained for fixed $N_f=2$ and $N_c=3$ are fairly in decent agreement with experimentally calculated statistics and previous model calculations based on the Schwinger-Dyson equation (SDE) and Bethe-Salpeter equation (BSE).",
        "comments": "11 pages, 16 figures",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11186"
    },
    {
        "doc_id": 86,
        "title": "Fermionic signal of vacuum polarization in strong laser fields",
        "authors": [
            "Ya-Nan Dai",
            "Karen Z. Hatsagortsyan",
            "Christoph H. Keitel",
            "Yue-Yue Chen"
        ],
        "subjects": [
            "High Energy Physics - Theory",
            "Plasma Physics"
        ],
        "abstract": "Vacuum polarization (VP) is investigated for the interaction of a polarized $\u03b3$-ray beam of GeV photons with a counterpropagating ultraintense laser pulse. In a conventional setup of a vacuum birefringence measurement, a VP signal is the emerging small circular (linear) polarization of the initially linearly (circularly) polarized probe photons. The pair production via the nonlinear Breit-Wheeler process in such a high-energy environment eliminates part of the $\u03b3$-photons in the outgoing $\u03b3$-beam, increasing the statistical error and decreasing the accuracy of this VP signal. In contrast, we investigate the conversion of the emerging circular polarization of $\u03b3$-photons into longitudinal polarization of the created positrons, considering the latter as the main VP signal. To study the VP effects in the highly nonlinear regime, where the Euler-Heisenberg effective Lagrangian method breaks down, we have developed a Monte-Carlo simulation method, incorporating vacuum birefringence and dichroism via the one-loop QED probabilities in the locally constant field approximation. Our Monte Carlo method will enable the study of VP effects in strong fields of arbitrary configuration. With 10~PW laser systems, we demonstrate the feasibility of detecting the fermionic signal of the VP effect at the 5$\u03c3$ confidence level with a few hours of measurement time.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11168"
    },
    {
        "doc_id": 87,
        "title": "Generalizing Speaker Verification for Spoof Awareness in the Embedding Space",
        "authors": [
            "Xuechen Liu",
            "Md Sahidullah",
            "Kong Aik Lee",
            "Tomi Kinnunen"
        ],
        "subjects": [
            "Cryptography and Security",
            "Artificial Intelligence",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "It is now well-known that automatic speaker verification (ASV) systems can be spoofed using various types of adversaries. The usual approach to counteract ASV systems against such attacks is to develop a separate spoofing countermeasure (CM) module to classify speech input either as a bonafide, or a spoofed utterance. Nevertheless, such a design requires additional computation and utilization efforts at the authentication stage. An alternative strategy involves a single monolithic ASV system designed to handle both zero-effort imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have the potential to provide stronger protections and more economic computations. To this end, we propose to generalize the standalone ASV (G-SASV) against spoofing attacks, where we leverage limited training data from CM to enhance a simple backend in the embedding space, without the involvement of a separate CM module during the test (authentication) phase. We propose a novel yet simple backend classifier based on deep neural networks and conduct the study via domain adaptation and multi-task integration of spoof embeddings at the training stage. Experiments are conducted on the ASVspoof 2019 logical access dataset, where we improve the performance of statistical ASV backends on the joint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and 49.8% in terms of equal error rates, respectively.",
        "comments": "To appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11156"
    },
    {
        "doc_id": 88,
        "title": "Weaving classical turbulence with quantum skeleton",
        "authors": [
            "Weiyu Shen",
            "Jie Yao",
            "Yue Yang"
        ],
        "subjects": [
            "Fluid Dynamics",
            "Quantum Gases",
            "Superconductivity"
        ],
        "abstract": "Matter entanglement is a common chaotic structure in both quantum and classical systems. Turbulence can be pictured as a tangle of vortex filaments in superfluids and viscous vortices in classical fluids. However, it is hard to explain how the statistical properties of turbulence arise from elemental structures. Here we use the quantum vortex tangle as a skeleton to generate an instantaneous classical turbulent field with intertwined vortex tubes. Combining the quantum skeleton and tunable vortex thickness makes the synthetic turbulence satisfy key statistical laws and provides valuable insights for elucidating energy cascade and extreme events. By manipulating the elemental structures, we customize turbulence with desired statistical features. This bottom-up approach of \"weaving\" turbulence provides a testbed for analyzing and modeling turbulence.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11149"
    },
    {
        "doc_id": 89,
        "title": "Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable",
        "authors": [
            "Yuta Kawakami",
            "Manabu Kuroki",
            "Jin Tian"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "There has been considerable recent interest in estimating heterogeneous causal effects. In this paper, we introduce conditional average partial causal effects (CAPCE) to reveal the heterogeneity of causal effects with continuous treatment. We provide conditions for identifying CAPCE in an instrumental variable setting. We develop three families of CAPCE estimators: sieve, parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze their statistical properties. We illustrate the proposed CAPCE estimators on synthetic and real-world data.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11130"
    },
    {
        "doc_id": 90,
        "title": "Regularized Estimation of Sparse Spectral Precision Matrices",
        "authors": [
            "Navonil Deb",
            "Amy Kuceyeski",
            "Sumanta Basu"
        ],
        "subjects": [
            "Methodology",
            "Computation"
        ],
        "abstract": "Spectral precision matrix, the inverse of a spectral density matrix, is an object of central interest in frequency-domain analysis of multivariate time series. Estimation of spectral precision matrix is a key step in calculating partial coherency and graphical model selection of stationary time series. When the dimension of a multivariate time series is moderate to large, traditional estimators of spectral density matrices such as averaged periodograms tend to be severely ill-conditioned, and one needs to resort to suitable regularization strategies involving optimization over complex variables.\n  In this work, we propose complex graphical Lasso (CGLASSO), an $\\ell_1$-penalized estimator of spectral precision matrix based on local Whittle likelihood maximization. We develop fast $\\textit{pathwise coordinate descent}$ algorithms for implementing CGLASSO on large dimensional time series data sets. At its core, our algorithmic development relies on a ring isomorphism between complex and real matrices that helps map a number of optimization problems over complex variables to similar optimization problems over real variables. This finding may be of independent interest and more broadly applicable for high-dimensional statistical analysis with complex-valued data. We also present a complete non-asymptotic theory of our proposed estimator which shows that consistent estimation is possible in high-dimensional regime as long as the underlying spectral precision matrix is suitably sparse. We compare the performance of CGLASSO with competing alternatives on simulated data sets, and use it to construct partial coherence network among brain regions from a real fMRI data set.",
        "comments": "55 pages, 8 figures",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11128"
    },
    {
        "doc_id": 91,
        "title": "Measures determined by their values on balls and Gromov-Wasserstein convergence",
        "authors": [
            "Anne van Delft",
            "Andrew J. Blumberg"
        ],
        "subjects": [
            "Algebraic Topology",
            "Probability"
        ],
        "abstract": "A classical question about a metric space is whether Borel measures on the space are determined by their values on balls. We show that for any given measure this property is stable under Gromov-Wasserstein convergence of metric measure spaces. We then use this result to show that suitable bounded subspaces of the space of persistence diagrams have the property that any Borel measure is determined by its values on balls. This justifies the use of empirical ball volumes for statistical testing in topological data analysis (TDA). Our intended application is to deploy the statistical foundations of van Delft and Blumberg (2023) for time series of random geometric objects in the context of TDA invariants, specifically persistent homology and zigzag persistence.",
        "comments": "MSC Class:          62R40; 55N31",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11125"
    },
    {
        "doc_id": 92,
        "title": "Constraint-based measures of shift and relative shift for discrete frequency distributions",
        "authors": [
            "Kenneth J. Locey",
            "Brian D. Stein"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Comparisons of frequency distributions often invoke the concept of shift to describe directional changes in properties such as the mean. In the present study, we sought to define shift as a property in and of itself. Specifically, we define distributional shift (DS) as the concentration of frequencies away from the discrete class having the greatest value (e.g., the right-most bin of a histogram). We derive a measure of DS using the normalized sum of exponentiated cumulative frequencies. We then define relative distributional shift (RDS) as the difference in DS between two distributions, revealing the magnitude and direction by which one distribution is concentrated to lesser or greater discrete classes relative to another. We find that RDS is highly related to popular measures that, while based on the comparison of frequency distributions, do not explicitly consider shift. While RDS provides a useful complement to other comparative measures, DS allows shift to be quantified as a property of individual distributions, similar in concept to a statistical moment.",
        "comments": "21 pages, 1 table, 6 figures",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11119"
    },
    {
        "doc_id": 93,
        "title": "A Finger on the Pulse of Cardiovascular Health: Smartphone Photoplethysmography-Based Pulse Waveform Analysis for Blood Pressure Measurement",
        "authors": [
            "Ivan Liu",
            "Fangyuan Liu",
            "Qi Zhong",
            "Shiguang Ni"
        ],
        "subjects": [
            "Signal Processing",
            "Computers and Society"
        ],
        "abstract": "Routine blood pressure (BP) monitoring, crucial for health assessment, faces challenges such as limited access to medical-grade equipment and expertise. Portable cuff BP devices, on the other hand, are cumbersome to carry all day and often cost-prohibitive in less developed countries. Besides, these sphygmomanometer-based devices can cause discomfort and disrupt blood flow during measurement. This study explores the use of smartphones for continuous BP monitoring, focusing on overcoming the trust barriers associated with the opacity of machine learning models in predicting BP from low-quality PPG signals. Our approach included developing models based on cardiovascular literature, using simple statistical methods to estimate BP from smartphone PPG signals with comprehensive data pre-processing, applying SHAP for enhanced interpretability and feature identification, and comparing our methods against standard references using Bland-Altman analysis. Validated with data from 125 participants, the study demonstrated significant correlations in waveform features between smartphone and reference BP monitoring devices. The cross-validation of linear regression [MAE=9.86 and 8.01 mmHg for systolic blood pressure (SBP) and diastolic blood pressure (DBP), respectively] and random forest model (MAE=8.91 and 6.68 mmHg for SBP and DBP) using waveform-only variables demonstrated the feasibility of using a smartphone to estimate BP. Although SHAP analysis identified key feature sets, Bland-Altman results did not fully meet established thresholds (84.64% and 94.69% of MAE<15 mmHg for SBP and DBP, respectively). The study suggests the potential of smartphone cameras to enhance the accuracy and interpretability of machine learning models for daily BP estimation, but also indicates that smartphone PPG-based BP prediction is not yet a replacement for traditional medical devices.",
        "comments": "33 pages, 9 figures",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11117"
    },
    {
        "doc_id": 94,
        "title": "Efficient Data Shapley for Weighted Nearest Neighbor Algorithms",
        "authors": [
            "Jiachen T. Wang",
            "Prateek Mittal",
            "Ruoxi Jia"
        ],
        "subjects": [
            "Data Structures and Algorithms",
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "This work aims to address an open problem in data valuation literature concerning the efficient computation of Data Shapley for weighted $K$ nearest neighbor algorithm (WKNN-Shapley). By considering the accuracy of hard-label KNN with discretized weights as the utility function, we reframe the computation of WKNN-Shapley into a counting problem and introduce a quadratic-time algorithm, presenting a notable improvement from $O(N^K)$, the best result from existing literature. We develop a deterministic approximation algorithm that further improves computational efficiency while maintaining the key fairness properties of the Shapley value. Through extensive experiments, we demonstrate WKNN-Shapley's computational efficiency and its superior performance in discerning data quality compared to its unweighted counterpart.",
        "comments": "AISTATS 2024 Oral",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.11103"
    },
    {
        "doc_id": 95,
        "title": "Asymptotic Normality of the Conditional Value-at-Risk based Pickands Estimator",
        "authors": [
            "Yizhou Li",
            "Pawel Polak"
        ],
        "subjects": [
            "Statistics Theory",
            "Other Statistics"
        ],
        "abstract": "The Pickands estimator for the extreme value index is beneficial due to its universal consistency, location, and scale invariance, which sets it apart from other types of estimators. However, similar to many extreme value index estimators, it is marked by poor asymptotic efficiency. Chen (2021) introduces a Conditional Value-at-Risk (CVaR)-based Pickands estimator, establishes its consistency, and demonstrates through simulations that this estimator significantly reduces mean squared error while preserving its location and scale invariance. The initial focus of this paper is on demonstrating the weak convergence of the empirical CVaR in functional space. Subsequently, based on the established weak convergence, the paper presents the asymptotic normality of the CVaR-based Pickands estimator. It further supports these theoretical findings with empirical evidence obtained through simulation studies.",
        "comments": " ",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.11096"
    },
    {
        "doc_id": 96,
        "title": "Learned Image Compression with Dual-Branch Encoder and Conditional Information Coding",
        "authors": [
            "Haisheng Fu",
            "Feng Liang",
            "Jie Liang",
            "Zhenman Fang",
            "Guohe Zhang",
            "Jingning Han"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "Recent advancements in deep learning-based image compression are notable. However, prevalent schemes that employ a serial context-adaptive entropy model to enhance rate-distortion (R-D) performance are markedly slow. Furthermore, the complexities of the encoding and decoding networks are substantially high, rendering them unsuitable for some practical applications. In this paper, we propose two techniques to balance the trade-off between complexity and performance. First, we introduce two branching coding networks to independently learn a low-resolution latent representation and a high-resolution latent representation of the input image, discriminatively representing the global and local information therein. Second, we utilize the high-resolution latent representation as conditional information for the low-resolution latent representation, furnishing it with global information, thus aiding in the reduction of redundancy between low-resolution information. We do not utilize any serial entropy models. Instead, we employ a parallel channel-wise auto-regressive entropy model for encoding and decoding low-resolution and high-resolution latent representations. Experiments demonstrate that our method is approximately twice as fast in both encoding and decoding compared to the parallelizable checkerboard context model, and it also achieves a 1.2% improvement in R-D performance compared to state-of-the-art learned image compression schemes. Our method also outperforms classical image codecs including H.266/VVC-intra (4:4:4) and some recent learned methods in rate-distortion performance, as validated by both PSNR and MS-SSIM metrics on the Kodak dataset.",
        "comments": "Accepted by DCC2024",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.11093"
    },
    {
        "doc_id": 97,
        "title": "Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions",
        "authors": [
            "Adel Javanmard",
            "Lin Chen",
            "Vahab Mirrokni",
            "Ashwinkumar Badanidiyuru",
            "Gang Fu"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Statistics Theory",
            "Machine Learning"
        ],
        "abstract": "Due to the rise of privacy concerns, in many practical applications the training data is aggregated before being shared with the learner, in order to protect privacy of users' sensitive responses. In an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is available only with an aggregate response, providing a summary of individuals' responses in that bag. In this paper, we study two natural loss functions for learning from aggregate responses: bag-level loss and the instance-level loss. In the former, the model is learnt by minimizing a loss between aggregate responses and aggregate model predictions, while in the latter the model aims to fit individual predictions to the aggregate responses. In this work, we show that the instance-level loss can be perceived as a regularized form of the bag-level loss. This observation lets us compare the two approaches with respect to bias and variance of the resulting estimators, and introduce a novel interpolating estimator which combines the two approaches. For linear regression tasks, we provide a precise characterization of the risk of the interpolating estimator in an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis allows us to theoretically understand the effect of different factors, such as bag size on the model prediction risk. In addition, we propose a mechanism for differentially private learning from aggregate responses and derive the optimal bag size in terms of prediction risk-privacy trade-off. We also carry out thorough experiments to corroborate our theory and show the efficacy of the interpolating estimator.",
        "comments": "To appear in the Twelfth International Conference on Learning Representations (ICLR 2024)",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.11081"
    },
    {
        "doc_id": 98,
        "title": "Estimating the Hawkes process from a discretely observed sample path",
        "authors": [
            "Feng Chen",
            "Jeffrey Kwan",
            "Tom Stindl"
        ],
        "subjects": [
            "Methodology",
            "Computation"
        ],
        "abstract": "The Hawkes process is a widely used model in many areas, such as\n  finance, seismology, neuroscience, epidemiology, and social\n  sciences. Estimation of the Hawkes process from continuous\n  observations of a sample path is relatively straightforward using\n  either the maximum likelihood or other methods. However, estimating\n  the parameters of a Hawkes process from observations of a sample\n  path at discrete time points only is challenging due to the\n  intractability of the likelihood with such data. In this work, we\n  introduce a method to estimate the Hawkes process from a discretely\n  observed sample path. The method takes advantage of a state-space\n  representation of the incomplete data problem and use the sequential\n  Monte Carlo (aka particle filtering) to approximate the likelihood\n  function. As an estimator of the likelihood function the SMC\n  approximation is unbiased, and therefore it can be used together\n  with the Metropolis-Hastings algorithm to construct Markov Chains to\n  approximate the likelihood distribution, or more generally, the\n  posterior distribution of model parameters. The performance of the\n  methodology is assessed using simulation experiments and compared\n  with other recently published methods. The proposed estimator is\n  found to have a smaller mean square error than the two benchmark\n  estimators. The proposed method has the additional advantage that\n  confidence intervals for the parameters are easily available. We\n  apply the proposed estimator to the analysis of weekly count data on\n  measles cases in Tokyo Japan and compare the results to those by\n  one of the benchmark methods.",
        "comments": "23 page, 5 figures",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.11075"
    },
    {
        "doc_id": 99,
        "title": "Efficient Data Reduction Strategies for Big Data and High-Dimensional LASSO Regressions",
        "authors": [
            "Xin Wang",
            "Min Yang",
            "William Li"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "The IBOSS approach proposed by Wang et al. (2019) selects the most informative subset of n points. It assumes that the ordinary least squares method is used and requires that the number of variables, p, is not large. However, in many practical problems, p is very large and penalty-based model fitting methods such as LASSO is used. We study the big data problems, in which both n and p are large. In the first part, we focus on reduction in data points. We develop theoretical results showing that the IBOSS type of approach can be applicable to penalty-based regressions such as LASSO. In the second part, we consider the situations where p is extremely large. We propose a two-step approach that involves first reducing the number of variables and then reducing the number of data points. Two separate algorithms are developed, whose performances are studied through extensive simulation studies. Compared to existing methods including well-known split-and-conquer approach, the proposed methods enjoy advantages in terms of estimation accuracy, prediction accuracy, and computation time.",
        "comments": " ",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.11070"
    },
    {
        "doc_id": 100,
        "title": "Biological species delimitation based on genetic and spatial dissimilarity: a comparative study",
        "authors": [
            "Gabriele d'Angella",
            "Christian Hennig"
        ],
        "subjects": [
            "Populations and Evolution",
            "Applications",
            "Methodology"
        ],
        "abstract": "The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. This paper investigates the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Various approaches are compared (some of which already exist in the literature) based on simulated metapopulations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Approaches involve partial Mantel testing, maximum likelihood mixed-effects models with a population effect, and jackknife-based homogeneity tests. A key challenge is that most tests perform on genetic and geographical distance data, violating standard independence assumptions. Simulations showed that partial Mantel tests and mixed-effects models have larger power than jackknife-based methods, but tend to display type-I-error rates slightly above the significance level. Moreover, a multiple regression model neglecting the dependence in the dissimilarities did not show inflated type-I-error rate. An application on brassy ringlets concludes the paper.",
        "comments": "paper of 23 pages with 4 figures; appendix of 11 pages with 4 figures",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12126"
    },
    {
        "doc_id": 101,
        "title": "Matching biomolecular structures by registration of point clouds",
        "authors": [
            "Michael Habeck",
            "Andreas Kr\u00f6pelin",
            "Nima Vakili"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "Motivation: Assessing the match between two biomolecular structures is at the heart of structural analyses such as superposition, alignment and docking. These tasks are typically solved with specialized structure-matching techniques implemented in software for protein structural alignment, rigid-body docking, or rigid fitting into cryo-EM maps. Results: We present a unifying framework to compare biomolecular structures by applying ideas from computer vision. The structures are represented as three-dimensional point clouds and compared by quantifying their overlap. We use the kernel correlation to measure point cloud overlap, and discuss local and global optimization strategies for maximizing the kernel correlation over the space of rigid transformations. We derive a majorization-minimization procedure that can be used to register two point clouds without establishing a point-to-point correspondence. We demonstrate that the majorization-minimization algorithms outperform the commonly used Iterative Closest Point registration algorithm. Furthermore, we discuss and benchmark a randomization strategy for globally optimizing the kernel correlation. We illustrate the approach on various 3D fitting problems such as the comparison of circularly permuted structures and rigid fitting of cryo-EM maps or bead models from small-angle scattering.",
        "comments": "18 pages (main text), 7 figures (main text)",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12082"
    },
    {
        "doc_id": 102,
        "title": "DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI",
        "authors": [
            "Sergio Morell-Ortega",
            "Marina Ruiz-Perez",
            "Marien Gadea",
            "Roberto Vivo-Hernando",
            "Gregorio Rubio",
            "Fernando Aparici",
            "Mariam de la Iglesia-Vaya",
            "Gwenaelle Catheline",
            "Pierrick Coup\u00e9",
            "Jos\u00e9 V. Manj\u00f3n"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Neurons and Cognition"
        ],
        "abstract": "This paper introduces a novel multimodal and high-resolution human brain cerebellum lobule segmentation method. Unlike current tools that operate at standard resolution ($1 \\text{ mm}^{3}$) or using mono-modal data, the proposed method improves cerebellum lobule segmentation through the use of a multimodal and ultra-high resolution ($0.125 \\text{ mm}^{3}$) training dataset. To develop the method, first, a database of semi-automatically labelled cerebellum lobules was created to train the proposed method with ultra-high resolution T1 and T2 MR images. Then, an ensemble of deep networks has been designed and developed, allowing the proposed method to excel in the complex cerebellum lobule segmentation task, improving precision while being memory efficient. Notably, our approach deviates from the traditional U-Net model by exploring alternative architectures. We have also integrated deep learning with classical machine learning methods incorporating a priori knowledge from multi-atlas segmentation, which improved precision and robustness. Finally, a new online pipeline, named DeepCERES, has been developed to make available the proposed method to the scientific community requiring as input only a single T1 MR image at standard resolution.",
        "comments": "20 pages",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.12074"
    },
    {
        "doc_id": 103,
        "title": "Approximating a linear dynamical system from non-sequential data",
        "authors": [
            "Cliff Stein",
            "Pratik Worah"
        ],
        "subjects": [
            "Genomics"
        ],
        "abstract": "Given non-sequential snapshots from instances of a dynamical system, we design a compressed sensing based algorithm that reconstructs the dynamical system. We formally prove that successful reconstruction is possible under the assumption that we can construct an approximate clock from a subset of the coordinates of the underlying system.\n  As an application, we argue that our assumption is likely true for genomic datasets, and we recover the underlying nuclear receptor networks and predict pathways, as opposed to genes, that may differentiate phenotypes in some publicly available datasets.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11858"
    },
    {
        "doc_id": 104,
        "title": "The NOSTRA model: coherent estimation of infection sources in the case of possible nosocomial transmission",
        "authors": [
            "David J Pascall",
            "Chris Jackson",
            "Stephanie Evans",
            "Theodore Gouliouris",
            "Chris Illingworth",
            "Stefan Piatek",
            "Julie V Robotham",
            "Oliver Stirrup",
            "Ben Warne",
            "Judith Breuer",
            "Daniela De Angelis"
        ],
        "subjects": [
            "Applications",
            "Quantitative Methods"
        ],
        "abstract": "Nosocomial infections have important consequences for patients and hospital staff: they worsen patient outcomes and their management stresses already overburdened health systems. Accurate judgements of whether an infection is nosocomial helps staff make appropriate choices to protect other patients within the hospital. Nosocomiality cannot be properly assessed without considering whether the infected patient came into contact with high risk potential infectors within the hospital. We developed a Bayesian model that integrates epidemiological, contact and pathogen genetic data to determine how likely an infection is to be nosocomial and the probability of given infection candidates being the source of the infection.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11837"
    },
    {
        "doc_id": 105,
        "title": "Full-dimensional characterisation of time-warped spike-time stimulus-response distribution geometries",
        "authors": [
            "James B Isbister"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Characterising the representation of sensory stimuli in the brain is a fundamental scientific endeavor, which can illuminate principles of information coding. Most characterizations reduce the dimensionality of neural data by converting spike trains to firing rates or binned spike counts, applying explicitly named methods of ``dimensionality reduction'', or collapsing trial-to-trial variability. Characterisation of the full-dimensional geometry of timing-based representations may provide unexpected insights into how complex high-dimensional information is encoded. Recent research shows that the distribution of representations elicited over trials of a single stimulus can be geometrically characterized without the application of dimensionality reduction, maintaining the temporal spiking information of individual neurons in a cell assembly and illuminating rich geometric structure. We extend these results, showing that precise spike time patterns for larger cell assemblies are time-warped (i.e. stretched or compressed) on each trial. Moreover, by geometrically characterizing distributions of large spike time patterns, our analysis supports the hypothesis that the degree to which a spike time pattern is time-warped depends on the cortical area's background activity level on a single trial. Finally, we suggest that the proliferation of large electrophysiology datasets and the increasing concentration of ``neural geometrists'', creates ideal conditions for characterization of full-dimensional spike time representations, in complement to dimensionality reduction approaches.",
        "comments": "Accepted as an extended abstract at the NeurReps workshop at NeurIPS 2023. The workshop doesn't publish extended abstracts so submitting here",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11784"
    },
    {
        "doc_id": 106,
        "title": "Impact of temporal interaction on the evolution of cooperation",
        "authors": [
            "Yujie He",
            "Tianyu Ren",
            "Junjun Zheng",
            "Huawen Liang"
        ],
        "subjects": [
            "Physics and Society",
            "Social and Information Networks",
            "Populations and Evolution"
        ],
        "abstract": "This research investigates the impact of dynamic interactions with time-varying topologies on the evolution of cooperative behaviours in social dilemmas. Traditional research has focused on deterministic rules governing pairwise interactions, yet the impact of interaction frequency and synchronicity on cooperation remains underexplored. Addressing this gap, our work introduces two temporal interaction mechanisms to model the stochastic or periodic participation of individuals in these games, acknowledging real-life variances due to exogenous temporal factors and geographical time differences. We consider that the interaction state significantly influences both game payoff calculations and the strategy updating process, offering new insights into the emergence and sustainability of cooperation. Our results indicate that maximum game participation frequency is suboptimal under a stochastic interaction mechanism. Instead, an intermediate region of activation probability yields the highest cooperation level, especially under strong dilemma conditions. This suggests that a balance between inactivity security and interaction frequency is crucial. Furthermore, local synchronization of interactions within specific areas is shown to be beneficial, as time differences hinder the spread of cross-structures but promote the formation of dense cooperative clusters with smoother boundaries. Our findings provide an intuitive understanding of node-based temporality and probabilistic interactions, contributing to the broader discourse on resolving social dilemmas.",
        "comments": "7 pages, 6 figures",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11782"
    },
    {
        "doc_id": 107,
        "title": "Combining oligo pools and Golden Gate cloning to create protein variant libraries or guide RNA libraries for CRISPR applications",
        "authors": [
            "Alicia Maci\u00e1 Valero",
            "Rianne C. Prins",
            "Thijs de Vroet",
            "Sonja Billerbeck"
        ],
        "subjects": [
            "Quantitative Methods",
            "Biomolecules"
        ],
        "abstract": "Oligo pools are array-synthesized, user-defined mixtures of single-stranded oligonucleotides that can be used as a source of synthetic DNA for library cloning. While currently offering the most affordable source of synthetic DNA, oligo pools also come with limitations such as a maximum synthesis length (approximately 350 bases), a higher error rate compared to alternative synthesis methods, and the presence of truncated molecules in the pool due to incomplete synthesis. Here, we provide users with a comprehensive protocol that details how oligo pools can be used in combination with Golden Gate cloning to create user-defined protein mutant libraries, as well as single guide RNA libraries for CRISPR applications. Our methods are optimized to work within the Yeast Toolkit Golden Gate scheme, but are in principle compatible with any other Golden Gate-based modular cloning toolkit and extendable to other restriction enzyme-based cloning methods beyond Golden Gate. Our methods yield high-quality, affordable, in-house variant libraries.",
        "comments": " ",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.11746"
    },
    {
        "doc_id": 108,
        "title": "Evolutionary dynamics of any multiplayer game on regular graphs",
        "authors": [
            "Chaoqian Wang",
            "Matja\u017e Perc",
            "Attila Szolnoki"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Statistical Mechanics",
            "Computational Complexity",
            "Cellular Automata and Lattice Gases",
            "Populations and Evolution"
        ],
        "abstract": "Multiplayer games on graphs are at the heart of theoretical descriptions of key evolutionary processes that govern vital social and natural systems. However, a comprehensive theoretical framework for solving multiplayer games with an arbitrary number of strategies on graphs is still missing. Here, we solve this by drawing an analogy with the Ball-and-Box problem, based on which we show that the local configuration of multiplayer games on graphs is equivalent to distributing $k$ identical co-players among $n$ distinct strategies. We use this to derive the replicator equation for any $n$-strategy multiplayer game under weak selection, which can be solved in polynomial time. As an example, we revisit the second-order free-riding problem, where costly punishment cannot truly resolve social dilemmas in a well-mixed population. Yet, in structured populations, we derive an accurate threshold for the punishment strength, beyond which punishment can either lead to the extinction of defection or transform the system into a rock-paper-scissors-like cycle. The analytical solution also qualitatively agrees with the phase diagrams that were previously obtained for non-marginal selection strengths. Our framework thus allows an exploration of any multi-strategy multiplayer game on regular graphs.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11686"
    },
    {
        "doc_id": 109,
        "title": "Accelerating Seed Location Filtering in DNA Read Mapping Using a Commercial Compute-in-SRAM Architecture",
        "authors": [
            "Courtney Golden",
            "Dan Ilan",
            "Nicholas Cebry",
            "Christopher Batten"
        ],
        "subjects": [
            "Hardware Architecture",
            "Genomics"
        ],
        "abstract": "DNA sequence alignment is an important workload in computational genomics. Reference-guided DNA assembly involves aligning many read sequences against candidate locations in a long reference genome. To reduce the computational load of this alignment, candidate locations can be pre-filtered using simpler alignment algorithms like edit distance. Prior work has explored accelerating filtering on simulated compute-in-DRAM, due to the massive parallelism of compute-in-memory architectures. In this paper, we present work-in-progress on accelerating filtering using a commercial compute-in-SRAM accelerator. We leverage the recently released Gemini accelerator platform from GSI Technology, which is the first, to our knowledge, commercial-scale compute-in-SRAM system. We accelerate the Myers' bit-parallel edit distance algorithm, producing average speedups of 14.1x over single-core CPU performance. Individual query/candidate alignments produce speedups of up to 24.1x. These early results suggest this novel architecture is well-suited to accelerating the filtering step of sequence-to-sequence DNA alignment.",
        "comments": "Journal ref:        5th Workshop on Accelerator Architecture in Computational Biology and Bioinformatics (AACBB), June 2023",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11685"
    },
    {
        "doc_id": 110,
        "title": "Modern approaches to improving phase contrast electron microscopy",
        "authors": [
            "Jeremy J. Axelrod",
            "Jessie T. Zhang",
            "Petar N. Petrov",
            "Robert M. Glaeser",
            "Holger Mueller"
        ],
        "subjects": [
            "Quantitative Methods",
            "Optics",
            "Biomolecules"
        ],
        "abstract": "Although defocus can be used to generate partial phase contrast in transmission electron microscope images, cryo-electron microscopy (cryo-EM) can be further improved by the development of phase plates which increase contrast by applying a phase shift to the unscattered part of the electron beam. Many approaches have been investigated, including the ponderomotive interaction between light and electrons. We review the recent successes achieved with this method in high-resolution, single-particle cryo-EM. We also review the status of using pulsed or near-field enhanced laser light as alternatives, along with approaches that use scanning transmission electron microscopy (STEM) with a segmented detector rather than a phase plate.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11678"
    },
    {
        "doc_id": 111,
        "title": "Enhancing selectivity using Wasserstein distance based reweighing",
        "authors": [
            "Pratik Worah"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "Given two labeled data-sets $\\mathcal{S}$ and $\\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\\mathcal{T}$.\n  On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.\n  Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1 (a highly similar protein). We tune the algorithm's parameter so that overall change in holdout loss is negligible, but the selectivity, i.e., the fraction of top 100 MNK2 binders that are MNK1 non-binders, increases from 54\\% to 95\\%, as a result of our reweighing. Of the 43 distinct small molecules predicted to be most selective from the enamine catalog, 2 small molecules were experimentally verified to be selective, i.e., they reduced the enzyme activity of MNK2 below 50\\% but not MNK1, at 10$\u03bc$M -- a 5\\% success rate.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11562"
    },
    {
        "doc_id": 112,
        "title": "Understanding Hepatitis B Virus Infection through Hepatocyte Proliferation and Capsid Recycling",
        "authors": [
            "Rupchand Sutradhar",
            "D C Dalal"
        ],
        "subjects": [
            "Populations and Evolution",
            "Dynamical Systems"
        ],
        "abstract": "Proliferation of uninfected as well as infected hepatocytes and recycling of DNA-containing\n  capsids are two major mechanisms playing significant roles in the clearance of hepatitis B\n  virus (HBV) infection. In this study, the temporal dynamics of this infection are investigated\n  through two in silico bio-mathematical models considering both proliferation of hepatocytes\n  and the recycling of capsids. Both models are formulated on the basis of a key finding in the existing literature: mitosis of infected yields in two uninfected progenies. In the first model,\n  we examine regular proliferation (occurs continuously), while the second model deals with the\n  irregular proliferation (happens when the total number of liver cells decreases to less than 70%\n  of its initial volume). The models are calibrated with the experimental data obtained from\n  an adult chimpanzee. Results of this study suggest that when both hepatocytes proliferate\n  with equal rate, proliferation aids the individual in a rapid recovery from the acute infection\n  whereas in the case of chronic infection, the severity of the infection increases if the proliferation\n  occurs frequently. On the other hand, if the infected cells proliferate at a slower rate than uninfected cells, the proliferation of uninfected hepatocytes contributes to increase the infection,\n  but the proliferation of infected hepatocytes acts to reduce the infection from the long-term\n  perspective. Furthermore, it is also observed that the differences between the outcomes of\n  regular and irregular proliferations are substantial and noteworthy.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11481"
    },
    {
        "doc_id": 113,
        "title": "Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis",
        "authors": [
            "Li Yin",
            "Xiong Yu",
            "Fan Wenxin",
            "Wang Kai",
            "Yu Qingqing",
            "Si Liping",
            "van der Smagt Patrick",
            "Tang Jun",
            "Chen Nutan"
        ],
        "subjects": [
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT.\n  Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities.\n  Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\\,\\%$ to $72\\%$, and for LSTM models, it is $66\\,\\%$ to $84\\,\\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for LSTM models it is between $1.09$ and $1.77$. Notably, these RMSEs are significantly lower than the random prediction error of $4.55$.\n  Conclusion: We creatively apply sequential models in the long-term management of SCIT with promising accuracy in the prediction of SCIT nonadherence in Allergic Rhinitis (AR) patients. While LSTM outperforms SLAC in adherence prediction, SLAC excels in score prediction for patients undergoing SCIT for AR. The state-action-based SLAC adds flexibility, presenting a novel and effective approach for managing long-term AIT.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.11447"
    },
    {
        "doc_id": 114,
        "title": "MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks via Text Prompts",
        "authors": [
            "Haoqiang Guo",
            "Sendong Zhao",
            "Haochun Wang",
            "Yanrui Du",
            "Bing Qin"
        ],
        "subjects": [
            "Machine Learning",
            "Computation and Language",
            "Biomolecules"
        ],
        "abstract": "Deep learning is now widely used in drug discovery, providing significant acceleration and cost reduction. As the most fundamental building block, molecular representation is essential for predicting molecular properties to enable various downstream applications. Most existing methods attempt to incorporate more information to learn better representations. However, not all features are equally important for a specific task. Ignoring this would potentially compromise the training efficiency and predictive accuracy. To address this issue, we propose a novel approach, which treats language models as an agent and molecular pretraining models as a knowledge base. The agent accentuates task-relevant features in the molecular representation by understanding the natural language description of the task, just as a tailor customizes clothes for clients. Thus, we call this approach MolTailor. Evaluations demonstrate MolTailor's superior performance over baselines, validating the efficacy of enhancing relevance for molecular representation learning. This illustrates the potential of language model guided optimization to better exploit and unleash the capabilities of existing powerful molecular representation methods. Our codes and appendix are available at https://github.com/SCIR-HI/MolTailor.",
        "comments": "Accepted by AAAI 2024",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11403"
    },
    {
        "doc_id": 115,
        "title": "PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding",
        "authors": [
            "Ruochi Zhang",
            "Haoran Wu",
            "Chang Liu",
            "Huaping Li",
            "Yuqian Wu",
            "Kewei Li",
            "Yifan Wang",
            "Yifan Deng",
            "Jiahui Chen",
            "Fengfeng Zhou",
            "Xin Gao"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "Recent advances in protein language models have catalyzed significant progress in peptide sequence representation. Despite extensive exploration in this field, pre-trained models tailored for peptide-specific needs remain largely unaddressed due to the difficulty in capturing the complex and sometimes unstable structures of peptides. This study introduces a novel multi-view contrastive learning framework PepHarmony for the sequence-based peptide encoding task. PepHarmony innovatively combines both sequence- and structure-level information into a sequence-level encoding module through contrastive learning. We carefully select datasets from the Protein Data Bank (PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences and structures. The experimental data highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures compared with the baseline and fine-tuned models. The robustness of our model is confirmed through extensive ablation studies, which emphasize the crucial roles of contrastive loss and strategic data sorting in enhancing predictive performance. The proposed PepHarmony framework serves as a notable contribution to peptide representations, and offers valuable insights for future applications in peptide drug discovery and peptide engineering. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony or http://www.healthinformaticslab.org/supp/.",
        "comments": "25 pages, 5 figures, 3 tables",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11360"
    },
    {
        "doc_id": 116,
        "title": "Sensory adaptation in a continuum model of bacterial chemotaxis -- working range, cost-accuracy relation, and coupled systems",
        "authors": [
            "Vansh Kharbanda",
            "Benedikt Sabass"
        ],
        "subjects": [
            "Cell Behavior",
            "Soft Condensed Matter"
        ],
        "abstract": "Sensory adaptation enables organisms to adjust their perception in a changing environment. A paradigm is bacterial chemotaxis, where the output activity of chemoreceptors is adapted to different baseline concentrations via receptor methylation. The range of internal receptor states limits the stimulus magnitude to which these systems can adapt. Here, we employ a highly idealized, Langevin-equation based model to study how the finite range of state variables affects the adaptation accuracy and the energy dissipation in individual and coupled systems. Maintaining an adaptive state requires constant energy dissipation. We show that the steady-state dissipation rate increases approximately linearly with the adaptation accuracy for varying stimulus magnitudes in the so-called perfect adaptation limit. This result complements the well-known logarithmic cost-accuracy relationship for varying chemical driving. Next, we study linearly coupled pairs of sensory units. We find that the interaction reduces the dissipation rate per unit and affects the overall cost-accuracy relationship. A coupling of the slow methylation variables results in a better accuracy than a coupling of activities. Overall, the findings highlight the significance of both the working range and collective operation mode as crucial design factors that impact the accuracy and energy expenditure of molecular adaptation networks.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11341"
    },
    {
        "doc_id": 117,
        "title": "Uncertainty quantification of receptor ligand binding sites prediction",
        "authors": [
            "Nanjie Chen",
            "Dongliang Yu",
            "Dmitri Beglov",
            "Mark Kon",
            "Julio Enrique Castrillon Candas"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "Recent advancements in protein docking site prediction have highlighted the limitations of traditional rigid docking algorithms, like PIPER, which often neglect critical stochastic elements such as solvent-induced fluctuations. These oversights can lead to inaccuracies in identifying viable docking sites due to the complexity of high-dimensional, stochastic energy manifolds with low regularity. To address this issue, our research introduces a novel model where the molecular shapes of ligands and receptors are represented using multi-variate Karhunen-Lo `eve (KL) expansions. This method effectively captures the stochastic nature of energy manifolds, allowing for a more accurate representation of molecular interactions.Developed as a plugin for PIPER, our scientific computing software enhances the platform, delivering robust uncertainty measures for the energy manifolds of ranked binding sites. Our results demonstrate that top-ranked binding sites, characterized by lower uncertainty in the stochastic energy manifold, align closely with actual docking sites. Conversely, sites with higher uncertainty correlate with less optimal docking positions. This distinction not only validates our approach but also sets a new standard in protein docking predictions, offering substantial implications for future molecular interaction research and drug development.",
        "comments": " ",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11312"
    },
    {
        "doc_id": 118,
        "title": "Seasonality of primary productivity affects coastal species more than its magnitude",
        "authors": [
            "Carlota Muniz",
            "Christopher McQuaid",
            "Nicolas Weidberg"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "While the importance of extreme conditions is recognised, patterns in species abundances are often interpreted through average environmental conditions within their distributional range. For marine species with pelagic larvae, temperature and phytoplankton concentration are key variables. Along the south coast of South Africa, conspicuous spatial patterns in recruitment rates and the abundances of different mussel species exist, with focal areas characterized by large populations. We studied 15 years of sea surface temperature (SST) and chlorophyll-a (chl-a) satellite data, using spectral analyses to partition their temporal variability over ecologically relevant time periods, including seasonal (101 to 365 days) and intra-seasonal cycles (20 to 100 days). Adult cover and mussel recruitment were measured at 10 sites along the south coast and regression models showed that about 70 percent of the variability in recruitment and adult cover was explained by seasonal variability in chl-a, while mean annual chl-a and SST only explained 30 percent of the recruitment, with no significant effect for adult cover. SST and chl-a at two upwelling centres showed less predictable seasonal cycles during the second half of the study period with a significant cooling trend during austral autumn, coinciding with one of the mussel reproductive peaks. This likely reflects recent changes in the Agulhas Current, the world largest western boundary current, which affects coastal ecosystems by driving upwelling.",
        "comments": "Journal ref:        Science of the Total Environment, 757:143740, 2021",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11289"
    },
    {
        "doc_id": 119,
        "title": "Smart Drug-Delivery Systems for Cancer Nanotherapy",
        "authors": [
            "Paola Sanchez-Moreno",
            "Juan Luis Ortega-Vinuesa",
            "Jose Manuel Peula-Garcia",
            "Juan Antonio Marchal",
            "Houria Boulaiz"
        ],
        "subjects": [
            "Tissues and Organs",
            "Mesoscale and Nanoscale Physics",
            "Biological Physics"
        ],
        "abstract": "Despite all the advances achieved in the field of tumor-biology research, in most cases conventional therapies including chemotherapy are still the leading choices. The main disadvantage of these treatments, in addition to the low solubility of many antitumor drugs, is their lack of specificity, which explains the frequent occurrence of serious side effects due to nonspecific drug uptake by healthy cells. Progress in nanotechnology and its application in medicine have provided new opportunities and different smart systems. Such systems can improve the intracellular delivery of the drugs due to their multifunctionality and targeting potential. The purpose of this manuscript is to review and analyze the recent progress made in nanotherapy applied to cancer treatment. First, we provide a global overview of cancer and different smart nanoparticles currently used in oncology. Then, we analyze in detail the development of drug-delivery strategies in cancer therapy, focusing mainly on the intravenously administered smart nanoparticles with protein corona to avoid immune-system clearance. Finally, we discuss the challenges, clinical trials, and future directions of the nanoparticle-based therapy in cancer.",
        "comments": "Preprint version, 25 pages, 7 figures, 3 tables. Authors thank to Bentham Science the posibility of deposit the ACCEPTED VERSION of the peer-reviewed article after 12 months of publication on journal web site on arXiv repository. The published manuscript is available at EurekaSelect via https://www.eurekaselect.com/openurl/content.php?genre=article&doi=10.2174/1389450117666160527142544",
        "date": "2024-01-20",
        "pdf_url": "https://arxiv.org/pdf/2401.11192"
    },
    {
        "doc_id": 120,
        "title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
        "authors": [
            "Chih-Hsuan Wei",
            "Alexis Allot",
            "Po-Ting Lai",
            "Robert Leaman",
            "Shubo Tian",
            "Ling Luo",
            "Qiao Jin",
            "Zhizheng Wang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "subjects": [
            "Computation and Language",
            "Quantitative Methods"
        ],
        "abstract": "PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art AI techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.",
        "comments": " ",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.11048"
    },
    {
        "doc_id": 121,
        "title": "Equivariant Graph Neural Operator for Modeling 3D Dynamics",
        "authors": [
            "Minkai Xu",
            "Jiaqi Han",
            "Aaron Lou",
            "Jean Kossaifi",
            "Arvind Ramanathan",
            "Kamyar Azizzadenesheli",
            "Jure Leskovec",
            "Stefano Ermon",
            "Anima Anandkumar"
        ],
        "subjects": [
            "Machine Learning",
            "Numerical Analysis",
            "Quantitative Methods"
        ],
        "abstract": "Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stacking the Fourier layers over equivariant networks. EGNO is the first operator learning framework that is capable of modeling solution dynamics functions over time while retaining 3D equivariance. Comprehensive experiments in multiple domains, including particle simulations, human motion capture, and molecular dynamics, demonstrate the significantly superior performance of EGNO against existing methods, thanks to the equivariant temporal modeling.",
        "comments": " ",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.11037"
    },
    {
        "doc_id": 122,
        "title": "Clustering Molecular Energy Landscapes by Adaptive Network Embedding",
        "authors": [
            "Paula Mercurio",
            "Di Liu"
        ],
        "subjects": [
            "Biomolecules",
            "Statistical Mechanics",
            "Machine Learning"
        ],
        "abstract": "In order to efficiently explore the chemical space of all possible small molecules, a common approach is to compress the dimension of the system to facilitate downstream machine learning tasks. Towards this end, we present a data driven approach for clustering potential energy landscapes of molecular structures by applying recently developed Network Embedding techniques, to obtain latent variables defined through the embedding function. To scale up the method, we also incorporate an entropy sensitive adaptive scheme for hierarchical sampling of the energy landscape, based on Metadynamics and Transition Path Theory. By taking into account the kinetic information implied by a system's energy landscape, we are able to interpret dynamical node-node relationships in reduced dimensions. We demonstrate the framework through Lennard-Jones (LJ) clusters and a human DNA sequence.",
        "comments": "19 pages, 10 figures",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.10972"
    },
    {
        "doc_id": 123,
        "title": "Homogenisation of nonlinear blood flow in periodic networks: the limit of small haematocrit heterogeneity",
        "authors": [
            "Y. Ben-Ami",
            "B. D. Wood",
            "J. M. Pitt-Francis",
            "P. K. Maini",
            "H. M. Byrne"
        ],
        "subjects": [
            "Tissues and Organs",
            "Soft Condensed Matter",
            "Biological Physics"
        ],
        "abstract": "In this work we develop a homogenisation methodology to upscale mathematical descriptions of microcirculatory blood flow from the microscale (where individual vessels are resolved) to the macroscopic (or tissue) scale. Due to the assumed two-phase nature of blood and specific features of red blood cells (RBCs), mathematical models for blood flow in the microcirculation are highly nonlinear, coupling the flow and RBC concentrations (haematocrit). In contrast to previous works which accomplished blood-flow homogenisation by assuming that the haematocrit level remains constant, here we allow for spatial heterogeneity in the haematocrit concentration and thus begin with a nonlinear microscale model. We simplify the analysis by considering the limit of small haematocrit heterogeneity which prevails when variations in haematocrit concentration between neighbouring vessels are small. Homogenisation results in a system of coupled, nonlinear partial differential equations describing the flow and haematocrit transport at the macroscale, in which a nonlinear Darcy-type model relates the flow and pressure gradient via a haematocrit-dependent permeability tensor. During the analysis we obtain further that haematocrit transport at the macroscale is governed by a purely advective equation. Applying the theory to particular examples of two- and three-dimensional geometries of periodic networks, we calculate the effective permeability tensor associated with blood flow in these vascular networks. We demonstrate how the statistical distribution of vessel lengths and diameters, together with the average haematocrit level, affect the statistical properties of the macroscopic permeability tensor. These data can be used to simulate blood flow and haematocrit transport at the macroscale.",
        "comments": "34 pages, 8 figures",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.10932"
    },
    {
        "doc_id": 124,
        "title": "A Chaotic Associative Memory",
        "authors": [
            "Nurani Rajagopal Rohan",
            "Sayan Gupta",
            "V. Srinivasa Chakravarthy"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Chaotic Dynamics"
        ],
        "abstract": "We propose a novel Chaotic Associative Memory model using a network of chaotic Rossler systems and investigate the storage capacity and retrieval capabilities of this model as a function of increasing periodicity and chaos. In early models of associate memory networks, memories were modeled as fixed points, which may be mathematically convenient but has poor neurobiological plausibility. Since brain dynamics is inherently oscillatory, attempts have been made to construct associative memories using nonlinear oscillatory networks. However, oscillatory associative memories are plagued by the problem of poor storage capacity, though efforts have been made to improve capacity by adding higher order oscillatory modes. The chaotic associative memory proposed here exploits the continuous spectrum of chaotic elements and has higher storage capacity than previously described oscillatory associate memories.",
        "comments": "10 pages, 8 Figures, Submitted to \"Chaos: An Interdisciplinary Journal of Nonlinear Science\"",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.10922"
    },
    {
        "doc_id": 125,
        "title": "Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior",
        "authors": [
            "Jason Toy",
            "Josh MacAdam",
            "Phil Tabor"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Artificial Intelligence"
        ],
        "abstract": "Recent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face challenges such as limited context windows and difficulties in generalization. In this paper, we introduce a metacognition module for generative agents, enabling them to observe their own thought processes and actions. This metacognitive approach, designed to emulate System 1 and System 2 cognitive processes, allows agents to significantly enhance their performance by modifying their strategy. We tested the metacognition module on a variety of scenarios, including a situation where generative agents must survive a zombie apocalypse, and observe that our system outperform others, while agents adapt and improve their strategies to complete tasks over time.",
        "comments": "9 pages, 4 figures",
        "date": "2024-01-09",
        "pdf_url": "https://arxiv.org/pdf/2401.10910"
    },
    {
        "doc_id": 126,
        "title": "Novel community data in ecology -- properties and prospects",
        "authors": [
            "Florian Hartig",
            "Nerea Abrego",
            "Alex Bush",
            "Jonathan M. Chase",
            "Gurutzeta Guillera-Arroita",
            "Mathew A. Leibold",
            "Otso Ovaskainen",
            "Lo\u00efc Pellissier",
            "Maximilian Pichler",
            "Giovanni Poggiato",
            "Laura Pollock",
            "Sara Si-Moussi",
            "Wilfried Thuiller",
            "Duarte S. Viana",
            "David I. Warton",
            "Damaris Zurell",
            "Douglas W. Yu"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "New technologies for acquiring biological information such as eDNA, acoustic or optical sensors, make it possible to generate spatial community observations at unprecedented scales. The potential of these novel community data to standardize community observations at high spatial, temporal, and taxonomic resolution and at large spatial scale ('many rows and many columns') has been widely discussed, but so far, there has been little integration of these data with ecological models and theory. Here, we review these developments and highlight emerging solutions, focusing on statistical methods for analyzing novel community data, in particular joint species distribution models; the new ecological questions that can be answered with these data; and the potential implications of these developments for policy and conservation.",
        "comments": "Journal ref:        Trends in Ecology & Evolution, 2024",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.10860"
    },
    {
        "doc_id": 127,
        "title": "Exploring the role of structure in a time constrained decision task",
        "authors": [
            "Naomi Chaix-Eichel",
            "Gautham Venugopal",
            "Thomas Boraud",
            "Nicolas P. Rougier"
        ],
        "subjects": [
            "Neural and Evolutionary Computing",
            "Neurons and Cognition"
        ],
        "abstract": "The structure of the basal ganglia is remarkably similar across a number of species (often described in terms of direct, indirect and hyperdirect pathways) and is deeply involved in decision making and action selection. In this article, we are interested in exploring the role of structure when solving a decision task while avoiding to make any strong assumption regarding the actual structure. To do so, we exploit the echo state network paradigm that allows to solve complex task based on a random architecture. Considering a temporal decision task, the question is whether a specific structure allows for better performance and if so, whether this structure shares some similarity with the basal ganglia. Our results highlight the advantage of having a slow (direct) and a fast (hyperdirect) pathway that allows to deal with late information during a decision making task.",
        "comments": " ",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.10849"
    },
    {
        "doc_id": 128,
        "title": "Neural Population Decoding and Imbalanced Multi-Omic Datasets For Cancer Subtype Diagnosis",
        "authors": [
            "Charles Theodore Kent",
            "Leila Bagheriye",
            "Johan Kwisthout"
        ],
        "subjects": [
            "Neural and Evolutionary Computing",
            "Machine Learning",
            "Genomics",
            "Quantitative Methods",
            "Applications"
        ],
        "abstract": "Recent strides in the field of neural computation has seen the adoption of Winner Take All (WTA) circuits to facilitate the unification of hierarchical Bayesian inference and spiking neural networks as a neurobiologically plausible model of information processing. Current research commonly validates the performance of these networks via classification tasks, particularly of the MNIST dataset. However, researchers have not yet reached consensus about how best to translate the stochastic responses from these networks into discrete decisions, a process known as population decoding. Despite being an often underexamined part of SNNs, in this work we show that population decoding has a significanct impact on the classification performance of WTA networks. For this purpose, we apply a WTA network to the problem of cancer subtype diagnosis from multi omic data, using datasets from The Cancer Genome Atlas (TCGA). In doing so we utilise a novel implementation of gene similarity networks, a feature encoding technique based on Kohoens self organising map algorithm. We further show that the impact of selecting certain population decoding methods is amplified when facing imbalanced datasets.",
        "comments": "This paper has been accepted in BIOINFORMATICS 2024 (BIOSTEC 2024)",
        "date": "2024-01-06",
        "pdf_url": "https://arxiv.org/pdf/2401.10844"
    },
    {
        "doc_id": 129,
        "title": "DeepRLI: A Multi-objective Framework for Universal Protein--Ligand Interaction Prediction",
        "authors": [
            "Haoyu Lin",
            "Shiwei Wang",
            "Jintao Zhu",
            "Yibo Li",
            "Jianfeng Pei",
            "Luhua Lai"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "Protein (receptor)--ligand interaction prediction is a critical component in computer-aided drug design, significantly influencing molecular docking and virtual screening processes. Despite the development of numerous scoring functions in recent years, particularly those employing machine learning, accurately and efficiently predicting binding affinities for protein--ligand complexes remains a formidable challenge. Most contemporary methods are tailored for specific tasks, such as binding affinity prediction, binding pose prediction, or virtual screening, often failing to encompass all aspects. In this study, we put forward DeepRLI, a novel protein--ligand interaction prediction architecture. It encodes each protein--ligand complex into a fully connected graph, retaining the integrity of the topological and spatial structure, and leverages the improved graph transformer layers with cosine envelope as the central module of the neural network, thus exhibiting superior scoring power. In order to equip the model to generalize to conformations beyond the confines of crystal structures and to adapt to molecular docking and virtual screening tasks, we propose a multi-objective strategy, that is, the model outputs three scores for scoring and ranking, docking, and screening, and the training process optimizes these three objectives simultaneously. For the latter two objectives, we augment the dataset through a docking procedure, incorporate suitable physics-informed blocks and employ an effective contrastive learning approach. Eventually, our model manifests a balanced performance across scoring, ranking, docking, and screening, thereby demonstrating its ability to handle a range of tasks. Overall, this research contributes a multi-objective framework for universal protein--ligand interaction prediction, augmenting the landscape of structure-based drug design.",
        "comments": " ",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.10806"
    },
    {
        "doc_id": 130,
        "title": "FIMBA: Evaluating the Robustness of AI in Genomics via Feature Importance Adversarial Attacks",
        "authors": [
            "Heorhii Skovorodnikov",
            "Hoda Alkhzaimi"
        ],
        "subjects": [
            "Machine Learning",
            "Cryptography and Security",
            "Genomics"
        ],
        "abstract": "With the steady rise of the use of AI in bio-technical applications and the widespread adoption of genomics sequencing, an increasing amount of AI-based algorithms and tools is entering the research and production stage affecting critical decision-making streams like drug discovery and clinical outcomes. This paper demonstrates the vulnerability of AI models often utilized downstream tasks on recognized public genomics datasets. We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance. Further, we enhance our approach by generating poisoned data using a variational autoencoder-based model. Our empirical findings unequivocally demonstrate a decline in model performance, underscored by diminished accuracy and an upswing in false positives and false negatives. Furthermore, we analyze the resulting adversarial samples via spectral analysis yielding conclusions for countermeasures against such attacks.",
        "comments": "15 pages, core code available at: https://github.com/HeorhiiS/fimba-attack",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.10657"
    },
    {
        "doc_id": 131,
        "title": "Exact analytical algorithm for solvent accessible surface area and derivatives in implicit solvent molecular simulations on GPUs",
        "authors": [
            "Xin Cao",
            "Michelle H. Hummel",
            "Yuzhang Wang",
            "Carlos Simmerling",
            "Evangelos A. Coutsias"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "In this paper, we present dSASA (differentiable SASA), an exact geometric method to calculate solvent accessible surface area (SASA) analytically along with atomic derivatives on GPUs. The atoms in a molecule are first assigned to tetrahedra in groups of four atoms by Delaunay tetrahedrization adapted for efficient GPU implementation and the SASA values for atoms and molecules are calculated based on the tetrahedrization information and inclusion-exclusion method. The SASA values from the numerical icosahedral-based method can be reproduced with more than 98% accuracy for both proteins and RNAs. Having been implemented on GPUs and incorporated into the software Amber, we can apply dSASA to implicit solvent molecular dynamics simulations with inclusion of this nonpolar term. The current GPU version of GB/SA simulations has been accelerated up to nearly 20-fold compared to the CPU version and it outperforms LCPO as the system size increases. The performance and importance of the nonpolar part in implicit solvent modeling are demonstrated in GB/SA simulations of proteins and accurate SASA calculation of nucleic acids.",
        "comments": " ",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10462"
    },
    {
        "doc_id": 132,
        "title": "Ecosystem models cannot predict the consequences of conservation decisions",
        "authors": [
            "Larissa Lubiana Botelho",
            "Cailan Jeynes-Smith",
            "Sarah Vollert",
            "Michael Bode"
        ],
        "subjects": [
            "Populations and Evolution",
            "Quantitative Methods"
        ],
        "abstract": "Ecosystem models are often used to predict the consequences of management decisions in applied ecology, including fisheries management and threatened species conservation. These models are high-dimensional, parameter-rich, and nonlinear, yet limited data is available to calibrate them, and they are rarely tested or validated. Consequently, the accuracy of their forecasts, and their utility as decision-support tools is a matter of debate. In this paper, we calibrate ecosystem models to time-series data from 110 different experimental microcosm ecosystems, each containing between three and five interacting species. We then assess how often these calibrated models offer accurate and useful predictions about how the ecosystem will respond to a set of standard management interventions. Our results show that for each timeseries dataset, a large number of very different parameter sets offer equivalent, good fits. However, these calibrated ecosystem models have poor predictive accuracy when forecasting future dynamics and offer ambiguous predictions about how species in the ecosystem will respond to management interventions. Closer inspection reveals that the ecosystem models fail because calibration cannot determine the types of interactions that occur within the ecosystem. Our findings call into question claims that ecosystem modelling can support applied ecological decision-making when they are calibrated against real-world datasets.",
        "comments": "23 pages (main text + supplementary material) 9 figures (main text + supplementary material)",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10439"
    },
    {
        "doc_id": 133,
        "title": "Diffusion of intrinsically disordered proteins within viscoelastic membraneless droplets",
        "authors": [
            "Fuga Watanabe",
            "Takuma Akimoto",
            "Robert B. Best",
            "Kresten Lindorff-Larsen",
            "Ralf Metzler",
            "Eiji Yamamoto"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Statistical Mechanics",
            "Biological Physics",
            "Computational Physics",
            "Biomolecules"
        ],
        "abstract": "In living cells, intrinsically disordered proteins (IDPs), such as FUS and DDX4, undergo phase separation, forming biomolecular condensates. Using molecular dynamics simulations, we investigate their behavior in their respective homogenous droplets. We find that the proteins exhibit transient subdiffusion due to the viscoelastic nature and confinement effects in the droplets. The conformation and the instantaneous diffusivity of the proteins significantly vary between the interior and the interface of the droplet, resulting in non-Gaussianity in the displacement distributions. This study highlights key aspects of IDP behavior in biomolecular condensates.",
        "comments": " ",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10438"
    },
    {
        "doc_id": 134,
        "title": "Exploring General Intelligence via Gated Graph Transformer in Functional Connectivity Studies",
        "authors": [
            "Gang Qu",
            "Anton Orlichenko",
            "Junqi Wang",
            "Gemeng Zhang",
            "Li Xiao",
            "Aiying Zhang",
            "Zhengming Ding",
            "Yu-Ping Wang"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Artificial Intelligence"
        ],
        "abstract": "Functional connectivity (FC) as derived from fMRI has emerged as a pivotal tool in elucidating the intricacies of various psychiatric disorders and delineating the neural pathways that underpin cognitive and behavioral dynamics inherent to the human brain. While Graph Neural Networks (GNNs) offer a structured approach to represent neuroimaging data, they are limited by their need for a predefined graph structure to depict associations between brain regions, a detail not solely provided by FCs. To bridge this gap, we introduce the Gated Graph Transformer (GGT) framework, designed to predict cognitive metrics based on FCs. Empirical validation on the Philadelphia Neurodevelopmental Cohort (PNC) underscores the superior predictive prowess of our model, further accentuating its potential in identifying pivotal neural connectivities that correlate with human cognitive processes.",
        "comments": " ",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10348"
    },
    {
        "doc_id": 135,
        "title": "DrugAssist: A Large Language Model for Molecule Optimization",
        "authors": [
            "Geyan Ye",
            "Xibao Cai",
            "Houtim Lai",
            "Xing Wang",
            "Junhong Huang",
            "Longyue Wang",
            "Wei Liu",
            "Xiangxiang Zeng"
        ],
        "subjects": [
            "Quantitative Methods",
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning"
        ],
        "abstract": "Recently, the impressive performance of large language models (LLMs) on a wide range of tasks has attracted an increasing number of attempts to apply LLMs in drug discovery. However, molecule optimization, a critical task in the drug discovery pipeline, is currently an area that has seen little involvement from LLMs. Most of existing approaches focus solely on capturing the underlying patterns in chemical structures provided by the data, without taking advantage of expert feedback. These non-interactive approaches overlook the fact that the drug discovery process is actually one that requires the integration of expert experience and iterative refinement. To address this gap, we propose DrugAssist, an interactive molecule optimization model which performs optimization through human-machine dialogue by leveraging LLM's strong interactivity and generalizability. DrugAssist has achieved leading results in both single and multiple property optimization, simultaneously showcasing immense potential in transferability and iterative optimization. In addition, we publicly release a large instruction-based dataset called MolOpt-Instructions for fine-tuning language models on molecule optimization tasks. We have made our code and data publicly available at https://github.com/blazerye/DrugAssist, which we hope to pave the way for future research in LLMs' application for drug discovery.",
        "comments": "Geyan Ye and Xibao Cai are equal contributors; Longyue Wang is corresponding author",
        "date": "2023-12-28",
        "pdf_url": "https://arxiv.org/pdf/2401.10334"
    },
    {
        "doc_id": 136,
        "title": "Fine scale depth regulation of invertebrate larvae around coastal fronts",
        "authors": [
            "Nicolas Weidberg",
            "Wayne Goschen",
            "Jennifer M. Jackson",
            "Paula Pattrick",
            "Christopher D. McQuaid",
            "Francesca Porri"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "Vertical migrations of zooplankters have been widely described, but their active movements through shallow, highly dynamic water columns within the inner shelf may be more complex and difficult to characterize. In this study, invertebrate larvae, currents, and hydrographic variables were sampled at different depths during and after the presence of fronts on three different cruises off the southern coast of South Africa. Internal wave dynamics were observed in the hydrographic data set but also through satellite imagery, although strong surface convergent currents were absent and thermal stratification was weak. During the first two cruises, fronts were more conspicuous and they preceded strong onshore currents at depth which developed with the rising tide. Vertical distributions of larvae changed accordingly, with higher abundances at these deep layers once the front disappeared. The third cruise was carried out during slack tides, the front was not conspicuous, deep strong onshore currents did not occur afterward and larval distributions did not change consistently through time. Overall, the vertical distributions of many larval taxa matched the vertical profiles of shoreward currents and multivariate analyses revealed that these flows structured the larval community, which was neither influenced by temperature nor chlorophyll. Thus, the ability to regulate active vertical positioning may enhance shoreward advection and determine nearshore larval distributions.",
        "comments": "Journal ref:        Limnology and Oceanography. 64 - 2, pp. 785 - 802, 2019",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10303"
    },
    {
        "doc_id": 137,
        "title": "Forecasting dengue outbreaks with uncertainty using seasonal weather patterns",
        "authors": [
            "Piumi Chathurangika",
            "Sanjeewa Perera",
            "Kushani De Silva"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "Dengue is a vector-borne disease transmitted to humans by vectors of genus Aedes and is a global threat with health, social, and economic impact in many of the tropical countries including Sri Lanka. The virus transmission is significantly impacted by environmental conditions, with a notable contribution from elevated per-capita vector density. These conditions are dynamic in nature and specially having the tropical climate, Sri Lanka experiences seasonal weather patterns dominated by monsoons. In this work, we investigate the dynamic influence of environmental conditions on dengue emergence in Colombo district where dengue is extremely prevalent in Sri Lanka. A novel approach leveraging the Markov chain Monte Carlo simulations has been employed to identify seasonal patterns of dengue disease emergence, utilizing the dynamics of weather patterns governing in the region. The newly developed algorithm allows us to estimate the timing of dengue outbreaks with uncertainty, enabling accurate forecasts of upcoming disease emergence patterns for better preparedness.",
        "comments": " ",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.10295"
    },
    {
        "doc_id": 138,
        "title": "Mechanisms of nearshore retention and offshore export of mussel larvae over the Agulhas Bank",
        "authors": [
            "Nicolas Weidberg",
            "Francesca Porri",
            "Charles von der Meden",
            "Jennifer M. Jackson",
            "Wayne Goschen",
            "Christopher McQuaid"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "Ecological connectivity is critical for population dynamics but in many benthic species it is complicated by a planktonic larval phase, whose dispersal remains poorly understood. Using a plankton pump, we examine the distribution of intertidal mussel larvae along three axes: alongshore, cross-shelf and by depth during a large scale (600 km) cruise over the Agulhas Bank off southern Africa in August/September 2010. As a general pattern, higher veliger abundances were found close to the coast. Our analyses of the nearshore flow, estimated from ADCP data and the vertical distribution of larvae, show that onshore larval retention may be mediated by active vertical swimming through the water column guided by light and wind-induced turbulence. A massive offshore export of larvae off St Francis Bay was, however, observed during an Agulhas Current meander which influenced inner shelf waters. We hypothesize that, by increasing and homogenizing flow, the Agulhas Current may erase the effects of larval vertical positioning on onshore retention and transport larvae offshore. Our study highlights the need to integrate the effects of complex, region-specific physical dynamics with the swimming behaviour of larvae in order to explain their spatial distribution, population connectivity and the consequences for population dynamics.",
        "comments": "Journal ref:        Journal of Plankton Research. 37 - 6, pp. 1166 - 1180. Oxford Journals, 11/2015",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.10292"
    },
    {
        "doc_id": 139,
        "title": "Analyzing Brain Activity During Learning Tasks with EEG and Machine Learning",
        "authors": [
            "Ryan Cho",
            "Mobasshira Zaman",
            "Kyu Taek Cho",
            "Jaejin Hwang"
        ],
        "subjects": [
            "Signal Processing",
            "Machine Learning",
            "Neurons and Cognition"
        ],
        "abstract": "This study aimed to analyze brain activity during various STEM activities, exploring the feasibility of classifying between different tasks. EEG brain data from twenty subjects engaged in five cognitive tasks were collected and segmented into 4-second clips. Power spectral densities of brain frequency waves were then analyzed. Testing different k-intervals with XGBoost, Random Forest, and Bagging Classifier revealed that Random Forest performed best, achieving a testing accuracy of 91.07% at an interval size of two. When utilizing all four EEG channels, cognitive flexibility was most recognizable. Task-specific classification accuracy showed the right frontal lobe excelled in mathematical processing and planning, the left frontal lobe in cognitive flexibility and mental flexibility, and the left temporoparietal lobe in connections. Notably, numerous connections between frontal and temporoparietal lobes were observed during STEM activities. This study contributes to a deeper understanding of implementing machine learning in analyzing brain activity and sheds light on the brain's mechanisms.",
        "comments": "20 pages, 7 figures",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.10285"
    },
    {
        "doc_id": 140,
        "title": "EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model",
        "authors": [
            "Yuqi Chen",
            "Kan Ren",
            "Kaitao Song",
            "Yansen Wang",
            "Yifan Wang",
            "Dongsheng Li",
            "Lili Qiu"
        ],
        "subjects": [
            "Signal Processing",
            "Artificial Intelligence",
            "Machine Learning",
            "Multimedia",
            "Neurons and Cognition"
        ],
        "abstract": "Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance on various downstream tasks but also provide interpretable outcomes of the useful patterns within the data. To validate the effectiveness of our model, we extensively evaluate it on various downstream tasks and assess the performance under different transfer settings. Furthermore, we demonstrate how the learned model exhibits transferable anomaly detection performance and provides valuable interpretability of the acquired patterns via self-supervised learning.",
        "comments": "A preprint version of an ongoing work",
        "date": "2024-01-11",
        "pdf_url": "https://arxiv.org/pdf/2401.10278"
    },
    {
        "doc_id": 141,
        "title": "Evolving Diploid Boolean and Multi-Valued Gene Networks",
        "authors": [
            "Larry Bull"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "Boolean networks have been widely used to explore aspects of gene regulation, traditionally with a single network. A modified form of the model to explore the effects of increasing the number of gene states has also recently been introduced. In this paper, these discrete dynamical networks are evolved as diploids within rugged fitness landscapes to explore their behaviour. Results suggest the general properties of haploid networks in similar circumstances remain for diploids. The previously proposed inherent fitness landscape smoothing properties of eukaryotic sex are shown to be exhibited in these dynamical systems, as is their propensity to change in size based upon the characteristics of the network and fitness landscape.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2302.01694",
        "date": "2023-11-19",
        "pdf_url": "https://arxiv.org/pdf/2401.10237"
    },
    {
        "doc_id": 142,
        "title": "Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products",
        "authors": [
            "Shengjie Luo",
            "Tianlang Chen",
            "Aditi S. Krishnapriyan"
        ],
        "subjects": [
            "Machine Learning",
            "Materials Science",
            "Group Theory",
            "Chemical Physics",
            "Biomolecules"
        ],
        "abstract": "Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations increases significantly as higher-order tensors are used. In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps. We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics. Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics. This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis. Consequently, the multiplication between spherical functions represented by a 2D Fourier basis can be efficiently computed via the convolution theorem and Fast Fourier Transforms. This transformation reduces the complexity of full tensor products of irreps from $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^3)$, where $L$ is the max degree of irreps. Leveraging this approach, we introduce the Gaunt Tensor Product, which serves as a new method to construct efficient equivariant operations across different model architectures. Our experiments on the Open Catalyst Project and 3BPA datasets demonstrate both the increased efficiency and improved performance of our approach.",
        "comments": "36 pages; ICLR 2024 (Spotlight Presentation); Code: https://github.com/lsj2408/Gaunt-Tensor-Product",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10216"
    },
    {
        "doc_id": 143,
        "title": "Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation",
        "authors": [
            "Zhengyi Li",
            "Menglu Li",
            "Lida Zhu",
            "Wen Zhang"
        ],
        "subjects": [
            "Quantitative Methods",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context sequence information, and motif generated by aligning all context sequences of PTM sites assists the prediction. Extensive experiments on three datasets show that PTM-CMGMS outperforms the state-of-the-art methods.",
        "comments": " ",
        "date": "2024-01-04",
        "pdf_url": "https://arxiv.org/pdf/2401.10211"
    },
    {
        "doc_id": 144,
        "title": "Exploiting Hierarchical Interactions for Protein Surface Learning",
        "authors": [
            "Yiqun Lin",
            "Liang Pan",
            "Yi Li",
            "Ziwei Liu",
            "Xiaomeng Li"
        ],
        "subjects": [
            "Biomolecules",
            "Machine Learning"
        ],
        "abstract": "Predicting interactions between proteins is one of the most important yet challenging problems in structural bioinformatics. Intrinsically, potential function sites in protein surfaces are determined by both geometric and chemical features. However, existing works only consider handcrafted or individually learned chemical features from the atom type and extract geometric features independently. Here, we identify two key properties of effective protein surface learning: 1) relationship among atoms: atoms are linked with each other by covalent bonds to form biomolecules instead of appearing alone, leading to the significance of modeling the relationship among atoms in chemical feature learning. 2) hierarchical feature interaction: the neighboring residue effect validates the significance of hierarchical feature interaction among atoms and between surface points and atoms (or residues). In this paper, we present a principled framework based on deep learning techniques, namely Hierarchical Chemical and Geometric Feature Interaction Network (HCGNet), for protein surface analysis by bridging chemical and geometric features with hierarchical interactions. Extensive experiments demonstrate that our method outperforms the prior state-of-the-art method by 2.3% in site prediction task and 3.2% in interaction matching task, respectively. Our code is available at https://github.com/xmed-lab/HCGNet.",
        "comments": "Accepted to J-BHI",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.10144"
    },
    {
        "doc_id": 145,
        "title": "Correlating fluorescence microscopy, optical and magnetic tweezers to study single chiral biopolymers, tested on DNA plectoneme formation dynamics",
        "authors": [
            "Jack W Shepherd",
            "Sebastien Guilbaud",
            "Zhaokun Zhou",
            "Jamieson Howard",
            "Matthew Burman",
            "Charley Schaefer",
            "Adam Kerrigan",
            "Clare Steele-King",
            "Agnes Noy",
            "Mark C Leake"
        ],
        "subjects": [
            "Biological Physics",
            "Biomolecules"
        ],
        "abstract": "Biopolymer topology is critical for determining interactions inside cell environments, exemplified by DNA where its response to mechanical perturbation is as important as biochemical properties to its cellular roles. The dynamic structures of chiral biopolymers exhibit complex dependence with extension and torsion, however the physical mechanisms underpinning the emergence of structural motifs upon physiological twisting and stretching are poorly understood due to technological limitations in correlating force, torque and spatial localization information. We present COMBI-Tweez (Combined Optical and Magnetic BIomolecule TWEEZers), a transformative tool that overcomes these challenges by integrating optical trapping, time-resolved electromagnetic tweezers, and fluorescence microscopy, demonstrated on single DNA molecules, that can controllably form and visualise higher order structural motifs including plectonemes. This technology combined with cutting-edge MD simulations provides quantitative insight into complex dynamic structures relevant to DNA cellular processes and can be adapted to study a range of filamentous biopolymers.",
        "comments": " ",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10087"
    },
    {
        "doc_id": 146,
        "title": "GPU Acceleration of a Conjugate Exponential Model for Cancer Tissue Heterogeneity",
        "authors": [
            "Anik Chaudhuri",
            "Anwoy Mohanty",
            "Manoranjan Satpathy"
        ],
        "subjects": [
            "Distributed, Parallel, and Cluster Computing",
            "Quantitative Methods"
        ],
        "abstract": "Heterogeneity in the cell population of cancer tissues poses many challenges in cancer diagnosis and treatment. Studying the heterogeneity in cell populations from gene expression measurement data in the context of cancer research is a problem of paramount importance. In addition, reducing the computation time of the algorithms that deal with high volumes of data has its obvious merits. Parallelizable models using Markov chain Monte Carlo methods are typically slow. This paper shows a novel, computationally efficient, and parallelizable model to analyze heterogeneity in cancer tissues using GPUs. Because our model is parallelizable, the input data size does not affect the computation time much, provided the hardware resources are not exhausted. Our model uses qPCR (quantitative polymerase chain reaction) gene expression measurements to study heterogeneity in cancer tissue. We compute the cell proportion breakup by accelerating variational methods on a GPU. We test this model on synthetic and real-world gene expression data collected from fibroblasts and compare the performance of our algorithm with those of MCMC and Expectation Maximization. Our new model is computationally less complex and faster than existing Bayesian models for cancer tissue heterogeneity.",
        "comments": " ",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10068"
    },
    {
        "doc_id": 147,
        "title": "Cardiac Digital Twin Pipeline for Virtual Therapy Evaluation",
        "authors": [
            "Julia Camps",
            "Zhinuo Jenny Wang",
            "Ruben Doste",
            "Maxx Holmes",
            "Brodie Lawson",
            "Jakub Tomek",
            "Kevin Burrage",
            "Alfonso Bueno-Orovio",
            "Blanca Rodriguez"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Tissues and Organs"
        ],
        "abstract": "Cardiac digital twins are computational tools capturing key functional and anatomical characteristics of patient hearts for investigating disease phenotypes and predicting responses to therapy. When paired with large-scale computational resources and large clinical datasets, digital twin technology can enable virtual clinical trials on virtual cohorts to fast-track therapy development. Here, we present an automated pipeline for personalising ventricular anatomy and electrophysiological function based on routinely acquired cardiac magnetic resonance (CMR) imaging data and the standard 12-lead electrocardiogram (ECG). Using CMR-based anatomical models, a sequential Monte-Carlo approximate Bayesian computational inference method is extended to infer electrical activation and repolarisation characteristics from the ECG. Fast simulations are conducted with a reaction-Eikonal model, including the Purkinje network and biophysically-detailed subcellular ionic current dynamics for repolarisation. For each patient, parameter uncertainty is represented by inferring a population of ventricular models rather than a single one, which means that parameter uncertainty can be propagated to therapy evaluation. Furthermore, we have developed techniques for translating from reaction-Eikonal to monodomain simulations, which allows more realistic simulations of cardiac electrophysiology. The pipeline is demonstrated in a healthy female subject, where our inferred reaction-Eikonal models reproduced the patient's ECG with a Pearson's correlation coefficient of 0.93, and the translated monodomain simulations have a correlation coefficient of 0.89. We then apply the effect of Dofetilide to the monodomain population of models for this subject and show dose-dependent QT and T-peak to T-end prolongations that are in keeping with large population drug response data.",
        "comments": " ",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10029"
    },
    {
        "doc_id": 148,
        "title": "An optimization-based equilibrium measure describes non-equilibrium steady state dynamics: application to edge of chaos",
        "authors": [
            "Junbin Qiu",
            "Haiping Huang"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Statistical Mechanics",
            "Neural and Evolutionary Computing"
        ],
        "abstract": "Understanding neural dynamics is a central topic in machine learning, non-linear physics and neuroscience. However, the dynamics is non-linear, stochastic and particularly non-gradient, i.e., the driving force can not be written as gradient of a potential. These features make analytic studies very challenging. The common tool is to use path integral approach or dynamical mean-field theory, but the drawback is one has to solve the integro-differential or dynamical mean-field equations, which is computationally expensive and has no closed form solutions in general. From the aspect of associated Fokker-Planck equation, the steady state solution is generally unknown. Here, we treat searching for the steady state as an optimization problem, and construct an approximate potential closely related to the speed of the dynamics, and find that searching for the ground state of this potential is equivalent to running a stochastic gradient dynamics. The resultant stationary state follows exactly the canonical Boltzmann measure. Within this framework, the quenched disorder intrinsic in the neural networks can be averaged out by applying the replica method. Our theory reproduces the well-known result of edge-of-chaos, and further the order parameters characterizing the continuous transition are derived, and different scaling behavior with respect to inverse temperature in both sides of the transition is also revealed. Our method opens the door to analytically study the steady state landscape of the deterministic or stochastic high dimensional dynamics.",
        "comments": "16 pages, 7 figures",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.10009"
    },
    {
        "doc_id": 149,
        "title": "Artificial Intelligence-based algorithms in medical image scan seg-mentation and intelligent visual-content generation -- a concise overview",
        "authors": [
            "Zofia Rudnicka",
            "Janusz Szczepanski",
            "Agnieszka Pregowska"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Recently, Artificial Intelligence (AI)-based algorithms have revolutionized the medical image segmentation processes. Thus, the precise segmentation of organs and their lesions may contribute to an efficient diagnostics process and a more effective selection of targeted therapies as well as increasing the effectiveness of the training process. In this context, AI may contribute to the automatization of the image scan segmentation process and increase the quality of the resulting 3D objects, which may lead to the generation of more realistic virtual objects. In this paper, we focus on the AI-based solutions applied in the medical image scan segmentation, and intelligent visual-content generation, i.e. computer-generated three-dimensional (3D) images in the context of Extended Reality (XR). We consider different types of neural networks used with a special emphasis on the learning rules applied, taking into account algorithm accuracy and performance, as well as open data availability. This paper attempts to summarize the current development of AI-based segmentation methods in medical imaging and intelligent visual content generation that are applied in XR. It concludes also with possible developments and open challenges in AI application in Extended Reality-based solutions. Finally, the future lines of research and development directions of Artificial Intelligence applications both in medical image segmentation and Extended Reality-based medical solutions are discussed",
        "comments": " ",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.09857"
    },
    {
        "doc_id": 150,
        "title": "FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction",
        "authors": [
            "Alexander Telepov",
            "Artem Tsypin",
            "Kuzma Khrabrov",
            "Sergey Yakukhnov",
            "Pavel Strashnov",
            "Petr Zhilyaev",
            "Egor Rumiantsev",
            "Daniel Ezhov",
            "Manvel Avetisian",
            "Olga Popova",
            "Artur Kadurin"
        ],
        "subjects": [
            "Biomolecules",
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (arXiv:2110.01219). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches.",
        "comments": "37 pages, 10 figures, to be published in TMLR journal (https://www.jmlr.org/tmlr/)",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.09840"
    },
    {
        "doc_id": 151,
        "title": "The impact of Covid-19 vaccination in Aotearoa New Zealand: a modelling study",
        "authors": [
            "Samik Datta",
            "Giorgia Vattiato",
            "Oliver J Maclaren",
            "Ning Hua",
            "Andrew Sporle",
            "Michael J Plank"
        ],
        "subjects": [
            "Populations and Evolution",
            "Physics and Society"
        ],
        "abstract": "Aotearoa New Zealand implemented a Covid-19 elimination strategy in 2020 and 2021, which enabled a large majority of the population to be vaccinated before being exposed to the virus. This strategy delivered one of the lowest pandemic mortality rates in the world. However, quantitative estimates of the population-level health benefits of vaccination are lacking. Here, we use a validated mathematical model to investigate counterfactual scenarios with differing levels of vaccine coverage in different age and ethnicity groups. The model builds on earlier research by adding age- and time-dependent case ascertainment, the effect of antiviral medications, improved hospitalisation rate estimates, and the impact of relaxing control measures. The model was used for scenario analysis and policy advice for the New Zealand Government in 2022 and 2023. We compare the number of Covid-19 hospitalisations, deaths, and years of life lost in each counterfactual scenario to a baseline scenario that is fitted to epidemiological data between January 2022 and June 2023. Our results estimate that vaccines saved 6650 (95% credible interval [4424, 10180]) lives, and prevented 74500 [51000, 115400] years of life lost and 45100 [34400, 55600] hospitalisations during this 18-month period. Making the same comparison before the benefit of antiviral medications is accounted for, the estimated number of lives saved by vaccines increases to 7604 [5080, 11942]. Due to inequities in the vaccine rollout, vaccination rates among M\u0101ori were lower than in people of European ethnicity. Our results show that, if vaccination rates had been equitable, an estimated 11-26% of the 292 M\u0101ori Covid-19 deaths that were recorded in this time period could have been prevented. We conclude that Covid-19 vaccination greatly reduced health burden in New Zealand and that equity needs to be a key focus of future vaccination programmes.",
        "comments": " ",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09679"
    },
    {
        "doc_id": 152,
        "title": "Functional Linear Non-Gaussian Acyclic Model for Causal Discovery",
        "authors": [
            "Tian-Le Yang",
            "Kuang-Yao Lee",
            "Kun Zhang",
            "Joe Suzuki"
        ],
        "subjects": [
            "Machine Learning",
            "Statistics Theory",
            "Neurons and Cognition",
            "Methodology"
        ],
        "abstract": "In causal discovery, non-Gaussianity has been used to characterize the complete configuration of a Linear Non-Gaussian Acyclic Model (LiNGAM), encompassing both the causal ordering of variables and their respective connection strengths. However, LiNGAM can only deal with the finite-dimensional case. To expand this concept, we extend the notion of variables to encompass vectors and even functions, leading to the Functional Linear Non-Gaussian Acyclic Model (Func-LiNGAM). Our motivation stems from the desire to identify causal relationships in brain-effective connectivity tasks involving, for example, fMRI and EEG datasets. We demonstrate why the original LiNGAM fails to handle these inherently infinite-dimensional datasets and explain the availability of functional data analysis from both empirical and theoretical perspectives. {We establish theoretical guarantees of the identifiability of the causal relationship among non-Gaussian random vectors and even random functions in infinite-dimensional Hilbert spaces.} To address the issue of sparsity in discrete time points within intrinsic infinite-dimensional functional data, we propose optimizing the coordinates of the vectors using functional principal component analysis. Experimental results on synthetic data verify the ability of the proposed framework to identify causal relationships among multivariate functions using the observed samples. For real data, we focus on analyzing the brain connectivity patterns derived from fMRI data.",
        "comments": " ",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09641"
    },
    {
        "doc_id": 153,
        "title": "Molecular causality in the advent of foundation models",
        "authors": [
            "Sebastian Lobentanzer",
            "Pablo Rodriguez-Mier",
            "Stefan Bauer",
            "Julio Saez-Rodriguez"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "Correlation is not causation. As simple as this widely agreed-upon statement may seem, scientifically defining causality and using it to drive our modern biomedical research is immensely challenging. In this perspective, we attempt to synergise the partly disparate fields of systems biology, causal reasoning, and machine learning, to inform future approaches in the field of systems biology and molecular networks.",
        "comments": "22 pages, 0 figures, 87 references; submitted to MSB",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09558"
    },
    {
        "doc_id": 154,
        "title": "Dimensional Neuroimaging Endophenotypes: Neurobiological Representations of Disease Heterogeneity Through Machine Learning",
        "authors": [
            "Junhao Wen",
            "Mathilde Antoniades",
            "Zhijian Yang",
            "Gyujoon Hwang",
            "Ioanna Skampardoni",
            "Rongguang Wang",
            "Christos Davatzikos"
        ],
        "subjects": [
            "Machine Learning",
            "Image and Video Processing",
            "Quantitative Methods"
        ],
        "abstract": "Machine learning has been increasingly used to obtain individualized neuroimaging signatures for disease diagnosis, prognosis, and response to treatment in neuropsychiatric and neurodegenerative disorders. Therefore, it has contributed to a better understanding of disease heterogeneity by identifying disease subtypes that present significant differences in various brain phenotypic measures. In this review, we first present a systematic literature overview of studies using machine learning and multimodal MRI to unravel disease heterogeneity in various neuropsychiatric and neurodegenerative disorders, including Alzheimer disease, schizophrenia, major depressive disorder, autism spectrum disorder, multiple sclerosis, as well as their potential in transdiagnostic settings. Subsequently, we summarize relevant machine learning methodologies and discuss an emerging paradigm which we call dimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological heterogeneity of neuropsychiatric and neurodegenerative disorders into a low dimensional yet informative, quantitative brain phenotypic representation, serving as a robust intermediate phenotype (i.e., endophenotype) largely reflecting underlying genetics and etiology. Finally, we discuss the potential clinical implications of the current findings and envision future research avenues.",
        "comments": " ",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09517"
    },
    {
        "doc_id": 155,
        "title": "Is the Emergence of Life an Expected Phase Transition in the Evolving Universe?",
        "authors": [
            "Stuart Kauffman",
            "Andrea Roli"
        ],
        "subjects": [
            "Populations and Evolution",
            "Biological Physics"
        ],
        "abstract": "We propose a novel definition of life in terms of which its emergence in the universe is expected, and its ever-creative open-ended evolution is entailed by no law. Living organisms are Kantian Wholes that achieve Catalytic Closure, Constraint Closure, and Spatial Closure. We here unite for the first time two established mathematical theories, namely Collectively Autocatalytic Sets and the Theory of the Adjacent Possible. The former establishes that a first-order phase transition to molecular reproduction is expected in the chemical evolution of the universe where the diversity and complexity of molecules increases; the latter posits that, under loose hypotheses, if the system starts with a small number of beginning molecules, each of which can combine with copies of itself or other molecules to make new molecules, over time the number of kinds of molecules increases slowly but then explodes upward hyperbolically. Together these theories imply that life is expected as a phase transition in the evolving universe. The familiar distinction between software and hardware loses its meaning in living cells. We propose new ways to study the phylogeny of metabolisms, new astronomical ways to search for life on exoplanets, new experiments to seek the emergence of the most rudimentary life, and the hint of a coherent testable pathway to prokaryotes with template replication and coding.",
        "comments": " ",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09514"
    },
    {
        "doc_id": 156,
        "title": "Role of Upwelling on Larval Dispersal and Productivity of Gooseneck Barnacle Populations in the Cantabrian Sea: Management Implications",
        "authors": [
            "Antonella Rivera",
            "Nicolas Weidberg",
            "Antonio F. Pardi\u00f1as",
            "Ricardo Gonzalez-Gil",
            "Luc\u0131a Garc\u0131a- Florez",
            "Jose Luis Acu\u00f1a"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "The effect of coastal upwelling on the recruitment and connectivity of coastal marine populations has rarely been characterized to a level of detail to be included into sound fishery management strategies. The gooseneck barnacle (Pollicipes pollicipes) fishery at the Cantabrian Coast (Northern Spain) is located at the fringes of the NW Spanish Upwelling system. This fishery is being co-managed through a fine-scale, interspersed set of protected rocks where each rock receives a distinct level of protection. Such interspersion is potentially beneficial, but the extent to which such spacing is consistent with mean larval dispersal distances is as yet unknown. We have simulated the spread of gooseneck barnacle larvae in the Central Cantabrian Coast using a high-resolution time-series of current profiles measured at a nearshore location. During a year of high upwelling activity (2009), theoretical recruitment success was 94% with peak recruitment predicted 56 km west of the emission point. However, for a year of low upwelling activity (2011) theoretical recruitment success dropped to 15.4% and peak recruitment was expected 13 km east of the emission point. This is consistent with a positive correlation between catch rates and the Integrated Upwelling Index, using a 4-year lag to allow recruits to reach commercial size. Furthermore, a net long-term westward larval transport was estimated by means of mitochondrial cytochrome c oxidase subunit I (COI) sequences for five populations in the Cantabrian Sea. Our results call into question the role of long distance dispersal, driven by the mesoscale processes in the area, in gooseneck barnacle populations and point to the prevalent role of small-scale, asymmetric connectivity more consistent with the typical scale of the co-management process in this fishery.",
        "comments": " ",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09513"
    },
    {
        "doc_id": 157,
        "title": "A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation",
        "authors": [
            "Nianzu Yang",
            "Kaipeng Zeng",
            "Haotian Lu",
            "Yexin Wu",
            "Zexin Yuan",
            "Shengdian Jiang",
            "Jiaxiang Wu",
            "Yimin Wang",
            "Junchi Yan"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Neuronal morphology is essential for studying brain functioning and understanding neurodegenerative disorders. As the acquiring of real-world morphology data is expensive, computational approaches especially learning-based ones e.g. MorphVAE for morphology generation were recently studied, which are often conducted in a way of randomly augmenting a given authentic morphology to achieve plausibility. Under such a setting, this paper proposes \\textbf{MorphGrower} which aims to generate more plausible morphology samples by mimicking the natural growth mechanism instead of a one-shot treatment as done in MorphVAE. Specifically, MorphGrower generates morphologies layer by layer synchronously and chooses a pair of sibling branches as the basic generation block, and the generation of each layer is conditioned on the morphological structure of previous layers and then generate morphologies via a conditional variational autoencoder with spherical latent space. Extensive experimental results on four real-world datasets demonstrate that MorphGrower outperforms MorphVAE by a notable margin. Our code will be publicly available to facilitate future research.",
        "comments": " ",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09500"
    },
    {
        "doc_id": 158,
        "title": "Gene-associated Disease Discovery Powered by Large Language Models",
        "authors": [
            "Jiayu Chang",
            "Shiyu Wang",
            "Chen Ling",
            "Zhaohui Qin",
            "Liang Zhao"
        ],
        "subjects": [
            "Quantitative Methods",
            "Information Retrieval"
        ],
        "abstract": "The intricate relationship between genetic variation and human diseases has been a focal point of medical research, evidenced by the identification of risk genes regarding specific diseases. The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment. To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes. This framework aims to automate the labor-intensive process of sifting through medical literature for evidence linking genetic variations to diseases, thereby enhancing the efficiency of disease identification. Our approach involves using LLMs to conduct literature searches, summarize relevant findings, and pinpoint diseases related to specific genes. This paper details the development and application of our LLM-powered framework, demonstrating its potential in streamlining the complex process of literature retrieval and summarization to identify diseases associated with specific genetic variations.",
        "comments": "This is the official paper accepted by AAAI 2024 Workshop on Large Language Models for Biological Discoveries",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.09490"
    },
    {
        "doc_id": 159,
        "title": "The Interplay Between Logical Phenomena and the Cognitive System of the Mind",
        "authors": [
            "Kazem Haghnejad Azar"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "In this article, we employ mathematical concepts as a tool to examine the phenomenon of consciousness experience and logical phenomena. Through our investigation, we aim to demonstrate that our experiences, while not confined to limitations, cannot be neatly encapsulated within a singular collection. Our conscious experience emerges as a result of the developmental and augmentative trajectory of our cognitive system. As our cognitive abilities undergo refinement and advancement, our capacity for logical thinking likewise evolves, thereby manifesting a heightened level of conscious experience. The primary objective of this article is to embark upon a profound exploration of the concept of logical experience, delving into the intricate process by which these experiences are derived from our mind.",
        "comments": " ",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.09465"
    },
    {
        "doc_id": 160,
        "title": "Diffusion-Driven Generative Framework for Molecular Conformation Prediction",
        "authors": [
            "Bobin Yang",
            "Jie Deng",
            "Zhenghan Chen",
            "Ruoxue Wu"
        ],
        "subjects": [
            "Biomolecules",
            "Artificial Intelligence",
            "Machine Learning",
            "Chemical Physics"
        ],
        "abstract": "The task of deducing three-dimensional molecular configurations from their two-dimensional graph representations holds paramount importance in the fields of computational chemistry and pharmaceutical development. The rapid advancement of machine learning, particularly within the domain of deep generative networks, has revolutionized the precision of predictive modeling in this context. Traditional approaches often adopt a two-step strategy: initially estimating interatomic distances and subsequently refining the spatial molecular structure by solving a distance geometry problem. However, this sequential approach occasionally falls short in accurately capturing the intricacies of local atomic arrangements, thereby compromising the fidelity of the resulting structural models. Addressing these limitations, this research introduces a cutting-edge generative framework named \\method{}. This framework is grounded in the principles of diffusion observed in classical non-equilibrium thermodynamics. \\method{} views atoms as discrete entities and excels in guiding the reversal of diffusion, transforming a distribution of stochastic noise back into coherent molecular structures through a process akin to a Markov chain. This transformation commences with the initial representation of a molecular graph in an abstract latent space, culminating in the realization of three-dimensional structures via a sophisticated bilevel optimization scheme meticulously tailored to meet the specific requirements of the task. One of the formidable challenges in this modeling endeavor involves preserving roto-translational invariance to ensure that the generated molecular conformations adhere to the laws of physics. Extensive experimental evaluations confirm the efficacy of the proposed \\method{} in comparison to state-of-the-art methods.",
        "comments": "arXiv admin note: text overlap with arXiv:2105.07246 by other authors",
        "date": "2024-01-21",
        "pdf_url": "https://arxiv.org/pdf/2401.09451"
    },
    {
        "doc_id": 161,
        "title": "Regenerative Medicine for Tendon/Ligament Injuries: De Novo Equine Tendon/Ligament Neotissue Generation and Application",
        "authors": [
            "Takashi Taguchi"
        ],
        "subjects": [
            "Tissues and Organs"
        ],
        "abstract": "Tendon and ligament injuries are debilitating conditions across species. Poor regenerative capacities of these tissues limit restoration of original functions. The first study of this dissertation evaluated the effect of cellular administration on tendon/ligament injuries in horses using meta-analysis. The findings led to the second study that engineered implantable de novo tendon neotissue using equine adipose-derived multipotent stromal cells and collagen type I. The neotendon was evaluated for its biocompatibility and therapeutic potential in the third study using immunocompetent and immunocompromised rat bilateral calcaneal tendon elongation model. The fourth study investigated the therapeutic effects of neotendon in surgically-induced non-terminal equine accessory ligament of deep digital flexor tendon injury model.",
        "comments": " ",
        "date": "2023-10-24",
        "pdf_url": "https://arxiv.org/pdf/2401.09423"
    },
    {
        "doc_id": 162,
        "title": "PERMUTOOLS: A MATLAB Package for Multivariate Permutation Testing",
        "authors": [
            "Michael J. Crosse",
            "John J. Foxe",
            "Sophie Molholm"
        ],
        "subjects": [
            "Methodology",
            "Quantitative Methods",
            "Computation"
        ],
        "abstract": "Statistical hypothesis testing and effect size measurement are routine parts of quantitative research. Advancements in computer processing power have greatly improved the capability of statistical inference through the availability of resampling methods. However, many of the statistical practices used today are based on traditional, parametric methods that rely on assumptions about the underlying population. These assumptions may not always be valid, leading to inaccurate results and misleading interpretations. Permutation testing, on the other hand, generates the sampling distribution empirically by permuting the observed data, providing distribution-free hypothesis testing. Furthermore, this approach lends itself to a powerful method for multiple comparison correction - known as max correction - which is less prone to type II errors than conventional correction methods. Parametric methods have also traditionally been utilized for estimating the confidence interval of various test statistics and effect size measures. However, these too can be estimated empirically using permutation or bootstrapping techniques. Whilst resampling methods are generally considered preferable, many popular programming languages and statistical software packages lack efficient implementations. Here, we introduce PERMUTOOLS, a MATLAB package for multivariate permutation testing and effect size measurement.",
        "comments": "7 pages, 2 figures, for PERMUTOOLS toolbox, see https://github.com/mickcrosse/PERMUTOOLS",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09401"
    },
    {
        "doc_id": 163,
        "title": "Graph-based vulnerability assessment of resting-state functional brain networks in full-term neonates",
        "authors": [
            "Mahshid Fouladivanda",
            "Kamran Kazemi",
            "Habibollah Danyali",
            "Ardalan Aarabi"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Quantitative Methods"
        ],
        "abstract": "Network disruption during early brain development can result in long-term cognitive impairments. In this study, we investigated rich-club organization in resting-state functional brain networks in full-term neonates using a multiscale connectivity analysis. We further identified the most influential nodes, also called spreaders, having higher impacts on the flow of information throughout the network. The network vulnerability to damage to rich-club (RC) connectivity within and between resting-state networks was also assessed using a graph-based vulnerability analysis. Our results revealed a rich club organization and small-world topology for resting-state functional brain networks in full term neonates, regardless of the network size. Interconnected mostly through short-range connections, functional rich-club hubs were confined to sensory-motor, cognitive-attention-salience (CAS), default mode, and language-auditory networks with an average cross-scale overlap of 36%, 20%, 15% and 12%, respectively. The majority of the functional hubs also showed high spreading potential, except for several non-RC spreaders within CAS and temporal networks. The functional networks exhibited high vulnerability to loss of RC nodes within sensorimotor cortices, resulting in a significant increase and decrease in network segregation and integration, respectively. The network vulnerability to damage to RC nodes within the language-auditory, cognitive-attention-salience, and default mode networks was also significant but relatively less prominent. Our findings suggest that the network integration in neonates can be highly compromised by damage to RC connectivity due to brain immaturity.",
        "comments": " ",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09255"
    },
    {
        "doc_id": 164,
        "title": "Reproducibility via neural fields of visual illusions induced by localized stimuli",
        "authors": [
            "Cyprien Tamekue",
            "Dario Prandi",
            "Yacine Chitour"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Analysis of PDEs",
            "Numerical Analysis",
            "Pattern Formation and Solitons"
        ],
        "abstract": "This paper investigates the replication of experiments by Billock and Tsou [PNAS, 2007] using the controllability of neural fields of Amari-type modelling the cortical activity in the primary visual cortex (V1), focusing on a regular funnel pattern localised in the fovea or the peripheral visual field. The aim is to understand and model the visual phenomena observed in these experiments, emphasising their nonlinear nature. The study involves designing sensory inputs simulating the visual stimuli from Billock and Tsou's experiments. The after-images induced by these inputs are then theoretically and numerically studied to determine their capacity to replicate the experimentally observed visual effects. A key aspect of this research is investigating the effects induced by the nonlinear nature of neural responses. In particular, by highlighting the importance of both excitatory and inhibitory neurons in the emergence of certain visual phenomena, this study suggests that an interplay of both types of neuronal activities plays an essential role in visual processes, challenging the assumption that the latter is mainly driven by excitatory activities alone.",
        "comments": "MSC Class:          92C20; 35B36; 45A05; 45G15; 45K05; 65R20",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.09108"
    },
    {
        "doc_id": 165,
        "title": "A hybrid tau-leap for simulating chemical kinetics with applications to parameter estimation",
        "authors": [
            "Thomas Trigo Trindade",
            "Konstantinos C. Zygalakis"
        ],
        "subjects": [
            "Molecular Networks",
            "Numerical Analysis",
            "Computation"
        ],
        "abstract": "We consider the problem of efficiently simulating stochastic models of chemical kinetics. The Gillespie Stochastic Simulation algorithm (SSA) is often used to simulate these models, however, in many scenarios of interest, the computational cost quickly becomes prohibitive. This is further exasperated in the Bayesian inference context when estimating parameters of chemical models, as the intractability of the likelihood requires multiple simulations of the underlying system. To deal with issues of computational complexity in this paper, we propose a novel hybrid $\u03c4$-leap algorithm for simulating well-mixed chemical systems. In particular, the algorithm uses $\u03c4$-leap when appropriate (high population densities), and SSA when necessary (low population densities, when discrete effects become non-negligible). In the intermediate regime, a combination of the two methods, which leverages the properties of the underlying Poisson formulation, is employed. As illustrated through a number of numerical experiments the hybrid $\u03c4$ offers significant computational savings when compared to SSA without however sacrificing the overall accuracy. This feature is particularly welcomed in the Bayesian inference context, as it allows for parameter estimation of stochastic chemical kinetics at reduced computational cost.",
        "comments": "25 pages, 8 figures",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.09097"
    },
    {
        "doc_id": 166,
        "title": "Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction",
        "authors": [
            "Ziyang Yu",
            "Wenbing Huang",
            "Yang Liu"
        ],
        "subjects": [
            "Machine Learning",
            "Biomolecules"
        ],
        "abstract": "The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we propose a novel learning-based method called ElliDock, which predicts an elliptic paraboloid to represent the protein-protein docking interface. To be specific, our model estimates elliptic paraboloid interfaces for the two input proteins respectively, and obtains the roto-translation transformation for docking by making two interfaces coincide. By its design, ElliDock is independently equivariant with respect to arbitrary rotations/translations of the proteins, which is an indispensable property to ensure the generalization of the docking process. Experimental evaluations show that ElliDock achieves the fastest inference time among all compared methods and is strongly competitive with current state-of-the-art learning-based models such as DiffDock-PP and Multimer particularly for antibody-antigen docking.",
        "comments": "ICLR 2024",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.08986"
    },
    {
        "doc_id": 167,
        "title": "From Physics to Sentience: Deciphering the Semantics of the Free-Energy Principle and Evaluating its Claims",
        "authors": [
            "Zahra Sheikhbahaee",
            "Adam Safron",
            "Casper Hesp",
            "Guillaume Dumas"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "The Free-Energy Principle (FEP) [1-3] has been adopted in a variety of ambitious proposals that aim to characterize all adaptive, sentient, and cognitive systems within a unifying framework. Judging by the amount of attention it has received from the scientific community, the FEP has gained significant traction in these pursuits. The current target article represents an important iteration of this research paradigm in formally describing emergent dynamics rather than merely (quasi-)steady states. This affords more in-depth considerations of the spatio-temporal complexities of cross-scale causality - as we have encouraged and built towards in previous publications (e.g., [4-9]). In this spirit of constructive feedback, we submit a few technical comments on some of the matters that appear to require further attention, in order to improve the clarity, rigour, and applicability of this framework.",
        "comments": " ",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.08873"
    },
    {
        "doc_id": 168,
        "title": "Using i-vectors for subject-independent cross-session EEG transfer learning",
        "authors": [
            "Jonathan Lasko",
            "Jeff Ma",
            "Mike Nicoletti",
            "Jonathan Sussman-Fort",
            "Sooyoung Jeong",
            "William Hartmann"
        ],
        "subjects": [
            "Machine Learning",
            "Computation and Language",
            "Sound",
            "Audio and Speech Processing",
            "Neurons and Cognition"
        ],
        "abstract": "Cognitive load classification is the task of automatically determining an individual's utilization of working memory resources during performance of a task based on physiologic measures such as electroencephalography (EEG). In this paper, we follow a cross-disciplinary approach, where tools and methodologies from speech processing are used to tackle this problem. The corpus we use was released publicly in 2021 as part of the first passive brain-computer interface competition on cross-session workload estimation. We present our approach which used i-vector-based neural network classifiers to accomplish inter-subject cross-session EEG transfer learning, achieving 18% relative improvement over equivalent subject-dependent models. We also report experiments showing how our subject-independent models perform competitively on held-out subjects and improve with additional subject data, suggesting that subject-dependent training is not required for effective cognitive load determination.",
        "comments": "11 pages",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.08851"
    },
    {
        "doc_id": 169,
        "title": "On the maximum value of the stairs2 index",
        "authors": [
            "Bryan Currie",
            "Kristina Wicke"
        ],
        "subjects": [
            "Combinatorics",
            "Populations and Evolution"
        ],
        "abstract": "Measures of tree balance play an important role in different research areas such as mathematical phylogenetics or theoretical computer science. The balance of a tree is usually quantified in a single number, called a balance or imbalance index, and several such indices exist in the literature. Here, we focus on the stairs2 balance index for rooted binary trees, which was first introduced in the context of viral phylogenetics but has not been fully analyzed from a mathematical viewpoint yet. While it is known that the caterpillar tree uniquely minimizes the stairs2 index for all leaf numbers and the fully balanced tree uniquely maximizes the stairs2 index for leaf numbers that are powers of two, understanding the maximum value and maximal trees for arbitrary leaf numbers is an open problem in the literature. In this note, we fill this gap by showing that for all leaf numbers, there is a unique rooted binary tree maximizing the stairs2 index. Additionally, we obtain recursive and closed expressions for the maximum value of the stairs2 index of a rooted binary tree with $n$ leaves.",
        "comments": "12 pages, 1 figure",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.08838"
    },
    {
        "doc_id": 170,
        "title": "Mechanical constraints and cell cycle regulation in models of collective cell migration",
        "authors": [
            "Carles Falc\u00f3",
            "Daniel J. Cohen",
            "Jos\u00e9 A. Carrillo",
            "Ruth E. Baker"
        ],
        "subjects": [
            "Quantitative Methods",
            "Biological Physics"
        ],
        "abstract": "The spatiotemporal coordination and regulation of cell proliferation is fundamental in many aspects of development and tissue maintenance. Cells have the ability to adapt their division rates in response to mechanical checkpoints, yet we do not fully understand how cell proliferation regulation impacts cell migration phenomena. Here, we present a minimal continuum model of cell migration with cell cycle dynamics, which includes mechanical constraints and hence can account for cell proliferation regulation. By combining minimal mathematical modelling, Bayesian inference, and recent experimental data, we quantify the impact of mechanical constraints across different cell cycle stages in epithelial tissue expansion experiments. Our model suggests that cells sense local density and adapt cell cycle progression in response, during G1 and the combined S/G2/M phases, providing an explicit relationship between each cell cycle stage duration and local tissue density, which is consistent with several experimental observations. Finally, we compare our mathematical model predictions to different experiments studying cell cycle regulation and present a quantitative analysis on the impact of mechanical constraints on cell migration patterns. Our work presents a systematic approach for investigating and analysing cell cycle data, providing mechanistic insights into how individual cells regulate proliferation, based on population-based experimental measurements.",
        "comments": " ",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.08805"
    },
    {
        "doc_id": 171,
        "title": "Machine Learning-Based Analysis of Ebola Virus' Impact on Gene Expression in Nonhuman Primates",
        "authors": [
            "Mostafa Rezapour",
            "Muhammad Khalid Khan Niazi",
            "Hao Lu",
            "Aarthi Narayanan",
            "Metin Nafi Gurcan"
        ],
        "subjects": [
            "Genomics",
            "Machine Learning"
        ],
        "abstract": "This study introduces the Supervised Magnitude-Altitude Scoring (SMAS) methodology, a machine learning-based approach, for analyzing gene expression data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV). We utilize a comprehensive dataset of NanoString gene expression profiles from Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen interaction analysis. SMAS effectively combines gene selection based on statistical significance and expression changes, employing linear classifiers such as logistic regression to accurately differentiate between RT-qPCR positive and negative NHP samples. A key finding of our research is the identification of IFI6 and IFI27 as critical biomarkers, demonstrating exceptional predictive performance with 100% accuracy and Area Under the Curve (AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6 and IFI27, genes, including MX1, OAS1, and ISG15, were significantly upregulated, highlighting their essential roles in the immune response to EBOV. Our results underscore the efficacy of the SMAS method in revealing complex genetic interactions and response mechanisms during EBOV infection. This research provides valuable insights into EBOV pathogenesis and aids in developing more precise diagnostic tools and therapeutic strategies to address EBOV infection in particular and viral infection in general.",
        "comments": "28 pages, 8 figures, 2 tables",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.08738"
    },
    {
        "doc_id": 172,
        "title": "Survival Analysis of Young Triple-Negative Breast Cancer Patients",
        "authors": [
            "M. Mehdi Owrang O",
            "Fariba Jafari Horestani",
            "Ginger Schwarz"
        ],
        "subjects": [
            "Quantitative Methods",
            "Machine Learning",
            "Applications"
        ],
        "abstract": "Breast cancer prognosis is crucial for effective treatment, with the disease more common in women over 40 years old but rare under 40 years old, where less than 5 percent of cases occur in the U.S. Studies indicate a worse prognosis in younger women, which varies by ethnicity. Breast cancers are classified based on receptors like estrogen, progesterone, and HER2. Triple-negative breast cancer (TNBC), lacking these receptors, accounts for about 15 percent of cases and is more prevalent in younger patients, often resulting in poorer outcomes. Nevertheless, the impact of age on TNBC prognosis remains unclear. Factors like age, race, tumor grade, size, and lymph node status are studied for their role in TNBC's clinical outcomes, but current research is inconclusive about age-related differences. This study uses SEER data set to examine the influence of younger age on survivability in TNBC patients, aiming to determine if age is a significant prognostic factor. Our experimental results on SEER dataset confirm the existing research reports that TNBC patients have worse prognosis compared to non-TNBC based on age. Our main goal was to investigate whether younger age has any significance on the survivability of TNBC patients. Experimental results do not show that younger age has any significance on the prognosis and survival rate of the TNBC patients",
        "comments": "31 Pages, 11 Figures, 7 Tables, Peer-reviewed article",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.08712"
    },
    {
        "doc_id": 173,
        "title": "On Image Search in Histopathology",
        "authors": [
            "H. R. Tizhoosh",
            "Liron Pantanowitz"
        ],
        "subjects": [
            "Image and Video Processing",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition",
            "Information Retrieval",
            "Quantitative Methods"
        ],
        "abstract": "Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.",
        "comments": " ",
        "date": "2024-01-14",
        "pdf_url": "https://arxiv.org/pdf/2401.08699"
    },
    {
        "doc_id": 174,
        "title": "Concept Alignment",
        "authors": [
            "Sunayana Rane",
            "Polyphony J. Bruna",
            "Ilia Sucholutsky",
            "Christopher Kello",
            "Thomas L. Griffiths"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Neurons and Cognition"
        ],
        "abstract": "Discussion of AI alignment (alignment between humans and AI systems) has focused on value alignment, broadly referring to creating AI systems that share human values. We argue that before we can even attempt to align values, it is imperative that AI systems and humans align the concepts they use to understand the world. We integrate ideas from philosophy, cognitive science, and deep learning to explain the need for concept alignment, not just value alignment, between humans and machines. We summarize existing accounts of how humans and machines currently learn concepts, and we outline opportunities and challenges in the path towards shared concepts. Finally, we explain how we can leverage the tools already being developed in cognitive science and AI research to accelerate progress towards concept alignment.",
        "comments": "NeurIPS MP2 Workshop 2023",
        "date": "2024-01-09",
        "pdf_url": "https://arxiv.org/pdf/2401.08672"
    },
    {
        "doc_id": 175,
        "title": "Validation and Comparison of Non-Stationary Cognitive Models: A Diffusion Model Application",
        "authors": [
            "Lukas Schumacher",
            "Martin Schnuerch",
            "Andreas Voss",
            "Stefan T. Radev"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Methodology"
        ],
        "abstract": "Cognitive processes undergo various fluctuations and transient states across different temporal scales. Superstatistics are emerging as a flexible framework for incorporating such non-stationary dynamics into existing cognitive model classes. In this work, we provide the first experimental validation of superstatistics and formal comparison of four non-stationary diffusion decision models in a specifically designed perceptual decision-making task. Task difficulty and speed-accuracy trade-off were systematically manipulated to induce expected changes in model parameters. To validate our models, we assess whether the inferred parameter trajectories align with the patterns and sequences of the experimental manipulations. To address computational challenges, we present novel deep learning techniques for amortized Bayesian estimation and comparison of models with time-varying parameters. Our findings indicate that transition models incorporating both gradual and abrupt parameter shifts provide the best fit to the empirical data. Moreover, we find that the inferred parameter trajectories closely mirror the sequence of experimental manipulations. Posterior re-simulations further underscore the ability of the models to faithfully reproduce critical data patterns. Accordingly, our results suggest that the inferred non-stationary dynamics may reflect actual changes in the targeted psychological constructs. We argue that our initial experimental validation paves the way for the widespread application of superstatistics in cognitive modeling and beyond.",
        "comments": " ",
        "date": "2023-12-07",
        "pdf_url": "https://arxiv.org/pdf/2401.08626"
    },
    {
        "doc_id": 176,
        "title": "Dynamic Brain Behaviours in Stroke: A Longitudinal Investigation Based on fMRI Analysis",
        "authors": [
            "Kaichao Wu",
            "Beth Jelfs",
            "Katrina Neville",
            "Qiang Fang"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Background: The brain's functional network constantly adapts to external changes. However, the mechanisms underlying this dynamic adaptive behavior in stroke patients with motor injuries and its role in post-stroke motor recovery remain poorly understood.\n  Method: This study conducted a long-term investigation involving 15 first-stroke patients. Each participant underwent five fMRI scans distributed equally over a six-month period. Using functional neuroimaging data, time-varying functional modularity in post-stroke patients was detected, and subsequently, the dynamic brain behaviors, including recruitment, integration, and flexibility, along with their longitudinal changes, were assessed.\n  Results: Our findings reveal that stroke lesions lead to significant and enduring alterations in all three dynamic behaviors within functional brain networks. Furthermore, during the six-month recovery period, patients who exhibited good and poor recovery showed notable differences in recruitment and flexibility, indicating distinct recovery trajectories for these groups. Notably, when predicting post-stroke recovery status, whole-brain recruitment emerged as a robust and reliable feature, achieving an AUC of 85.93\n  Significance: Our study offers a comprehensive depiction of dynamic brain behavior in the post-ischemic-stroke brain, with a focus on longitudinal changes concurrent with functional recovery. These dynamic patterns hold promise as valuable tools for evaluating and predicting motor recovery following stroke.",
        "comments": " ",
        "date": "2023-11-28",
        "pdf_url": "https://arxiv.org/pdf/2401.08607"
    },
    {
        "doc_id": 177,
        "title": "Long cycles in linear thresholding systems",
        "authors": [
            "Anna Laddach",
            "Michael Shapiro"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Linear thresholding systems have been used as a model of neural activation and more recently proposed as a model of gene regulation. Here we exhibit linear thresholding systems whose dynamics produce surprisingly long cycles.",
        "comments": "3 pages",
        "date": "2024-01-18",
        "pdf_url": "https://arxiv.org/pdf/2401.08605"
    },
    {
        "doc_id": 178,
        "title": "From Conceptual Spaces to Quantum Concepts: Formalising and Learning Structured Conceptual Models",
        "authors": [
            "Sean Tull",
            "Razin A. Shaikh",
            "Sara Sabrina Zemljic",
            "Stephen Clark"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Artificial Intelligence",
            "Quantum Physics"
        ],
        "abstract": "In this article we present a new modelling framework for structured concepts using a category-theoretic generalisation of conceptual spaces, and show how the conceptual representations can be learned automatically from data, using two very different instantiations: one classical and one quantum. A contribution of the work is a thorough category-theoretic formalisation of our framework. We claim that the use of category theory, and in particular the use of string diagrams to describe quantum processes, helps elucidate some of the most important features of our approach. We build upon Gardenfors' classical framework of conceptual spaces, in which cognition is modelled geometrically through the use of convex spaces, which in turn factorise in terms of simpler spaces called domains. We show how concepts from the domains of shape, colour, size and position can be learned from images of simple shapes, where concepts are represented as Gaussians in the classical implementation, and quantum effects in the quantum one. In the classical case we develop a new model which is inspired by the Beta-VAE model of concepts, but is designed to be more closely connected with language, so that the names of concepts form part of the graphical model. In the quantum case, concepts are learned by a hybrid classical-quantum network trained to perform concept classification, where the classical image processing is carried out by a convolutional neural network and the quantum representations are produced by a parameterised quantum circuit. Finally, we consider the question of whether our quantum models of concepts can be considered conceptual spaces in the Gardenfors sense.",
        "comments": "This article consolidates our previous reports on concept formalisation and learning: arXiv:2302.14822 and arXiv:2203.11216",
        "date": "2023-11-06",
        "pdf_url": "https://arxiv.org/pdf/2401.08585"
    },
    {
        "doc_id": 179,
        "title": "How cytoskeletal crosstalk makes cells move: bridging cell-free and cell studies",
        "authors": [
            "James P. Conboy",
            "Irene Ist\u00fariz Petitjean",
            "Anouk van der Net",
            "Gijsje H. Koenderink"
        ],
        "subjects": [
            "Biological Physics",
            "Cell Behavior"
        ],
        "abstract": "Cell migration is a fundamental process for life and is highly dependent on the dynamical and mechanical properties of the cytoskeleton. Intensive physical and biochemical crosstalk between actin, microtubules, and intermediate filaments ensures their coordination to facilitate and enable migration. In this review we discuss the different mechanical aspects that govern cell migration and provide, for each mechanical aspect, a novel perspective by juxtaposing two complementary approaches to the biophysical study of cytoskeletal crosstalk: live-cell studies (often referred to as top-down studies) and cell-free studies (often referred to as bottom-up studies). We summarize the main findings from both experimental approaches, and we provide our perspective on bridging the two perspectives to address the open questions of how cytoskeletal crosstalk governs cell migration and makes cells move.",
        "comments": "4 figures",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.08368"
    },
    {
        "doc_id": 180,
        "title": "dabih -- encrypted data storage and sharing platform",
        "authors": [
            "Michael Huttner",
            "Jakob Simeth",
            "Renato Liguori",
            "Fulvia Ferrazzi",
            "Rainer Spang"
        ],
        "subjects": [
            "Cryptography and Security",
            "Software Engineering",
            "Genomics"
        ],
        "abstract": "Background: The secure management of sensitive clinical data, particularly human genomics data, has become a critical requirement in modern biomedical research. Although the necessary software and algorithms are readily available, their use by non-IT experts poses significant challenges.\n  Methods: We developed dabih, an open-source web application specifically designed to facilitate user-friendly encrypted data management. dabih enables web-based uploading, storing, sharing, and downloading of sensitive data in any format. Its approach to data security involves a two-stage envelope encryption process. We combine symmetric-key encryption for data and public-key encryption as key encapsulation mechanism. The private key necessary for decrypting the data remains exclusively on the owner's device. Thus, accessing data is impossible without explicit permission from the keyholder.\n  Results: dabih is available open-source on GitHub https://github.com/spang-lab/dabih, as ready to use containers on docker hub and includes a command line interface and a graphical bulk upload tool as pre-built binaries. Documentation is available as part of the web application.\n  Conclusions: dabih enables everyone to use strong cryptography for their data, while being just as simple to use as other, non-encrypted, data storage solutions. All the cryptography occurs seamlessly in the background as users interact with a secure web portal, simply by dragging and dropping files.",
        "comments": "16 pages including 4 figures and 5 appendices",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.08333"
    },
    {
        "doc_id": 181,
        "title": "Multifractal organization of EEG signals in Multiple Sclerosis",
        "authors": [
            "Marcin W\u0105torek",
            "Wojciech Tomczyk",
            "Magda Gaw\u0142owska",
            "Natalia Golonka-Afek",
            "Aleksandra \u017byrkowska",
            "Monika Marona",
            "Marcin Wnuk",
            "Agnieszka S\u0142owik",
            "Jeremi K. Ochab",
            "Magdalena Fafrowicz",
            "Tadeusz Marek",
            "Pawe\u0142 O\u015bwi\u0119cimka"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Disordered Systems and Neural Networks",
            "Adaptation and Self-Organizing Systems",
            "Quantitative Methods"
        ],
        "abstract": "Quantifying the complex/multifractal organization of the brain signals is crucial to fully understanding the brain processes and structure. In this contribution, we performed the multifractal analysis of the electroencephalographic (EEG) data obtained from a controlled multiple sclerosis (MS) study, focusing on the correlation between the degree of multifractality, disease duration, and disability level. Our results reveal a significant correspondence between the complexity of the time series and multiple sclerosis development, quantified respectively by scaling exponents and the Expanded Disability Status Scale (EDSS). Namely, for some brain regions, a well-developed multifractality and little persistence of the time series were identified in patients with a high level of disability, whereas the control group and patients with low EDSS were characterised by persistence and monofractality of the signals. The analysis of the cross-correlations between EEG signals supported these results, with the most significant differences identified for patients with EDSS $> 1$ and the combined group of patients with EDSS $\\leq 1$ and controls. No association between the multifractality and disease duration was observed, indicating that the multifractal organisation of the data is a hallmark of developing the disease. The observed complexity/multifractality of EEG signals is hypothetically a result of neuronal compensation -- i.e., of optimizing neural processes in the presence of structural brain degeneration. The presented study is highly relevant due to the multifractal formalism used to quantify complexity and due to scarce resting-state EEG evidence for cortical reorganization associated with compensation.",
        "comments": "39 pages, including supplementary materials (11 figures, 4 tables)",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.08321"
    },
    {
        "doc_id": 182,
        "title": "Sources of HIV infections among MSM with a migration background: a viral phylogenetic case study in Amsterdam, the Netherlands",
        "authors": [
            "Alexandra Blenkinsop",
            "Nikos Pantazis",
            "Evangelia Georgia Kostaki",
            "Lysandros Sofocleous",
            "Ard van Sighem",
            "Daniela Bezemer",
            "Thijs van de Laar",
            "Marc van der Valk",
            "Peter Reiss",
            "Godelieve de Bree",
            "Oliver Ratmann"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "Background: Men and women with a migration background comprise an increasing proportion of incident HIV cases across Western Europe. Several studies indicate a substantial proportion acquire HIV post-migration.\n  Methods: We used partial HIV consensus sequences with linked demographic and clinical data from the opt-out ATHENA cohort of people with HIV in the Netherlands to quantify population-level sources of transmission to Dutch-born and foreign-born Amsterdam men who have sex with men (MSM) between 2010-2021. We identified phylogenetically and epidemiologically possible transmission pairs in local transmission chains and interpreted these in the context of estimated infection dates, quantifying transmission dynamics between sub-populations by world region of birth.\n  Results: We estimate the majority of Amsterdam MSM who acquired their infection locally had a Dutch-born Amsterdam MSM source (56% [53-58%]). Dutch-born MSM were the predominant source population of infections among almost all foreign-born Amsterdam MSM sub-populations. Stratifying by two-year intervals indicated shifts in transmission dynamics, with a majority of infections originating from foreign-born MSM since 2018, although uncertainty ranges remained wide.\n  Conclusions: In the context of declining HIV incidence among Amsterdam MSM, our data suggest whilst native-born MSM have predominantly driven transmissions in 2010-2021, the contribution from foreign-born MSM living in Amsterdam is increasing.",
        "comments": " ",
        "date": "2024-01-16",
        "pdf_url": "https://arxiv.org/pdf/2401.08308"
    },
    {
        "doc_id": 183,
        "title": "Attention-Based CNN-BiLSTM for Sleep State Classification of Spatiotemporal Wide-Field Calcium Imaging Data",
        "authors": [
            "Xiaohui Zhang",
            "Eric C. Landsness",
            "Hanyang Miao",
            "Wei Chen",
            "Michelle Tang",
            "Lindsey M. Brier",
            "Joseph P. Culver",
            "Jin-Moo Lee",
            "Mark A. Anastasio"
        ],
        "subjects": [
            "Image and Video Processing",
            "Neurons and Cognition"
        ],
        "abstract": "Background: Wide-field calcium imaging (WFCI) with genetically encoded calcium indicators allows for spatiotemporal recordings of neuronal activity in mice. When applied to the study of sleep, WFCI data are manually scored into the sleep states of wakefulness, non-REM (NREM) and REM by use of adjunct EEG and EMG recordings. However, this process is time-consuming, invasive and often suffers from low inter- and intra-rater reliability. Therefore, an automated sleep state classification method that operates on spatiotemporal WFCI data is desired. New Method: A hybrid network architecture consisting of a convolutional neural network (CNN) to extract spatial features of image frames and a bidirectional long short-term memory network (BiLSTM) with attention mechanism to identify temporal dependencies among different time points was proposed to classify WFCI data into states of wakefulness, NREM and REM sleep. Results: Sleep states were classified with an accuracy of 84% and Cohen's kappa of 0.64. Gradient-weighted class activation maps revealed that the frontal region of the cortex carries more importance when classifying WFCI data into NREM sleep while posterior area contributes most to the identification of wakefulness. The attention scores indicated that the proposed network focuses on short- and long-range temporal dependency in a state-specific manner. Comparison with Existing Method: On a 3-hour WFCI recording, the CNN-BiLSTM achieved a kappa of 0.67, comparable to a kappa of 0.65 corresponding to the human EEG/EMG-based scoring. Conclusions: The CNN-BiLSTM effectively classifies sleep states from spatiotemporal WFCI data and will enable broader application of WFCI in sleep.",
        "comments": " ",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.08098"
    },
    {
        "doc_id": 184,
        "title": "A new model of trust based on neural information processing",
        "authors": [
            "Scott E. Allen",
            "Ren\u00e9 F. Kizilcec",
            "A. David Redish"
        ],
        "subjects": [
            "General Economics",
            "Human-Computer Interaction",
            "Neurons and Cognition"
        ],
        "abstract": "More than 30 years of research has firmly established the vital role of trust in human organizations and relationships, but the underlying mechanisms by which people build, lose, and rebuild trust remains incompletely understood. We propose a mechanistic model of trust that is grounded in the modern neuroscience of decision making. Since trust requires anticipating the future actions of others, any mechanistic model must be built upon up-to-date theories on how the brain learns, represents, and processes information about the future within its decision-making systems. Contemporary neuroscience has revealed that decision making arises from multiple parallel systems that perform distinct, complementary information processing. Each system represents information in different forms, and therefore learns via different mechanisms. When an act of trust is reciprocated or violated, this provides new information that can be used to anticipate future actions. The taxonomy of neural information representations that is the basis for the system boundaries between neural decision-making systems provides a taxonomy for categorizing different forms of trust and generating mechanistic predictions about how these forms of trust are learned and manifested in human behavior. Three key predictions arising from our model are (1) strategic risk-taking can reveal how to best proceed in a relationship, (2) human organizations and environments can be intentionally designed to encourage trust among their members, and (3) violations of trust need not always degrade trust, but can also provide opportunities to build trust.",
        "comments": " ",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.08064"
    },
    {
        "doc_id": 185,
        "title": "Understanding YTHDF2-mediated mRNA Degradation By m6A-BERT-Deg",
        "authors": [
            "Ting-He Zhang",
            "Sumin Jo",
            "Michelle Zhang",
            "Kai Wang",
            "Shou-Jiang Gao",
            "Yufei Huang"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "N6-methyladenosine (m6A) is the most abundant mRNA modification within mammalian cells, holding pivotal significance in the regulation of mRNA stability, translation, and splicing. Furthermore, it plays a critical role in the regulation of RNA degradation by primarily recruiting the YTHDF2 reader protein. However, the selective regulation of mRNA decay of the m6A-methylated mRNA through YTHDF2 binding is poorly understood. To improve our understanding, we developed m6A-BERT-Deg, a BERT model adapted for predicting YTHDF2-mediated degradation of m6A-methylated mRNAs. We meticulously assembled a high-quality training dataset by integrating multiple data sources for the HeLa cell line. To overcome the limitation of small training samples, we employed a pre-training-fine-tuning strategy by first performing a self-supervised pre-training of the model on 427,760 unlabeled m6A site sequences. The test results demonstrated the importance of this pre-training strategy in enabling m6A-BERT-Deg to outperform other benchmark models. We further conducted a comprehensive model interpretation and revealed a surprising finding that the presence of co-factors in proximity to m6A sites may disrupt YTHDF2-mediated mRNA degradation, subsequently enhancing mRNA stability. We also extended our analyses to the HEK293 cell line, shedding light on the context-dependent YTHDF2-mediated mRNA degradation.",
        "comments": " ",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.08004"
    },
    {
        "doc_id": 186,
        "title": "Discovery of Generalizable TBI Phenotypes Using Multivariate Time-Series Clustering",
        "authors": [
            "Hamid Ghaderi",
            "Brandon Foreman",
            "Chandan K. Reddy",
            "Vignesh Subbian"
        ],
        "subjects": [
            "Machine Learning",
            "Quantitative Methods",
            "Applications"
        ],
        "abstract": "Traumatic Brain Injury (TBI) presents a broad spectrum of clinical presentations and outcomes due to its inherent heterogeneity, leading to diverse recovery trajectories and varied therapeutic responses. While many studies have delved into TBI phenotyping for distinct patient populations, identifying TBI phenotypes that consistently generalize across various settings and populations remains a critical research gap. Our research addresses this by employing multivariate time-series clustering to unveil TBI's dynamic intricates. Utilizing a self-supervised learning-based approach to clustering multivariate time-Series data with missing values (SLAC-Time), we analyzed both the research-centric TRACK-TBI and the real-world MIMIC-IV datasets. Remarkably, the optimal hyperparameters of SLAC-Time and the ideal number of clusters remained consistent across these datasets, underscoring SLAC-Time's stability across heterogeneous datasets. Our analysis revealed three generalizable TBI phenotypes (\u03b1, \\b{eta}, and \u03b3), each exhibiting distinct non-temporal features during emergency department visits, and temporal feature profiles throughout ICU stays. Specifically, phenotype \u03b1 represents mild TBI with a remarkably consistent clinical presentation. In contrast, phenotype \\b{eta} signifies severe TBI with diverse clinical manifestations, and phenotype \u03b3 represents a moderate TBI profile in terms of severity and clinical diversity. Age is a significant determinant of TBI outcomes, with older cohorts recording higher mortality rates. Importantly, while certain features varied by age, the core characteristics of TBI manifestations tied to each phenotype remain consistent across diverse populations.",
        "comments": "25 pages, 10 figures, 4 tables, submitted to Computers in Biology and Medicine",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.08002"
    },
    {
        "doc_id": 187,
        "title": "Integrate Any Omics: Towards genome-wide data integration for patient stratification",
        "authors": [
            "Shihao Ma",
            "Andy G. X. Zeng",
            "Benjamin Haibe-Kains",
            "Anna Goldenberg",
            "John E Dick",
            "Bo Wang"
        ],
        "subjects": [
            "Genomics",
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "High-throughput omics profiling advancements have greatly enhanced cancer patient stratification. However, incomplete data in multi-omics integration presents a significant challenge, as traditional methods like sample exclusion or imputation often compromise biological diversity and dependencies. Furthermore, the critical task of accurately classifying new patients with partial omics data into existing subtypes is commonly overlooked. To address these issues, we introduce IntegrAO (Integrate Any Omics), an unsupervised framework for integrating incomplete multi-omics data and classifying new samples. IntegrAO first combines partially overlapping patient graphs from diverse omics sources and utilizes graph neural networks to produce unified patient embeddings. Our systematic evaluation across five cancer cohorts involving six omics modalities demonstrates IntegrAO's robustness to missing data and its accuracy in classifying new samples with partial profiles. An acute myeloid leukemia case study further validates its capability to uncover biological and clinical heterogeneity in incomplete datasets. IntegrAO's ability to handle heterogeneous and incomplete data makes it an essential tool for precision oncology, offering a holistic approach to patient characterization.",
        "comments": " ",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.07937"
    },
    {
        "doc_id": 188,
        "title": "Predicting heteropolymer interactions: demixing and hypermixing of disordered protein sequences",
        "authors": [
            "Kyosuke Adachi",
            "Kyogo Kawaguchi"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Biological Physics",
            "Biomolecules"
        ],
        "abstract": "Cells contain multiple condensates which spontaneously form due to the heterotypic interactions between their components. Although the proteins and disordered region sequences that are responsible for condensate formation have been extensively studied, the rule of interactions between the components that allow demixing, i.e., the coexistence of multiple condensates, is yet to be elucidated. Here we construct an effective theory of the interaction between heteropolymers by fitting it to the molecular dynamics simulation results obtained for more than 200 sequences sampled from the disordered regions of human proteins. We find that the sum of amino acid pair interactions across two heteropolymers predicts the Boyle temperature qualitatively well, which can be quantitatively improved by the dimer pair approximation, where we incorporate the effect of neighboring amino acids in the sequences. The improved theory, combined with the finding of a metric that captures the effective interaction strength between distinct sequences, allowed the selection of up to three disordered region sequences that demix with each other in multicomponent simulations, as well as the generation of artificial sequences that demix with a given sequence. The theory points to a generic sequence design strategy to demix or hypermix thanks to the low dimensional nature of the space of the interactions that we identify. As a consequence of the geometric arguments in the space of interactions, we find that the number of distinct sequences that can demix with each other is strongly constrained, irrespective of the choice of the coarse-grained model. Altogether, we construct a theoretical basis for methods to estimate the effective interaction between heteropolymers, which can be utilized in predicting phase separation properties as well as rules of assignment in the localization and functions of disordered proteins.",
        "comments": "20 pages, 21 figures",
        "date": "2024-01-19",
        "pdf_url": "https://arxiv.org/pdf/2401.07826"
    },
    {
        "doc_id": 189,
        "title": "Phenotyping calcification in vascular tissues using artificial intelligence",
        "authors": [
            "Mehdi Ramezanpour",
            "Anne M. Robertson",
            "Yasutaka Tobe",
            "Xiaowei Jia",
            "Juan R. Cebral"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Data Analysis, Statistics and Probability",
            "Quantitative Methods",
            "Tissues and Organs"
        ],
        "abstract": "Vascular calcification is implicated as an important factor in major adverse cardiovascular events (MACE), including heart attack and stroke. A controversy remains over how to integrate the diverse forms of vascular calcification into clinical risk assessment tools. Even the commonly used calcium score for coronary arteries, which assumes risk scales positively with total calcification, has important inconsistencies. Fundamental studies are needed to determine how risk is influenced by the diverse calcification phenotypes. However, studies of these kinds are hindered by the lack of high-throughput, objective, and non-destructive tools for classifying calcification in imaging data sets. Here, we introduce a new classification system for phenotyping calcification along with a semi-automated, non-destructive pipeline that can distinguish these phenotypes in even atherosclerotic tissues. The pipeline includes a deep-learning-based framework for segmenting lipid pools in noisy micro-CT images and an unsupervised clustering framework for categorizing calcification based on size, clustering, and topology. This approach is illustrated for five vascular specimens, providing phenotyping for thousands of calcification particles across as many as 3200 images in less than seven hours. Average Dice Similarity Coefficients of 0.96 and 0.87 could be achieved for tissue and lipid pool, respectively, with training and validation needed on only 13 images despite the high heterogeneity in these tissues. By introducing an efficient and comprehensive approach to phenotyping calcification, this work enables large-scale studies to identify a more reliable indicator of the risk of cardiovascular events, a leading cause of global mortality and morbidity.",
        "comments": " ",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.07825"
    },
    {
        "doc_id": 190,
        "title": "Animal-associated marine Acidobacteria with a rich natural product repertoire",
        "authors": [
            "Stefan Leopold-Messer",
            "Clara Chepkirui",
            "Mathijs F. J. Mabesoone",
            "Joshua Mayer",
            "Lucas Paoli",
            "Shinichi Sunagawa",
            "Agustinus R. Uria",
            "Toshiyuki Wakimoto",
            "J\u00f6rn Piel"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "Sponges have long been recognized as a rich source of bioactive natural products. Various studies suggest that many of these compounds are produced by symbiotic bacteria. However, substance supplies and functional insights about the producers remain limited because cultivation remains unsuccessful. To identify alternative, sustainable sources of sponge-derived polyketides, we computationally analyzed 5289 characterized and orphan trans-acyltransferase polyketide synthases, enzymes with widespread roles in polyketide biosynthesis by bacterial symbionts. The analytical workflow predicted marine animal-derived Acidobacteria of the family Acanthopleuribacteraceae with large sets of biosynthetic gene clusters to be enriched in sponge-type chemistry. Targeted compound isolation from a chiton-associated strain yielded new congeners of the phorboxazoles and calyculins, potent and scarce cytotoxins exclusively known from sponges. These first natural products of Acidobacteria and new coral metagenomic data on a third family member suggest animal-associated Acanthopleuribacteraceae as a rich source of sponge-type as well as novel metabolites",
        "comments": "Journal ref:        Chem, 9 (12), 2023, pp. 3696-3713",
        "date": "2024-01-17",
        "pdf_url": "https://arxiv.org/pdf/2401.07730"
    },
    {
        "doc_id": 191,
        "title": "Comprehensive Joint Modeling of First-Line Therapeutics in Non-Small Cell Lung Cancer",
        "authors": [
            "Benjamin Schneider",
            "S\u00e9bastien Benzekry",
            "Jonathan Mochel"
        ],
        "subjects": [
            "Cell Behavior",
            "Tissues and Organs"
        ],
        "abstract": "First-line antiproliferatives for non-small cell lung cancer (NSCLC) have a relatively high failure rate due to high intrinsic resistance rates and acquired resistance rates to therapy. 57% patients are diagnosed in late-stage disease due to the tendency of early-stage NSCLC to be asymptomatic. For patients first diagnosed with metastatic disease the 5-year survival rate is approximately 5%. To help accelerate the development of novel therapeutics and computer-based tools for optimizing individual therapy, we have collated data from 11 different clinical trials in NSCLC and developed a semi-mechanistic, clinical model of NSCLC growth and pharmacodynamics relative to the various therapeutics represented in the study. In this study, we have produced extremely precise estimates of clinical parameters fundamental to cancer modeling such as the rate of acquired resistance to various pharmaceuticals, the relationship between drug concentration and rate of cancer cell death, as well as the fine temporal dynamics of anti-VEGF therapy. In the simulation sets documented in this study, we have used the model to make meaningful descriptions of efficacy gain in making bevacizumab-antiproliferative combination therapy sequential, over a series of days, rather than concurrent.",
        "comments": " ",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.07719"
    },
    {
        "doc_id": 192,
        "title": "Ion channels in critical membranes: clustering, cooperativity, and memory effects",
        "authors": [
            "Antonio Suma",
            "Daniel Sigg",
            "Seamus Gallagher",
            "Giuseppe Gonnella",
            "Vincenzo Carnevale"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Biological Physics",
            "Biomolecules"
        ],
        "abstract": "Much progress has been made in elucidating the inner workings of voltage-gated ion channels, but less understood is the influence of lipid rafts on gating kinetics. Here we propose that state-dependent channel affinity for different lipid species provides a unified explanation for the experimentally observed behaviors of clustering, cooperativity, and hysteresis. We develop models of diffusing lipids and channels engaged in Ising-like interactions to investigate the collective behaviors driven by raft formation in critical membranes close to the demixing transition. The model channels demonstrate lipid-mediated long-range interactions, activation curve steepening, and long-term memory in ionic currents. These behaviors likely play a role in channel-mediated cellular signaling and suggest a universal mechanism for self-organization of biomolecular assemblies.",
        "comments": "14 pages, 6 figures",
        "date": "2024-01-22",
        "pdf_url": "https://arxiv.org/pdf/2401.07660"
    },
    {
        "doc_id": 193,
        "title": "Empirical Evidence for the Fragment level Understanding on Drug Molecular Structure of LLMs",
        "authors": [
            "Xiuyuan Hu",
            "Guoqing Liu",
            "Yang Zhao",
            "Hao Zhang"
        ],
        "subjects": [
            "Machine Learning",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "AI for drug discovery has been a research hotspot in recent years, and SMILES-based language models has been increasingly applied in drug molecular design. However, no work has explored whether and how language models understand the chemical spatial structure from 1D sequences. In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments. The results indicate that language models can understand chemical structures from the perspective of molecular fragments, and the structural knowledge learned through fine-tuning is reflected in the high-frequency SMILES substrings generated by the model.",
        "comments": "Accepted by AAAI 2024 workshop: Large Language Models for Biological Discoveries (LLMs4Bio)",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.07657"
    },
    {
        "doc_id": 194,
        "title": "Measuring multisensory integration in reaction time: the relative entropy approach",
        "authors": [
            "Hans Colonius",
            "Adele Diederich"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "A classic definition of multisensory integration (MI) has been proposed as ``the presence of a (statistically) significant change in the response to a cross-modal stimulus complex compared to unimodal stimuli''. However, this general definition did not result in a broad consensus on how to quantify the amount of MI in the context of reaction time (RT). In this brief note, we argue that numeric measures of reaction times that only involve mean or median RTs do not uncover the information required to fully assess the effect of multisensory integration. We suggest instead novel measures that include the entire RT distributions functions. The central role is played by relative entropy (aka Kullback-Leibler divergence), a statistical concept in information theory, statistics, and machine learning to measure the (non-symmetric) distance between probability distributions. We provide a number of theoretical examples, but empirical applications and statistical testing are postponed to later study.",
        "comments": "9 pages, 1 figure",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.07568"
    },
    {
        "doc_id": 195,
        "title": "Vitamin K content of Australian-grown horticultural commodities",
        "authors": [
            "Eleanor Dunlop",
            "Judy Cunningham",
            "Paul Adorno",
            "Georgios Dabos",
            "Stuart K Johnson",
            "Lucinda J Black"
        ],
        "subjects": [
            "Other Quantitative Biology"
        ],
        "abstract": "Vitamin K is emerging as a multi-function vitamin that plays a role in bone, brain and vascular health. Vitamin K composition data remain limited globally and Australia has lacked nationally representative data for vitamin K1 (phylloquinone, PK) in horticultural commodities. Primary samples (n = 927) of 90 different Australian-grown fruit, vegetable and nut commodities were purchased in three Australian cities. We measured PK in duplicate in 95 composite samples using liquid chromatography with electrospray ionisation-tandem mass spectrometry. The greatest mean concentrations of PK were found in kale (565 ug/100 g), baby spinach (255 ug/100 g) and Brussels sprouts (195 ug/100 g). The data contribute to the global collection of vitamin K food composition data. They add to the evidence that PK concentrations vary markedly between geographic regions, supporting development of region-specific datasets for national food composition databases that do not yet contain data for vitamin K.",
        "comments": "22 pages, 2 tables",
        "date": "2024-01-15",
        "pdf_url": "https://arxiv.org/pdf/2401.07473"
    },
    {
        "doc_id": 196,
        "title": "Inference of dynamical gene regulatory networks from single-cell data with physics informed neural networks",
        "authors": [
            "Maria Mircea",
            "Diego Garlaschelli",
            "Stefan Semrau"
        ],
        "subjects": [
            "Quantitative Methods",
            "Artificial Intelligence",
            "Machine Learning",
            "Biological Physics",
            "Molecular Networks"
        ],
        "abstract": "One of the main goals of developmental biology is to reveal the gene regulatory networks (GRNs) underlying the robust differentiation of multipotent progenitors into precisely specified cell types. Most existing methods to infer GRNs from experimental data have limited predictive power as the inferred GRNs merely reflect gene expression similarity or correlation. Here, we demonstrate, how physics-informed neural networks (PINNs) can be used to infer the parameters of predictive, dynamical GRNs that provide mechanistic understanding of biological processes. Specifically we study GRNs that exhibit bifurcation behavior and can therefore model cell differentiation. We show that PINNs outperform regular feed-forward neural networks on the parameter inference task and analyze two relevant experimental scenarios: 1. a system with cell communication for which gene expression trajectories are available and 2. snapshot measurements of a cell population in which cell communication is absent. Our analysis will inform the design of future experiments to be analyzed with PINNs and provides a starting point to explore this powerful class of neural network models further.",
        "comments": "25 pages, 8 figures",
        "date": "2024-01-14",
        "pdf_url": "https://arxiv.org/pdf/2401.07379"
    },
    {
        "doc_id": 197,
        "title": "Robust Genomic Prediction and Heritability Estimation using Density Power Divergence",
        "authors": [
            "Upama Paul Chowdhury",
            "Susmita Das",
            "Abhik Ghosh"
        ],
        "subjects": [
            "Methodology",
            "Genomics",
            "Applications"
        ],
        "abstract": "This manuscript delves into the intersection of genomics and phenotypic prediction, focusing on the statistical innovation required to navigate the complexities introduced by noisy covariates and confounders. The primary emphasis is on the development of advanced robust statistical models tailored for genomic prediction from single nucleotide polymorphism (SNP) data collected from genome-wide association studies (GWAS) in plant and animal breeding and multi-field trials. The manuscript explores the limitations of traditional marker-assisted recurrent selection, highlighting the significance of incorporating all estimated effects of marker loci into the statistical framework and aiming to reduce the high dimensionality of GWAS data while preserving critical information. This paper introduces a new robust statistical framework for genomic prediction, employing one-stage and two-stage linear mixed model analyses along with utilizing the popular robust minimum density power divergence estimator (MDPDE) to estimate genetic effects on phenotypic traits. The study illustrates the superior performance of the proposed MDPDE-based genomic prediction and associated heritability estimation procedures over existing competitors through extensive empirical experiments on artificial datasets and application to a real-life maize breeding dataset. The results showcase the robustness and accuracy of the proposed MDPDE-based approaches, especially in the presence of data contamination, emphasizing their potential applications in improving breeding programs and advancing genomic prediction of phenotyping traits.",
        "comments": "Under Review",
        "date": "2024-01-14",
        "pdf_url": "https://arxiv.org/pdf/2401.07344"
    },
    {
        "doc_id": 198,
        "title": "Phenotypic switching mechanisms determine the structure of cell migration into extracellular matrix under the `go-or-grow' hypothesis",
        "authors": [
            "Rebecca M. Crossley",
            "Kevin J. Painter",
            "Tommaso Lorenzi",
            "Philip K. Maini",
            "Ruth E. Baker"
        ],
        "subjects": [
            "Cell Behavior"
        ],
        "abstract": "A fundamental feature of collective cell migration is phenotypic heterogeneity which, for example, influences tumour progression and relapse. While current mathematical models often consider discrete phenotypic structuring of the cell population, in-line with the `go-or-grow' hypothesis \\cite{hatzikirou2012go, stepien2018traveling}, they regularly overlook the role that the environment may play in determining the cells' phenotype during migration. Comparing a previously studied volume-filling model for a homogeneous population of generalist cells that can proliferate, move and degrade extracellular matrix (ECM) \\cite{crossley2023travelling} to a novel model for a heterogeneous population comprising two distinct sub-populations of specialist cells that can either move and degrade ECM or proliferate, this study explores how different hypothetical phenotypic switching mechanisms affect the speed and structure of the invading cell populations. Through a continuum model derived from its individual-based counterpart, insights into the influence of the ECM and the impact of phenotypic switching on migrating cell populations emerge. Notably, specialist cell populations that cannot switch phenotype show reduced invasiveness compared to generalist cell populations, while implementing different forms of switching significantly alters the structure of migrating cell fronts. This key result suggests that the structure of an invading cell population could be used to infer the underlying mechanisms governing phenotypic switching.",
        "comments": "34 pages, 11 figures",
        "date": "2024-01-14",
        "pdf_url": "https://arxiv.org/pdf/2401.07279"
    },
    {
        "doc_id": 199,
        "title": "Balancing reaction-diffusion network for cell polarization pattern with stability and asymmetry",
        "authors": [
            "Yixuan Chen",
            "Guoye Guan",
            "Lei-Han Tang",
            "Chao Tang"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "Cell polarization is a critical process that separates molecules into two distinct regions in prokaryotic and eukaryotic cells, guiding biological processes such as cell division and cell differentiation. Although several underlying antagonistic reaction-diffusion networks capable of setting up cell polarization have been identified experimentally and theoretically, our understanding of how to manipulate pattern stability and asymmetry remains incomplete, especially when only a subset of network components are known. Here we present numerical results to show that the polarized pattern of an antagonistic 2-node network collapses into a homogeneous state when subjected to single-sided self-regulation, single-sided additional regulation, or unequal system parameters. However, polarity can be restored through a combination of two modifications that have opposing effects. Additionally, spatially inhomogeneous parameters favoring respective domains stabilize their interface at designated locations. To connect our findings to cell polarity studies of the nematode Caenorhabditis elegans zygote, we reconstituted a 5-node network where a 4-node circuit with full mutual inhibitions between anterior and posterior is modified by a mutual activation in the anterior and an additional mutual inhibition between the anterior and the posterior. Once again, a generic set of kinetic parameters moves the interface towards either the anterior or posterior end, yet a polarized pattern can be stabilized through spatial tuning of one or more parameters coupled to intracellular or extracellular cues. A user-friendly software, PolarSim, is introduced to facilitate the exploration of networks with alternative node numbers, parameter values, and regulatory pathways.",
        "comments": " ",
        "date": "2024-01-14",
        "pdf_url": "https://arxiv.org/pdf/2401.07227"
    }
]