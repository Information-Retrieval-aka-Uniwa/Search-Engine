[
    {
        "doc_id": 0,
        "title": "General duality and dual attainment for adapted transport",
        "authors": [
            "Daniel Kr\u0161ek",
            "Gudmund Pammer"
        ],
        "subjects": [
            "Probability",
            "Optimization and Control",
            "Mathematical Finance"
        ],
        "abstract": "We investigate duality and existence of dual optimizers for several adapted optimal transport problems under minimal assumptions. This includes the causal and bicausal transport, the barycenter problem, and a general multimarginal problem incorporating causality constraints. Moreover, we discuss applications of our results in robust finance. We consider a non-dominated model of several financial markets where stocks are traded dynamically, but the joint stock dynamics are unknown. We show that a no-arbitrage assumption in a quasi-sure sense naturally leads to sets of multicausal couplings. Consequently, computing the robust superhedging price is equivalent to solving an adapted transport problem, and finding a superhedging strategy means solving the corresponding dual.",
        "comments": "32 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11958"
    },
    {
        "doc_id": 1,
        "title": "Forecasting and Backtesting Gradient Allocations of Expected Shortfall",
        "authors": [
            "Takaaki Koike",
            "Cathy W. S. Chen",
            "Edward M. H. Lin"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "Capital allocation is a procedure for quantifying the contribution of each source of risk to aggregated risk. The gradient allocation rule, also known as the Euler principle, is a prevalent rule of capital allocation under which the allocated capital captures the diversification benefit of the marginal risk as a component of overall risk. This research concentrates on Expected Shortfall (ES) as a regulatory standard and focuses on the gradient allocations of ES, also called ES contributions. We achieve the comprehensive treatment of backtesting the tuple of ES contributions in the framework of the traditional and comparative backtests based on the concepts of joint identifiability and multi-objective elicitability. For robust forecast evaluation against the choice of scoring function, we further develop Murphy diagrams for ES contributions as graphical tools to check whether one forecast dominates another under a class of scoring functions. Finally, leveraging the recent concept of multi-objective elicitability, we propose a novel semiparametric model for forecasting dynamic ES contributions based on a compositional regression model. In an empirical analysis of stock returns we evaluate and compare a variety of models for forecasting dynamic ES contributions and demonstrate the outstanding performance of the proposed model.",
        "comments": "MSC Class:          62F07; 62P05; 91B30",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11701"
    },
    {
        "doc_id": 2,
        "title": "A Novel Decision Ensemble Framework: Customized Attention-BiLSTM and XGBoost for Speculative Stock Price Forecasting",
        "authors": [
            "Riaz Ud Din",
            "Salman Ahmed",
            "Saddam Hussain Khan"
        ],
        "subjects": [
            "Statistical Finance",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "Forecasting speculative stock prices is essential for effective investment risk management that drives the need for the development of innovative algorithms. However, the speculative nature, volatility, and complex sequential dependencies within financial markets present inherent challenges which necessitate advanced techniques. This paper proposes a novel framework, CAB-XDE (customized attention BiLSTM-XGB decision ensemble), for predicting the daily closing price of speculative stock Bitcoin-USD (BTC-USD). CAB-XDE framework integrates a customized bi-directional long short-term memory (BiLSTM) with the attention mechanism and the XGBoost algorithm. The customized BiLSTM leverages its learning capabilities to capture the complex sequential dependencies and speculative market trends. Additionally, the new attention mechanism dynamically assigns weights to influential features, thereby enhancing interpretability, and optimizing effective cost measures and volatility forecasting. Moreover, XGBoost handles nonlinear relationships and contributes to the proposed CAB-XDE framework robustness. Additionally, the weight determination theory-error reciprocal method further refines predictions. This refinement is achieved by iteratively adjusting model weights. It is based on discrepancies between theoretical expectations and actual errors in individual customized attention BiLSTM and XGBoost models to enhance performance. Finally, the predictions from both XGBoost and customized attention BiLSTM models are concatenated to achieve diverse prediction space and are provided to the ensemble classifier to enhance the generalization capabilities of CAB-XDE. The proposed CAB-XDE framework is empirically validated on volatile Bitcoin market, sourced from Yahoo Finance and outperforms state-of-the-art models with a MAPE of 0.0037, MAE of 84.40, and RMSE of 106.14.",
        "comments": "30 pages, 16 Figures, 4 Tables",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11621"
    },
    {
        "doc_id": 3,
        "title": "The geometry of multi-curve interest rate models",
        "authors": [
            "Claudio Fontana",
            "Giacomo Lanaro",
            "Agatha Murgoci"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "We study the problems of consistency and of the existence of finite-dimensional realizations for multi-curve interest rate models of Heath-Jarrow-Morton type, generalizing the geometric approach developed by T. Bj\u00f6rk and co-authors in the classical single-curve setting. We characterize when a multi-curve interest rate model is consistent with a given parameterized family of forward curves and spreads and when a model can be realized by a finite-dimensional state process. We illustrate the general theory in a number of model classes and examples, providing explicit constructions of finite-dimensional realizations. Based on these theoretical results, we perform the calibration of a three-curve Hull-White model to market data and analyse the stability of the estimated parameters.",
        "comments": "28 pages, 2 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11619"
    },
    {
        "doc_id": 4,
        "title": "Functional Limit Theorems for Hawkes Processes",
        "authors": [
            "Ulrich Horst",
            "Wei Xu"
        ],
        "subjects": [
            "Probability",
            "Statistics Theory",
            "Mathematical Finance"
        ],
        "abstract": "We prove that the long-run behavior of Hawkes processes is fully determined by the average number and the dispersion of child events. For subcritical processes we provide FLLNs and FCLTs under minimal conditions on the kernel of the process with the precise form of the limit theorems depending strongly on the dispersion of child events. For a critical Hawkes process with weakly dispersed child events, functional central limit theorems do not hold. Instead, we prove that the rescaled intensity processes and rescaled Hawkes processes behave like CIR-processes without mean-reversion, respectively integrated CIR-processes. We provide the rate of convergence by establishing an upper bound on the Wasserstein distance between the distributions of rescaled Hawkes process and the corresponding limit process. By contrast, critical Hawkes process with heavily dispersed child events share many properties of subcritical ones. In particular, functional limit theorems hold. However, unlike subcritical processes critical ones with heavily dispersed child events display long-range dependencies.",
        "comments": "59 pages; Keywords and phrases: Hawkes process, functional limit theorem, regular variation, convergence rate",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11495"
    },
    {
        "doc_id": 5,
        "title": "PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding",
        "authors": [
            "Ruochi Zhang",
            "Haoran Wu",
            "Chang Liu",
            "Huaping Li",
            "Yuqian Wu",
            "Kewei Li",
            "Yifan Wang",
            "Yifan Deng",
            "Jiahui Chen",
            "Fengfeng Zhou",
            "Xin Gao"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "Recent advances in protein language models have catalyzed significant progress in peptide sequence representation. Despite extensive exploration in this field, pre-trained models tailored for peptide-specific needs remain largely unaddressed due to the difficulty in capturing the complex and sometimes unstable structures of peptides. This study introduces a novel multi-view contrastive learning framework PepHarmony for the sequence-based peptide encoding task. PepHarmony innovatively combines both sequence- and structure-level information into a sequence-level encoding module through contrastive learning. We carefully select datasets from the Protein Data Bank (PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences and structures. The experimental data highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures compared with the baseline and fine-tuned models. The robustness of our model is confirmed through extensive ablation studies, which emphasize the crucial roles of contrastive loss and strategic data sorting in enhancing predictive performance. The proposed PepHarmony framework serves as a notable contribution to peptide representations, and offers valuable insights for future applications in peptide drug discovery and peptide engineering. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony or http://www.healthinformaticslab.org/supp/.",
        "comments": "25 pages, 5 figures, 3 tables",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11360"
    },
    {
        "doc_id": 6,
        "title": "Data-driven Option Pricing",
        "authors": [
            "Min Dai",
            "Hanqing Jin",
            "Xi Yang"
        ],
        "subjects": [
            "Pricing of Securities"
        ],
        "abstract": "We propose an innovative data-driven option pricing methodology that relies exclusively on the dataset of historical underlying asset prices. While the dataset is rooted in the objective world, option prices are commonly expressed as discounted expectations of their terminal payoffs in a risk-neutral world. Bridging this gap motivates us to identify a pricing kernel process, transforming option pricing into evaluating expectations in the objective world. We recover the pricing kernel by solving a utility maximization problem, and evaluate the expectations in terms of a functional optimization problem. Leveraging the deep learning technique, we design data-driven algorithms to solve both optimization problems over the dataset. Numerical experiments are presented to demonstrate the efficiency of our methodology.",
        "comments": "15 pages, 3 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11158"
    },
    {
        "doc_id": 7,
        "title": "BioFinBERT: Finetuning Large Language Models (LLMs) to Analyze Sentiment of Press Releases and Financial Text Around Inflection Points of Biotech Stocks",
        "authors": [
            "Valentina Aparicio",
            "Daniel Gordon",
            "Sebastian G. Huayamares",
            "Yuhuai Luo"
        ],
        "subjects": [
            "General Finance",
            "Computational Finance",
            "Trading and Market Microstructure"
        ],
        "abstract": "Large language models (LLMs) are deep learning algorithms being used to perform natural language processing tasks in various fields, from social sciences to finance and biomedical sciences. Developing and training a new LLM can be very computationally expensive, so it is becoming a common practice to take existing LLMs and finetune them with carefully curated datasets for desired applications in different fields. Here, we present BioFinBERT, a finetuned LLM to perform financial sentiment analysis of public text associated with stocks of companies in the biotechnology sector. The stocks of biotech companies developing highly innovative and risky therapeutic drugs tend to respond very positively or negatively upon a successful or failed clinical readout or regulatory approval of their drug, respectively. These clinical or regulatory results are disclosed by the biotech companies via press releases, which are followed by a significant stock response in many cases. In our attempt to design a LLM capable of analyzing the sentiment of these press releases,we first finetuned BioBERT, a biomedical language representation model designed for biomedical text mining, using financial textual databases. Our finetuned model, termed BioFinBERT, was then used to perform financial sentiment analysis of various biotech-related press releases and financial text around inflection points that significantly affected the price of biotech stocks.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11011"
    },
    {
        "doc_id": 8,
        "title": "Forecasting Cryptocurrency Staking Rewards",
        "authors": [
            "Sauren Gupta",
            "Apoorva Hathi Katharaki",
            "Yifan Xu",
            "Bhaskar Krishnamachari",
            "Rajarshi Gupta"
        ],
        "subjects": [
            "Statistical Finance",
            "Cryptography and Security",
            "Machine Learning"
        ],
        "abstract": "This research explores a relatively unexplored area of predicting cryptocurrency staking rewards, offering potential insights to researchers and investors. We investigate two predictive methodologies: a) a straightforward sliding-window average, and b) linear regression models predicated on historical data. The findings reveal that ETH staking rewards can be forecasted with an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day look-aheads respectively, using a 7-day sliding-window average approach. Additionally, we discern diverse prediction accuracies across various cryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is identified as superior to the moving-window average for perdicting in the short term for XTZ and ATOM. The results underscore the generally stable and predictable nature of staking rewards for most assets, with MATIC presenting a noteworthy exception.",
        "comments": "9 pages, 18 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10931"
    },
    {
        "doc_id": 9,
        "title": "Application of Machine Learning in Stock Market Forecasting: A Case Study of Disney Stock",
        "authors": [
            "Dengxin Huang"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning",
            "Applications"
        ],
        "abstract": "This document presents a stock market analysis conducted on a dataset consisting of 750 instances and 16 attributes donated in 2014-10-23. The analysis includes an exploratory data analysis (EDA) section, feature engineering, data preparation, model selection, and insights from the analysis. The Fama French 3-factor model is also utilized in the analysis. The results of the analysis are presented, with linear regression being the best-performing model.",
        "comments": "9 pages, 7 figures",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10903"
    },
    {
        "doc_id": 10,
        "title": "Stylized Facts and Market Microstructure: An In-Depth Exploration of German Bond Futures Market",
        "authors": [
            "Hamza Bodor",
            "Laurent Carlier"
        ],
        "subjects": [
            "Statistical Finance",
            "Trading and Market Microstructure"
        ],
        "abstract": "This paper presents an in-depth analysis of stylized facts in the context of futures on German bonds. The study examines four futures contracts on German bonds: Schatz, Bobl, Bund and Buxl, using tick-by-tick limit order book datasets. It uncovers a range of stylized facts and empirical observations, including the distribution of order sizes, patterns of order flow, and inter-arrival times of orders. The findings reveal both commonalities and unique characteristics across the different futures, thereby enriching our understanding of these markets. Furthermore, the paper introduces insightful realism metrics that can be used to benchmark market simulators. The study contributes to the literature on financial stylized facts by extending empirical observations to this class of assets, which has been relatively underexplored in existing research. This work provides valuable guidance for the development of more accurate and realistic market simulators.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10722"
    },
    {
        "doc_id": 11,
        "title": "Dynamic Programming: Finite States",
        "authors": [
            "Thomas J. Sargent",
            "John Stachurski"
        ],
        "subjects": [
            "General Economics",
            "Optimization and Control"
        ],
        "abstract": "This book is about dynamic programming and its applications in economics, finance, and adjacent fields. It brings together recent innovations in the theory of dynamic programming and provides applications and code that can help readers approach the research frontier. The book is aimed at graduate students and researchers, although most chapters are accessible to undergraduate students with solid quantitative backgrounds.",
        "comments": "MSC Class:          90C39",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10473"
    },
    {
        "doc_id": 12,
        "title": "Deep Generative Modeling for Financial Time Series with Application in VaR: A Comparative Review",
        "authors": [
            "Lars Ericson",
            "Xuejun Zhu",
            "Xusi Han",
            "Rao Fu",
            "Shuang Li",
            "Steve Guo",
            "Ping Hu"
        ],
        "subjects": [
            "Computational Finance",
            "Machine Learning",
            "Risk Management",
            "Statistical Finance"
        ],
        "abstract": "In the financial services industry, forecasting the risk factor distribution conditional on the history and the current market environment is the key to market risk modeling in general and value at risk (VaR) model in particular. As one of the most widely adopted VaR models in commercial banks, Historical simulation (HS) uses the empirical distribution of daily returns in a historical window as the forecast distribution of risk factor returns in the next day. The objectives for financial time series generation are to generate synthetic data paths with good variety, and similar distribution and dynamics to the original historical data. In this paper, we apply multiple existing deep generative methods (e.g., CGAN, CWGAN, Diffusion, and Signature WGAN) for conditional time series generation, and propose and test two new methods for conditional multi-step time series generation, namely Encoder-Decoder CGAN and Conditional TimeVAE. Furthermore, we introduce a comprehensive framework with a set of KPIs to measure the quality of the generated time series for financial modeling. The KPIs cover distribution distance, autocorrelation and backtesting. All models (HS, parametric and neural networks) are tested on both historical USD yield curve data and additional data simulated from GARCH and CIR processes. The study shows that top performing models are HS, GARCH and CWGAN models. Future research directions in this area are also discussed.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10370"
    },
    {
        "doc_id": 13,
        "title": "Interplay between Cryptocurrency Transactions and Online Financial Forums",
        "authors": [
            "Ana Fern\u00e1ndez Vilas",
            "Rebeca P. D\u00edaz Redondo",
            "Daniel Couto Cancela",
            "Alejandro Torrado Pazos"
        ],
        "subjects": [
            "General Finance",
            "Computers and Society",
            "Machine Learning"
        ],
        "abstract": "Cryptocurrencies are a type of digital money meant to provide security and anonymity while using cryptography techniques. Although cryptocurrencies represent a breakthrough and provide some important benefits, their usage poses some risks that are a result of the lack of supervising institutions and transparency. Because disinformation and volatility is discouraging for personal investors, cryptocurrencies emerged hand-in-hand with the proliferation of online users' communities and forums as places to share information that can alleviate users' mistrust. This research focuses on the study of the interplay between these cryptocurrency forums and fluctuations in cryptocurrency values. In particular, the most popular cryptocurrency Bitcoin (BTC) and a related active discussion community, Bitcointalk, are analyzed. This study shows that the activity of Bitcointalk forum keeps a direct relationship with the trend in the values of BTC, therefore analysis of this interaction would be a perfect base to support personal investments in a non-regulated market and, to confirm whether cryptocurrency forums show evidences to detect abnormal behaviors in BTC values as well as to predict or estimate these values. The experiment highlights that forum data can explain specific events in the financial field. It also underlines the relevance of quotes (regular mechanism to response a post) at periods: (1) when there is a high concentration of posts around certain topics; (2) when peaks in the BTC price are observed; and, (3) when the BTC price gradually shifts downwards and users intend to sell.",
        "comments": "Journal ref:        Mathematics 2021, 9(4), 411;",
        "date": "27 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10238"
    },
    {
        "doc_id": 14,
        "title": "An Exploration to the Correlation Structure and Clustering of Macroeconomic Variables (MEV)",
        "authors": [
            "Garvit Arora",
            "Shubhangi Shubhangi",
            "Ying Wu",
            "Xuan Mei"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "As a quantitative characterization of the complicated economy, Macroeconomic Variables (MEVs), including GDP, inflation, unemployment, income, spending, interest rate, etc., are playing a crucial role in banks' portfolio management and stress testing exercise. In recent years, especially during the COVID-19 period and the current high inflation environment, people are frequently talking about the changing \"correlation structure\" of MEVs. In this paper, we use a principal component based algorithm to better understand MEVs' correlation structure in a given period. We also demonstrate how this method can be used to visualize historical MEVs pattern changes between 2000 and 2022. Further, we use this method to compare different hypothetical or historical macroeconomic scenarios and present our key findings.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10162"
    },
    {
        "doc_id": 15,
        "title": "Cardiac Digital Twin Pipeline for Virtual Therapy Evaluation",
        "authors": [
            "Julia Camps",
            "Zhinuo Jenny Wang",
            "Ruben Doste",
            "Maxx Holmes",
            "Brodie Lawson",
            "Jakub Tomek",
            "Kevin Burrage",
            "Alfonso Bueno-Orovio",
            "Blanca Rodriguez"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Tissues and Organs"
        ],
        "abstract": "Cardiac digital twins are computational tools capturing key functional and anatomical characteristics of patient hearts for investigating disease phenotypes and predicting responses to therapy. When paired with large-scale computational resources and large clinical datasets, digital twin technology can enable virtual clinical trials on virtual cohorts to fast-track therapy development. Here, we present an automated pipeline for personalising ventricular anatomy and electrophysiological function based on routinely acquired cardiac magnetic resonance (CMR) imaging data and the standard 12-lead electrocardiogram (ECG). Using CMR-based anatomical models, a sequential Monte-Carlo approximate Bayesian computational inference method is extended to infer electrical activation and repolarisation characteristics from the ECG. Fast simulations are conducted with a reaction-Eikonal model, including the Purkinje network and biophysically-detailed subcellular ionic current dynamics for repolarisation. For each patient, parameter uncertainty is represented by inferring a population of ventricular models rather than a single one, which means that parameter uncertainty can be propagated to therapy evaluation. Furthermore, we have developed techniques for translating from reaction-Eikonal to monodomain simulations, which allows more realistic simulations of cardiac electrophysiology. The pipeline is demonstrated in a healthy female subject, where our inferred reaction-Eikonal models reproduced the patient's ECG with a Pearson's correlation coefficient of 0.93, and the translated monodomain simulations have a correlation coefficient of 0.89. We then apply the effect of Dofetilide to the monodomain population of models for this subject and show dose-dependent QT and T-peak to T-end prolongations that are in keeping with large population drug response data.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10029"
    },
    {
        "doc_id": 16,
        "title": "Consistent asset modelling with random coefficients and switches between regimes",
        "authors": [
            "Felix L. Wolf",
            "Griselda Deelstra",
            "Lech A. Grzelak"
        ],
        "subjects": [
            "Pricing of Securities",
            "Computational Finance",
            "Risk Management"
        ],
        "abstract": "We explore a stochastic model that enables capturing external influences in two specific ways. The model allows for the expression of uncertainty in the parametrisation of the stochastic dynamics and incorporates patterns to account for different behaviours across various times or regimes. To establish our framework, we initially construct a model with random parameters, where the switching between regimes can be dictated either by random variables or deterministically. Such a model is highly interpretable. We further ensure mathematical consistency by demonstrating that the framework can be elegantly expressed through local volatility models taking the form of standard jump diffusions. Additionally, we consider a Markov-modulated approach for the switching between regimes characterised by random parameters. For all considered models, we derive characteristic functions, providing a versatile tool with wide-ranging applications. In a numerical experiment, we apply the framework to the financial problem of option pricing. The impact of parameter uncertainty is analysed in a two-regime model, where the asset process switches between periods of high and low volatility imbued with high and low uncertainty, respectively.",
        "comments": "MSC Class:          91G20 91G30",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09955"
    },
    {
        "doc_id": 17,
        "title": "Cross-Domain Behavioral Credit Modeling: transferability from private to central data",
        "authors": [
            "O. Didkovskyi",
            "N. Jean",
            "G. Le Pera",
            "C. Nordio"
        ],
        "subjects": [
            "Risk Management",
            "Statistical Finance"
        ],
        "abstract": "This paper introduces a credit risk rating model for credit risk assessment in quantitative finance, aiming to categorize borrowers based on their behavioral data. The model is trained on data from Experian, a widely recognized credit bureau, to effectively identify instances of loan defaults among bank customers. Employing state-of-the-art statistical and machine learning techniques ensures the model's predictive accuracy. Furthermore, we assess the model's transferability by testing it on behavioral data from the Bank of Italy, demonstrating its potential applicability across diverse datasets during prediction. This study highlights the benefits of incorporating external behavioral data to improve credit risk assessment in financial institutions.",
        "comments": "25 pages, 15 figures",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09778"
    },
    {
        "doc_id": 18,
        "title": "Neural Hawkes: Non-Parametric Estimation in High Dimension and Causality Analysis in Cryptocurrency Markets",
        "authors": [
            "Timoth\u00e9e Fabre",
            "Ioane Muni Toke"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Mathematical Finance"
        ],
        "abstract": "We propose a novel approach to marked Hawkes kernel inference which we name the moment-based neural Hawkes estimation method. Hawkes processes are fully characterized by their first and second order statistics through a Fredholm integral equation of the second kind. Using recent advances in solving partial differential equations with physics-informed neural networks, we provide a numerical procedure to solve this integral equation in high dimension. Together with an adapted training pipeline, we give a generic set of hyperparameters that produces robust results across a wide range of kernel shapes. We conduct an extensive numerical validation on simulated data. We finally propose two applications of the method to the analysis of the microstructure of cryptocurrency markets. In a first application we extract the influence of volume on the arrival rate of BTC-USD trades and in a second application we analyze the causality relationships and their directions amongst a universe of 15 cryptocurrency pairs in a centralized exchange.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09361"
    },
    {
        "doc_id": 19,
        "title": "A closer look at the chemical potential of an ideal agent system",
        "authors": [
            "Christoph J. B\u00f6rner",
            "Ingo Hoffmann",
            "John H. Stiebel"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "Models for spin systems known from statistical physics are used in econometrics in the form of agent-based models. Econophysics research in econometrics is increasingly developing general market models that describe exchange phenomena and use the chemical potential $\u03bc$ known from physics in the context of particle number changes. In statistical physics, equations of state are known for the chemical potential, which take into account the respective model framework and the corresponding state variables. A simple transfer of these equations of state to problems in econophysics appears difficult. To the best of our knowledge, the equation of state for the chemical potential is currently missing even for the simplest conceivable model of an ideal agent system. In this paper, this research gap is closed and the equation of state for the chemical potential is derived from the econophysical model assumptions of the ideal agent system. An interpretation of the equation of state leads to fundamental relationships that could also have been guessed, but are shown here by the theory.",
        "comments": "11 Pages, 0 Figures, Working Paper, Theoretical Contribution",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09233"
    },
    {
        "doc_id": 20,
        "title": "Mean-Field SDEs driven by $G$-Brownian Motion",
        "authors": [
            "Karl-Wilhelm Georg Bollweg",
            "Thilo Meyer-Brandis"
        ],
        "subjects": [
            "Probability",
            "Mathematical Finance"
        ],
        "abstract": "We extend the notion of mean-field SDEs to SDEs driven by $G$-Brownian motion. More precisely, we consider a $G$-SDE where the coefficients depend not only on time and the current state but also on the solution as random variable.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09113"
    },
    {
        "doc_id": 21,
        "title": "AI Thrust: Ranking Emerging Powers for Tech Startup Investment in Latin America",
        "authors": [
            "Abraham Ramos Torres",
            "Laura N Montoya"
        ],
        "subjects": [
            "General Economics",
            "Risk Management"
        ],
        "abstract": "Artificial intelligence (AI) is rapidly transforming the global economy, and Latin America is no exception. In recent years, there has been a growing interest in AI development and implementation in the region. This paper presents a ranking of Latin American (LATAM) countries based on their potential to become emerging powers in AI. The ranking is based on three pillars: infrastructure, education, and finance. Infrastructure is measured by the availability of electricity, high-speed internet, the quality of telecommunications networks, and the availability of supercomputers. Education is measured by the quality of education and the research status. Finance is measured by the cost of investments, history of investments, economic metrics, and current implementation of AI.\n  While Brazil, Chile, and Mexico have established themselves as major players in the AI industry in Latin America, our ranking demonstrates the new emerging powers in the region. According to the results, Argentina, Colombia, Uruguay, Costa Rica, and Ecuador are leading as new emerging powers in AI in Latin America. These countries have strong education systems, well-developed infrastructure, and growing financial resources. The ranking provides a useful tool for policymakers, investors, and businesses interested in AI development in Latin America. It can help to identify emerging LATAM countries with the greatest potential for AI growth and success.",
        "comments": "9 pages, 4 tables, 9 figures",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09056"
    },
    {
        "doc_id": 22,
        "title": "On continuity of state-dependent utilities",
        "authors": [
            "Edoardo Berton",
            "Alessandro Doldi",
            "Marco Maggis"
        ],
        "subjects": [
            "Mathematical Finance",
            "Theoretical Economics"
        ],
        "abstract": "State-dependent preferences for a general Savage's state space were shown in Wakker and Zank (1999) to admit a numerical representation in the form of the integral of a state-dependent utility, as soon as pointwise continuity of the preference ordering is assumed. In this paper we prove that such a state-dependent function inherits pointwise continuity from the preference ordering, providing in this way a positive answer to a conjecture posed in the aforementioned seminal work. We further apply this result to obtain an explicit representation of conditional Chisini means in the form of a conditional certainty equivalent.",
        "comments": "MSC Class:          91B06; 91B08; 60A05",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09054"
    },
    {
        "doc_id": 23,
        "title": "Spurious Default Probability Projections in Credit Risk Stress Testing Models",
        "authors": [
            "Bernd Engelmann"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "Credit risk stress testing has become an important risk management device which is used both by banks internally and by regulators. Stress testing is complex because it essentially means projecting a bank's full balance sheet conditional on a macroeconomic scenario over multiple years. Part of the complexity stems from using a wide range of model parameters for, e.g., rating transition, write-off rules, prepayment, or origination of new loans. A typical parameterization of a credit risk stress test model specifies parameters linked to an average economic, the through-the-cycle, state. These parameters are transformed to a stressed state by utilizing a macroeconomic model. It will be shown that the model parameterization implies a unique through-the-cycle portfolio which is unrelated to a bank's current portfolio. Independent of the stress imposed to the model, the current portfolio will have a tendency to propagate towards the through-the-cycle portfolio. This could create unwanted spurious effects on projected portfolio default rates especially when a stress test model's parameterization is inconsistent with a bank's current portfolio.",
        "comments": "15 pages, 4 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08892"
    },
    {
        "doc_id": 24,
        "title": "Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities and Risks",
        "authors": [
            "Xihan Xiong",
            "Zhipeng Wang",
            "Xi Chen",
            "William Knottenbelt",
            "Michael Huth"
        ],
        "subjects": [
            "General Finance",
            "Cryptography and Security"
        ],
        "abstract": "Lido, the leading Liquid Staking Derivative (LSD) provider on Ethereum, allows users to stake an arbitrary amount of ETH to receive stETH, which can be integrated with Decentralized Finance (DeFi) protocols such as Aave. The composability between Lido and Aave enables a novel strategy called \"leverage staking\", where users stake ETH on Lido to acquire stETH, utilize stETH as collateral on Aave to borrow ETH, and then restake the borrowed ETH on Lido. Users can iteratively execute this process to optimize potential returns based on their risk profile.\n  This paper systematically studies the opportunities and risks associated with leverage staking. We are the first to formalize the leverage staking strategy within the Lido-Aave ecosystem. Our empirical study identifies 262 leverage staking positions on Ethereum, with an aggregated staking amount of 295,243 ETH (482M USD). We discover that 90.13% of leverage staking positions have achieved higher returns than conventional staking. Furthermore, we perform stress tests to evaluate the risk introduced by leverage staking under extreme conditions. We find that leverage staking significantly amplifies the risk of cascading liquidations. We hope this paper can inform and encourage the development of robust risk management approaches to protect the Lido-Aave LSD ecosystem.",
        "comments": " ",
        "date": "28 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08610"
    },
    {
        "doc_id": 25,
        "title": "Forking paths in financial economics",
        "authors": [
            "Guillaume Coqueret"
        ],
        "subjects": [
            "General Finance",
            "Methodology"
        ],
        "abstract": "We argue that spanning large numbers of degrees of freedom in empirical analysis allows better characterizations of effects and thus improves the trustworthiness of conclusions. Our ideas are illustrated in three studies: equity premium prediction, asset pricing anomalies and risk premia estimation. In the first, we find that each additional degree of freedom in the protocol expands the average range of $t$-statistics by at least 30%. In the second, we show that resorting to forking paths instead of bootstrapping in multiple testing raises the bar of significance for anomalies: at the 5% confidence level, the threshold for bootstrapped statistics is 4.5, whereas with paths, it is at least 8.2, a bar much higher than those currently used in the literature. In our third application, we reveal the importance of particular steps in the estimation of premia. In addition, we use paths to corroborate prior findings in the three topics. We document heterogeneity in our ability to replicate prior studies: some conclusions seem robust, others do not align with the paths we were able to generate.",
        "comments": " ",
        "date": "25 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08606"
    },
    {
        "doc_id": 26,
        "title": "Reinforcement Learning and Deep Stochastic Optimal Control for Final Quadratic Hedging",
        "authors": [
            "Bernhard Hientzsch"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "We consider two data driven approaches, Reinforcement Learning (RL) and Deep Trajectory-based Stochastic Optimal Control (DTSOC) for hedging a European call option without and with transaction cost according to a quadratic hedging P&L objective at maturity (\"variance-optimal hedging\" or \"final quadratic hedging\"). We study the performance of the two approaches under various market environments (modeled via the Black-Scholes and/or the log-normal SABR model) to understand their advantages and limitations. Without transaction costs and in the Black-Scholes model, both approaches match the performance of the variance-optimal Delta hedge. In the log-normal SABR model without transaction costs, they match the performance of the variance-optimal Barlett's Delta hedge. Agents trained on Black-Scholes trajectories with matching initial volatility but used on SABR trajectories match the performance of Bartlett's Delta hedge in average cost, but show substantially wider variance. To apply RL approaches to these problems, P&L at maturity is written as sum of step-wise contributions and variants of RL algorithms are implemented and used that minimize expectation of second moments of such sums.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2302.07996",
        "date": "20 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08600"
    },
    {
        "doc_id": 27,
        "title": "Fitting random cash management models to data",
        "authors": [
            "Francisco Salas-Molina"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "Organizations use cash management models to control balances to both avoid overdrafts and obtain a profit from short-term investments. Most management models are based on control bounds which are derived from the assumption of a particular cash flow probability distribution. In this paper, we relax this strong assumption to fit cash management models to data by means of stochastic and linear programming. We also introduce ensembles of random cash management models which are built by randomly selecting a subsequence of the original cash flow data set. We illustrate our approach by means of a real case study showing that a small random sample of data is enough to fit sufficiently good bound-based models.",
        "comments": "19 pages,6 figures, 1 table",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08548"
    },
    {
        "doc_id": 28,
        "title": "Dynamic portfolio selection under generalized disappointment aversion",
        "authors": [
            "Zongxia Liang",
            "Sheng Wang",
            "Jianming Xia",
            "Fengyi Yuan"
        ],
        "subjects": [
            "Mathematical Finance",
            "Portfolio Management"
        ],
        "abstract": "This paper addresses the continuous-time portfolio selection problem under generalized disappointment aversion (GDA). The implicit definition of the certainty equivalent within GDA preferences introduces time inconsistency to this problem. We provide the sufficient and necessary conditions for a strategy to be an equilibrium by a fully nonlinear ordinary differential equation (ODE). Through an exploration of the existence and uniqueness of solution to the ODE, we establish the existence and uniqueness of the equilibrium. Our findings indicate that under disappointment aversion (DA) preferences, non-participation in the stock market is the unique equilibrium. The numerical analysis reveals that, under GDA preferences, the investment proportion in the stock market consistently remains smaller than the investment proportion under the classical Expected Utility (EU) theory.",
        "comments": "27 pages, 4 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08323"
    },
    {
        "doc_id": 29,
        "title": "Do backrun auctions protect traders?",
        "authors": [
            "Andrew W. Macpherson"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Distributed, Parallel, and Cluster Computing",
            "Computer Science and Game Theory"
        ],
        "abstract": "We study a new \"laminated\" queueing model for orders on batched trading venues such as decentralised exchanges. The model aims to capture and generalise transaction queueing infrastructure that has arisen to organise MEV activity on public blockchains such as Ethereum, providing convenient channels for sophisticated agents to extract value by acting on end-user order flow by performing arbitrage and related HFT activities. In our model, market orders are interspersed with orders created by arbitrageurs that under idealised conditions reset the marginal price to a global equilibrium between each trade, improving predictability of execution for liquidity traders.\n  If an arbitrageur has a chance to land multiple opportunities in a row, he may attempt to manipulate the execution price of the intervening market order by a probabilistic blind sandwiching strategy. To study how bad this manipulation can get, we introduce and bound a price manipulation coefficient that measures the deviation from global equilibrium of local pricing quoted by a rational arbitrageur. We exhibit cases in which this coefficient is well approximated by a \"zeta value' with interpretable and empirically measurable parameters.",
        "comments": "Keywords: MEV, queue discipline, sandwich, CFMM, arbitrage, blockchain, Ethereum",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08302"
    },
    {
        "doc_id": 30,
        "title": "Optimal Insurance to Maximize Exponential Utility when Premium is Computed by a Convex Functional",
        "authors": [
            "Jingyi Cao",
            "Dongchen Li",
            "Virginia R. Young",
            "Bin Zou"
        ],
        "subjects": [
            "Mathematical Finance",
            "Optimization and Control",
            "Risk Management"
        ],
        "abstract": "We find the optimal indemnity to maximize the expected utility of terminal wealth of a buyer of insurance whose preferences are modeled by an exponential utility. The insurance premium is computed by a convex functional. We obtain a necessary condition for the optimal indemnity; then, because the candidate optimal indemnity is given implicitly, we use that necessary condition to develop a numerical algorithm to compute it. We prove that the numerical algorithm converges to a unique indemnity that, indeed, equals the optimal policy. We also illustrate our results with numerical examples.",
        "comments": "12 pages, 3 figures",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08094"
    },
    {
        "doc_id": 31,
        "title": "A Two-Step Longstaff Schwartz Monte Carlo Approach to Game Option Pricing",
        "authors": [
            "Ce Wang"
        ],
        "subjects": [
            "Computational Finance",
            "Pricing of Securities"
        ],
        "abstract": "We proposed a two-step Longstaff Schwartz Monte Carlo (LSMC) method with two regression models fitted at each time step to price game options. Although the original LSMC can be used to price game options with an enlarged range of path in regression and a modified cashflow updating rule, we identified a drawback of such approach, which motivated us to propose our approach. We implemented numerical examples with benchmarks using binomial tree and numerical PDE, and it showed that our method produces more reliable results comparing to the original LSMC.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08093"
    },
    {
        "doc_id": 32,
        "title": "Transformer-based approach for Ethereum Price Prediction Using Crosscurrency correlation and Sentiment Analysis",
        "authors": [
            "Shubham Singh",
            "Mayur Bhat"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Pricing of Securities"
        ],
        "abstract": "The research delves into the capabilities of a transformer-based neural network for Ethereum cryptocurrency price forecasting. The experiment runs around the hypothesis that cryptocurrency prices are strongly correlated with other cryptocurrencies and the sentiments around the cryptocurrency. The model employs a transformer architecture for several setups from single-feature scenarios to complex configurations incorporating volume, sentiment, and correlated cryptocurrency prices. Despite a smaller dataset and less complex architecture, the transformer model surpasses ANN and MLP counterparts on some parameters. The conclusion presents a hypothesis on the illusion of causality in cryptocurrency price movements driven by sentiments.",
        "comments": "12 pages",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08077"
    },
    {
        "doc_id": 33,
        "title": "Provisions and Economic Capital for Credit Losses",
        "authors": [
            "Dorinel Bastide",
            "St\u00e9phane Cr\u00e9pey"
        ],
        "subjects": [
            "Risk Management",
            "Probability",
            "General Finance"
        ],
        "abstract": "Based on supermodularity ordering properties, we show that convex risk measures of credit losses are nondecreasing  w.r.t. credit-credit and, in a wrong-way risk setup, credit-market, covariances of elliptically distributed latent factors. These results support the use of such setups for computing credit provisions and economic capital or for conducting stress test exercises and risk management analysis.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07728"
    },
    {
        "doc_id": 34,
        "title": "Cash and Card Acceptance in Retail Payments: Motivations and Factors",
        "authors": [
            "Samuel Vandak",
            "Geoffrey Goodell"
        ],
        "subjects": [
            "Computers and Society",
            "Computational Engineering, Finance, and Science",
            "General Finance"
        ],
        "abstract": "The landscape of payment methods in retail is a complex and evolving area. Vendors are motivated to conduct an appropriate analysis to decide what payment methods to accept out of a vast range of options. Many factors are included in this decision process, some qualitative and some quantitative. The following research project investigates vendors' acceptance of cards and cash from various viewpoints, all chosen to represent a novel perspective, including the barriers and preferences for each and correlations with external demographic factors. We observe that lower interchange fees, limited in this instance by the regulatory framework, play a crucial role in facilitating merchants' acceptance of card payments. The regulatory constraints on interchange fees create a favorable cost structure for merchants, making card payment adoption financially feasible. However, additional factors like technological readiness and consumer preferences might also play a significant role in their decision-making process. We also note that aggregate Merchant Service Providers (MSPs) have positively impacted the payment landscape by offering more competitive fee rates, particularly beneficial for small merchants and entrepreneurs. However, associated risks, such as account freezes or abrupt terminations, pose challenges and often lack transparency. Last, the quantitative analysis of the relationship between demographic variables and acceptance of payment types is presented. This analysis combines the current landscape of payment acceptance in the UK with data from the most recent census from 2021. We show that the unemployment rates shape card and cash acceptance, age affects contactless preference, and work-from-home impacts credit card preference.",
        "comments": "34 pages, 19 figures, 5 tables",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07682"
    },
    {
        "doc_id": 35,
        "title": "Empirical Evidence for the Fragment level Understanding on Drug Molecular Structure of LLMs",
        "authors": [
            "Xiuyuan Hu",
            "Guoqing Liu",
            "Yang Zhao",
            "Hao Zhang"
        ],
        "subjects": [
            "Machine Learning",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "AI for drug discovery has been a research hotspot in recent years, and SMILES-based language models has been increasingly applied in drug molecular design. However, no work has explored whether and how language models understand the chemical spatial structure from 1D sequences. In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments. The results indicate that language models can understand chemical structures from the perspective of molecular fragments, and the structural knowledge learned through fine-tuning is reflected in the high-frequency SMILES substrings generated by the model.",
        "comments": "Accepted by AAAI 2024 workshop: Large Language Models for Biological Discoveries (LLMs4Bio)",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07657"
    },
    {
        "doc_id": 36,
        "title": "Graph database while computationally efficient filters out quickly the ESG integrated equities in investment management",
        "authors": [
            "Partha Sen",
            "Sumana Sen"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "Design/methodology/approach This research evaluated the databases of SQL, No-SQL and graph databases to compare and contrast efficiency and performance. To perform this experiment the data were collected from multiple sources including stock price and financial news. Python is used as an interface to connect and query databases (to create database structures according to the feed file structure, to load data into tables, objects, to read data , to connect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM (Large language model) including RAG (Retrieval Augmented Generation) with Machine Learning, deep learning, NLP (natural language processing) or Decision Analytics are computationally expensive. Finding a better option to consume less resources and time to get the result. Findings The Graph database of ESG (Environmental, Social and Governance) is comparatively better and can be considered for extended analytics to integrate ESG in business and investment. Practical implications A graph ML with a RAG architecture model can be introduced as a new framework with less computationally expensive LLM application in the equity filtering process for portfolio management. Originality/value Filtering out selective stocks out of two thousand or more listed companies in any stock exchange for active investment, consuming less resource consumption especially memory and energy to integrate artificial intelligence and ESG in business and investment.",
        "comments": "10 pages, 17 figures",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07483"
    },
    {
        "doc_id": 37,
        "title": "Herd Behavior in Optimal Investment: A Dual-Agent Approach with Investment Opinion and Rational Decision Decomposition",
        "authors": [
            "Huisheng Wang",
            "H. Vicky Zhao"
        ],
        "subjects": [
            "Systems and Control",
            "Optimization and Control",
            "Mathematical Finance",
            "Portfolio Management"
        ],
        "abstract": "In this paper, we study the optimal investment problem involving two agents, where the decision of one agent is influenced by the other. To measure the distance between two agents' decisions, we introduce the average deviation. We formulate the stochastic optimal control problem considering herd behavior and derive the analytical solution through the variational method. We theoretically analyze the impact of users' herd behavior on the optimal decision by decomposing it into their rational decisions, which is called the rational decision decomposition. Furthermore, to quantify the preference for their rational decision over that of the other agent, we introduce the agent's investment opinion. Our study is validated through simulations on real stock data.",
        "comments": " ",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07183"
    },
    {
        "doc_id": 38,
        "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
        "authors": [
            "Varshini Reddy",
            "Rik Koncel-Kedziorski",
            "Viet Dac Lai",
            "Chris Tanner"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence"
        ],
        "abstract": "Research in quantitative reasoning within the financial domain indeed necessitates the use of realistic tasks and data, primarily because of the significant impact of decisions made in business and finance. Financial professionals often interact with documents hundreds of pages long, but most research datasets drastically reduce this context length. To address this, we introduce a long-document financial QA task. We augment 7,621 questions from the existing FinQA dataset with full-document context, extending the average context length for each question from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments of retrieval-based QA pipelines and long-context language models on the augmented data. Our results show that DocFinQA provides challenges for even the strongest, state-of-the-art systems.",
        "comments": "13 pages",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06915"
    },
    {
        "doc_id": 39,
        "title": "A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models",
        "authors": [
            "Emmanuil H. Georgoulis",
            "Antonis Papapantoleon",
            "Costas Smaragdakis"
        ],
        "subjects": [
            "Computational Finance",
            "Machine Learning",
            "Numerical Analysis",
            "Probability",
            "Machine Learning"
        ],
        "abstract": "We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assessed in a series of numerical experiments involving the Merton jump-diffusion model.",
        "comments": "16 pages, 11 figures",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06740"
    },
    {
        "doc_id": 40,
        "title": "Equity auction dynamics: latent liquidity models with activity acceleration",
        "authors": [
            "Mohammed Salek",
            "Damien Challet",
            "Ioane Muni Toke"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Statistical Finance"
        ],
        "abstract": "Equity auctions display several distinctive characteristics in contrast to continuous trading. As the auction time approaches, the rate of events accelerates causing a substantial liquidity buildup around the indicative price. This, in turn, results in a reduced price impact and decreased volatility of the indicative price. In this study, we adapt the latent/revealed order book framework to the specifics of equity auctions. We provide precise measurements of the model parameters, including order submissions, cancellations, and diffusion rates. Our setup allows us to describe the full dynamics of the average order book during closing auctions in Euronext Paris. These findings support the relevance of the latent liquidity framework in describing limit order book dynamics. Lastly, we analyze the factors contributing to a sub-diffusive indicative price and demonstrate the absence of indicative price predictability.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06724"
    },
    {
        "doc_id": 41,
        "title": "SpotV2Net: Multivariate Intraday Spot Volatility Forecasting via Vol-of-Vol-Informed Graph Attention Networks",
        "authors": [
            "Alessio Brini",
            "Giacomo Toscano"
        ],
        "subjects": [
            "Statistical Finance",
            "Computational Finance"
        ],
        "abstract": "This paper introduces SpotV2Net, a multivariate intraday spot volatility forecasting model based on a Graph Attention Network architecture. SpotV2Net represents financial assets as nodes within a graph and includes non-parametric high-frequency Fourier estimates of the spot volatility and co-volatility as node features. Further, it incorporates Fourier estimates of the spot volatility of volatility and co-volatility of volatility as features for node edges. We test the forecasting accuracy of SpotV2Net in an extensive empirical exercise, conducted with high-frequency prices of the components of the Dow Jones Industrial Average index. The results we obtain suggest that SpotV2Net shows improved accuracy, compared to alternative econometric and machine-learning-based models. Further, our results show that SpotV2Net maintains accuracy when performing intraday multi-step forecasts. To interpret the forecasts produced by SpotV2Net, we employ GNNExplainer, a model-agnostic interpretability tool and thereby uncover subgraphs that are critical to a node's predictions.",
        "comments": "34 pages, 9 figures",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06249"
    },
    {
        "doc_id": 42,
        "title": "CNN-DRL for Scalable Actions in Finance",
        "authors": [
            "Sina Montazeri",
            "Akram Mirzaeinia",
            "Haseebullah Jumakhan",
            "Amir Mirzaeinia"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "The published MLP-based DRL in finance has difficulties in learning the dynamics of the environment when the action scale increases. If the buying and selling increase to one thousand shares, the MLP agent will not be able to effectively adapt to the environment. To address this, we designed a CNN agent that concatenates the data from the last ninety days of the daily feature vector to create the CNN input matrix. Our extensive experiments demonstrate that the MLP-based agent experiences a loss corresponding to the initial environment setup, while our designed CNN remains stable, effectively learns the environment, and leads to an increase in rewards.",
        "comments": "10th Annual Conf. on Computational Science & Computational Intelligence",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06179"
    },
    {
        "doc_id": 43,
        "title": "CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine Learning Methods",
        "authors": [
            "Yue Chen",
            "Xingyi Andrew",
            "Salintip Supasanya"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "Historically, the economic recession often came abruptly and disastrously. For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from October 2007 to March 2009. If we could detect the signals of the crisis earlier, we could have taken preventive measures. Therefore, driven by such motivation, we use advanced machine learning techniques, including Random Forest and Extreme Gradient Boosting, to predict any potential market crashes mainly in the US market. Also, we would like to compare the performance of these methods and examine which model is better for forecasting US stock market crashes. We apply our models on the daily financial market data, which tend to be more responsive with higher reporting frequencies. We consider 75 explanatory variables, including general US stock market indexes, SP 500 sector indexes, as well as market indicators that can be used for the purpose of crisis prediction. Finally, we conclude, with selected classification metrics, that the Extreme Gradient Boosting method performs the best in predicting US stock market crisis events.",
        "comments": "14 pages, 9 figures",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06172"
    },
    {
        "doc_id": 44,
        "title": "Multimodal Gen-AI for Fundamental Investment Research",
        "authors": [
            "Lezhi Li",
            "Ting-Yu Chang",
            "Hai Wang"
        ],
        "subjects": [
            "General Finance",
            "Machine Learning"
        ],
        "abstract": "This report outlines a transformative initiative in the financial investment industry, where the conventional decision-making process, laden with labor-intensive tasks such as sifting through voluminous documents, is being reimagined. Leveraging language models, our experiments aim to automate information summarization and investment idea generation. We seek to evaluate the effectiveness of fine-tuning methods on a base model (Llama2) to achieve specific application-level goals, including providing insights into the impact of events on companies and sectors, understanding market condition relationships, generating investor-aligned investment ideas, and formatting results with stock recommendations and detailed explanations. Through state-of-the-art generative modeling techniques, the ultimate objective is to develop an AI agent prototype, liberating human investors from repetitive tasks and allowing a focus on high-level strategic thinking. The project encompasses a diverse corpus dataset, including research reports, investment memos, market news, and extensive time-series market data. We conducted three experiments applying unsupervised and supervised LoRA fine-tuning on the llama2_7b_hf_chat as the base model, as well as instruction fine-tuning on the GPT3.5 model. Statistical and human evaluations both show that the fine-tuned versions perform better in solving text modeling, summarization, reasoning, and finance domain questions, demonstrating a pivotal step towards enhancing decision-making processes in the financial domain. Code implementation for the project can be found on GitHub: https://github.com/Firenze11/finance_lm.",
        "comments": " ",
        "date": "23 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06164"
    },
    {
        "doc_id": 45,
        "title": "De novo Drug Design using Reinforcement Learning with Multiple GPT Agents",
        "authors": [
            "Xiuyuan Hu",
            "Guoqing Liu",
            "Yang Zhao",
            "Hao Zhang"
        ],
        "subjects": [
            "Biomolecules",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT.",
        "comments": "Accepted by NeurIPS 2023",
        "date": "21 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06155"
    },
    {
        "doc_id": 46,
        "title": "A Statistical Field Perspective on Capital Allocation and Accumulation: Individual dynamics",
        "authors": [
            "Pierre Gosselin",
            "A\u00efleen Lotz"
        ],
        "subjects": [
            "General Finance",
            "High Energy Physics - Theory"
        ],
        "abstract": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups. The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends to their investors. Financial capital is thus a function of dividends and stock valuations, whereas physical capital is a function of the total capital allocated by the financial sector. Whereas our previous work focused on the background fields that describe potential long-term equilibria, here we compute the transition functions of individual agents and study their probabilistic dynamics in the background field, as a function of their initial state. We show that capital accumulation depends on various factors. The probability associated with each firm's trajectories is the result of several contradictory effects: the firm tends to shift towards sectors with the greatest long-term return, but must take into account the impact of its shift on its attractiveness for investors throughout its trajectory. Since this trajectory depends largely on the average capital of transition sectors, a firm's attractiveness during its relocation depends on the relative level of capital in those sectors. Thus, an under-capitalized firm reaching a high-capital sector will experience a loss of attractiveness, and subsequently, in investors. Moreover, the firm must also consider the effects of competition in the intermediate sectors. An under-capitalized firm will tend to be ousted out towards sectors with lower average capital, while an over-capitalized firm will tend to shift towards higher averagecapital sectors. For investors, capital allocation depends on their short and long-term returns. These returns are not independent: in the short-term, returns are composed of both the firm's dividends and the increase in its stock prices. In the long-term, returns are based on the firm's growth expectations, but also, indirectly, on expectations of higher stock prices. Investors' capital allocation directly depends on the volatility of stock prices and {\\ldots}rms'dividends. Investors will tend to reallocate their capital to maximize their short and long-term returns. The higher their level of capital, the stronger the reallocation will be.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2312.16173, arXiv:2205.03087",
        "date": "30 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06142"
    },
    {
        "doc_id": 47,
        "title": "StockFormer: A Swing Trading Strategy Based on STL Decomposition and Self-Attention Networks",
        "authors": [
            "Bohan Ma",
            "Yiheng Wang",
            "Yuchao Lu",
            "Tianzixuan Hu",
            "Jinling Xu",
            "Patrick Houlihan"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Machine Learning"
        ],
        "abstract": "Amidst ongoing market recalibration and increasing investor optimism, the U.S. stock market is experiencing a resurgence, prompting the need for sophisticated tools to protect and grow portfolios. Addressing this, we introduce \"Stockformer,\" a cutting-edge deep learning framework optimized for swing trading, featuring the TopKDropout method for enhanced stock selection. By integrating STL decomposition and self-attention networks, Stockformer utilizes the S&P 500's complex data to refine stock return predictions. Our methodology entailed segmenting data for training and validation (January 2021 to January 2023) and testing (February to June 2023). During testing, Stockformer's predictions outperformed ten industry models, achieving superior precision in key predictive accuracy indicators (MAE, RMSE, MAPE), with a remarkable accuracy rate of 62.39% in detecting market trends. In our backtests, Stockformer's swing trading strategy yielded a cumulative return of 13.19% and an annualized return of 30.80%, significantly surpassing current state-of-the-art models. Stockformer has emerged as a beacon of innovation in these volatile times, offering investors a potent tool for market forecasting. To advance the field and foster community collaboration, we have open-sourced Stockformer, available at https://github.com/Eric991005/Stockformer.",
        "comments": "Currently under consideration for publication in the International Journal of Forecasting",
        "date": "22 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06139"
    },
    {
        "doc_id": 48,
        "title": "Quantum Probability Theoretic Asset Return Modeling: A Novel Schr\u00f6dinger-Like Trading Equation and Multimodal Distribution",
        "authors": [
            "Li Lin"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "Quantum theory provides a comprehensive framework for quantifying uncertainty, often applied in quantum finance to explore the stochastic nature of asset returns. This perspective likens returns to microscopic particle motion, governed by quantum probabilities akin to physical laws. However, such approaches presuppose specific microscopic quantum effects in return changes, a premise criticized for lack of guarantee. This paper diverges by asserting that quantum probability is a mathematical extension of classical probability to complex numbers. It isn't exclusively tied to microscopic quantum phenomena, bypassing the need for quantum effects in returns.By directly linking quantum probability's mathematical structure to traders' decisions and market behaviors, it avoids assuming quantum effects for returns and invoking the wave function. The complex phase of quantum probability, capturing transitions between long and short decisions while considering information interaction among traders, offers an inherent advantage over classical probability in characterizing the multimodal distribution of asset returns.Utilizing Fourier decomposition, we derive a Schr\u00f6dinger-like trading equation, where each term explicitly corresponds to implications of market trading. The equation indicates discrete energy levels in financial trading, with returns following a normal distribution at the lowest level. As the market transitions to higher trading levels, a phase shift occurs in the return distribution, leading to multimodality and fat tails. Empirical research on the Chinese stock market supports the existence of energy levels and multimodal distributions derived from this quantum probability asset returns model.",
        "comments": " ",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05823"
    },
    {
        "doc_id": 49,
        "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis",
        "authors": [
            "Frank Xing"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence",
            "Multiagent Systems",
            "General Finance"
        ],
        "abstract": "Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed.",
        "comments": "15 pages",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05799"
    },
    {
        "doc_id": 50,
        "title": "Super-hedging-pricing formulas and Immediate-Profit arbitrage for market models under random horizon",
        "authors": [
            "Tahir Choulli",
            "Emmanuel Lepinette"
        ],
        "subjects": [
            "Mathematical Finance",
            "Optimization and Control",
            "Probability",
            "Pricing of Securities"
        ],
        "abstract": "In this paper, we consider the discrete-time setting, and the market model described by (S,F,T)$. Herein F is the ``public\" flow of information which is available to all agents overtime, S is the discounted price process of d-tradable assets, and T is an arbitrary random time whose occurrence might not be observable via F. Thus, we consider the larger flow G which incorporates F and makes T an observable random time. This framework covers the credit risk theory setting, the life insurance setting and the setting of employee stock option valuation. For the stopped model (S^T,G) and for various vulnerable claims, based on this model, we address the super-hedging pricing valuation problem and its intrinsic Immediate-Profit arbitrage (IP hereafter for short). Our first main contribution lies in singling out the impact of change of prior and/or information on conditional essential supremum, which is a vital tool in super-hedging pricing. The second main contribution consists of describing as explicit as possible how the set of super-hedging prices expands under the stochasticity of T and its risks, and we address the IP arbitrage for (S^T,G) as well. The third main contribution resides in elaborating as explicit as possible pricing formulas for vulnerable claims, and singling out the various informational risks in the prices' dynamics.",
        "comments": " ",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05713"
    },
    {
        "doc_id": 51,
        "title": "Boundary conditions at infinity for Black-Scholes equations",
        "authors": [
            "Yukihiro Tsuzuki"
        ],
        "subjects": [
            "Mathematical Finance",
            "Computational Finance"
        ],
        "abstract": "We propose numerical procedures for computing the prices of forward contracts where the underlying asset price is a Markovian local martingale. If the underlying process is a strict local martingale, multiple solutions exist for the corresponding Black-Scholes equations, and the derivative prices are characterized as the minimal solutions. Our prices are upper and lower bounds obtained using numerical methods on a finite grid under the respective boundary conditions. These bounds and the boundary values converge to the exact value as the underlying price approaches infinity. The proposed procedures are demonstrated through numerical tests.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05549"
    },
    {
        "doc_id": 52,
        "title": "Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market Wraps?",
        "authors": [
            "Baptiste Lefort",
            "Eric Benhamou",
            "Jean-Jacques Ohana",
            "David Saltiel",
            "Beatrice Guez",
            "Damien Challet"
        ],
        "subjects": [
            "Statistical Finance",
            "Artificial Intelligence"
        ],
        "abstract": "We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to 2023, reposted on large financial media, to determine how global news headlines may affect stock market movements using ChatGPT and a two-stage prompt approach. We document a statistically significant positive correlation between the sentiment score and future equity market returns over short to medium term, which reverts to a negative correlation over longer horizons. Validation of this correlation pattern across multiple equity markets indicates its robustness across equity regions and resilience to non-linearity, evidenced by comparison of Pearson and Spearman correlations. Finally, we provide an estimate of the optimal horizon that strikes a balance between reactivity to new information and correlation.",
        "comments": " ",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05447"
    },
    {
        "doc_id": 53,
        "title": "An adaptive network-based approach for advanced forecasting of cryptocurrency values",
        "authors": [
            "Ali Mehrban",
            "Pegah Ahadian"
        ],
        "subjects": [
            "Statistical Finance",
            "Computational Engineering, Finance, and Science",
            "Cryptography and Security",
            "Machine Learning"
        ],
        "abstract": "This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time.",
        "comments": "11 pages",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05441"
    },
    {
        "doc_id": 54,
        "title": "Multi-relational Graph Diffusion Neural Network with Parallel Retention for Stock Trends Classification",
        "authors": [
            "Zinuo You",
            "Pengju Zhang",
            "Jin Zheng",
            "John Cartlidge"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning",
            "Neural and Evolutionary Computing"
        ],
        "abstract": "Stock trend classification remains a fundamental yet challenging task, owing to the intricate time-evolving dynamics between and within stocks. To tackle these two challenges, we propose a graph-based representation learning approach aimed at predicting the future movements of multiple stocks. Initially, we model the complex time-varying relationships between stocks by generating dynamic multi-relational stock graphs. This is achieved through a novel edge generation algorithm that leverages information entropy and signal energy to quantify the intensity and directionality of inter-stock relations on each trading day. Then, we further refine these initial graphs through a stochastic multi-relational diffusion process, adaptively learning task-optimal edges. Subsequently, we implement a decoupled representation learning scheme with parallel retention to obtain the final graph representation. This strategy better captures the unique temporal features within individual stocks while also capturing the overall structure of the stock graph. Comprehensive experiments conducted on real-world datasets from two US markets (NASDAQ and NYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the effectiveness of our method. Our approach consistently outperforms state-of-the-art baselines in forecasting next trading day stock trends across three test periods spanning seven years. Datasets and code have been released (https://github.com/pixelhero98/MGDPR).",
        "comments": "5 pages, 2 figures. Author manuscript accepted for ICASSP 2024 (IEEE International Conference on Acoustics, Speech and Signal Processing)",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05430"
    },
    {
        "doc_id": 55,
        "title": "Introduction of L0 norm and application of L1 and C1 norm in the study of time-series",
        "authors": [
            "Victor Ujaldon Garcia"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "Four markets are considered: Cryptocurrencies / South American exchange rate / Spanish Banking indices and European Indices and studied using TDA (Topological Data Analysis) tools. These tools are used to predict and showcase both strengths and weakness of the current TDA tools. In this paper a new tool $L0$ norm is defined and complemented with the already existing $C1$ norm.",
        "comments": "14 pages 8 figures",
        "date": "30 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05423"
    },
    {
        "doc_id": 56,
        "title": "Multiple-bubble testing in the cryptocurrency market: a case study of bitcoin",
        "authors": [
            "Sanaz Behzadi",
            "Mahmonir Bayanati",
            "Hamed Nozari"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "Economic periods and financial crises have highlighted the importance of evaluating financial markets to investors and researchers in recent decades.",
        "comments": " ",
        "date": "29 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05417"
    },
    {
        "doc_id": 57,
        "title": "On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors",
        "authors": [
            "Xinshuai Dong",
            "Haoyue Dai",
            "Yewen Fan",
            "Songyao Jin",
            "Sathyamoorthy Rajendran",
            "Kun Zhang"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning",
            "Methodology"
        ],
        "abstract": "Financial data is generally time series in essence and thus suffers from three fundamental issues: the mismatch in time resolution, the time-varying property of the distribution - nonstationarity, and causal factors that are important but unknown/unobserved. In this paper, we follow a causal perspective to systematically look into these three demons in finance. Specifically, we reexamine these issues in the context of causality, which gives rise to a novel and inspiring understanding of how the issues can be addressed. Following this perspective, we provide systematic solutions to these problems, which hopefully would serve as a foundation for future research in the area.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05414"
    },
    {
        "doc_id": 58,
        "title": "RIVCoin: an alternative, integrated, CeFi/DeFi-Vaulted Cryptocurrency",
        "authors": [
            "Roberto Rivera",
            "Guido Rocco",
            "Massimiliano Marzo",
            "Enrico Talin"
        ],
        "subjects": [
            "General Finance",
            "General Economics"
        ],
        "abstract": "This whitepaper introduces RIVCoin, a cryptocurrency built on Cosmos, fully stabilized by a diversified portfolio of both CeFi and DeFi assets, available in a digital, non-custodial wallet called RIV Wallet, that aims to provide Users an easy way to access the cryptocurrency markets, compliant to the strictest AML laws and regulations up to date. The token is a cryptocurrency at any time stabilized by a basket of assets: reserves are invested in a portfolio composed long term by 50% of CeFi assets, comprised of Fixed Income, Equity, Mutual and Hedge Funds and 50% of diversified strategies focused on digital assets, mainly staking and LP farming on the major, battle tested DeFi protocols. The cryptocurrency, as well as the dollar before Bretton Woods, is always fully stabilized by vaulted proof of assets: it is born and managed as a decentralized token, minted by a Decentralized Autonomous Organization, and entirely stabilized by assets evaluated by professional independent third parties. Users will trade, pool, and exchange the token without any intermediary, being able to merge them into a Liquidity Pool whose rewards will be composed by both the trading fees and the liquidity rewards derived from the reserve's seigniorage.\n  Users who wish and decide to pool RIVCoin in the Liquidity Pool will receive additional RIVCoin for themselves, and new RIVCoin are minted when the reserves increase in value or in case of purchase of new RIVCoin. The proposed model allows for alignment of incentives: decreasing the risk exposure by wealthier Users, but implicitly increasing that of smaller ones to a level perceived by them as still sustainable. Users indirectly benefit from the access to the rewards of sophisticated cryptocurrency portfolios hitherto precluded to them, without this turning into a disadvantage for the wealthy User.",
        "comments": " ",
        "date": "19 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05393"
    },
    {
        "doc_id": 59,
        "title": "Optimal Linear Signal: An Unsupervised Machine Learning Framework to Optimize PnL with Linear Signals",
        "authors": [
            "Pierre Renucci"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "This study presents an unsupervised machine learning approach for optimizing Profit and Loss (PnL) in quantitative finance. Our algorithm, akin to an unsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL generated from signals constructed linearly from exogenous variables. The methodology employs a linear relationship between exogenous variables and the trading signal, with the objective of maximizing the Sharpe Ratio through parameter optimization. Empirical application on an ETF representing U.S. Treasury bonds demonstrates the model's effectiveness, supported by regularization techniques to mitigate overfitting. The study concludes with potential avenues for further development, including generalized time steps and enhanced corrective terms.",
        "comments": "The code of the model and the empiric strategy are available on my GitHub: Cnernc/OptimalLinearSignal",
        "date": "22 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05337"
    },
    {
        "doc_id": 60,
        "title": "Comparison of Markowitz Model and Single-Index Model on Portfolio Selection of Malaysian Stocks",
        "authors": [
            "Zhang Chern Lee",
            "Wei Yun Tan",
            "Hoong Khen Koo",
            "Wilson Pang"
        ],
        "subjects": [
            "Portfolio Management"
        ],
        "abstract": "Our article is focused on the application of Markowitz Portfolio Theory and the Single Index Model on 10-year historical monthly return data for 10 stocks included in FTSE Bursa Malaysia KLCI, which is also our market index, as well as a risk-free asset which is the monthly fixed deposit rate. We will calculate the minimum variance portfolio and maximum Sharpe portfolio for both the Markowitz model and Single Index model subject to five different constraints, with the results presented in the form of tables and graphs such that comparisons between the different models and constraints can be made. We hope this article will help provide useful information for future investors who are interested in the Malaysian stock market and would like to construct an efficient investment portfolio. Keywords: Markowitz Portfolio Theory, Single Index Model, FTSE Bursa Malaysia KLCI, Efficient Portfolio",
        "comments": "19 pages, 5 figures",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05264"
    },
    {
        "doc_id": 61,
        "title": "A Mean Field Game between Informed Traders and a Broker",
        "authors": [
            "Philippe Bergault",
            "Leandro S\u00e1nchez-Betancourt"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Optimization and Control"
        ],
        "abstract": "We find closed-form solutions to the stochastic game between a broker and a mean-field of informed traders. In the finite player game, the informed traders observe a common signal and a private signal. The broker, on the other hand, observes the trading speed of each of his clients and provides liquidity to the informed traders. Each player in the game optimises wealth adjusted by inventory penalties. In the mean field version of the game, using a G\u00e2teaux derivative approach, we characterise the solution to the game with a system of forward-backward stochastic differential equations that we solve explicitly. We find that the optimal trading strategy of the broker is linear on his own inventory, on the average inventory among informed traders, and on the common signal or the average trading speed of the informed traders. The Nash equilibrium we find helps informed traders decide how to use private information, and helps brokers decide how much of the order flow they should externalise or internalise when facing a large number of clients.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05257"
    },
    {
        "doc_id": 62,
        "title": "On the Martingale Schr\u00f6dinger Bridge between Two Distributions",
        "authors": [
            "Marcel Nutz",
            "Johannes Wiesel"
        ],
        "subjects": [
            "Probability",
            "Mathematical Finance"
        ],
        "abstract": "We study a martingale Schr\u00f6dinger bridge problem: given two probability distributions, find their martingale coupling with minimal relative entropy. Our main result provides Schr\u00f6dinger potentials for this coupling. Namely, under certain conditions, the log-density of the optimal coupling is given by a triplet of real functions representing the marginal and martingale constraints. The potentials are also described as the solution of a dual problem.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05209"
    },
    {
        "doc_id": 63,
        "title": "Markowitz Portfolio Construction at Seventy",
        "authors": [
            "Stephen Boyd",
            "Kasper Johansson",
            "Ronald Kahn",
            "Philipp Schiele",
            "Thomas Schmelzer"
        ],
        "subjects": [
            "Portfolio Management",
            "Optimization and Control"
        ],
        "abstract": "More than seventy years ago Harry Markowitz formulated portfolio construction as an optimization problem that trades off expected return and risk, defined as the standard deviation of the portfolio returns. Since then the method has been extended to include many practical constraints and objective terms, such as transaction cost or leverage limits. Despite several criticisms of Markowitz's method, for example its sensitivity to poor forecasts of the return statistics, it has become the dominant quantitative method for portfolio construction in practice. In this article we describe an extension of Markowitz's method that addresses many practical effects and gracefully handles the uncertainty inherent in return statistics forecasting. Like Markowitz's original formulation, the extension is also a convex optimization problem, which can be solved with high reliability and speed.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05080"
    },
    {
        "doc_id": 64,
        "title": "Scaling Laws And Statistical Properties of The Transaction Flows And Holding Times of Bitcoin",
        "authors": [
            "Didier Sornette",
            "Yu Zhang"
        ],
        "subjects": [
            "Trading and Market Microstructure"
        ],
        "abstract": "We study the temporal evolution of the holding-time distribution of bitcoins and find that the average distribution of holding-time is a heavy-tailed power law extending from one day to over at least $200$ weeks with an exponent approximately equal to $0.9$, indicating very long memory effects. We also report significant sample-to-sample variations of the distribution of holding times, which can be best characterized as multiscaling, with power-law exponents varying between $0.3$ and $2.5$ depending on bitcoin price regimes. We document significant differences between the distributions of book-to-market and of realized returns, showing that traders obtain far from optimal performance. We also report strong direct qualitative and quantitative evidence of the disposition effect in the Bitcoin Blockchain data. Defining age-dependent transaction flows as the fraction of bitcoins that are traded at a given time and that were born (last traded) at some specific earlier time, we document that the time-averaged transaction flow fraction has a power law dependence as a function of age, with an exponent close to $-1.5$, a value compatible with priority queuing theory. We document the existence of multifractality on the measure defined as the normalized number of bitcoins exchanged at a given time.",
        "comments": " ",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04702"
    },
    {
        "doc_id": 65,
        "title": "Proof of Efficient Liquidity: A Staking Mechanism for Capital Efficient Liquidity",
        "authors": [
            "Arman Abgaryan",
            "Utkarsh Sharma",
            "Joshua Tobkin"
        ],
        "subjects": [
            "General Finance"
        ],
        "abstract": "The Proof of Efficient Liquidity (PoEL) protocol, designed for specialised Proof of Stake (PoS) consensus-based blockchain infrastructures that incorporate intrinsic DeFi applications, aims to support sustainable liquidity bootstrapping and network security. This innovative mechanism efficiently utilises budgeted staking rewards to attract and sustain liquidity through a risk structuring engine and incentive allocation strategy, both of which are designed to maximise capital efficiency. The proposed protocol seeks to serve the dual objective of - (i) capital creation, by efficiently attracting risk capital, and maximising its operational utility for intrinsic DeFi applications, thereby asserting sustainability; and (ii) enhancing the adopting blockchain network's economic security, by augmenting their staking (PoS) mechanism with a harmonious layer seeking to attract a diversity of digital assets. Finally, in the appendix, we seek to generalise the financial incentivisation protocol to the notion of service fee credits, such that it utilises the network's auxiliary services as a means to propagate incentives to attract liquidity and facilitate the network to achieve the critical mass of usage necessary for sustained operations and growth.",
        "comments": " ",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04521"
    },
    {
        "doc_id": 66,
        "title": "Computing the Gerber-Shiu function with interest and a constant dividend barrier by physics-informed neural networks",
        "authors": [
            "Zan Yu",
            "Lianzeng Zhang"
        ],
        "subjects": [
            "Numerical Analysis",
            "Probability",
            "Risk Management"
        ],
        "abstract": "In this paper, we propose a new efficient method for calculating the Gerber-Shiu discounted penalty function. Generally, the Gerber-Shiu function usually satisfies a class of integro-differential equation. We introduce the physics-informed neural networks (PINN) which embed a differential equation into the loss of the neural network using automatic differentiation. In addition, PINN is more free to set boundary conditions and does not rely on the determination of the initial value. This gives us an idea to calculate more general Gerber-Shiu functions. Numerical examples are provided to illustrate the very good performance of our approximation.",
        "comments": "23 pages; 5 figures",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04378"
    },
    {
        "doc_id": 67,
        "title": "Expiring Assets in Automated Market Makers",
        "authors": [
            "Kenan Wood",
            "Maurice Herlihy",
            "Hammurabi Mendes",
            "Jonad Pulaj"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Distributed, Parallel, and Cluster Computing",
            "Mathematical Finance",
            "Trading and Market Microstructure"
        ],
        "abstract": "An automated market maker (AMM) is a state machine that manages pools of assets, allowing parties to buy and sell those assets according to a fixed mathematical formula. AMMs are typically implemented as smart contracts on blockchains, and its prices are kept in line with the overall market price by arbitrage: if the AMM undervalues an asset with respect to the market, an \"arbitrageur\" can make a risk-free profit by buying just enough of that asset to bring the AMM's price back in line with the market.\n  AMMs, however, are not designed for assets that expire: that is, assets that cannot be produced or resold after a specified date. As assets approach expiration, arbitrage may not be able to reconcile supply and demand, and the liquidity providers that funded the AMM may have excessive exposure to risk due to rapid price variations.\n  This paper formally describes the design of a decentralized exchange (DEX) for assets that expire, combining aspects of AMMs and limit-order books. We ensure liveness and market clearance, providing mechanisms for liquidity providers to control their exposure to risk and adjust prices dynamically in response to situations where arbitrage may fail.",
        "comments": "33 pages",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04289"
    },
    {
        "doc_id": 68,
        "title": "Economic Forces in Stock Returns",
        "authors": [
            "Yue Chen",
            "Mohan Li"
        ],
        "subjects": [
            "General Economics",
            "Statistical Finance"
        ],
        "abstract": "When analyzing the components influencing the stock prices, it is commonly believed that economic activities play an important role. More specifically, asset prices are more sensitive to the systematic economic news that impose a pervasive effect on the whole market. Moreover, the investors will not be rewarded for bearing idiosyncratic risks as such risks are diversifiable. In the paper Economic Forces and the Stock Market 1986, the authors introduced an attribution model to identify the specific systematic economic forces influencing the market. They first defined and examined five classic factors from previous research papers: Industrial Production, Unanticipated Inflation, Change in Expected Inflation, Risk Premia, and The Term Structure. By adding in new factors, the Market Indices, Consumptions and Oil Prices, one by one, they examined the significant contribution of each factor to the stock return. The paper concluded that the stock returns are exposed to the systematic economic news, and they are priced with respect to their risk exposure. Also, the significant factors can be identified by simply adopting their model. Driven by such motivation, we conduct an attribution analysis based on the general framework of their model to further prove the importance of the economic factors and identify the specific identity of significant factors.",
        "comments": "11 pages, 10 figures",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04132"
    },
    {
        "doc_id": 69,
        "title": "Decomposing Smiles: A Time Change Approach",
        "authors": [
            "Liexin Cheng",
            "Xue Cheng"
        ],
        "subjects": [
            "Pricing of Securities",
            "Mathematical Finance"
        ],
        "abstract": "We develop a novel time-change approach to study the shape of implied volatility smiles. The method is applicable to common semimartingale models, including jump-diffusion, rough volatility and infinite activity models. We approximate the at-the-money skew and curvature with an improved moment-based formula. The moments are further explicitly computed under a time change framework. The limiting skew and curvature for several models are considered. We also test the accuracy of the short-term approximation results on models via numerical methods and on empirical data. Finally, we apply the method to the calibration problem.",
        "comments": " ",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03776"
    },
    {
        "doc_id": 70,
        "title": "Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection",
        "authors": [
            "Georgios Fatouros",
            "Konstantinos Metaxas",
            "John Soldatos",
            "Dimosthenis Kyriazis"
        ],
        "subjects": [
            "Computational Finance",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Computation and Language",
            "Machine Learning"
        ],
        "abstract": "In the dynamic and data-driven landscape of financial markets, this paper introduces MarketSenseAI, a novel AI-driven framework leveraging the advanced reasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI incorporates Chain of Thought and In-Context Learning methodologies to analyze a wide array of data sources, including market price dynamics, financial news, company fundamentals, and macroeconomic reports emulating the decision making process of prominent financial investment teams. The development, implementation, and empirical validation of MarketSenseAI are detailed, with a focus on its ability to provide actionable investment signals (buy, hold, sell) backed by cogent explanations. A notable aspect of this study is the use of GPT-4 not only as a predictive tool but also as an evaluator, revealing the significant impact of the AI-generated explanations on the reliability and acceptance of the suggested investment signals. In an extensive empirical evaluation with S&P 100 stocks, MarketSenseAI outperformed the benchmark index by 13%, achieving returns up to 40%, while maintaining a risk profile comparable to the market. These results demonstrate the efficacy of Large Language Models in complex financial decision-making and mark a significant advancement in the integration of AI into financial analysis and investment strategies. This research contributes to the financial AI field, presenting an innovative approach and underscoring the transformative potential of AI in revolutionizing traditional financial analysis investment methodologies.",
        "comments": "15 pages, 12 figures, 12 tables",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03737"
    },
    {
        "doc_id": 71,
        "title": "Structured factor copulas for modeling the systemic risk of European and United States banks",
        "authors": [
            "Hoang Nguyen",
            "Audron\u0117 Virbickait\u0117",
            "M. Concepci\u00f3n Aus\u00edn",
            "Pedro Galeano"
        ],
        "subjects": [
            "Statistical Finance",
            "Applications"
        ],
        "abstract": "In this paper, we employ Credit Default Swaps (CDS) to model the joint and conditional distress probabilities of banks in Europe and the U.S. using factor copulas. We propose multi-factor, structured factor, and factor-vine models where the banks in the sample are clustered according to their geographic location. We find that within each region, the co-dependence between banks is best described using both, systematic and idiosyncratic, financial contagion channels. However, if we consider the banking system as a whole, then the systematic contagion channel prevails, meaning that the distress probabilities are driven by a latent global factor and region-specific factors. In all cases, the co-dependence structure of bank CDS spreads is highly correlated in the tail. The out-of-sample forecasts of several measures of systematic risk allow us to identify the periods of distress in the banking sector over the recent years including the COVID-19 pandemic, the interest rate hikes in 2022, and the banking crisis in 2023.",
        "comments": " ",
        "date": "7 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03443"
    },
    {
        "doc_id": 72,
        "title": "Modelling and Predicting the Conditional Variance of Bitcoin Daily Returns: Comparsion of Markov Switching GARCH and SV Models",
        "authors": [
            "Dennis Koch",
            "Vahidin Jeleskovic",
            "Zahid I. Younas"
        ],
        "subjects": [
            "Statistical Finance",
            "Risk Management"
        ],
        "abstract": "This paper introduces a unique and valuable research design aimed at analyzing Bitcoin price volatility. To achieve this, a range of models from the Markov Switching-GARCH and Stochastic Autoregressive Volatility (SARV) model classes are considered and their out-of-sample forecasting performance is thoroughly examined. The paper provides insights into the rationale behind the recommendation for a two-stage estimation approach, emphasizing the separate estimation of coefficients in the mean and variance equations. The results presented in this paper indicate that Stochastic Volatility models, particularly SARV models, outperform MS-GARCH models in forecasting Bitcoin price volatility. Moreover, the study suggests that in certain situations, persistent simple GARCH models may even outperform Markov-Switching GARCH models in predicting the variance of Bitcoin log returns. These findings offer valuable guidance for risk management experts, highlighting the potential advantages of SARV models in managing and forecasting Bitcoin price volatility.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03393"
    },
    {
        "doc_id": 73,
        "title": "Volatility models in practice: Rough, Path-dependent or Markovian?",
        "authors": [
            "Eduardo Abi Jaber",
            "Shaun",
            "Li"
        ],
        "subjects": [
            "Mathematical Finance",
            "Computational Finance",
            "Pricing of Securities"
        ],
        "abstract": "An extensive empirical study of the class of Volterra Bergomi models using SPX options data between 2011 and 2022 reveals the following fact-check on two fundamental claims echoed in the rough volatility literature:\n  Do rough volatility models with Hurst index $H \\in (0,1/2)$ really capture well SPX implied volatility surface with very few parameters? No, rough volatility models are inconsistent with the global shape of SPX smiles. They suffer from severe structural limitations imposed by the roughness component, with the Hurst parameter $H \\in (0,1/2)$ controlling the smile in a poor way. In particular, the SPX at-the-money skew is incompatible with the power-law shape generated by rough volatility models. The skew of rough volatility models increases too fast on the short end, and decays too slow on the longer end where \"negative\" $H$ is sometimes needed.\n  Do rough volatility models really outperform consistently their classical Markovian counterparts? No, for short maturities they underperform their one-factor Markovian counterpart with the same number of parameters. For longer maturities, they do not systematically outperform the one-factor model and significantly underperform when compared to an under-parametrized two-factor Markovian model with only one additional calibratable parameter.\n  On the positive side: our study identifies a (non-rough) path-dependent Bergomi model and an under-parametrized two-factor Markovian Bergomi model that consistently outperform their rough counterpart in capturing SPX smiles between one week and three years with only 3 to 4 calibratable parameters. \\end{abstract}",
        "comments": " ",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03345"
    },
    {
        "doc_id": 74,
        "title": "Negatively dependent optimal risk sharing",
        "authors": [
            "Jean-Gabriel Lauzier",
            "Liyuan Lin",
            "Ruodu Wang"
        ],
        "subjects": [
            "Theoretical Economics",
            "Risk Management"
        ],
        "abstract": "We analyze the problem of optimally sharing risk using allocations that exhibit counter-monotonicity, the most extreme form of negative dependence. Counter-monotonic allocations take the form of either \"winner-takes-all\" lotteries or \"loser-loses-all\" lotteries, and we respectively refer to these (normalized) cases as jackpot or scapegoat allocations. Our main theorem, the counter-monotonic improvement theorem, states that for a given set of random variables that are either all bounded from below or all bounded from above, one can always find a set of counter-monotonic random variables such that each component is greater or equal than its counterpart in the convex order. We show that Pareto optimal allocations, if they exist, must be jackpot allocations when all agents are risk seeking. We essentially obtain the opposite when all agents have discontinuous Bernoulli utility functions, as scapegoat allocations maximize the probability of being above the discontinuity threshold. We also consider the case of rank-dependent expected utility (RDU) agents and find conditions which guarantee that RDU agents prefer jackpot allocations. We provide an application for the mining of cryptocurrencies and show that in contrast to risk-averse miners, RDU miners with small computing power never join a mining pool. Finally, we characterize the competitive equilibria with risk-seeking agents, providing a first and second fundamental theorem of welfare economics where all equilibrium allocations are jackpot allocations.",
        "comments": "35 pages, 1 figure, Keywords: Pareto optimality, Risk sharing, Counter-monotonicity, Risk seeking, Rank-dependent expected utility, Cryptocurrency mining pools",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03328"
    },
    {
        "doc_id": 75,
        "title": "Optimal Order Execution subject to Reservation Strategies under Execution Risk",
        "authors": [
            "Xue Cheng",
            "Peng Guo",
            "Tai-ho Wang"
        ],
        "subjects": [
            "Trading and Market Microstructure"
        ],
        "abstract": "The paper addresses the problem of meta order execution from a broker-dealer's point of view in Almgren-Chriss model under order fill uncertainty. A broker-dealer agency is authorized to execute an order of trading on client's behalf. The strategies that the agent is allowed to deploy is subject to a benchmark, referred to as the reservation strategy, regulated by the client. We formulate the broker's problem as a utility maximization problem in which the broker seeks to maximize his utility of excess profit-and-loss at the execution horizon. Optimal strategy in feedback form is obtained in closed form. In the absence of execution risk, the optimal strategies subject to reservation strategies are deterministic. We establish an affine structure among the trading trajectories under optimal strategies subject to general reservation strategies using implementation shortfall and target close orders as basis. We conclude the paper with numerical experiments illustrating the trading trajectories as well as histograms of terminal wealth and utility at investment horizon under optimal strategies versus those under TWAP strategies.",
        "comments": " ",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03305"
    },
    {
        "doc_id": 76,
        "title": "Synergistic Formulaic Alpha Generation for Quantitative Trading based on Reinforcement Learning",
        "authors": [
            "Hong-Gi Shin",
            "Sukhyun Jeong",
            "Eui-Yeon Kim",
            "Sungho Hong",
            "Young-Jin Cho",
            "Yong-Hoon Choi"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Artificial Intelligence"
        ],
        "abstract": "Mining of formulaic alpha factors refers to the process of discovering and developing specific factors or indicators (referred to as alpha factors) for quantitative trading in stock market. To efficiently discover alpha factors in vast search space, reinforcement learning (RL) is commonly employed. This paper proposes a method to enhance existing alpha factor mining approaches by expanding a search space and utilizing pretrained formulaic alpha set as initial seed values to generate synergistic formulaic alpha. We employ information coefficient (IC) and rank information coefficient (Rank IC) as performance evaluation metrics for the model. Using CSI300 market data, we conducted real investment simulations and observed significant performance improvement compared to existing techniques.",
        "comments": "Accepted by ICOIN 2024",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02710"
    },
    {
        "doc_id": 77,
        "title": "Displaying risk in mergers: a diagrammatic approach for exchange ratio determination",
        "authors": [
            "Alessandra Mainini",
            "Enrico Moretto",
            "Daniela Visetti"
        ],
        "subjects": [
            "General Finance"
        ],
        "abstract": "This article extends, in a stochastic setting, previous results in the determination of feasible exchange ratios for merging companies. A first outcome is that shareholders of the companies involved in the merging process face both an upper and a lower bounds for acceptable exchange ratios. Secondly, in order for the improved `bargaining region' to be intelligibly displayed, the diagrammatic approach developed by Kulpa is exploited.",
        "comments": " ",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02681"
    },
    {
        "doc_id": 78,
        "title": "Constrained Max Drawdown: a Fast and Robust Portfolio Optimization Approach",
        "authors": [
            "Albert Dorador"
        ],
        "subjects": [
            "Portfolio Management",
            "Optimization and Control"
        ],
        "abstract": "We propose an alternative linearization to the classical Markowitz quadratic portfolio optimization model, based on maximum drawdown. This model, which minimizes maximum portfolio drawdown, is particularly appealing during times of financial distress, like during the COVID-19 pandemic. In addition, we will present a Mixed-Integer Linear Programming variation of our new model that, based on our out-of-sample results and sensitivity analysis, delivers a more profitable and robust solution with a 200 times faster solving time compared to the standard Markowitz quadratic formulation.",
        "comments": " ",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02601"
    },
    {
        "doc_id": 79,
        "title": "Opinion formation in the world trade network",
        "authors": [
            "C\u00e9lestin Coquid\u00e9",
            "Jos\u00e9 Lages",
            "Dima L. Shepelyansky"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Statistical Mechanics",
            "Social and Information Networks",
            "Physics and Society"
        ],
        "abstract": "We extend the opinion formation approach to probe the world influence of economical organizations. Our opinion formation model mimics a battle between currencies within the international trade network. Based on the United Nations Comtrade database, we construct the world trade network for the years of the last decade from 2010 to 2020. We consider different core groups constituted by countries preferring to trade in a specific currency. We will consider principally two core groups, namely, 5 Anglo-Saxon countries which prefer to trade in US dollar and the 11 BRICS+ which prefer to trade in a hypothetical currency, hereafter called BRI, pegged to their economies. We determine the trade currency preference of the other countries via a Monte Carlo process depending on the direct transactions between the countries. The results obtained in the frame of this mathematical model show that starting from year 2014 the majority of the world countries would have preferred to trade in BRI than USD. The Monte Carlo process reaches a steady state with 3 distinct groups: two groups of countries preferring, whatever is the initial distribution of the trade currency preferences, to trade, one in BRI and the other in USD, and a third group of countries swinging as a whole between USD and BRI depending on the initial distribution of the trade currency preferences. We also analyze the battle between USD, EUR and BRI, and present the reduced Google matrix description of the trade relations between the Anglo-Saxon countries and the BRICS+.",
        "comments": "16 pages, 19 figures (including 9 figures present in Appendix section) and 1 table",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02378"
    },
    {
        "doc_id": 80,
        "title": "ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach",
        "authors": [
            "Zeynep Hilal Kilimci",
            "Mustafa Yalcin"
        ],
        "subjects": [
            "Biomolecules",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "Anticancer peptides (ACPs) are a class of molecules that have gained significant attention in the field of cancer research and therapy. ACPs are short chains of amino acids, the building blocks of proteins, and they possess the ability to selectively target and kill cancer cells. One of the key advantages of ACPs is their ability to selectively target cancer cells while sparing healthy cells to a greater extent. This selectivity is often attributed to differences in the surface properties of cancer cells compared to normal cells. That is why ACPs are being investigated as potential candidates for cancer therapy. ACPs may be used alone or in combination with other treatment modalities like chemotherapy and radiation therapy. While ACPs hold promise as a novel approach to cancer treatment, there are challenges to overcome, including optimizing their stability, improving selectivity, and enhancing their delivery to cancer cells, continuous increasing in number of peptide sequences, developing a reliable and precise prediction model. In this work, we propose an efficient transformer-based framework to identify anticancer peptides for by performing accurate a reliable and precise prediction model. For this purpose, four different transformer models, namely ESM, ProtBert, BioBERT, and SciBERT are employed to detect anticancer peptides from amino acid sequences. To demonstrate the contribution of the proposed framework, extensive experiments are carried on widely-used datasets in the literature, two versions of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of proposed model enhances classification accuracy when compared to the state-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of accuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and 88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.",
        "comments": " ",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02124"
    },
    {
        "doc_id": 81,
        "title": "Forecasting Bitcoin Volatility: A Comparative Analysis of Volatility Approaches",
        "authors": [
            "Cristina Chinazzo",
            "Vahidin Jeleskovic"
        ],
        "subjects": [
            "Trading and Market Microstructure"
        ],
        "abstract": "This paper conducts an extensive analysis of Bitcoin return series, with a primary focus on three volatility metrics: historical volatility (calculated as the sample standard deviation), forecasted volatility (derived from GARCH-type models), and implied volatility (computed from the emerging Bitcoin options market). These measures of volatility serve as indicators of market expectations for conditional volatility and are compared to elucidate their differences and similarities. The central finding of this study underscores a notably high expected level of volatility, both on a daily and annual basis, across all the methodologies employed. However, it's crucial to emphasize the potential challenges stemming from suboptimal liquidity in the Bitcoin options market. These liquidity constraints may lead to discrepancies in the computed values of implied volatility, particularly in scenarios involving extreme moneyness or maturity. This analysis provides valuable insights into Bitcoin's volatility landscape, shedding light on the unique characteristics and dynamics of this cryptocurrency within the context of financial markets.",
        "comments": " ",
        "date": "3 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02049"
    },
    {
        "doc_id": 82,
        "title": "Notes on the SWIFT method based on Shannon Wavelets for Option Pricing -- Revisited",
        "authors": [
            "Fabien Le Floc'h"
        ],
        "subjects": [
            "Computational Finance",
            "Numerical Analysis"
        ],
        "abstract": "This note revisits the SWIFT method based on Shannon wavelets to price European options under models with a known characteristic function in 2023. In particular, it discusses some possible improvements and exposes some concrete drawbacks of the method.",
        "comments": " ",
        "date": "7 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01758"
    },
    {
        "doc_id": 83,
        "title": "Text mining arXiv: a look through quantitative finance papers",
        "authors": [
            "Michele Leonardo Bianchi"
        ],
        "subjects": [
            "Digital Libraries",
            "Information Retrieval",
            "General Finance"
        ],
        "abstract": "This paper explores articles hosted on the arXiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research. Employing text mining techniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arXiv from 1997 to 2022. We extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain. Additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches.",
        "comments": " ",
        "date": "3 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01751"
    },
    {
        "doc_id": 84,
        "title": "Non-Atomic Arbitrage in Decentralized Finance",
        "authors": [
            "Lioba Heimbach",
            "Vabuk Pahari",
            "Eric Schertenleib"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "General Finance"
        ],
        "abstract": "The prevalence of maximal extractable value (MEV) in the Ethereum ecosystem has led to a characterization of the latter as a dark forest. Studies of MEV have thus far largely been restricted to purely on-chain MEV, i.e., sandwich attacks, cyclic arbitrage, and liquidations. In this work, we shed light on the prevalence of non-atomic arbitrage on decentralized exchanges (DEXes) on the Ethereum blockchain. Importantly, non-atomic arbitrage exploits price differences between DEXes on the Ethereum blockchain as well as exchanges outside the Ethereum blockchain (i.e., centralized exchanges or DEXes on other blockchains). Thus, non-atomic arbitrage is a type of MEV that involves actions on and off the Ethereum blockchain.\n  In our study of non-atomic arbitrage, we uncover that more than a fourth of the volume on Ethereum's biggest five DEXes from the merge until 31 October 2023 can likely be attributed to this type of MEV. We further highlight that only eleven searchers are responsible for more than 80% of the identified non-atomic arbitrage volume sitting at a staggering 137 billion US$ and draw a connection between the centralization of the block construction market and non-atomic arbitrage. Finally, we discuss the security implications of these high-value transactions that account for more than 10% of Ethereum's total block value and outline possible mitigations.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01622"
    },
    {
        "doc_id": 85,
        "title": "An arbitrage driven price dynamics of Automated Market Makers in the presence of fees",
        "authors": [
            "Joseph Najnudel",
            "Shen-Ning Tung",
            "Kazutoshi Yamazaki",
            "Ju-Yi Yen"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "We present a model for price dynamics in the Automated Market Makers (AMM) setting. Within this framework, we propose a reference market price following a geometric Brownian motion. The AMM price is constrained by upper and lower bounds, determined by constant multiplications of the reference price. Through the utilization of local times and excursion-theoretic approaches, we derive several analytical results, including its time-changed representation and limiting behavior.",
        "comments": " ",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01526"
    },
    {
        "doc_id": 86,
        "title": "Nash Equilibria in Greenhouse Gas Offset Credit Markets",
        "authors": [
            "Liam Welsh",
            "Sebastian Jaimungal"
        ],
        "subjects": [
            "General Finance",
            "Computational Finance",
            "Risk Management"
        ],
        "abstract": "In response to the global climate crisis, governments worldwide are introducing legislation to reduce greenhouse gas (GHG) emissions to help mitigate environmental catastrophes. One method to encourage emission reductions is to incentivize carbon capturing and carbon reducing projects while simultaneously penalising excess GHG output. Firms that invest in carbon capturing projects or reduce their emissions can receive offset credits (OCs) in return. These OCs can be used for regulatory purposes to offset their excess emissions in a compliance period. OCs may also be traded between firms. Thus, firms have the choice between investing in projects to generate OCs or to trade OCs. In this work, we present a novel market framework and characterise the optimal behaviour of GHG OC market participants in both single-player and two-player settings. We analyse both a single-period and multi-period setting. As the market model does not elicit a closed form solution, we develop a numerical methodology to estimate players' optimal behaviours in accordance to the Nash equilibria. Our findings indicate the actions players take are dependent on the scale of their project opportunities as well as their fellow market participants. We demonstrate the importance of behaving optimally via simulations in order to offset emission penalties and the importance of investing in GHG reducing or capturing projects from a financial perspective.",
        "comments": "MSC Class:          91G99; 35Q91; 91-08; 91A80; 91B74",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01427"
    },
    {
        "doc_id": 87,
        "title": "Accelerating Black-Box Molecular Property Optimization by Adaptively Learning Sparse Subspaces",
        "authors": [
            "Farshud Sorourifar",
            "Thomas Banker",
            "Joel A. Paulson"
        ],
        "subjects": [
            "Biomolecules",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "Molecular property optimization (MPO) problems are inherently challenging since they are formulated over discrete, unstructured spaces and the labeling process involves expensive simulations or experiments, which fundamentally limits the amount of available data. Bayesian optimization (BO) is a powerful and popular framework for efficient optimization of noisy, black-box objective functions (e.g., measured property values), thus is a potentially attractive framework for MPO. To apply BO to MPO problems, one must select a structured molecular representation that enables construction of a probabilistic surrogate model. Many molecular representations have been developed, however, they are all high-dimensional, which introduces important challenges in the BO process -- mainly because the curse of dimensionality makes it difficult to define and perform inference over a suitable class of surrogate models. This challenge has been recently addressed by learning a lower-dimensional encoding of a SMILE or graph representation of a molecule in an unsupervised manner and then performing BO in the encoded space. In this work, we show that such methods have a tendency to \"get stuck,\" which we hypothesize occurs since the mapping from the encoded space to property values is not necessarily well-modeled by a Gaussian process. We argue for an alternative approach that combines numerical molecular descriptors with a sparse axis-aligned Gaussian process model, which is capable of rapidly identifying sparse subspaces that are most relevant to modeling the unknown property function. We demonstrate that our proposed method substantially outperforms existing MPO methods on a variety of benchmark and real-world problems. Specifically, we show that our method can routinely find near-optimal molecules out of a set of more than $>100$k alternatives within 100 or fewer expensive queries.",
        "comments": "9 pages, 2 figures consisting of 6 and 4 plots, accepted to NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01398"
    },
    {
        "doc_id": 88,
        "title": "Almost Perfect Shadow Prices",
        "authors": [
            "Eberhard Mayerhofer"
        ],
        "subjects": [
            "Portfolio Management",
            "Optimization and Control",
            "Probability"
        ],
        "abstract": "Shadow prices simplify the derivation of optimal trading strategies in markets with transaction costs by transferring optimization into a more tractable, frictionless market. This paper establishes that a na\u00efve shadow price Ansatz for maximizing long term returns given average volatility yields a strategy that is, for small bid-ask-spreads, asymptotically optimal at third order. Considering the second-order impact of transaction costs, such a strategy is essentially optimal. However, for risk aversion different from one, we devise alternative strategies that outperform the shadow market at fourth order. Finally, it is shown that the risk-neutral objective rules out the existence of shadow prices.",
        "comments": "15 pages",
        "date": "1 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.00970"
    },
    {
        "doc_id": 89,
        "title": "A Portfolio's Common Causal Conditional Risk-neutral PDE",
        "authors": [
            "Alejandro Rodriguez Dominguez"
        ],
        "subjects": [
            "Portfolio Management",
            "Mathematical Finance"
        ],
        "abstract": "Portfolio's optimal drivers for diversification are common causes of the constituents' correlations. A closed-form formula for the conditional probability of the portfolio given its optimal common drivers is presented, with each pair constituent-common driver joint distribution modelled by Gaussian copulas. A conditional risk-neutral PDE is obtained for this conditional probability as a system of copulas' PDEs, allowing for dynamical risk management of a portfolio as shown in the experiments. Implied conditional portfolio volatilities and implied weights are new risk metrics that can be dynamically monitored from the PDEs or obtained from their solution.",
        "comments": "6 pages, 4 figures, Mathematical and Statistical Methods for Actuarial Sciences and Finance - MAF2024",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.00949"
    },
    {
        "doc_id": 90,
        "title": "Intraday Trading Algorithm for Predicting Cryptocurrency Price Movements Using Twitter Big Data Analysis",
        "authors": [
            "Vahidin Jeleskovic",
            "Stephen Mackay"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "Cryptocurrencies have emerged as a novel financial asset garnering significant attention in recent years. A defining characteristic of these digital currencies is their pronounced short-term market volatility, primarily influenced by widespread sentiment polarization, particularly on social media platforms such as Twitter. Recent research has underscored the correlation between sentiment expressed in various networks and the price dynamics of cryptocurrencies. This study delves into the 15-minute impact of informative tweets disseminated through foundation channels on trader behavior, with a focus on potential outcomes related to sentiment polarization. The primary objective is to identify factors that can predict positive price movements and potentially be leveraged through a trading algorithm. To accomplish this objective, we conduct a conditional examination of return and excess return rates within the 15 minutes following tweet publication. The empirical findings reveal statistically significant increases in return rates, particularly within the initial three minutes following tweet publication. Notably, adverse effects resulting from the messages were not observed. Surprisingly, sentiments were found to have no discerni-ble impact on cryptocurrency price movements. Our analysis further identifies that inves-tors are primarily influenced by the quality of tweet content, as reflected in the choice of words and tweet volume. While the basic trading algorithm presented in this study does yield some benefits within the 15-minute timeframe, these benefits are not statistically significant. Nevertheless, it serves as a foundational framework for potential enhance-ments and further investigations.",
        "comments": " ",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00603"
    },
    {
        "doc_id": 91,
        "title": "On the implied volatility of Inverse and Quanto Inverse options under stochastic volatility models",
        "authors": [
            "Elisa Al\u00f2s",
            "Eulalia Nualart",
            "Makar Pravosud"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "In this paper we study short-time behavior of the at-the-money implied volatility for Inverse and Quanto Inverse European options with fixed strike price. The asset price is assumed to follow a general stochastic volatility process. Using techniques of the Malliavin calculus such as the anticipating Ito's formula we first compute the level of the implied volatility of the option when the maturity converges to zero. Then, we find a short maturity asymptotic formula for the skew of the implied volatility that depends on the roughness of the volatility model. We apply our general results to the SABR and fractional Bergomi models, and provide some numerical simulations that confirm the accurateness of the asymptotic formula for the skew.",
        "comments": "arXiv admin note: text overlap with arXiv:2308.15341, arXiv:2208.01353",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00539"
    },
    {
        "doc_id": 92,
        "title": "Financial Time-Series Forecasting: Towards Synergizing Performance And Interpretability Within a Hybrid Machine Learning Approach",
        "authors": [
            "Shun Liu",
            "Kexin Wu",
            "Chufeng Jiang",
            "Bin Huang",
            "Danqing Ma"
        ],
        "subjects": [
            "Machine Learning",
            "Statistical Finance"
        ],
        "abstract": "In the realm of cryptocurrency, the prediction of Bitcoin prices has garnered substantial attention due to its potential impact on financial markets and investment strategies. This paper propose a comparative study on hybrid machine learning algorithms and leverage on enhancing model interpretability. Specifically, linear regression(OLS, LASSO), long-short term memory(LSTM), decision tree regressors are introduced. Through the grounded experiments, we observe linear regressor achieves the best performance among candidate models. For the interpretability, we carry out a systematic overview on the preprocessing techniques of time-series statistics, including decomposition, auto-correlational function, exponential triple forecasting, which aim to excavate latent relations and complex patterns appeared in the financial time-series forecasting. We believe this work may derive more attention and inspire more researches in the realm of time-series analysis and its realistic applications.",
        "comments": " ",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00534"
    },
    {
        "doc_id": 93,
        "title": "Optimization of portfolios with cryptocurrencies: Markowitz and GARCH-Copula model approach",
        "authors": [
            "Vahidin Jeleskovic",
            "Claudio Latini",
            "Zahid I. Younas",
            "Mamdouh A. S. Al-Faryan"
        ],
        "subjects": [
            "Portfolio Management",
            "Applications"
        ],
        "abstract": "The growing interest in cryptocurrencies has drawn the attention of the financial world to this innovative medium of exchange. This study aims to explore the impact of cryptocurrencies on portfolio performance. We conduct our analysis retrospectively, assessing the performance achieved within a specific time frame by three distinct portfolios: one consisting solely of equities, bonds, and commodities; another composed exclusively of cryptocurrencies; and a third, which combines both 'traditional' assets and the best-performing cryptocurrency from the second portfolio.To achieve this, we employ the classic variance-covariance approach, utilizing the GARCH-Copula and GARCH-Vine Copula methods to calculate the risk structure. The optimal asset weights within the optimized portfolios are determined through the Markowitz optimization problem. Our analysis predominantly reveals that the portfolio comprising both cryptocurrency and traditional assets exhibits a higher Sharpe ratio from a retrospective viewpoint and demonstrates more stable performances from a prospective perspective. We also provide an explanation for our choice of portfolio optimization based on the Markowitz approach rather than CVaR and ES.",
        "comments": " ",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00507"
    },
    {
        "doc_id": 94,
        "title": "A framework for the valuation of insurance liabilities by production cost",
        "authors": [
            "Christoph Moehr"
        ],
        "subjects": [
            "Pricing of Securities"
        ],
        "abstract": "This paper sets out a framework for the valuation of insurance liabilities that is intended to be economically realistic, elementary, reasonably practically applicable, and as a special case to provide a basis for the valuation in regulatory solvency systems such as Solvency II and the SST. The valuation framework is based on the cost of producing the liabilities to an insurance company that is subject to solvency regulation (regulatory solvency capital requirements) and insolvency laws (consequences of failure) in finite discrete time. Starting from the replication approach of classical no-arbitrage theory, the framework additionally considers the nature and cost of capital (expressed by a ``financiability condition\"), that the liabilities may be required to be fulfilled only ``in sufficiently many cases\" (expressed by a ``fulfillment condition\"), production using ``fully illiquid\" assets in addition to tradables, and the asymmetry between assets and liabilities. We identify necessary and sufficient conditions on the capital investment under which the framework recovers the market prices of tradables, investigate extending production to take account of insolvency, implications of using illiquid assets in the production, and show how Solvency II and SST valuation can be derived with specific assumptions.",
        "comments": "35 pages, no figures",
        "date": "30 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00263"
    },
    {
        "doc_id": 95,
        "title": "Enhancing CVaR portfolio optimisation performance with GAM factor models",
        "authors": [
            "Davide Lauria",
            "W. Brent Lindquist",
            "Svetlozar T. Rachev"
        ],
        "subjects": [
            "Portfolio Management"
        ],
        "abstract": "We propose a discrete-time econometric model that combines autoregressive filters with factor regressions to predict stock returns for portfolio optimisation purposes. In particular, we test both robust linear regressions and general additive models on two different investment universes composed of the Dow Jones Industrial Average and the Standard & Poor's 500 indexes, and we compare the out-of-sample performances of mean-CVaR optimal portfolios over a horizon of six years. The results show a substantial improvement in portfolio performances when the factor model is estimated with general additive models.",
        "comments": " ",
        "date": "30 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00188"
    },
    {
        "doc_id": 96,
        "title": "Representation of forward performance criteria with random endowment via FBSDE and application to forward optimized certainty equivalent",
        "authors": [
            "Gechun Liang",
            "Yifan Sun",
            "Thaleia Zariphopoulou"
        ],
        "subjects": [
            "Portfolio Management",
            "Probability"
        ],
        "abstract": "We extend the notion of forward performance criteria to settings with random endowment in incomplete markets. Building on these results, we introduce and develop the novel concept of forward optimized certainty equivalent (forward OCE), which offers a genuinely dynamic valuation mechanism that accommodates progressively adaptive market model updates, stochastic risk preferences, and incoming claims with arbitrary maturities.\n  In parallel, we develop a new methodology to analyze the emerging stochastic optimization problems by directly studying the candidate optimal control processes for both the primal and dual problems. Specifically, we derive two new systems of forward-backward stochastic differential equations (FBSDEs) and establish necessary and sufficient conditions for optimality, and various equivalences between the two problems. This new approach is general and complements the existing one based on backward stochastic partial differential equations (backward SPDEs) for the related value functions. We, also, consider representative examples for both forward performance criteria with random endowment and forward OCE, and for the case of exponential criteria, we investigate the connection between forward OCE and forward entropic risk measures.",
        "comments": "50 pages",
        "date": "29 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00103"
    },
    {
        "doc_id": 97,
        "title": "Synthetic Data Applications in Finance",
        "authors": [
            "Vamsi K. Potluru",
            "Daniel Borrajo",
            "Andrea Coletta",
            "Niccol\u00f2 Dalmasso",
            "Yousef El-Laham",
            "Elizabeth Fons",
            "Mohsen Ghassemi",
            "Sriram Gopalakrishnan",
            "Vikesh Gosai",
            "Eleonora Krea\u010di\u0107",
            "Ganapathy Mani",
            "Saheed Obitayo",
            "Deepak Paramanand",
            "Natraj Raman",
            "Mikhail Solonin",
            "Srijan Sood",
            "Svitlana Vyetrenko",
            "Haibei Zhu",
            "Manuela Veloso",
            "Tucker Balch"
        ],
        "subjects": [
            "Machine Learning",
            "General Finance"
        ],
        "abstract": "Synthetic data has made tremendous strides in various commercial settings including finance, healthcare, and virtual reality. We present a broad overview of prototypical applications of synthetic data in the financial sector and in particular provide richer details for a few select ones. These cover a wide variety of data modalities including tabular, time-series, event-series, and unstructured arising from both markets and retail financial applications. Since finance is a highly regulated industry, synthetic data is a potential approach for dealing with issues related to privacy, fairness, and explainability. Various metrics are utilized in evaluating the quality and effectiveness of our approaches in these applications. We conclude with open directions in synthetic data in the context of the financial domain.",
        "comments": "50 pages, journal submission",
        "date": "29 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00081"
    },
    {
        "doc_id": 98,
        "title": "Sector Rotation by Factor Model and Fundamental Analysis",
        "authors": [
            "Runjia Yang",
            "Beining Shi"
        ],
        "subjects": [
            "Portfolio Management"
        ],
        "abstract": "This study presents an analytical approach to sector rotation, leveraging both factor models and fundamental metrics. We initiate with a systematic classification of sectors, followed by an empirical investigation into their returns. Through factor analysis, the paper underscores the significance of momentum and short-term reversion in dictating sectoral shifts. A subsequent in-depth fundamental analysis evaluates metrics such as PE, PB, EV-to-EBITDA, Dividend Yield, among others. Our primary contribution lies in developing a predictive framework based on these fundamental indicators. The constructed models, post rigorous training, exhibit noteworthy predictive capabilities. The findings furnish a nuanced understanding of sector rotation strategies, with implications for asset management and portfolio construction in the financial domain.",
        "comments": " ",
        "date": "18 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00001"
    },
    {
        "doc_id": 99,
        "title": "Causal Discovery in Financial Markets: A Framework for Nonstationary Time-Series Data",
        "authors": [
            "Agathe Sadeghi",
            "Achintya Gopal",
            "Mohammad Fesanghary"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "A deeper comprehension of financial markets necessitates understanding not only the statistical dependencies among various entities but also the causal dependencies. This paper extends the Constraint-based Causal Discovery from Heterogeneous Data algorithm to account for lagged relationships in time-series data (an algorithm we call CD-NOTS), shedding light on the complex causal relations between different financial assets and variables. We compare the performance of different algorithmic choices, such as the choice of conditional independence test, to give general advice on the effective way to use CD-NOTS. Using the results from the simulated data, we apply CD-NOTS to a broad range of indices and factors in order to identify causal connections among the entities, thereby showing how causal discovery can serve as a valuable tool for factor-based investing, portfolio diversification, and comprehension of market dynamics. Further, we show our algorithm is a more effective alternative to other causal discovery algorithms since the assumptions of our algorithm are more realistic in terms of financial data, a conclusion we find is statistically significant.",
        "comments": "24 pages, 25 figures",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2312.17375"
    },
    {
        "doc_id": 100,
        "title": "Exploring Simple Open-Vocabulary Semantic Segmentation",
        "authors": [
            "Zihang Lai"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "Open-vocabulary semantic segmentation models aim to accurately assign a semantic label to each pixel in an image from a set of arbitrary open-vocabulary texts. In order to learn such pixel-level alignment, current approaches typically rely on a combination of (i) image-level VL model (e.g. CLIP), (ii) ground truth masks, and (iii) custom grouping encoders. In this paper, we introduce S-Seg, a novel model that can achieve surprisingly strong performance without depending on any of the above elements. S-Seg leverages pseudo-mask and language to train a MaskFormer, and can be easily trained from publicly available image-text datasets. Contrary to prior works, our model directly trains for pixel-level features and language alignment. Once trained, S-Seg generalizes well to multiple testing datasets without requiring fine-tuning. In addition, S-Seg has the extra benefits of scalability with data and consistently improvement when augmented with self-training. We believe that our simple yet effective approach will serve as a solid baseline for future research.",
        "comments": "Code is available at: https://github.com/zlai0/S-Seg",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12217"
    },
    {
        "doc_id": 101,
        "title": "Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning",
        "authors": [
            "Philip Amortila",
            "Tongyi Cao",
            "Akshay Krishnamurthy"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Optimization and Control"
        ],
        "abstract": "A pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ. As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. In this paper, we study the effect of distribution shift in the presence of model misspecification, specifically focusing on $L_{\\infty}$-misspecified regression and adversarial covariate shift, where the regression target remains fixed while the covariate distribution changes arbitrarily. We show that empirical risk minimization, or standard least squares regression, can result in undesirable misspecification amplification where the error due to misspecification is amplified by the density ratio between the training and testing distributions. As our main result, we develop a new algorithm -- inspired by robust optimization techniques -- that avoids this undesirable behavior, resulting in no misspecification amplification while still obtaining optimal statistical rates. As applications, we use this regression procedure to obtain new guarantees in offline and online reinforcement learning with misspecification and establish new separations between previously studied structural conditions and notions of coverage.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12216"
    },
    {
        "doc_id": 102,
        "title": "Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models",
        "authors": [
            "Chenyu Lian",
            "Hong-Yu Zhou",
            "Yizhou Yu",
            "Liansheng Wang"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at https://github.com/RL4M/MED-PEFT.",
        "comments": "Technical report",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12215"
    },
    {
        "doc_id": 103,
        "title": "Genericity Through Stratification",
        "authors": [
            "Victor Arrial",
            "Giulio Guerrieri",
            "Delia Kesner"
        ],
        "subjects": [
            "Logic in Computer Science",
            "Programming Languages"
        ],
        "abstract": "A fundamental issue in the $\u03bb$-calculus is to find appropriate notions for meaningfulness. Inspired by well-known results for the call-by-name $\u03bb$-calculus (CbN), where meaningful terms are identified to the solvable ones, this paper validates the challenging claim that the notion of potential valuability (aka scrutability), previously introduced in the literature, adequately represents meaningfulness in the call-by-value $\u03bb$-calculus (CbV). Akin to CbN, this claim is corroborated by proving two essential properties. The first one is genericity, stating that meaningless subterms have no bearing on evaluating normalizing terms. To prove this, we use a novel approach based on stratified reduction, indifferently applicable to CbN and CbV. The second property concerns consistency of the smallest congruence relation resulting from equating all meaningless terms (without equating all terms). We also show that such a congruence has a unique consistent and maximal extension, which coincides with a natural notion of observational equivalence. Our results thus supply the formal concepts and tools that validate the informal notion of meaningfulness underlying CbN and CbV.",
        "comments": "ACM Class:          F.3.2; F.4.1; D.3.1",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12212"
    },
    {
        "doc_id": 104,
        "title": "Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks for Accurate Bangla Sign Language Recognition",
        "authors": [
            "Haz Sameen Shahgir",
            "Khondker Salman Sayeed",
            "Md Toki Tahmid",
            "Tanjeem Azwad Zaman",
            "Md. Zarif Ul Alam"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Recent advances in Deep Learning and Computer Vision have been successfully leveraged to serve marginalized communities in various contexts. One such area is Sign Language - a primary means of communication for the deaf community. However, so far, the bulk of research efforts and investments have gone into American Sign Language, and research activity into low-resource sign languages - especially Bangla Sign Language - has lagged significantly. In this research paper, we present a new word-level Bangla Sign Language dataset - BdSL40 - consisting of 611 videos over 40 words, along with two different approaches: one with a 3D Convolutional Neural Network model and another with a novel Graph Neural Network approach for the classification of BdSL40 dataset. This is the first study on word-level BdSL recognition, and the dataset was transcribed from Indian Sign Language (ISL) using the Bangla Sign Language Dictionary (1997). The proposed GNN model achieved an F1 score of 89%. The study highlights the significant lexical and semantic similarity between BdSL, West Bengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in the literature. We release the dataset and source code to stimulate further research.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12210"
    },
    {
        "doc_id": 105,
        "title": "CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation",
        "authors": [
            "Zhihong Chen",
            "Maya Varma",
            "Jean-Benoit Delbrouck",
            "Magdalini Paschali",
            "Louis Blankemeier",
            "Dave Van Veen",
            "Jeya Maria Jose Valanarasu",
            "Alaa Youssef",
            "Joseph Paul Cohen",
            "Eduardo Pontes Reis",
            "Emily B. Tsai",
            "Andrew Johnston",
            "Cameron Olsen",
            "Tanishq Mathew Abraham",
            "Sergios Gatidis",
            "Akshay S. Chaudhari",
            "Curtis Langlotz"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Computation and Language"
        ],
        "abstract": "Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \\emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \\emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \\url{https://stanford-aimi.github.io/chexagent.html}.",
        "comments": "24 pages, 8 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12208"
    },
    {
        "doc_id": 106,
        "title": "Rate-Distortion-Perception Tradeoff Based on the Conditional-Distribution Perception Measure",
        "authors": [
            "Sadaf Salehkalaibar",
            "Jun Chen",
            "Ashish Khisti",
            "Wei Yu"
        ],
        "subjects": [
            "Information Theory",
            "Machine Learning"
        ],
        "abstract": "We study the rate-distortion-perception (RDP) tradeoff for a memoryless source model in the asymptotic limit of large block-lengths. Our perception measure is based on a divergence between the distributions of the source and reconstruction sequences conditioned on the encoder output, which was first proposed in [1], [2]. We consider the case when there is no shared randomness between the encoder and the decoder. For the case of discrete memoryless sources we derive a single-letter characterization of the RDP function, thus settling a problem that remains open for the marginal metric introduced in Blau and Michaeli [3] (with no shared randomness). Our achievability scheme is based on lossy source coding with a posterior reference map proposed in [4]. For the case of continuous valued sources under squared error distortion measure and squared quadratic Wasserstein perception measure we also derive a single-letter characterization and show that a noise-adding mechanism at the decoder suffices to achieve the optimal representation. For the case of zero perception loss, we show that our characterization interestingly coincides with the results for the marginal metric derived in [5], [6] and again demonstrate that zero perception loss can be achieved with a $3$-dB penalty in the minimum distortion. Finally we specialize our results to the case of Gaussian sources. We derive the RDP function for vector Gaussian sources and propose a waterfilling type solution. We also partially characterize the RDP function for a mixture of vector Gaussians.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12207"
    },
    {
        "doc_id": 107,
        "title": "Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization",
        "authors": [
            "Animesh Basak Chowdhury",
            "Marco Romanelli",
            "Benjamin Tan",
            "Ramesh Karri",
            "Siddharth Garg"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Hardware Architecture"
        ],
        "abstract": "Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates. The process involves a sequential application of logic minimization heuristics (``synthesis recipe\"), with their arrangement significantly impacting crucial metrics such as area and delay. Addressing the challenge posed by the broad spectrum of design complexities - from variations of past designs (e.g., adders and multipliers) to entirely novel configurations (e.g., innovative processor instructions) - requires a nuanced `synthesis recipe` guided by human expertise and intuition. This study conducts a thorough examination of learning and search techniques for logic synthesis, unearthing a surprising revelation: pre-trained agents, when confronted with entirely novel designs, may veer off course, detrimentally affecting the search trajectory. We present ABC-RL, a meticulously tuned $\u03b1$ parameter that adeptly adjusts recommendations from pre-trained agents during the search process. Computed based on similarity scores through nearest neighbor retrieval from the training dataset, ABC-RL yields superior synthesis recipes tailored for a wide array of hardware designs. Our findings showcase substantial enhancements in the Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to 24.8% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an impressive up to 9x reduction in runtime (iso-QoR) when compared to current state-of-the-art methodologies.",
        "comments": "Accepted in ICLR 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12205"
    },
    {
        "doc_id": 108,
        "title": "Unsupervised Machine Learning for the Classification of Astrophysical X-ray Sources",
        "authors": [
            "V\u00edctor Samuel P\u00e9rez-D\u00edaz",
            "Juan Rafael Mart\u00ednez-Galarza",
            "Alexander Caicedo",
            "Raffaele D'Abrusco"
        ],
        "subjects": [
            "Instrumentation and Methods for Astrophysics",
            "Artificial Intelligence"
        ],
        "abstract": "The automatic classification of X-ray detections is a necessary step in extracting astrophysical information from compiled catalogs of astrophysical sources. Classification is useful for the study of individual objects, statistics for population studies, as well as for anomaly detection, i.e., the identification of new unexplored phenomena, including transients and spectrally extreme sources. Despite the importance of this task, classification remains challenging in X-ray astronomy due to the lack of optical counterparts and representative training sets. We develop an alternative methodology that employs an unsupervised machine learning approach to provide probabilistic classes to Chandra Source Catalog sources with a limited number of labeled sources, and without ancillary information from optical and infrared catalogs. We provide a catalog of probabilistic classes for 8,756 sources, comprising a total of 14,507 detections, and demonstrate the success of the method at identifying emission from young stellar objects, as well as distinguishing between small-scale and large-scale compact accretors with a significant level of confidence. We investigate the consistency between the distribution of features among classified objects and well-established astrophysical hypotheses such as the unified AGN model. This provides interpretability to the probabilistic classifier. Code and tables are available publicly through GitHub. We provide a web playground for readers to explore our final classification at https://umlcaxs-playground.streamlit.app.",
        "comments": "21 pages, 11 figures. Accepted in MNRAS",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12203"
    },
    {
        "doc_id": 109,
        "title": "OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics",
        "authors": [
            "Peiqi Liu",
            "Yaswanth Orru",
            "Chris Paxton",
            "Nur Muhammad Mahi Shafiullah",
            "Lerrel Pinto"
        ],
        "subjects": [
            "Robotics",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot's performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. Videos of our experiments are available on our website: https://ok-robot.github.io",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12202"
    },
    {
        "doc_id": 110,
        "title": "APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference",
        "authors": [
            "Bowen Zhao",
            "Hannaneh Hajishirzi",
            "Qingqing Cao"
        ],
        "subjects": [
            "Computation and Language",
            "Machine Learning"
        ],
        "abstract": "Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%.",
        "comments": "19 pages, 6 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12200"
    },
    {
        "doc_id": 111,
        "title": "LONEStar: The Lunar Flashlight Optical Navigation Experiment",
        "authors": [
            "Michael Krause",
            "Ava Thrasher",
            "Priyal Soni",
            "Liam Smego",
            "Reuben Isaac",
            "Jennifer Nolan",
            "Micah Pledger",
            "E. Glenn Lightsey",
            "W. Jud Ready",
            "John Christian"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Instrumentation and Methods for Astrophysics",
            "Space Physics"
        ],
        "abstract": "This paper documents the results from the highly successful Lunar flashlight Optical Navigation Experiment with a Star tracker (LONEStar). Launched in December 2022, Lunar Flashlight (LF) was a NASA-funded technology demonstration mission. After a propulsion system anomaly prevented capture in lunar orbit, LF was ejected from the Earth-Moon system and into heliocentric space. NASA subsequently transferred ownership of LF to Georgia Tech to conduct an unfunded extended mission to demonstrate further advanced technology objectives, including LONEStar. From August-December 2023, the LONEStar team performed on-orbit calibration of the optical instrument and a number of different OPNAV experiments. This campaign included the processing of nearly 400 images of star fields, Earth and Moon, and four other planets (Mercury, Mars, Jupiter, and Saturn). LONEStar provided the first on-orbit demonstrations of heliocentric navigation using only optical observations of planets. Of special note is the successful in-flight demonstration of (1) instantaneous triangulation with simultaneous sightings of two planets with the LOST algorithm and (2) dynamic triangulation with sequential sightings of multiple planets.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12198"
    },
    {
        "doc_id": 112,
        "title": "Programmable EM Sensor Array for Golden-Model Free Run-time Trojan Detection and Localization",
        "authors": [
            "Hanqiu Wang",
            "Max Panoff",
            "Zihao Zhan",
            "Shuo Wang",
            "Christophe Bobda",
            "Domenic Forte"
        ],
        "subjects": [
            "Cryptography and Security",
            "Signal Processing"
        ],
        "abstract": "Side-channel analysis has been proven effective at detecting hardware Trojans in integrated circuits (ICs). However, most detection techniques rely on large external probes and antennas for data collection and require a long measurement time to detect Trojans. Such limitations make these techniques impractical for run-time deployment and ineffective in detecting small Trojans with subtle side-channel signatures. To overcome these challenges, we propose a Programmable Sensor Array (PSA) for run-time hardware Trojan detection, localization, and identification. PSA is a tampering-resilient integrated on-chip magnetic field sensor array that can be re-programmed to change the sensors' shape, size, and location. Using PSA, EM side-channel measurement results collected from sensors at different locations on an IC can be analyzed to localize and identify the Trojan. The PSA has better performance than conventional external magnetic probes and state-of-the-art on-chip single-coil magnetic field sensors. We fabricated an AES-128 test chip with four AES Hardware Trojans. They were successfully detected, located, and identified with the proposed on-chip PSA within 10 milliseconds using our proposed cross-domain analysis.",
        "comments": "6 pages, 5 figures, Accepted at DATE2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12193"
    },
    {
        "doc_id": 113,
        "title": "Text Embedding Inversion Attacks on Multilingual Language Models",
        "authors": [
            "Yiyi Chen",
            "Heather Lent",
            "Johannes Bjerva"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence",
            "Cryptography and Security"
        ],
        "abstract": "Representing textual information as real-numbered embeddings has become the norm in NLP. Moreover, with the rise of public interest in large language models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a business model. This is not without outstanding security risks, as previous research has demonstrated that sensitive data can be reconstructed from embeddings, even without knowledge of the underlying model that generated them. However, such work is limited by its sole focus on English, leaving all other languages vulnerable to attacks by malicious actors. %As many international and multilingual companies leverage EaaS, there is an urgent need for research into multilingual LLM security. To this end, this work investigates LLM security from the perspective of multilingual embedding inversion. Concretely, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario. Our findings reveal that multilingual models are potentially more vulnerable to inversion attacks than their monolingual counterparts. This stems from the reduced data requirements for achieving comparable inversion performance in settings where the underlying language is not known a-priori. To our knowledge, this work is the first to delve into multilinguality within the context of inversion attacks, and our findings highlight the need for further investigation and enhanced defenses in the area of NLP Security.",
        "comments": "13 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12192"
    },
    {
        "doc_id": 114,
        "title": "WARM: On the Benefits of Weight Averaged Reward Models",
        "authors": [
            "Alexandre Ram\u00e9",
            "Nino Vieillard",
            "L\u00e9onard Hussenot",
            "Robert Dadashi",
            "Geoffrey Cideron",
            "Olivier Bachem",
            "Johan Ferret"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ],
        "abstract": "Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM.",
        "comments": "14 pages, 9 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12187"
    },
    {
        "doc_id": 115,
        "title": "Is Your Kettle Smarter Than a Hacker? A Scalable Tool for Assessing Replay Attack Vulnerabilities on Consumer IoT Devices",
        "authors": [
            "Sara Lazzaro",
            "Vincenzo De Angelis",
            "Anna Maria Mandalari",
            "Francesco Buccafurri"
        ],
        "subjects": [
            "Cryptography and Security"
        ],
        "abstract": "Consumer Internet of Things (IoT) devices often leverage the local network to communicate with the corresponding companion app or other devices. This has benefits in terms of efficiency since it offloads the cloud. ENISA and NIST security guidelines underscore the importance of enabling default local communication for safety and reliability. Indeed, an IoT device should continue to function in case the cloud connection is not available. While the security of cloud-device connections is typically strengthened through the usage of standard protocols, local connectivity security is frequently overlooked. Neglecting the security of local communication opens doors to various threats, including replay attacks. In this paper, we investigate this class of attacks by designing a systematic methodology for automatically testing IoT devices vulnerability to replay attacks. Specifically, we propose a tool, named REPLIOT, able to test whether a replay attack is successful or not, without prior knowledge of the target devices. We perform thousands of automated experiments using popular commercial devices spanning various vendors and categories. Notably, our study reveals that among these devices, 51% of them do not support local connectivity, thus they are not compliant with the reliability and safety requirements of the ENISA/NIST guidelines. We find that 75% of the remaining devices are vulnerable to replay attacks with REPLIOT having a detection accuracy of 0.98-1. Finally, we investigate the possible causes of this vulnerability, discussing possible mitigation strategies.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12184"
    },
    {
        "doc_id": 116,
        "title": "Tracking before detection using partial orders and optimization",
        "authors": [
            "Michael Robinson",
            "Michael Stein",
            "Henry S. Owen"
        ],
        "subjects": [
            "Dynamical Systems",
            "Computational Engineering, Finance, and Science"
        ],
        "abstract": "This article addresses the problem of multi-object tracking by using a non-deterministic model of target behaviors with hard constraints. To capture the evolution of target features as well as their locations, we permit objects to lie in a general topological target configuration space, rather than a Euclidean space. We obtain tracker performance bounds based on sample rates, and derive a flexible, agnostic tracking algorithm. We demonstrate our algorithm on two scenarios involving laboratory and field data.",
        "comments": "MSC Class:          37N99",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12182"
    },
    {
        "doc_id": 117,
        "title": "Universal Neurons in GPT2 Language Models",
        "authors": [
            "Wes Gurnee",
            "Theo Horsley",
            "Zifan Carl Guo",
            "Tara Rezaei Kheirkhah",
            "Qinyi Sun",
            "Will Hathaway",
            "Neel Nanda",
            "Dimitris Bertsimas"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ],
        "abstract": "A basic question within the emerging field of mechanistic interpretability is the degree to which neural networks learn the same underlying mechanisms. In other words, are neural mechanisms universal across different models? In this work, we study the universality of individual neurons across GPT2 models trained from different initial random seeds, motivated by the hypothesis that universal neurons are likely to be interpretable. In particular, we compute pairwise correlations of neuron activations over 100 million tokens for every neuron pair across five different seeds and find that 1-5\\% of neurons are universal, that is, pairs of neurons which consistently activate on the same inputs. We then study these universal neurons in detail, finding that they usually have clear interpretations and taxonomize them into a small number of neuron families. We conclude by studying patterns in neuron weights to establish several universal functional roles of neurons in simple circuits: deactivating attention heads, changing the entropy of the next token distribution, and predicting the next token to (not) be within a particular set.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12181"
    },
    {
        "doc_id": 118,
        "title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation",
        "authors": [
            "Zachary Novack",
            "Julian McAuley",
            "Taylor Berg-Kirkpatrick",
            "Nicholas J. Bryan"
        ],
        "subjects": [
            "Sound",
            "Artificial Intelligence",
            "Machine Learning",
            "Audio and Speech Processing"
        ],
        "abstract": "We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose frame-work for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents. Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency. We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control - all without ever fine-tuning the underlying model. When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models. Sound examples can be found at https://DITTO-Music.github.io/web/.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12179"
    },
    {
        "doc_id": 119,
        "title": "In-Context Learning for Extreme Multi-Label Classification",
        "authors": [
            "Karel D'Oosterlinck",
            "Omar Khattab",
            "Fran\u00e7ois Remy",
            "Thomas Demeester",
            "Chris Develder",
            "Christopher Potts"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence"
        ],
        "abstract": "Multi-label classification problems with thousands of classes are hard to solve with in-context learning alone, as language models (LMs) might lack prior knowledge about the precise classes or how to assign them, and it is generally infeasible to demonstrate every class in a prompt. We propose a general program, $\\texttt{Infer--Retrieve--Rank}$, that defines multi-step interactions between LMs and retrievers to efficiently tackle such problems. We implement this program using the $\\texttt{DSPy}$ programming model, which specifies in-context systems in a declarative manner, and use $\\texttt{DSPy}$ optimizers to tune it towards specific datasets by bootstrapping only tens of few-shot examples. Our primary extreme classification program, optimized separately for each task, attains state-of-the-art results across three benchmarks (HOUSE, TECH, TECHWOLF). We apply the same program to a benchmark with vastly different characteristics and attain competitive performance as well (BioDEX). Unlike prior work, our proposed solution requires no finetuning, is easily applicable to new tasks, alleviates prompt engineering, and requires only tens of labeled examples. Our code is public at https://github.com/KarelDO/xmc.dspy.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12178"
    },
    {
        "doc_id": 120,
        "title": "Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses",
        "authors": [
            "Tahereh Zarrat Ehsan",
            "Seyed Mehdi Mohtavipour"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ],
        "abstract": "Detecting anomalies in poultry houses is crucial for maintaining optimal chicken health conditions, minimizing economic losses and bolstering profitability. This paper presents a novel real-time framework for analyzing chicken behavior in cage-free poultry houses to detect abnormal behaviors. Specifically, two significant abnormalities, namely inactive broiler and huddling behavior, are investigated in this study. The proposed framework comprises three key steps: (1) chicken detection utilizing a state-of-the-art deep learning model, (2) tracking individual chickens across consecutive frames with a fast tracker module, and (3) detecting abnormal behaviors within the video stream. Experimental studies are conducted to evaluate the efficacy of the proposed algorithm in accurately assessing chicken behavior. The results illustrate that our framework provides a precise and efficient solution for real-time anomaly detection, facilitating timely interventions to maintain chicken health and enhance overall productivity on poultry farms. Github: https://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis",
        "comments": "11 pages, 7 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12176"
    },
    {
        "doc_id": 121,
        "title": "Single-View 3D Human Digitalization with Large Reconstruction Models",
        "authors": [
            "Zhenzhen Weng",
            "Jingyuan Liu",
            "Hao Tan",
            "Zhan Xu",
            "Yang Zhou",
            "Serena Yeung-Levy",
            "Jimei Yang"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "In this paper, we introduce Human-LRM, a single-stage feed-forward Large Reconstruction Model designed to predict human Neural Radiance Fields (NeRF) from a single image. Our approach demonstrates remarkable adaptability in training using extensive datasets containing 3D scans and multi-view capture. Furthermore, to enhance the model's applicability for in-the-wild scenarios especially with occlusions, we propose a novel strategy that distills multi-view reconstruction into single-view via a conditional triplane diffusion model. This generative extension addresses the inherent variations in human body shapes when observed from a single view, and makes it possible to reconstruct the full body human from an occluded image. Through extensive experiments, we show that Human-LRM surpasses previous methods by a significant margin on several benchmarks.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12175"
    },
    {
        "doc_id": 122,
        "title": "IoT-Based Wireless Networkingfor Seismic Applications",
        "authors": [
            "Hadi Jamali-Rad",
            "Xander Campman"
        ],
        "subjects": [
            "Distributed, Parallel, and Cluster Computing"
        ],
        "abstract": "We propose to employ a recently developed IoT-based wireless technology, so called low-power wide-area networks (LPWANs), to exploit their long range, low power, and inherent compatibility to cloud storage and computing. We create a remotely-operated minimum-maintenance wireless solution for four major seismic applications of interest. By proposing appropriate network architecture and data coordination (aggregation and transmission) designs we show that neither the low data-rate nor the low duty-cycle of LPWANs impose fundamental issues in handling a considerable amount of data created by complex seismic scenarios as long as the application is delay-tolerant. In order to confirm this claim, we cast our ideas into a practical large-scale networking design for simultaneous seismic monitoring and interferometry and carry out an analysis on the data generation and transmission rates. Finally, we present some results from a small-scale field test in which we have employed our IoT-based wireless nodes for real-time seismic quality control (QC) over clouds.",
        "comments": "Journal ref:        Geophysical Prospecting 2018",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12174"
    },
    {
        "doc_id": 123,
        "title": "Natural Strategic Ability in Stochastic Multi-Agent Systems",
        "authors": [
            "Rapha\u00ebl Berthon",
            "Joost-Pieter Katoen",
            "Munyque Mittelmann",
            "Aniello Murano"
        ],
        "subjects": [
            "Logic in Computer Science",
            "Artificial Intelligence"
        ],
        "abstract": "Strategies synthesized using formal methods can be complex and often require infinite memory, which does not correspond to the expected behavior when trying to model Multi-Agent Systems (MAS). To capture such behaviors, natural strategies are a recently proposed framework striking a balance between the ability of agents to strategize with memory and the model-checking complexity, but until now has been restricted to fully deterministic settings. For the first time, we consider the probabilistic temporal logics PATL and PATL* under natural strategies (NatPATL and NatPATL*, resp.). As main result we show that, in stochastic MAS, NatPATL model-checking is NP-complete when the active coalition is restricted to deterministic strategies. We also give a 2NEXPTIME complexity result for NatPATL* with the same restriction. In the unrestricted case, we give an EXPSPACE complexity for NatPATL and 3EXPSPACE complexity for NatPATL*.",
        "comments": "Extended version of the paper accepted at AAAI 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12170"
    },
    {
        "doc_id": 124,
        "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities",
        "authors": [
            "Boyuan Chen",
            "Zhuo Xu",
            "Sean Kirmani",
            "Brian Ichter",
            "Danny Driess",
            "Pete Florence",
            "Dorsa Sadigh",
            "Leonidas Guibas",
            "Fei Xia"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Computation and Language",
            "Machine Learning",
            "Robotics"
        ],
        "abstract": "Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12168"
    },
    {
        "doc_id": 125,
        "title": "Dynamic Semantic Compression for CNN Inference in Multi-access Edge Computing: A Graph Reinforcement Learning-based Autoencoder",
        "authors": [
            "Nan Li",
            "Alexandros Iosifidis",
            "Qi Zhang"
        ],
        "subjects": [
            "Image and Video Processing",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "This paper studies the computational offloading of CNN inference in dynamic multi-access edge computing (MEC) networks. To address the uncertainties in communication time and computation resource availability, we propose a novel semantic compression method, autoencoder-based CNN architecture (AECNN), for effective semantic extraction and compression in partial offloading. In the semantic encoder, we introduce a feature compression module based on the channel attention mechanism in CNNs, to compress intermediate data by selecting the most informative features. In the semantic decoder, we design a lightweight decoder to reconstruct the intermediate data through learning from the received compressed data to improve accuracy. To effectively trade-off communication, computation, and inference accuracy, we design a reward function and formulate the offloading problem of CNN inference as a maximization problem with the goal of maximizing the average inference accuracy and throughput over the long term. To address this maximization problem, we propose a graph reinforcement learning-based AECNN (GRL-AECNN) method, which outperforms existing works DROO-AECNN, GRL-BottleNet++ and GRL-DeepJSCC under different dynamic scenarios. This highlights the advantages of GRL-AECNN in offloading decision-making in dynamic MEC.",
        "comments": "arXiv admin note: text overlap with arXiv:2211.13745",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12167"
    },
    {
        "doc_id": 126,
        "title": "Semi-supervised segmentation of land cover images using nonlinear canonical correlation analysis with multiple features and t-SNE",
        "authors": [
            "Hong Wei",
            "James Xiao",
            "Yichao Zhang",
            "Xia Hong"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ],
        "abstract": "Image segmentation is a clustering task whereby each pixel is assigned a cluster label. Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available. This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple bands and a local patch as appropriate. Similarly, multiple types of texture features based on a pixel's local patch would also be beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount of ground truth, then training a supervised model, which is sometimes impractical. In this work, by resorting to label only a small quantity of pixels, a new semi-supervised segmentation approach is proposed. Initially, over all pixels, an image data matrix is created in high dimensional feature space. Then, t-SNE projects the high dimensional data onto 3D embedding. By using radial basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix via the small labelled data set. The associated canonical variables, obtained for the full image, are applied by k-means clustering algorithm. The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral images, demonstrating excellent segmentation results.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12164"
    },
    {
        "doc_id": 127,
        "title": "Automated facial recognition system using deep learning for pain assessment in adults with cerebral palsy",
        "authors": [
            "\u00c1lvaro Sabater-G\u00e1rriz",
            "F. Xavier Gaya-Morey",
            "Jos\u00e9 Mar\u00eda Buades-Rubio",
            "Cristina Manresa Yee",
            "Pedro Montoya",
            "Inmaculada Riquelme"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Background: Pain assessment in individuals with neurological conditions, especially those with limited self-report ability and altered facial expressions, presents challenges. Existing measures, relying on direct observation by caregivers, lack sensitivity and specificity. In cerebral palsy, pain is a common comorbidity and a reliable evaluation protocol is crucial. Thus, having an automatic system that recognizes facial expressions could be of enormous help when diagnosing pain in this type of patient.\n  Objectives: 1) to build a dataset of facial pain expressions in individuals with cerebral palsy, and 2) to develop an automated facial recognition system based on deep learning for pain assessment addressed to this population.\n  Methods: Ten neural networks were trained on three pain image databases, including the UNBC-McMaster Shoulder Pain Expression Archive Database, the Multimodal Intensity Pain Dataset, and the Delaware Pain Database. Additionally, a curated dataset (CPPAIN) was created, consisting of 109 preprocessed facial pain expression images from individuals with cerebral palsy, categorized by two physiotherapists using the Facial Action Coding System observational scale.\n  Results: InceptionV3 exhibited promising performance on the CP-PAIN dataset, achieving an accuracy of 62.67% and an F1 score of 61.12%. Explainable artificial intelligence techniques revealed consistent essential features for pain identification across models.\n  Conclusion: This study demonstrates the potential of deep learning models for robust pain detection in populations with neurological conditions and communication disabilities. The creation of a larger dataset specific to cerebral palsy would further enhance model accuracy, offering a valuable tool for discerning subtle and idiosyncratic pain expressions. The insights gained could extend to other complex neurological conditions.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12161"
    },
    {
        "doc_id": 128,
        "title": "Transcending To Notions",
        "authors": [
            "Sama Sai Karthik",
            "Jayati Deshmukh",
            "Srinath Srinivasa"
        ],
        "subjects": [
            "Multiagent Systems"
        ],
        "abstract": "Social identities play an important role in the dynamics of human societies, and it can be argued that some sense of identification with a larger cause or idea plays a critical role in making humans act responsibly. Often social activists strive to get populations to identify with some cause or notion -- like green energy, diversity, etc. in order to bring about desired social changes. We explore the problem of designing computational models for social identities in the context of autonomous AI agents. For this, we propose an agent model that enables agents to identify with certain notions and show how this affects collective outcomes. We also contrast between associations of identity with rational preferences. The proposed model is simulated in an application context of urban mobility, where we show how changes in social identity affect mobility patterns and collective outcomes.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12159"
    },
    {
        "doc_id": 129,
        "title": "Extension property for partial automorphisms of the $n$-partite and semigeneric tournaments",
        "authors": [
            "Jan Hubi\u010dka",
            "Colin Jahel",
            "Mat\u011bj Kone\u010dn\u00fd",
            "Marcin Sabok"
        ],
        "subjects": [
            "Combinatorics",
            "Discrete Mathematics",
            "Logic"
        ],
        "abstract": "We present a proof of the extension property for partial automorphisms (EPPA) for classes of finite $n$-partite tournaments for $n \\in \\{2,3,\\ldots,\u03c9\\}$, and for the class of finite semigeneric tournaments. We also prove that the generic $\u03c9$-partite tournament and the generic semigeneric tournament have ample generics.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12153"
    },
    {
        "doc_id": 130,
        "title": "Uncoded Storage Coded Transmission Elastic Computing with Straggler Tolerance in Heterogeneous Systems",
        "authors": [
            "Xi Zhong",
            "Joerg Kliewer",
            "Mingyue Ji"
        ],
        "subjects": [
            "Information Theory",
            "Distributed, Parallel, and Cluster Computing",
            "Optimization and Control"
        ],
        "abstract": "In 2018, Yang et al. introduced a novel and effective approach, using maximum distance separable (MDS) codes, to mitigate the impact of elasticity in cloud computing systems. This approach is referred to as coded elastic computing. Some limitations of this approach include that it assumes all virtual machines have the same computing speeds and storage capacities, and it cannot tolerate stragglers for matrix-matrix multiplications. In order to resolve these limitations, in this paper, we introduce a new combinatorial optimization framework, named uncoded storage coded transmission elastic computing (USCTEC), for heterogeneous speeds and storage constraints, aiming to minimize the expected computation time for matrix-matrix multiplications, under the consideration of straggler tolerance. Within this framework, we propose optimal solutions with straggler tolerance under relaxed storage constraints. Moreover, we propose a heuristic algorithm that considers the heterogeneous storage constraints. Our results demonstrate that the proposed algorithm outperforms baseline solutions utilizing cyclic storage placements, in terms of both expected computation time and storage size.",
        "comments": "6 pages, 1 figure, accepted in ICC 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12151"
    },
    {
        "doc_id": 131,
        "title": "Personalized Over-the-Air Federated Learning with Personalized Reconfigurable Intelligent Surfaces",
        "authors": [
            "Jiayu Mao",
            "Aylin Yener"
        ],
        "subjects": [
            "Information Theory",
            "Machine Learning"
        ],
        "abstract": "Over-the-air federated learning (OTA-FL) provides bandwidth-efficient learning by leveraging the inherent superposition property of wireless channels. Personalized federated learning balances performance for users with diverse datasets, addressing real-life data heterogeneity. We propose the first personalized OTA-FL scheme through multi-task learning, assisted by personal reconfigurable intelligent surfaces (RIS) for each user. We take a cross-layer approach that optimizes communication and computation resources for global and personalized tasks in time-varying channels with imperfect channel state information, using multi-task learning for non-i.i.d data. Our PROAR-PFed algorithm adaptively designs power, local iterations, and RIS configurations. We present convergence analysis for non-convex objectives and demonstrate that PROAR-PFed outperforms state-of-the-art on the Fashion-MNIST dataset.",
        "comments": "Copyright 2024 IEEE. Published in ICASSP 2024, 14-19 April, Seoul, Korea. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12149"
    },
    {
        "doc_id": 132,
        "title": "Anisotropy Is Inherent to Self-Attention in Transformers",
        "authors": [
            "Nathan Godey",
            "\u00c9ric de la Clergerie",
            "Beno\u00eet Sagot"
        ],
        "subjects": [
            "Computation and Language"
        ],
        "abstract": "The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations suggest that anisotropy is actually inherent to Transformers-based models.",
        "comments": "Proceedings of EACL 2024. Previously presented at ACL-SRW 2023 (arXiv:2306.07656). arXiv admin note: substantial text overlap with arXiv:2306.07656",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12143"
    },
    {
        "doc_id": 133,
        "title": "Spin Wave Threshold Gate",
        "authors": [
            "Arne Van Zegbroeck",
            "Pantazis Anagnostou",
            "Said Hamdioui",
            "Christop Adelmann",
            "Florin Ciubotaru",
            "Sorin Cotofana"
        ],
        "subjects": [
            "Emerging Technologies"
        ],
        "abstract": "While Spin Waves (SW) interaction provides natural support for low power Majority (MAJ) gate implementations many hurdles still exists on the road towards the realization of practically relevant SW circuits. In this paper we leave the SW interaction avenue and propose Threshold Logic (TL) inspired SW computing, which relies on successive phase rotations applied to one single SW instead of on the interference of an odd number of SWs. After providing a short TL inside we introduce the SW TL gate concept and discuss the way to mirror TL gate weight and threshold values into physical phase-shifter parameters. Subsequently, we design and demonstrate proper operation of a SW TL based Full Adder (FA) by means of micro-magnetic simulations. We conclude the paper by providing inside on the potential advantages of our proposal by means of a conceptual comparison of MAJ and TL based FA implementations.",
        "comments": "This work has received funding from the Horizon Europe research and innovation program within the project \"Spider\" (grant agreement no. 101070417)",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12136"
    },
    {
        "doc_id": 134,
        "title": "Accelerating Continuous Variable Coherent Ising Machines via Momentum",
        "authors": [
            "Robin Brown",
            "Davide Venturelli",
            "Marco Pavone",
            "David E. Bernal Neira"
        ],
        "subjects": [
            "Optimization and Control",
            "Emerging Technologies",
            "Quantum Physics"
        ],
        "abstract": "The Coherent Ising Machine (CIM) is a non-conventional architecture that takes inspiration from physical annealing processes to solve Ising problems heuristically. Its dynamics are naturally continuous and described by a set of ordinary differential equations that have been proven to be useful for the optimization of continuous variables non-convex quadratic optimization problems. The dynamics of such Continuous Variable CIMs (CV-CIM) encourage optimization via optical pulses whose amplitudes are determined by the negative gradient of the objective; however, standard gradient descent is known to be trapped by local minima and hampered by poor problem conditioning. In this work, we propose to modify the CV-CIM dynamics using more sophisticated pulse injections based on tried-and-true optimization techniques such as momentum and Adam. Through numerical experiments, we show that the momentum and Adam updates can significantly speed up the CV-CIM's convergence and improve sample diversity over the original CV-CIM dynamics. We also find that the Adam-CV-CIM's performance is more stable as a function of feedback strength, especially on poorly conditioned instances, resulting in an algorithm that is more robust, reliable, and easily tunable. More broadly, we identify the CIM dynamical framework as a fertile opportunity for exploring the intersection of classical optimization and modern analog computing.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12135"
    },
    {
        "doc_id": 135,
        "title": "VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games",
        "authors": [
            "He Zhang",
            "Xinyang Li",
            "Yuanxi Sun",
            "Xinyi Fu",
            "Christine Qiu",
            "John M. Carroll"
        ],
        "subjects": [
            "Human-Computer Interaction",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "Understanding and recognizing emotions are important and challenging issues in the metaverse era. Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications. In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players. We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively. We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets. The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope. We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments. Moreover, we discussed the implications of this work for communities and applications. The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD.",
        "comments": "Accepted to IEEE VR 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12133"
    },
    {
        "doc_id": 136,
        "title": "Evaluation of QCNN-LSTM for Disability Forecasting in Multiple Sclerosis Using Sequential Multisequence MRI",
        "authors": [
            "John D. Mayfield",
            "Issam El Naqa"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Emerging Technologies",
            "Image and Video Processing"
        ],
        "abstract": "Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term Memory (LSTM) models were studied to provide sequential relationships for each timepoint in MRIs of patients with Multiple Sclerosis (MS). In this pilot study, we compared three QCNN-LSTM models for binary classification of MS disability benchmarked against classical neural network architectures. Our hypothesis is that quantum models will provide competitive performance. Methods Matrix Product State (MPS), reverse Multistate Entanglement Renormalization Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM layer to process near-annual MRI data of patients diagnosed with MS. These were benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision Transformer (ViViT). Predicted logits were measured against ground truth labels of each patient's Extended Disability Severity Score (EDSS) using binary cross-entropy loss. Training/validation/holdout testing was partitioned using 5-fold cross validation with a total split of 60:20:20. Levene's test of variance was used to measure statistical difference and Student's t-test for paired model differences in mean. Results The MPS-LSTM, reverse MERA-LSTM, and TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively (p-value 0.915). VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73 and 0.77, respectively (p-value 0.631). Overall variance and mean were not statistically significant (p-value 0.713), however, time to train was significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218, respectively, p-value <0.001). Conclusion QCNN-LSTM models perform competitively to their classical counterparts with greater efficiency in train time. Clinically, these can add value in terms of efficiency to time-dependent deep learning prediction of disease progression based upon medical imaging.",
        "comments": "ACM Class:          I.2.0; I.2.6",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12132"
    },
    {
        "doc_id": 137,
        "title": "NeuroSynt: A Neuro-symbolic Portfolio Solver for Reactive Synthesis",
        "authors": [
            "Matthias Cosler",
            "Christopher Hahn",
            "Ayham Omar",
            "Frederik Schmitt"
        ],
        "subjects": [
            "Logic in Computer Science",
            "Machine Learning"
        ],
        "abstract": "We introduce NeuroSynt, a neuro-symbolic portfolio solver framework for reactive synthesis. At the core of the solver lies a seamless integration of neural and symbolic approaches to solving the reactive synthesis problem. To ensure soundness, the neural engine is coupled with model checkers verifying the predictions of the underlying neural models. The open-source implementation of NeuroSynt provides an integration framework for reactive synthesis in which new neural and state-of-the-art symbolic approaches can be seamlessly integrated. Extensive experiments demonstrate its efficacy in handling challenging specifications, enhancing the state-of-the-art reactive synthesis solvers, with NeuroSynt contributing novel solves in the current SYNTCOMP benchmarks.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12131"
    },
    {
        "doc_id": 138,
        "title": "Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy",
        "authors": [
            "Will LeVine",
            "Benjamin Pikus",
            "Jacob Phillips",
            "Berk Norman",
            "Fernando Amat Gil",
            "Sean Hendryx"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "As deep neural networks become adopted in high-stakes domains, it is crucial to be able to identify when inference inputs are Out-of-Distribution (OOD) so that users can be alerted of likely drops in performance and calibration despite high confidence. Among many others, existing methods use the following two scores to do so without training on any apriori OOD examples: a learned temperature and an energy score. In this paper we introduce Ablated Learned Temperature Energy (or \"AbeT\" for short), a method which combines these prior methods in novel ways with effective modifications. Due to these contributions, AbeT lowers the False Positive Rate at $95\\%$ True Positive Rate (FPR@95) by $35.39\\%$ in classification (averaged across all ID and OOD datasets measured) compared to state of the art without training networks in multiple stages or requiring hyperparameters or test-time backward passes. We additionally provide empirical insights as to how our model learns to distinguish between In-Distribution (ID) and OOD samples while only being explicitly trained on ID samples via exposure to misclassified ID examples at training time. Lastly, we show the efficacy of our method in identifying predicted bounding boxes and pixels corresponding to OOD objects in object detection and semantic segmentation, respectively - with an AUROC increase of $5.15\\%$ in object detection and both a decrease in FPR@95 of $41.48\\%$ and an increase in AUPRC of $34.20\\%$ on average in semantic segmentation compared to previous state of the art.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12129"
    },
    {
        "doc_id": 139,
        "title": "CodeTailor: Personalized Parsons Puzzles are Preferred Over AI-Generated Solutions to Support Learning",
        "authors": [
            "Xinying Hou",
            "Zihan Wu",
            "Xu Wang",
            "Barbara J. Ericson"
        ],
        "subjects": [
            "Computers and Society",
            "Human-Computer Interaction"
        ],
        "abstract": "Programming can be challenging for novices, but it is difficult to provide high-quality, comprehensive, and timely support at scale. Generative AI and its products, like ChatGPT, can create a solution for most introductory programming problems. However, students may become overly reliant on these tools for quick code generation and homework completion, leading to reduced engagement and limited learning. In this work, we present \\sys{}, a system that utilizes large language models (LLM) while still promoting students' cognitive engagement. \\sys{} provides a personalized Parsons puzzle to support struggling students. In a Parsons puzzle, students place mixed-up code blocks in the correct order to solve a problem. A technical evaluation with 800 incorrect student code demonstrated that \\sys{} can efficiently create high-quality (correct, personalized, and concise) Parsons puzzles for students. In a within-subjects experiment with 18 novice programmers, most students rated using \\sys{} as more engaging, and they preferred \\sys{} for learning rather than simply receiving an AI-generated solution. Additionally, students recalled more new elements from the supported practice to the posttest after using \\sys{}, compared to when they simply received a direct solution. Qualitative observations and interviews provided evidence for the benefits of \\sys{} including emphasizing algorithmic thinking, fostering continuity in learning, promoting metacognitive reflection, and boosting student confidence. We conclude by suggesting future designs for applying generative AI in a way that minimizes over-reliance and enhances learning.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12125"
    },
    {
        "doc_id": 140,
        "title": "Improving genetic algorithms performance via deterministic population shrinkage",
        "authors": [
            "Juan Luis Jim\u00e9nez Laredo",
            "Carlos Fernandes",
            "Juan Juli\u00e1n Merelo",
            "Christian Gagn\u00e9"
        ],
        "subjects": [
            "Neural and Evolutionary Computing"
        ],
        "abstract": "Despite the intuition that the same population size is not needed throughout the run of an Evolutionary Algorithm (EA), most EAs use a fixed population size. This paper presents an empirical study on the possible benefits of a Simple Variable Population Sizing (SVPS) scheme on the performance of Genetic Algorithms (GAs). It consists in decreasing the population for a GA run following a predetermined schedule, configured by a speed and a severity parameter. The method uses as initial population size an estimation of the minimum size needed to supply enough building blocks, using a fixed-size selectorecombinative GA converging within some confidence interval toward good solutions for a particular problem. Following this methodology, a scalability analysis is conducted on deceptive, quasi-deceptive, and non-deceptive trap functions in order to assess whether SVPS-GA improves performances compared to a fixed-size GA under different problem instances and difficulty levels. Results show several combinations of speed-severity where SVPS-GA preserves the solution quality while improving performances, by reducing the number of evaluations needed for success.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12121"
    },
    {
        "doc_id": 141,
        "title": "Centralization in Block Building and Proposer-Builder Separation",
        "authors": [
            "Maryam Bahrani",
            "Pranav Garimidi",
            "Tim Roughgarden"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Cryptography and Security",
            "Distributed, Parallel, and Cluster Computing",
            "Theoretical Economics"
        ],
        "abstract": "The goal of this paper is to rigorously interrogate conventional wisdom about centralization in block-building (due to, e.g., MEV and private order flow) and the outsourcing of block-building by validators to specialists (i.e., proposer-builder separation):\n  1. Does heterogeneity in skills and knowledge across block producers inevitably lead to centralization?\n  2. Does proposer-builder separation eliminate heterogeneity and preserve decentralization among proposers?\n  This paper develops mathematical models and results that offer answers to these questions:\n  1. In a game-theoretic model with endogenous staking, heterogeneous block producer rewards, and staking costs, we quantify the extent to which heterogeneous rewards lead to concentration in the equilibrium staking distribution.\n  2. In a stochastic model in which heterogeneous block producers repeatedly reinvest rewards into staking, we quantify, as a function of the block producer heterogeneity, the rate at which stake concentrates on the most sophisticated block producers.\n  3. In a model with heterogeneous proposers and specialized builders, we quantify, as a function of the competitiveness of the builder ecosystem, the extent to which proposer-builder separation reduces the heterogeneity in rewards across different proposers.\n  Our models and results take advantage of connections to contest design, P\u00f3lya urn processes, and auction theory.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12120"
    },
    {
        "doc_id": 142,
        "title": "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models",
        "authors": [
            "Kian Ahrabian",
            "Zhivar Sourati",
            "Kexuan Sun",
            "Jiarui Zhang",
            "Yifan Jiang",
            "Fred Morstatter",
            "Jay Pujara"
        ],
        "subjects": [
            "Computation and Language"
        ],
        "abstract": "While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments expose the difficulty of solving such problems while showcasing the immense gap between open-source and closed-source models. We also reveal critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experiment with various methods, such as Chain-of-Thought prompting, resulting in a significant (up to 100%) boost in performance.",
        "comments": "Code and datasets are available at https://github.com/kahrabian/mllm-nvar",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12117"
    },
    {
        "doc_id": 143,
        "title": "Improved accuracy of continuum surface flux models for metal additive manufacturing melt pool simulations",
        "authors": [
            "Nils Much",
            "Magdalena Schreter-Fleischhacker",
            "Peter Munch",
            "Martin Kronbichler",
            "Wolfgang A. Wall",
            "Christoph Meier"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science"
        ],
        "abstract": "Computational modeling of the melt pool dynamics in laser-based powder bed fusion metal additive manufacturing (PBF-LB/M) promises to shed light on fundamental defect generation mechanisms. These processes are typically accompanied by rapid evaporation so that the evaporation-induced recoil pressure and cooling arise as major driving forces for fluid dynamics and temperature evolution. The magnitude of these interface fluxes depends exponentially on the melt pool surface temperature, which, therefore, must be predicted with high accuracy. The present work utilizes a diffuse interface model based on a continuum surface flux (CSF) description on the interfaces to study dimensionally reduced thermal two-phase problems representing PBF-LB/M in a finite element framework. It is demonstrated that the extreme temperature gradients combined with the high ratios of material properties between metal and ambient gas lead to significant errors in the interface temperatures and fluxes when classical CSF approaches, along with typical interface thicknesses and discretizations, are applied. A novel parameter-scaled CSF approach is proposed, which is constructed to yield a smoother temperature rate in the diffuse interface region, significantly increasing the solution accuracy. The interface thickness required to predict the temperature field with a given level of accuracy is less restrictive by at least one order of magnitude for the proposed parameter-scaled CSF approach compared to classical CSF, drastically reducing computational costs. Finally, we showcased the general applicability of the parameter-scaled CSF to a three-dimensional simulation of stationary laser melting of PBF-LB/M considering the fully coupled thermo-hydrodynamic multi-phase problem, including phase change.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12114"
    },
    {
        "doc_id": 144,
        "title": "Extracting Formulae in Many-Valued Logic from Deep Neural Networks",
        "authors": [
            "Yani Zhang",
            "Helmut B\u00f6lcskei"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Logic in Computer Science"
        ],
        "abstract": "We propose a new perspective on deep ReLU networks, namely as circuit counterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV) generalization of Boolean logic. An algorithm for extracting formulae in MV logic from deep ReLU networks is presented. As the algorithm applies to networks with general, in particular also real-valued, weights, it can be used to extract logical formulae from deep ReLU networks trained on data.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12113"
    },
    {
        "doc_id": 145,
        "title": "Constrained Multi-Tildes: Derived Term and Position Automata",
        "authors": [
            "Samira Attou",
            "Ludovic Mignot",
            "Cl\u00e9ment Miklarz",
            "Florent Nicart"
        ],
        "subjects": [
            "Formal Languages and Automata Theory"
        ],
        "abstract": "Multi-tildes are regular operators that were introduced to enhance the factorization power of regular expressions, allowing us to add the empty word in several factors of a catenation product of languages. In addition to multi-bars, which dually remove the empty word, they allow representing any acyclic automaton by a linear-sized expression, whereas the lower bound is exponential in the classic case.\n  In this paper, we extend multi-tildes from disjunctive combinations to any Boolean combination, allowing us to exponentially enhance the factorization power of tildes expressions. Moreover, we show how to convert these expressions into finite automata and give a Haskell implementation of them using advanced techniques of functional programming.",
        "comments": "Extended version of https://doi.org/10.1007/978-3-031-40247-0_4, submitted to International Journal of Foundations of Computer Science",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12111"
    },
    {
        "doc_id": 146,
        "title": "Weak second-order quantum state diffusion unraveling of the Lindblad master equation",
        "authors": [
            "Sayak Adhikari",
            "Roi Baer"
        ],
        "subjects": [
            "Quantum Physics"
        ],
        "abstract": "Abstract Simulating mixed-state evolution in open quantum systems is crucial for various chemical physics, quantum optics, and computer science applications. These simulations typically follow the Lindblad master equation dynamics. An alternative approach known as quantum state diffusion unraveling is based on the trajectories of pure states generated by random wave functions, which evolve according to a nonlinear It\u00f4-Schr\u00f6dinger equation (ISE). This study introduces weak first- and second-order solvers for the ISE based on directly applying the It\u00f4-Taylor expansion with exact derivatives in the interaction picture. We tested the method on free and driven Morse oscillators coupled to a thermal environment and found that both orders allowed practical estimation with a few dozen iterations. The variance was relatively small compared to the linear unraveling and did not grow with time. The second-order solver delivers much higher accuracy and stability with bigger time steps than the first-order scheme, with a small additional workload. However, the second-order algorithm has quadratic complexity with the number of Lindblad operators as opposed to the linear complexity of the first-order algorithm.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12109"
    },
    {
        "doc_id": 147,
        "title": "On-Time Delivery in Crowdshipping Systems: An Agent-Based Approach Using Streaming Data",
        "authors": [
            "Jeremias D\u00f6tterl",
            "Ralf Bruns",
            "J\u00fcrgen Dunkel",
            "Sascha Ossowski"
        ],
        "subjects": [
            "Artificial Intelligence",
            "Machine Learning",
            "Multiagent Systems"
        ],
        "abstract": "In parcel delivery, the \"last mile\" from the parcel hub to the customer is costly, especially for time-sensitive delivery tasks that have to be completed within hours after arrival. Recently, crowdshipping has attracted increased attention as a new alternative to traditional delivery modes. In crowdshipping, private citizens (\"the crowd\") perform short detours in their daily lives to contribute to parcel delivery in exchange for small incentives. However, achieving desirable crowd behavior is challenging as the crowd is highly dynamic and consists of autonomous, self-interested individuals. Leveraging crowdshipping for time-sensitive deliveries remains an open challenge. In this paper, we present an agent-based approach to on-time parcel delivery with crowds. Our system performs data stream processing on the couriers' smartphone sensor data to predict delivery delays. Whenever a delay is predicted, the system attempts to forge an agreement for transferring the parcel from the current deliverer to a more promising courier nearby. Our experiments show that through accurate delay predictions and purposeful task transfers many delays can be prevented that would occur without our approach.",
        "comments": "Journal ref:        Frontiers in Artificial Intelligence and Applications. Volume 325: ECAI 2020. Pages 51-58",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12108"
    },
    {
        "doc_id": 148,
        "title": "Energy-aware Trajectory Optimization for UAV-mounted RIS and Full-duplex Relay",
        "authors": [
            "Dimitrios Tyrovolas",
            "Nikos A. Mitsiou",
            "Thomas G. Boufikos",
            "Prodromos-Vasileios Mekikis",
            "Sotiris A. Tegos",
            "Panagiotis D. Diamantoulakis",
            "Sotiris Ioannidis",
            "Christos K. Liaskos",
            "George K. Karagiannidis"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "In the evolving landscape of sixth-generation (6G) wireless networks, unmanned aerial vehicles (UAVs) have emerged as transformative tools for dynamic and adaptive connectivity. However, dynamically adjusting their position to offer favorable communication channels introduces operational challenges in terms of energy consumption, especially when integrating advanced communication technologies like reconfigurable intelligent surfaces (RISs) and full-duplex relays (FDRs). To this end, by recognizing the pivotal role of UAV mobility, the paper introduces an energy-aware trajectory design for UAV-mounted RISs and UAV-mounted FDRs using the decode and forward (DF) protocol, aiming to maximize the network minimum rate and enhance user fairness, while taking into consideration the available on-board energy. Specifically, this work highlights their distinct energy consumption characteristics and their associated integration challenges by developing appropriate energy consumption models for both UAV-mounted RISs and FDRs that capture the intricate relationship between key factors such as weight, and their operational characteristics. Furthermore, a joint time-division multiple access (TDMA) user scheduling-UAV trajectory optimization problem is formulated, considering the power dynamics of both systems, while assuring that the UAV energy is not depleted mid-air. Finally, simulation results underscore the importance of energy considerations in determining the optimal trajectory and scheduling and provide insights into the performance comparison of UAV-mounted RISs and FDRs in UAV-assisted wireless networks.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12107"
    },
    {
        "doc_id": 149,
        "title": "LearnedWMP: Workload Memory Prediction Using Distribution of Query Templates",
        "authors": [
            "Shaikh Quader",
            "Andres Jaramillo",
            "Sumona Mukhopadhyay",
            "Ghadeer Abuoda",
            "Calisto Zuzarte",
            "David Kalmuk",
            "Marin Litoiu",
            "Manos Papagelis"
        ],
        "subjects": [
            "Databases",
            "Machine Learning"
        ],
        "abstract": "In a modern DBMS, working memory is frequently the limiting factor when processing in-memory analytic query operations such as joins, sorting, and aggregation. Existing resource estimation approaches for a DBMS estimate the resource consumption of a query by computing an estimate of each individual database operator in the query execution plan. Such an approach is slow and error-prone as it relies upon simplifying assumptions, such as uniformity and independence of the underlying data. Additionally, the existing approach focuses on individual queries separately and does not factor in other queries in the workload that may be executed concurrently. In this research, we are interested in query performance optimization under concurrent execution of a batch of queries (a workload). Specifically, we focus on predicting the memory demand for a workload rather than providing separate estimates for each query within it. We introduce the problem of workload memory prediction and formalize it as a distribution regression problem. We propose Learned Workload Memory Prediction (LearnedWMP) to improve and simplify estimating the working memory demands of workloads. Through a comprehensive experimental evaluation, we show that LearnedWMP reduces the memory estimation error of the state-of-the-practice method by up to 47.6%. Compared to an alternative single-query model, during training and inferencing, the LearnedWMP model and its variants were 3x to 10x faster. Moreover, LearnedWMP-based models were at least 50% smaller in most cases. Overall, the results demonstrate the advantages of the LearnedWMP approach and its potential for a broader impact on query performance optimization.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12103"
    },
    {
        "doc_id": 150,
        "title": "An Empirical Analysis of In-context Learning Abilities of LLMs for MT",
        "authors": [
            "Pranjal A. Chitale",
            "Jay Gala",
            "Varun Gumma",
            "Mitesh M. Khapra",
            "Raj Dabre"
        ],
        "subjects": [
            "Computation and Language"
        ],
        "abstract": "In-context learning (ICL) has consistently demonstrated superior performance over zero-shot performance in large language models (LLMs). However, the understanding of the dynamics of ICL and the aspects that influence downstream performance remains limited, especially for natural language generation (NLG) tasks. This work aims to address this gap by investigating the ICL capabilities of LLMs and studying the impact of different aspects of the in-context demonstrations for the task of machine translation (MT). Our preliminary investigations aim to discern whether in-context learning (ICL) is predominantly influenced by demonstrations or instructions by applying diverse perturbations to in-context demonstrations while preserving the task instruction. We observe varying behavior to perturbed examples across different model families, notably with BLOOM-7B derivatives being severely influenced by noise, whereas Llama 2 derivatives not only exhibit robustness but also tend to show enhancements over the clean baseline when subject to perturbed demonstrations. This suggests that the robustness of ICL may be governed by several factors, including the type of noise, perturbation direction (source or target), the extent of pretraining of the specific model, and fine-tuning for downstream tasks if applicable. Further investigation is warranted to develop a comprehensive understanding of these factors in future research.",
        "comments": "Work in progress",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12097"
    },
    {
        "doc_id": 151,
        "title": "CLIQUE as an AND of Polynomial-Sized Monotone Constant-Depth Circuits",
        "authors": [
            "Levente Bodn\u00e1r"
        ],
        "subjects": [
            "Computational Complexity"
        ],
        "abstract": "This paper shows that calculating $k$-CLIQUE on $n$ vertex graphs, requires the AND of at least $2^{n/4k}$ monotone, constant-depth, and polynomial-sized circuits, for sufficiently large values of $k$. The proof relies on a new, monotone, one-sided switching lemma, designed for cliques.",
        "comments": "11 pages, 1 figure",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12094"
    },
    {
        "doc_id": 152,
        "title": "Monitoring the Future of Smart Contracts",
        "authors": [
            "Margarita Capretto",
            "Martin Ceresa",
            "Cesar Sanchez"
        ],
        "subjects": [
            "Logic in Computer Science",
            "Cryptography and Security"
        ],
        "abstract": "Blockchains are decentralized systems that provide trustable execution guarantees. Smart contracts are programs written in specialized programming languages running on blockchains that govern how tokens and cryptocurrency are sent and received. Smart contracts can invoke other smart contracts during the execution of transactions always initiated by external users.\n  Once deployed, smart contracts cannot be modified, so techniques like runtime verification are very appealing for improving their reliability. However, the conventional model of computation of smart contracts is transactional: once operations commit, their effects are permanent and cannot be undone.\n  In this paper, we proposed the concept of future monitors which allows monitors to remain waiting for future transactions to occur before committing or aborting. This is inspired by optimistic rollups, which are modern blockchain implementations that increase efficiency (and reduce cost) by delaying transaction effects. We exploit this delay to propose a model of computation that allows (bounded) future monitors. We show our monitors correct respect of legacy transactions, how they implement future bounded monitors and how they guarantee progress. We illustrate the use of future bounded monitors to implement correctly multi-transaction flash loans.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12093"
    },
    {
        "doc_id": 153,
        "title": "Quantum Eigensolver for General Matrices",
        "authors": [
            "Xiao-Ming Zhang",
            "Yukun Zhang",
            "Wenhao He",
            "Xiao Yuan"
        ],
        "subjects": [
            "Quantum Physics",
            "Mesoscale and Nanoscale Physics",
            "Data Structures and Algorithms",
            "Numerical Analysis",
            "Computational Physics"
        ],
        "abstract": "The eigenvalue problem, a cornerstone in linear algebra, provides profound insights into studying matrix properties. Quantum algorithms addressing this problem have hitherto been constrained to special normal matrices assuming spectral decomposition, leaving the extension to general matrices an open challenge. In this work, we present a novel family of quantum algorithms tailored for solving the eigenvalue problem for general matrices, encompassing scenarios with complex eigenvalues or even defective matrices. Our approach begins by tackling the task of searching for an eigenvalue without additional constraints. For diagonalizable matrices, our algorithm has $\\tilde O(\\varepsilon^{-1})$ complexity with an error $\\varepsilon$, achieving the nearly Heisenberg scaling. Subsequently, we study the identification of eigenvalues closest to a specified point or line, extending the results for ground energy and energy gap problems in Hermitian matrices. We achieve an accuracy scaling of $\\tilde O(\\varepsilon^{-2})$ for general diagonalizable matrices, further refining to $\\tilde O(\\varepsilon^{-1})$ under the condition of real eigenvalues or constant distance from the reference point. The algorithm's foundation lies in the synergy of three techniques: the relationship between eigenvalues of matrix $A$ and the minimum singular value of $A-\u03bcI$, quantum singular value threshold subroutine extended from quantum singular-value estimation, and problem-specific searching algorithms. Our algorithms find applications in diverse domains, including estimating the relaxation time of Markov chains, solving Liouvillian gaps in open quantum systems, and verifying PT-symmetry broken/unbroken phases. These applications underscore the significance of our quantum eigensolvers for problems across various disciplines.",
        "comments": "6+10 pages, 1+1 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12091"
    },
    {
        "doc_id": 154,
        "title": "Unsupervised Learning of Graph from Recipes",
        "authors": [
            "Aissatou Diallo",
            "Antonis Bikakis",
            "Luke Dickens",
            "Anthony Hunter",
            "Rob Miller"
        ],
        "subjects": [
            "Computation and Language"
        ],
        "abstract": "Cooking recipes are one of the most readily available kinds of procedural text. They consist of natural language instructions that can be challenging to interpret. In this paper, we propose a model to identify relevant information from recipes and generate a graph to represent the sequence of actions in the recipe. In contrast with other approaches, we use an unsupervised approach. We iteratively learn the graph structure and the parameters of a $\\mathsf{GNN}$ encoding the texts (text-to-graph) one sequence at a time while providing the supervision by decoding the graph into text (graph-to-text) and comparing the generated text to the input. We evaluate the approach by comparing the identified entities with annotated datasets, comparing the difference between the input and output texts, and comparing our generated graphs with those generated by state of the art methods.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12088"
    },
    {
        "doc_id": 155,
        "title": "Revisiting Demonstration Selection Strategies in In-Context Learning",
        "authors": [
            "Keqin Peng",
            "Liang Ding",
            "Yancheng Yuan",
            "Xuebo Liu",
            "Min Zhang",
            "Yuanxin Ouyang",
            "Dacheng Tao"
        ],
        "subjects": [
            "Computation and Language"
        ],
        "abstract": "Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and it is still unclear why this happens or what factors will influence its choice. In this work, we first revisit the factors contributing to this variance from both data and model aspects, and find that the choice of demonstration is both data- and model-dependent. We further proposed a data- and model-dependent demonstration selection method, \\textbf{TopK + ConE}, based on the assumption that \\textit{the performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples}, resulting in a simple and effective recipe for ICL. Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code will be released.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12087"
    },
    {
        "doc_id": 156,
        "title": "West-of-N: Synthetic Preference Generation for Improved Reward Modeling",
        "authors": [
            "Aliz\u00e9e Pace",
            "Jonathan Mallinson",
            "Eric Malmi",
            "Sebastian Krause",
            "Aliaksei Severyn"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model. In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs. Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training. This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query. Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data. This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12086"
    },
    {
        "doc_id": 157,
        "title": "Consistency Based Unsupervised Self-training For ASR Personalisation",
        "authors": [
            "Jisi Zhang",
            "Vandana Rajan",
            "Haaris Mehmood",
            "David Tuckey",
            "Pablo Peso Parada",
            "Md Asif Jalal",
            "Karthikeyan Saravanan",
            "Gil Ho Lee",
            "Jungin Lee",
            "Seokyeong Jung"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "On-device Automatic Speech Recognition (ASR) models trained on speech data of a large population might underperform for individuals unseen during training. This is due to a domain shift between user data and the original training data, differed by user's speaking characteristics and environmental acoustic conditions. ASR personalisation is a solution that aims to exploit user data to improve model robustness. The majority of ASR personalisation methods assume labelled user data for supervision. Personalisation without any labelled data is challenging due to limited data size and poor quality of recorded audio samples. This work addresses unsupervised personalisation by developing a novel consistency based training method via pseudo-labelling. Our method achieves a relative Word Error Rate Reduction (WERR) of 17.3% on unlabelled training data and 8.1% on held-out data compared to a pre-trained model, and outperforms the current state-of-the art methods.",
        "comments": "Accepted for IEEE ASRU 2023",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12085"
    },
    {
        "doc_id": 158,
        "title": "Collaborative Reinforcement Learning Based Unmanned Aerial Vehicle (UAV) Trajectory Design for 3D UAV Tracking",
        "authors": [
            "Yujiao Zhu",
            "Mingzhe Chen",
            "Sihua Wang",
            "Ye Hu",
            "Yuchen Liu",
            "Changchuan Yin"
        ],
        "subjects": [
            "Multiagent Systems",
            "Machine Learning"
        ],
        "abstract": "In this paper, the problem of using one active unmanned aerial vehicle (UAV) and four passive UAVs to localize a 3D target UAV in real time is investigated. In the considered model, each passive UAV receives reflection signals from the target UAV, which are initially transmitted by the active UAV. The received reflection signals allow each passive UAV to estimate the signal transmission distance which will be transmitted to a base station (BS) for the estimation of the position of the target UAV. Due to the movement of the target UAV, each active/passive UAV must optimize its trajectory to continuously localize the target UAV. Meanwhile, since the accuracy of the distance estimation depends on the signal-to-noise ratio of the transmission signals, the active UAV must optimize its transmit power. This problem is formulated as an optimization problem whose goal is to jointly optimize the transmit power of the active UAV and trajectories of both active and passive UAVs so as to maximize the target UAV positioning accuracy. To solve this problem, a Z function decomposition based reinforcement learning (ZD-RL) method is proposed. Compared to value function decomposition based RL (VD-RL), the proposed method can find the probability distribution of the sum of future rewards to accurately estimate the expected value of the sum of future rewards thus finding better transmit power of the active UAV and trajectories for both active and passive UAVs and improving target UAV positioning accuracy. Simulation results show that the proposed ZD-RL method can reduce the positioning errors by up to 39.4% and 64.6%, compared to VD-RL and independent deep RL methods, respectively.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12079"
    },
    {
        "doc_id": 159,
        "title": "Temporal Blind Spots in Large Language Models",
        "authors": [
            "Jonas Wallat",
            "Adam Jatowt",
            "Avishek Anand"
        ],
        "subjects": [
            "Computation and Language"
        ],
        "abstract": "Large language models (LLMs) have recently gained significant attention due to their unparalleled ability to perform various natural language processing tasks. These models, benefiting from their advanced natural language understanding capabilities, have demonstrated impressive zero-shot performance. However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations. Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents. In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding. We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets. Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information. In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates. Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks. The code is available\\footnote{https://github.com/jwallat/temporalblindspots}.",
        "comments": "accepted at WSDM'24",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12078"
    },
    {
        "doc_id": 160,
        "title": "Human Impression of Humanoid Robots Mirroring Social Cues",
        "authors": [
            "Di Fu",
            "Fares Abawi",
            "Philipp Allgeuer",
            "Stefan Wermter"
        ],
        "subjects": [
            "Robotics",
            "Human-Computer Interaction"
        ],
        "abstract": "Mirroring non-verbal social cues such as affect or movement can enhance human-human and human-robot interactions in the real world. The robotic platforms and control methods also impact people's perception of human-robot interaction. However, limited studies have compared robot imitation across different platforms and control methods. Our research addresses this gap by conducting two experiments comparing people's perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control. We discovered that the iCub robot was perceived as more humanlike than the Pepper robot when mirroring affect. A vision-based controlled iCub outperformed the IMU-based controlled one in the movement mirroring task. Our findings suggest that different robotic platforms impact people's perception of robots' mirroring during HRI. The control method also contributes to the robot's mirroring performance. Our work sheds light on the design and application of different humanoid robots in the real world.",
        "comments": "Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI '24 Companion), March 11-14, 2024, Boulder, CO, USA. arXiv admin note: text overlap with arXiv:2302.09648",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12076"
    },
    {
        "doc_id": 161,
        "title": "NLP-based Relation Extraction Methods in RE",
        "authors": [
            "Quim Motger",
            "Xavier Franch"
        ],
        "subjects": [
            "Software Engineering"
        ],
        "abstract": "Mobile app repositories have been largely used in scientific research as large-scale, highly adaptive crowdsourced information systems. These software platforms can potentially nourish multiple software and requirements engineering tasks based on user reviews and other natural language documents, including feedback analysis, recommender systems and topic modelling. Consequently, researchers often endeavour to overcome domain-specific challenges, including integration of heterogeneous data sources, large-scale data collection and adaptation of a publicly available data set for a given research scenario. In this paper, we present MApp-KG, a combination of software resources and data artefacts in the field of mobile app repositories to support extended knowledge generation tasks. Our contribution aims to provide a framework for automatically constructing a knowledge graph modelling a domain-specific catalogue of mobile apps. Complementarily, we distribute MApp-KG in a public triplestore and as a static data snapshot, which may be promptly employed for future research and reproduction of our findings.",
        "comments": "This article will appear as a chapter in a book provisionally titled \"Natural Language Processing for Requirements Engineering\", to be published by Springer",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12075"
    },
    {
        "doc_id": 162,
        "title": "DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI",
        "authors": [
            "Sergio Morell-Ortega",
            "Marina Ruiz-Perez",
            "Marien Gadea",
            "Roberto Vivo-Hernando",
            "Gregorio Rubio",
            "Fernando Aparici",
            "Mariam de la Iglesia-Vaya",
            "Gwenaelle Catheline",
            "Pierrick Coup\u00e9",
            "Jos\u00e9 V. Manj\u00f3n"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Neurons and Cognition"
        ],
        "abstract": "This paper introduces a novel multimodal and high-resolution human brain cerebellum lobule segmentation method. Unlike current tools that operate at standard resolution ($1 \\text{ mm}^{3}$) or using mono-modal data, the proposed method improves cerebellum lobule segmentation through the use of a multimodal and ultra-high resolution ($0.125 \\text{ mm}^{3}$) training dataset. To develop the method, first, a database of semi-automatically labelled cerebellum lobules was created to train the proposed method with ultra-high resolution T1 and T2 MR images. Then, an ensemble of deep networks has been designed and developed, allowing the proposed method to excel in the complex cerebellum lobule segmentation task, improving precision while being memory efficient. Notably, our approach deviates from the traditional U-Net model by exploring alternative architectures. We have also integrated deep learning with classical machine learning methods incorporating a priori knowledge from multi-atlas segmentation, which improved precision and robustness. Finally, a new online pipeline, named DeepCERES, has been developed to make available the proposed method to the scientific community requiring as input only a single T1 MR image at standard resolution.",
        "comments": "20 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12074"
    },
    {
        "doc_id": 163,
        "title": "The time slot allocation problem in liberalised passenger railway markets: a multi-objective approach",
        "authors": [
            "Nikola Be\u0161inovi\u0107",
            "Ricardo Garc\u00eda-R\u00f3denas",
            "Mar\u00eda Luz L\u00f3pez-Garc\u00eda",
            "Julio Alberto L\u00f3pez-G\u00f3mez",
            "Jos\u00e9 \u00c1ngel Mart\u00edn-Baos"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science"
        ],
        "abstract": "The liberalisation of the European passenger railway markets through the European Directive EU 91/440/EEC states a new scenario where different Railway Undertakings compete with each other in a bidding process for time slots. The infrastructure resources are provided by the Infrastructure Manager, who analyses and assesses the bids received, allocating the resources to each Railway Undertaking. Time slot allocation is a fact that drastically influences the market equilibrium. In this paper, we address the time slot allocation problem within the context of a liberalized passenger railway market as a multi-objective model. The Infrastructure Manager is tasked with selecting a point from the Pareto front as the solution to the time slot allocation problem. We propose two criteria for making this selection: the first one allocates time slots to each company according to a set of priorities, while the second one introduces a criterion of fairness in the treatment of companies to incentive competition. The assessment of the impact of these rules on market equilibrium has been conducted on a liberalized high-speed corridor within the Spanish railway network.",
        "comments": "32 pages and 6 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12073"
    },
    {
        "doc_id": 164,
        "title": "Cross-lingual Transfer Learning for Javanese Dependency Parsing",
        "authors": [
            "Fadli Aulawi Al Ghiffari",
            "Ika Alfina",
            "Kurniawati Azizah"
        ],
        "subjects": [
            "Computation and Language"
        ],
        "abstract": "While structure learning achieves remarkable performance in high-resource languages, the situation differs for under-represented languages due to the scarcity of annotated data. This study focuses on assessing the efficacy of transfer learning in enhancing dependency parsing for Javanese, a language spoken by 80 million individuals but characterized by limited representation in natural language processing. We utilized the Universal Dependencies dataset consisting of dependency treebanks from more than 100 languages, including Javanese. We propose two learning strategies to train the model: transfer learning (TL) and hierarchical transfer learning (HTL). While TL only uses a source language to pre-train the model, the HTL method uses a source language and an intermediate language in the learning process. The results show that our best model uses the HTL method, which improves performance with an increase of 10% for both UAS and LAS evaluations compared to the baseline model.",
        "comments": "Accepted at IJCNLP-AACL 2023 SRW",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12072"
    },
    {
        "doc_id": 165,
        "title": "An Irredundant and Compressed Data Layout to Optimize Bandwidth Utilization of FPGA Accelerators",
        "authors": [
            "Corentin Ferry",
            "Nicolas Derumigny",
            "Steven Derrien",
            "Sanjay Rajopadhye"
        ],
        "subjects": [
            "Hardware Architecture"
        ],
        "abstract": "Memory bandwidth is known to be a performance bottleneck for FPGA accelerators, especially when they deal with large multi-dimensional data-sets. A large body of work focuses on reducing of off-chip transfers, but few authors try to improve the efficiency of transfers. This paper addresses the later issue by proposing (i) a compiler-based approach to accelerator's data layout to maximize contiguous access to off-chip memory, and (ii) data packing and runtime compression techniques that take advantage of this layout to further improve memory performance. We show that our approach can decrease the I/O cycles up to $7\\times$ compared to un-optimized memory accesses.",
        "comments": "11 pages, 11 figures, 2 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12071"
    },
    {
        "doc_id": 166,
        "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
        "authors": [
            "Abhimanyu Hans",
            "Avi Schwarzschild",
            "Valeriia Cherepanova",
            "Hamid Kazemi",
            "Aniruddha Saha",
            "Micah Goldblum",
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.",
        "comments": "20 pages, code available at https://github.com/ahans30/Binoculars",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12070"
    },
    {
        "doc_id": 167,
        "title": "Beyond TreeSHAP: Efficient Computation of Any-Order Shapley Interactions for Tree Ensembles",
        "authors": [
            "Maximilian Muschalik",
            "Fabian Fumagalli",
            "Barbara Hammer",
            "Eyke H\u00fcllermeier"
        ],
        "subjects": [
            "Machine Learning"
        ],
        "abstract": "While shallow decision trees may be interpretable, larger ensemble models like gradient-boosted trees, which often set the state of the art in machine learning problems involving tabular data, still remain black box models. As a remedy, the Shapley value (SV) is a well-known concept in explainable artificial intelligence (XAI) research for quantifying additive feature attributions of predictions. The model-specific TreeSHAP methodology solves the exponential complexity for retrieving exact SVs from tree-based models. Expanding beyond individual feature attribution, Shapley interactions reveal the impact of intricate feature interactions of any order. In this work, we present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley interactions for predictions of tree-based models. TreeSHAP-IQ is supported by a mathematical framework that exploits polynomial arithmetic to compute the interaction scores in a single recursive traversal of the tree, akin to Linear TreeSHAP. We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore interactions on well-established benchmark datasets.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12069"
    },
    {
        "doc_id": 168,
        "title": "Resource-constrained stereo singing voice cancellation",
        "authors": [
            "Clara Borrelli",
            "James Rae",
            "Dogac Basaran",
            "Matt McVicar",
            "Mehrez Souden",
            "Matthias Mauch"
        ],
        "subjects": [
            "Sound",
            "Machine Learning",
            "Audio and Speech Processing"
        ],
        "abstract": "We study the problem of stereo singing voice cancellation, a subtask of music source separation, whose goal is to estimate an instrumental background from a stereo mix. We explore how to achieve performance similar to large state-of-the-art source separation networks starting from a small, efficient model for real-time speech separation. Such a model is useful when memory and compute are limited and singing voice processing has to run with limited look-ahead. In practice, this is realised by adapting an existing mono model to handle stereo input. Improvements in quality are obtained by tuning model parameters and expanding the training set. Moreover, we highlight the benefits a stereo model brings by introducing a new metric which detects attenuation inconsistencies between channels. Our approach is evaluated using objective offline metrics and a large-scale MUSHRA trial, confirming the effectiveness of our techniques in stringent listening tests.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12068"
    },
    {
        "doc_id": 169,
        "title": "A concise proof of Commoner's theorem",
        "authors": [
            "Petr Jancar"
        ],
        "subjects": [
            "Logic in Computer Science"
        ],
        "abstract": "The textbook proofs of Commoner's theorem characterizing liveness in free-choice Petri nets are given in contexts of technical notions and claims that make the proofs look a bit long. The aim of this note is to give a concise self-contained proof.",
        "comments": "A slight elaboration of the 1-page text in Petri Net Newsletter, No 49, page 43 (October 1995)",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12067"
    },
    {
        "doc_id": 170,
        "title": "Market Responses to Genuine Versus Strategic Generosity: An Empirical Examination of NFT Charity Fundraisers",
        "authors": [
            "Chen Liang",
            "Murat Tunc",
            "Gordon Burtch"
        ],
        "subjects": [
            "General Economics",
            "Human-Computer Interaction"
        ],
        "abstract": "Crypto donations now represent a significant fraction of charitable giving worldwide. Nonfungible token (NFT) charity fundraisers, which involve the sale of NFTs of artistic works with the proceeds donated to philanthropic causes, have emerged as a novel development in this space. A unique aspect of NFT charity fundraisers is the significant potential for donors to reap financial gains from the rising value of purchased NFTs. Questions may arise about the motivations of donors in these charity fundraisers, resulting in a negative social image. NFT charity fundraisers thus offer a unique opportunity to understand the economic consequences of a donor's social image. We investigate these effects in the context of a large NFT charity fundraiser. We identify the causal effect of purchasing an NFT within the charity fundraiser on a donor's later market outcomes by leveraging random variation in transaction processing times on the blockchain. Further, we demonstrate a clear pattern of heterogeneity, based on an individual's decision to relist (versus hold) the purchased charity NFTs (a sign of strategic generosity), and based on an individual's degree of social exposure within the NFT marketplace. We show that charity-NFT \"relisters\" experience significant penalties in the market, in terms of the prices they are able to command on other NFT listings, particularly among those who relist quickly and those who are more socially exposed. Our study underscores the growing importance of digital visibility and traceability, features that characterize crypto-philanthropy, and online philanthropy more broadly.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12064"
    },
    {
        "doc_id": 171,
        "title": "Scalable Automated Verification for Cyber-Physical Systems in Isabelle/HOL",
        "authors": [
            "Jonathan Juli\u00e1n Huerta y Munive",
            "Simon Foster",
            "Mario Gleirscher",
            "Georg Struth",
            "Christian Pardillo Laursen",
            "Thomas Hickman"
        ],
        "subjects": [
            "Logic in Computer Science",
            "Mathematical Software"
        ],
        "abstract": "We formally introduce IsaVODEs (Isabelle verification with Ordinary Differential Equations), a framework for the verification of cyber-physical systems. We describe the semantic foundations of the framework's formalisation in the Isabelle/HOL proof assistant. A user-friendly language specification based on a robust state model makes our framework flexible and adaptable to various engineering workflows. New additions to the framework increase both its expressivity and proof automation. Specifically, formalisations related to forward diamond correctness specifications, certification of unique solutions to ordinary differential equations (ODEs) as flows, and invariant reasoning for systems of ODEs contribute to the framework's scalability and usability. Various examples and an evaluation validate the effectiveness of our framework.",
        "comments": "Submitted to the Journal of Automated Reasoning",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12061"
    },
    {
        "doc_id": 172,
        "title": "SEDAC: A CVAE-Based Data Augmentation Method for Security Bug Report Identification",
        "authors": [
            "Y. Liao",
            "T. Zhang"
        ],
        "subjects": [
            "Cryptography and Security",
            "Software Engineering"
        ],
        "abstract": "Bug tracking systems store many bug reports, some of which are related to security. Identifying those security bug reports (SBRs) may help us predict some security-related bugs and solve security issues promptly so that the project can avoid threats and attacks. However, in the real world, the ratio of security bug reports is severely low; thus, directly training a prediction model with raw data may result in inaccurate results. Faced with the massive challenge of data imbalance, many researchers in the past have attempted to use text filtering or clustering methods to minimize the proportion of non-security bug reports (NSBRs) or apply oversampling methods to synthesize SBRs to make the dataset as balanced as possible. Nevertheless, there are still two challenges to those methods: 1) They ignore long-distance contextual information. 2) They fail to generate an utterly balanced dataset. To tackle these two challenges, we propose SEDAC, a new SBR identification method that generates similar bug report vectors to solve data imbalance problems and accurately detect security bug reports. Unlike previous studies, it first converts bug reports into individual bug report vectors with distilBERT, which are based on word2vec. Then, it trains a generative model through conditional variational auto-encoder (CVAE) to generate similar vectors with security labels, which makes the number of SBRs equal to NSBRs'. Finally, balanced data are used to train a security bug report classifier. To evaluate the effectiveness of our framework, we conduct it on 45,940 bug reports from Chromium and four Apache projects. The experimental results show that SEDAC outperforms all the baselines in g-measure with improvements of around 14.24%-50.10%.",
        "comments": "11 pages, 3 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12060"
    },
    {
        "doc_id": 173,
        "title": "The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization",
        "authors": [
            "Matan Schliserman",
            "Uri Sherman",
            "Tomer Koren"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "We study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence. First, for full-batch gradient descent (GD) we give a construction of a learning problem in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance of the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\u03a9(1)$ population excess risk. Our bound translates to a lower bound of $\u03a9(\\sqrt{d})$ on the number of training examples required for standard GD to reach a non-trivial test error, answering an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b) and showing that a non-trivial dimension dependence is unavoidable. Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that an application of the same construction technique provides a similar $\u03a9(\\sqrt{d})$ lower bound for the sample complexity of SGD to reach a non-trivial empirical error, despite achieving optimal test performance. This again provides an exponential improvement in the dimension dependence compared to previous work (Koren, Livni, Mansour, and Sherman, 2022), resolving an open question left therein.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12058"
    },
    {
        "doc_id": 174,
        "title": "NEUROSEC: FPGA-Based Neuromorphic Audio Security",
        "authors": [
            "Murat Isik",
            "Hiruna Vishwamith",
            "Yusuf Sur",
            "Kayode Inadagbo",
            "I. Can Dikmen"
        ],
        "subjects": [
            "Cryptography and Security",
            "Emerging Technologies",
            "Machine Learning",
            "Neural and Evolutionary Computing",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "Neuromorphic systems, inspired by the complexity and functionality of the human brain, have gained interest in academic and industrial attention due to their unparalleled potential across a wide range of applications. While their capabilities herald innovation, it is imperative to underscore that these computational paradigms, analogous to their traditional counterparts, are not impervious to security threats. Although the exploration of neuromorphic methodologies for image and video processing has been rigorously pursued, the realm of neuromorphic audio processing remains in its early stages. Our results highlight the robustness and precision of our FPGA-based neuromorphic system. Specifically, our system showcases a commendable balance between desired signal and background noise, efficient spike rate encoding, and unparalleled resilience against adversarial attacks such as FGSM and PGD. A standout feature of our framework is its detection rate of 94%, which, when compared to other methodologies, underscores its greater capability in identifying and mitigating threats within 5.39 dB, a commendable SNR ratio. Furthermore, neuromorphic computing and hardware security serve many sensor domains in mission-critical and privacy-preserving applications.",
        "comments": "Audio processing, FPGA, Hardware Security, Neuromorphic Computing",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12055"
    },
    {
        "doc_id": 175,
        "title": "CloSe: A 3D Clothing Segmentation Dataset and Model",
        "authors": [
            "Dimitrije Anti\u0107",
            "Garvita Tiwari",
            "Batuhan Ozcomlekci",
            "Riccardo Marin",
            "Gerard Pons-Moll"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ],
        "abstract": "3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries. Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization. To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes. Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmentation from colored point clouds. CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving performance over baselines and prior work. The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data. We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing. We also introduce CloSe-T, a 3D interactive tool for refining segmentation labels. Combining the tool with CloSe-T in a continual learning setup demonstrates improved generalization on real-world data. Dataset, model, and tool can be found at https://virtualhumans.mpi-inf.mpg.de/close3dv24/.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12051"
    },
    {
        "doc_id": 176,
        "title": "HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report (Team KuzHum)",
        "authors": [
            "Volodymyr Kuzma",
            "Vladyslav Humennyy",
            "Ruslan Partsey"
        ],
        "subjects": [
            "Robotics",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "We report an improvements to NeurIPS 2023 HomeRobot: Open Vocabulary Mobile Manipulation (OVMM) Challenge reinforcement learning baseline. More specifically, we propose more accurate semantic segmentation module, along with better place skill policy, and high-level heuristic that outperforms the baseline by 2.4% of overall success rate (sevenfold improvement) and 8.2% of partial success rate (1.75 times improvement) on Test Standard split of the challenge dataset. With aforementioned enhancements incorporated our agent scored 3rd place in the challenge on both simulation and real-world stages.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12048"
    },
    {
        "doc_id": 177,
        "title": "Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D",
        "authors": [
            "Haojie Huang",
            "Owen Howell",
            "Xupeng Zhu",
            "Dian Wang",
            "Robin Walters",
            "Robert Platt"
        ],
        "subjects": [
            "Robotics",
            "Machine Learning"
        ],
        "abstract": "Many complex robotic manipulation tasks can be decomposed as a sequence of pick and place actions. Training a robotic agent to learn this sequence over many different starting conditions typically requires many iterations or demonstrations, especially in 3D environments. In this work, we propose Fourier Transporter (\\ours{}) which leverages the two-fold $\\SE(d)\\times\\SE(d)$ symmetry in the pick-place problem to achieve much higher sample efficiency. \\ours{} is an open-loop behavior cloning method trained using expert demonstrations to predict pick-place actions on new environments. \\ours{} is constrained to incorporate symmetries of the pick and place actions independently. Our method utilizes a fiber space Fourier transformation that allows for memory-efficient construction. We test our proposed network on the RLbench benchmark and achieve state-of-the-art results across various tasks.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12046"
    },
    {
        "doc_id": 178,
        "title": "Look, Listen and Recognise: Character-Aware Audio-Visual Subtitling",
        "authors": [
            "Bruno Korbar",
            "Jaesung Huh",
            "Andrew Zisserman"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "The goal of this paper is automatic character-aware subtitle generation. Given a video and a minimal amount of metadata, we propose an audio-visual method that generates a full transcript of the dialogue, with precise speech timestamps, and the character speaking identified. The key idea is to first use audio-visual cues to select a set of high-precision audio exemplars for each character, and then use these exemplars to classify all speech segments by speaker identity. Notably, the method does not require face detection or tracking. We evaluate the method over a variety of TV sitcoms, including Seinfeld, Fraiser and Scrubs. We envision this system being useful for the automatic generation of subtitles to improve the accessibility of the vast amount of videos available on modern streaming services. Project page : \\url{https://www.robots.ox.ac.uk/~vgg/research/look-listen-recognise/}",
        "comments": "Accepted for publication in ICASSP 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12039"
    },
    {
        "doc_id": 179,
        "title": "Joint Near-Field Target Tracking and Communications with Full Duplex Holographic MIMO",
        "authors": [
            "Ioannis Gavras",
            "George C. Alexandropoulos"
        ],
        "subjects": [
            "Information Theory",
            "Emerging Technologies"
        ],
        "abstract": "In this paper, we present a simultaneous target tracking and multi-user communications system realized by a full duplex holographic Multiple-Input Multiple-Output (MIMO) node equipped with Dynamic Metasurface Antennas (DMAs) at both its communication ends. Focusing on the near-field regime, we extend Fresnel's approximation to metasurfaces and devise a subspace tracking scheme with DMA-based hybrid Analog and Digital (A/D) reception as well as hybrid A/D transmission with a DMA for sum-rate maximization. The presented simulation results corroborate the efficiency of the proposed framework for various system parameters.",
        "comments": "5 pages, 3 figures, IEEE ICASSP 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12036"
    },
    {
        "doc_id": 180,
        "title": "Momentum-SAM: Sharpness Aware Minimization without Computational Overhead",
        "authors": [
            "Marlon Becker",
            "Frederick Altrock",
            "Benjamin Risse"
        ],
        "subjects": [
            "Machine Learning",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss. While significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam. We evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization. Code is available at https://github.com/MarlonBecker/MSAM.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12033"
    },
    {
        "doc_id": 181,
        "title": "MINT: A wrapper to make multi-modal and multi-image AI models interactive",
        "authors": [
            "Jan Freyberg",
            "Abhijit Guha Roy",
            "Terry Spitz",
            "Beverly Freeman",
            "Mike Schaekermann",
            "Patricia Strachan",
            "Eva Schnider",
            "Renee Wong",
            "Dale R Webster",
            "Alan Karthikesalingam",
            "Yun Liu",
            "Krishnamurthy Dvijotham",
            "Umesh Telang"
        ],
        "subjects": [
            "Human-Computer Interaction",
            "Artificial Intelligence"
        ],
        "abstract": "During the diagnostic process, doctors incorporate multimodal information including imaging and the medical history - and similarly medical AI development has increasingly become multimodal. In this paper we tackle a more subtle challenge: doctors take a targeted medical history to obtain only the most pertinent pieces of information; how do we enable AI to do the same? We develop a wrapper method named MINT (Make your model INTeractive) that automatically determines what pieces of information are most valuable at each step, and ask for only the most useful information. We demonstrate the efficacy of MINT wrapping a skin disease prediction model, where multiple images and a set of optional answers to $25$ standard metadata questions (i.e., structured medical history) are used by a multi-modal deep network to provide a differential diagnosis. We show that MINT can identify whether metadata inputs are needed and if so, which question to ask next. We also demonstrate that when collecting multiple images, MINT can identify if an additional image would be beneficial, and if so, which type of image to capture. We showed that MINT reduces the number of metadata and image inputs needed by 82% and 36.2% respectively, while maintaining predictive performance. Using real-world AI dermatology system data, we show that needing fewer inputs can retain users that may otherwise fail to complete the system submission and drop off without a diagnosis. Qualitative examples show MINT can closely mimic the step-by-step decision making process of a clinical workflow and how this is different for straight forward cases versus more difficult, ambiguous cases. Finally we demonstrate how MINT is robust to different underlying multi-model classifiers and can be easily adapted to user requirements without significant model re-training.",
        "comments": "15 pages, 7 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12032"
    },
    {
        "doc_id": 182,
        "title": "Near-Field Localization with $1$-bit Quantized Hybrid A/D Reception",
        "authors": [
            "Ioannis Gavras",
            "Italo Atzeni",
            "George C. Alexandropoulos"
        ],
        "subjects": [
            "Information Theory",
            "Emerging Technologies"
        ],
        "abstract": "In this paper, we consider a hybrid Analog and Digital (A/D) receiver architecture with an extremely large Dynamic Metasurface Antenna (DMA) and an $1$-bit resolution Analog-to-Digital Converter (ADC) at each of its reception radio-frequency chains, and present a localization approach for User Equipment (UE) lying in its near-field regime. The proposed algorithm scans the UE area of interest to identify the DMA-based analog combining configuration resulting to the peak in a received pseudo-spectrum, yielding the UE position estimation in three dimensions. Our simulation results demonstrate the validity of the proposed scheme, especially for increasing DMA sizes, and showcase the interplay among various system parameters.",
        "comments": "5 pages, 3 figures, IEEE ICASSP 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12029"
    },
    {
        "doc_id": 183,
        "title": "A Survey of Advances in Optimization Methods for Wireless Communication System Design",
        "authors": [
            "Ya-Feng Liu",
            "Tsung-Hui Chang",
            "Mingyi Hong",
            "Zheyu Wu",
            "Anthony Man-Cho So",
            "Eduard A. Jorswieck",
            "Wei Yu"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing",
            "Optimization and Control"
        ],
        "abstract": "Mathematical optimization is now widely regarded as an indispensable modeling and solution tool for the design of wireless communications systems. While optimization has played a significant role in the revolutionary progress in wireless communication and networking technologies from 1G to 5G and onto the future 6G, the innovations in wireless technologies have also substantially transformed the nature of the underlying mathematical optimization problems upon which the system designs are based and have sparked significant innovations in the development of methodologies to understand, to analyze, and to solve those problems. In this paper, we provide a comprehensive survey of recent advances in mathematical optimization theory and algorithms for wireless communication system design. We begin by illustrating common features of mathematical optimization problems arising in wireless communication system design. We discuss various scenarios and use cases and their associated mathematical structures from an optimization perspective. We then provide an overview of recent advances in mathematical optimization theory and algorithms, from nonconvex optimization, global optimization, and integer programming, to distributed optimization and learning-based optimization. The key to successful solution of mathematical optimization problems is in carefully choosing and/or developing suitable optimization algorithms (or neural network architectures) that can exploit the underlying problem structure. We conclude the paper by identifying several open research challenges and outlining future research directions.",
        "comments": "47 pages, 10 figures, submitted for possible publication",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12025"
    },
    {
        "doc_id": 184,
        "title": "Multimodal Visual-Tactile Representation Learning through Self-Supervised Contrastive Pre-Training",
        "authors": [
            "Vedant Dave",
            "Fotios Lygerakis",
            "Elmar Rueckert"
        ],
        "subjects": [
            "Robotics",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "The rapidly evolving field of robotics necessitates methods that can facilitate the fusion of multiple modalities. Specifically, when it comes to interacting with tangible objects, effectively combining visual and tactile sensory data is key to understanding and navigating the complex dynamics of the physical world, enabling a more nuanced and adaptable response to changing environments. Nevertheless, much of the earlier work in merging these two sensory modalities has relied on supervised methods utilizing datasets labeled by humans.This paper introduces MViTac, a novel methodology that leverages contrastive learning to integrate vision and touch sensations in a self-supervised fashion. By availing both sensory inputs, MViTac leverages intra and inter-modality losses for learning representations, resulting in enhanced material property classification and more adept grasping prediction. Through a series of experiments, we showcase the effectiveness of our method and its superiority over existing state-of-the-art self-supervised and supervised techniques. In evaluating our methodology, we focus on two distinct tasks: material classification and grasping success prediction. Our results indicate that MViTac facilitates the development of improved modality encoders, yielding more robust representations as evidenced by linear probing assessments.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12024"
    },
    {
        "doc_id": 185,
        "title": "A Simulation of Optimal Dryness When Moving in the Rain or Snow Using MATLAB",
        "authors": [
            "Neil Zhao",
            "Emilee Brockner",
            "Asia Winslow",
            "Megan Seraydarian"
        ],
        "subjects": [
            "Discrete Mathematics",
            "Mathematical Software"
        ],
        "abstract": "The classic question of whether one should walk or run in the rain to remain the least wet has inspired a myriad of solutions ranging from physically performing test runs in raining conditions to mathematically modeling human movement through rain. This manuscript approaches the classical problem by simulating movement through rainfall using MATLAB. Our simulation was generalizable to include snowfall as well. An increase in walking speed resulted in a corresponding decrease in raindrop and snowflake collisions. When raindrops or snowflakes were given a horizontal movement vector due to wind, a local minimum in collisions was achieved when moving in parallel with the same horizontal speed as the raindrop; no local minimum was detected with antiparallel movement. In general, our simulation revealed that the faster one moves, the drier one remains.",
        "comments": "15 pages, 9 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12023"
    },
    {
        "doc_id": 186,
        "title": "Stereo-Matching Knowledge Distilled Monocular Depth Estimation Filtered by Multiple Disparity Consistency",
        "authors": [
            "Woonghyun Ka",
            "Jae Young Lee",
            "Jaehyun Choi",
            "Junmo Kim"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "In stereo-matching knowledge distillation methods of the self-supervised monocular depth estimation, the stereo-matching network's knowledge is distilled into a monocular depth network through pseudo-depth maps. In these methods, the learning-based stereo-confidence network is generally utilized to identify errors in the pseudo-depth maps to prevent transferring the errors. However, the learning-based stereo-confidence networks should be trained with ground truth (GT), which is not feasible in a self-supervised setting. In this paper, we propose a method to identify and filter errors in the pseudo-depth map using multiple disparity maps by checking their consistency without the need for GT and a training process. Experimental results show that the proposed method outperforms the previous methods and works well on various configurations by filtering out erroneous areas where the stereo-matching is vulnerable, especially such as textureless regions, occlusion boundaries, and reflective surfaces.",
        "comments": "ICASSP 2024. The first two authors are equally contributed",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12019"
    },
    {
        "doc_id": 187,
        "title": "PairwiseHist: Fast, Accurate and Space-Efficient Approximate Query Processing with Data Compression",
        "authors": [
            "Aaron Hurst",
            "Daniel E. Lucani",
            "Qi Zhang"
        ],
        "subjects": [
            "Databases"
        ],
        "abstract": "Exponential growth in data collection is creating significant challenges for data storage and analytics latency.Approximate Query Processing (AQP) has long been touted as a solution for accelerating analytics on large datasets, however, there is still room for improvement across all key performance criteria. In this paper, we propose a novel histogram-based data synopsis called PairwiseHist that uses recursive hypothesis testing to ensure accurate histograms and can be built on top of data compressed using Generalized Deduplication (GD). We thus show that GD data compression can contribute to AQP. Compared to state-of-the-art AQP approaches, PairwiseHist achieves better performance across all key metrics, including 2.6$ \\times $ higher accuracy, 3.5$ \\times $ lower latency, 24$ \\times $ smaller synopses and 1.5--4$ \\times $ faster construction time.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12018"
    },
    {
        "doc_id": 188,
        "title": "Robustness to distribution shifts of compressed networks for edge devices",
        "authors": [
            "Lulan Shen",
            "Ali Edalati",
            "Brett Meyer",
            "Warren Gross",
            "James J. Clark"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "It is necessary to develop efficient DNNs deployed on edge devices with limited computation resources. However, the compressed networks often execute new tasks in the target domain, which is different from the source domain where the original network is trained. It is important to investigate the robustness of compressed networks in two types of data distribution shifts: domain shifts and adversarial perturbations. In this study, we discover that compressed models are less robust to distribution shifts than their original networks. Interestingly, larger networks are more vulnerable to losing robustness than smaller ones, even when they are compressed to a similar size as the smaller networks. Furthermore, compact networks obtained by knowledge distillation are much more robust to distribution shifts than pruned networks. Finally, post-training quantization is a reliable method for achieving significant robustness to distribution shifts, and it outperforms both pruned and distilled models in terms of robustness.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12014"
    },
    {
        "doc_id": 189,
        "title": "TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients",
        "authors": [
            "Mengdi Wang",
            "Anna Bodonhelyi",
            "Efe Bozkir",
            "Enkelejda Kasneci"
        ],
        "subjects": [
            "Machine Learning",
            "Distributed, Parallel, and Cluster Computing"
        ],
        "abstract": "Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are \"lazy\" and train their models solely for few epochs for next global aggregation. TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings. We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution. Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12012"
    },
    {
        "doc_id": 190,
        "title": "Architecting Data-Intensive Applications : From Data Architecture Design to Its Quality Assurance",
        "authors": [
            "Moamin Abughazala"
        ],
        "subjects": [
            "Software Engineering"
        ],
        "abstract": "Context - The exponential growth of data is becoming a significant concern. Managing this data has become incredibly challenging, especially when dealing with various sources in different formats and speeds. Moreover, Ensuring data quality has become increasingly crucial for effective decision-making and operational processes. Data Architecture is crucial in describing, collecting, storing, processing, and analyzing data to meet business needs. Providing an abstract view of data-intensive applications is essential to ensure that the data is transformed into valuable information. We must take these challenges seriously to ensure we can effectively manage and use the data to our advantage. Objective - To establish an architecture framework that enables a comprehensive description of the data architecture and effectively streamlines data quality monitoring. Method - The architecture framework utilizes Model Driven Engineering (MDE) techniques. Its backing of data-intensive architecture descriptions empowers with an automated generation for data quality checks. Result - The Framework offers a comprehensive solution for data-intensive applications to model their architecture efficiently and monitor the quality of their data. It automates the entire process and ensures precision and consistency in data. With DAT, architects and analysts gain access to a powerful tool that simplifies their workflow and empowers them to make informed decisions based on reliable data insights. Conclusion - We have evaluated the DAT on more than five cases within various industry domains, demonstrating its exceptional adaptability and effectiveness.",
        "comments": "PhD thesis",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12011"
    },
    {
        "doc_id": 191,
        "title": "On a class of interdiction problems with partition matroids: complexity and polynomial-time algorithms",
        "authors": [
            "Sergey S. Ketkov",
            "Oleg A. Prokopyev"
        ],
        "subjects": [
            "Computational Complexity",
            "Optimization and Control"
        ],
        "abstract": "In this study, we consider a class of linear matroid interdiction problems, where the feasible sets for the upper-level decision-maker (referred to as the leader) and the lower-level decision-maker (referred to as the follower) are given by partition matroids with a common ground set. In contrast to classical network interdiction models where the leader is subject to a single budget constraint, in our setting, both the leader and the follower are subject to several independent cardinality constraints and engage in a zero-sum game. While a single-level linear integer programming problem over a partition matroid is known to be polynomially solvable, we prove that the considered bilevel problem is NP-hard, even when the objective function coefficients are all binary. On a positive note, it turns out that, if the number of cardinality constraints is fixed for either the leader or the follower, then the considered class of bilevel problems admits several polynomial-time solution schemes. Specifically, these schemes are based on a single-level dual reformulation, a dynamic programming-based approach, and a 2-flip local search algorithm for the leader.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12010"
    },
    {
        "doc_id": 192,
        "title": "Tensor-view Topological Graph Neural Network",
        "authors": [
            "Tao Wen",
            "Elynn Chen",
            "Yuzhou Chen"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence"
        ],
        "abstract": "Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation learning modules that disentangle feature tensor aggregation and transformation and learn to preserve multi-modal structure with less computation. Theoretically, we derive high probability bounds on both the out-of-sample and in-sample mean squared approximation errors for our proposed Tensor Transformation Layer (TTL). Real data experiments show that the proposed TTG-NN outperforms 20 state-of-the-art methods on various graph benchmarks.",
        "comments": "Accepted at AISTATS 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12007"
    },
    {
        "doc_id": 193,
        "title": "ALMs: Authorial Language Models for Authorship Attribution",
        "authors": [
            "Weihang Huang",
            "Akira Murakami",
            "Jack Grieve"
        ],
        "subjects": [
            "Computation and Language"
        ],
        "abstract": "In this paper, we introduce an authorship attribution method called Authorial Language Models (ALMs) that involves identifying the most likely author of a questioned document based on the perplexity of the questioned document calculated for a set of causal language models fine-tuned on the writings of a set of candidate author. We benchmarked ALMs against state-of-art-systems using the CCAT50 dataset and the Blogs50 datasets. We find that ALMs achieves a macro-average accuracy score of 83.6% on Blogs50, outperforming all other methods, and 74.9% on CCAT50, matching the performance of the best method. To assess the performance of ALMs on shorter texts, we also conducted text ablation testing. We found that to reach a macro-average accuracy of 70%, ALMs needs 40 tokens on Blogs50 and 400 tokens on CCAT50, while to reach 60% ALMs requires 20 tokens on Blogs50 and 70 tokens on CCAT50.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12005"
    },
    {
        "doc_id": 194,
        "title": "NLCG-Net: A Model-Based Zero-Shot Learning Framework for Undersampled Quantitative MRI Reconstruction",
        "authors": [
            "Xinrui Jiang",
            "Yohan Jun",
            "Jaejin Cho",
            "Mengze Gao",
            "Xingwang Yong",
            "Berkin Bilgic"
        ],
        "subjects": [
            "Image and Video Processing",
            "Machine Learning",
            "Signal Processing"
        ],
        "abstract": "Typical quantitative MRI (qMRI) methods estimate parameter maps after image reconstructing, which is prone to biases and error propagation. We propose a Nonlinear Conjugate Gradient (NLCG) optimizer for model-based T2/T1 estimation, which incorporates U-Net regularization trained in a scan-specific manner. This end-to-end method directly estimates qMRI maps from undersampled k-space data using mono-exponential signal modeling with zero-shot scan-specific neural network regularization to enable high fidelity T1 and T2 mapping. T2 and T1 mapping results demonstrate the ability of the proposed NLCG-Net to improve estimation quality compared to subspace reconstruction at high accelerations.",
        "comments": "8 pages, 5 figures, submitted to International Society for Magnetic Resonance in Medicine 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12004"
    },
    {
        "doc_id": 195,
        "title": "HgbNet: predicting hemoglobin level/anemia degree from EHR data",
        "authors": [
            "Zhuo Zhi",
            "Moe Elbadawi",
            "Adam Daneshmend",
            "Mine Orlu",
            "Abdul Basit",
            "Andreas Demosthenous",
            "Miguel Rodrigues"
        ],
        "subjects": [
            "Machine Learning"
        ],
        "abstract": "Anemia is a prevalent medical condition that typically requires invasive blood tests for diagnosis and monitoring. Electronic health records (EHRs) have emerged as valuable data sources for numerous medical studies. EHR-based hemoglobin level/anemia degree prediction is non-invasive and rapid but still faces some challenges due to the fact that EHR data is typically an irregular multivariate time series containing a significant number of missing values and irregular time intervals. To address these issues, we introduce HgbNet, a machine learning-based prediction model that emulates clinicians' decision-making processes for hemoglobin level/anemia degree prediction. The model incorporates a NanDense layer with a missing indicator to handle missing values and employs attention mechanisms to account for both local irregularity and global irregularity. We evaluate the proposed method using two real-world datasets across two use cases. In our first use case, we predict hemoglobin level/anemia degree at moment T+1 by utilizing records from moments prior to T+1. In our second use case, we integrate all historical records with additional selected test results at moment T+1 to predict hemoglobin level/anemia degree at the same moment, T+1. HgbNet outperforms the best baseline results across all datasets and use cases. These findings demonstrate the feasibility of estimating hemoglobin levels and anemia degree from EHR data, positioning HgbNet as an effective non-invasive anemia diagnosis solution that could potentially enhance the quality of life for millions of affected individuals worldwide. To our knowledge, HgbNet is the first machine learning model leveraging EHR data for hemoglobin level/anemia degree prediction.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12002"
    },
    {
        "doc_id": 196,
        "title": "Modeling Stereo-Confidence Out of the End-to-End Stereo-Matching Network via Disparity Plane Sweep",
        "authors": [
            "Jae Young Lee",
            "Woonghyun Ka",
            "Jaehyun Choi",
            "Junmo Kim"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "We propose a novel stereo-confidence that can be measured externally to various stereo-matching networks, offering an alternative input modality choice of the cost volume for learning-based approaches, especially in safety-critical systems. Grounded in the foundational concepts of disparity definition and the disparity plane sweep, the proposed stereo-confidence method is built upon the idea that any shift in a stereo-image pair should be updated in a corresponding amount shift in the disparity map. Based on this idea, the proposed stereo-confidence method can be summarized in three folds. 1) Using the disparity plane sweep, multiple disparity maps can be obtained and treated as a 3-D volume (predicted disparity volume), like the cost volume is constructed. 2) One of these disparity maps serves as an anchor, allowing us to define a desirable (or ideal) disparity profile at every spatial point. 3) By comparing the desirable and predicted disparity profiles, we can quantify the level of matching ambiguity between left and right images for confidence measurement. Extensive experimental results using various stereo-matching networks and datasets demonstrate that the proposed stereo-confidence method not only shows competitive performance on its own but also consistent performance improvements when it is used as an input modality for learning-based stereo-confidence methods.",
        "comments": "AAAI 2024. The first two authors contributed equally",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12001"
    },
    {
        "doc_id": 197,
        "title": "Integrating Statistical Significance and Discriminative Power in Pattern Discovery",
        "authors": [
            "Leonardo Alexandre",
            "Rafael S. Costa",
            "Rui Henriques"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains. Actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power. Our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality. We also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria. To test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\u03b4$-Trimax and TriGen, that use various pattern quality criteria, such as Mean Squared Residual (MSR), Least Squared Lines (LSL), and Multi Slope Measure (MSL). Results from three case studies show the role of the proposed methodology in discovering patterns with pronounced improvements of discriminative power and statistical significance without quality deterioration, highlighting its importance in supervisedly guiding the search. Although the proposed methodology is motivated over multivariate time series data, it can be straightforwardly extended to pattern discovery tasks involving multivariate, N-way (N>3), transactional, and sequential data structures.\n  Availability: The code is freely available at https://github.com/JupitersMight/MOF_Triclustering under the MIT license.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12000"
    },
    {
        "doc_id": 198,
        "title": "Expert-Driven Monitoring of Operational ML Models",
        "authors": [
            "Joran Leest",
            "Claudia Raibulet",
            "Ilias Gerostathopoulos",
            "Patricia Lago"
        ],
        "subjects": [
            "Machine Learning",
            "Software Engineering"
        ],
        "abstract": "We propose Expert Monitoring, an approach that leverages domain expertise to enhance the detection and mitigation of concept drift in machine learning (ML) models. Our approach supports practitioners by consolidating domain expertise related to concept drift-inducing events, making this expertise accessible to on-call personnel, and enabling automatic adaptability with expert oversight.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11993"
    },
    {
        "doc_id": 199,
        "title": "Tight Bounds on the Message Complexity of Distributed Tree Verification",
        "authors": [
            "Shay Kutten",
            "Peter Robinson",
            "Ming Ming Tan"
        ],
        "subjects": [
            "Distributed, Parallel, and Cluster Computing",
            "Data Structures and Algorithms"
        ],
        "abstract": "We consider the message complexity of verifying whether a given subgraph of the communication network forms a tree with specific properties both in the KT-$\u03c1$ (nodes know their $\u03c1$-hop neighborhood, including node IDs) and the KT-$0$ (nodes do not have this knowledge) models. We develop a rather general framework that helps in establishing tight lower bounds for various tree verification problems. We also consider two different verification requirements: namely that every node detects in the case the input is incorrect, as well as the requirement that at least one node detects. The results are stronger than previous ones in the sense that we assume that each node knows the number $n$ of nodes in the graph (in some cases) or an $\u03b1$ approximation of $n$ (in other cases). For spanning tree verification, we show that the message complexity inherently depends on the quality of the given approximation of $n$: We show a tight lower bound of $\u03a9(n^2)$ for the case $\u03b1\\ge \\sqrt{2}$ and a much better upper bound (i.e., $O(n \\log n)$) when nodes are given a tighter approximation. On the other hand, our framework also yields an $\u03a9(n^2)$ lower bound on the message complexity of verifying a minimum spanning tree (MST), which reveals a polynomial separation between ST verification and MST verification. This result holds for randomized algorithms with perfect knowledge of the network size, and even when just one node detects illegal inputs, thus improving over the work of Kor, Korman, and Peleg (2013). For verifying a $d$-approximate BFS tree, we show that the same lower bound holds even if nodes know $n$ exactly, however, the lower bound is sensitive to $d$, which is the stretch parameter.",
        "comments": "Appeared at OPODIS 2023",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11991"
    },
    {
        "doc_id": 200,
        "title": "Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning",
        "authors": [
            "Philip Amortila",
            "Tongyi Cao",
            "Akshay Krishnamurthy"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Optimization and Control"
        ],
        "abstract": "A pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ. As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. In this paper, we study the effect of distribution shift in the presence of model misspecification, specifically focusing on $L_{\\infty}$-misspecified regression and adversarial covariate shift, where the regression target remains fixed while the covariate distribution changes arbitrarily. We show that empirical risk minimization, or standard least squares regression, can result in undesirable misspecification amplification where the error due to misspecification is amplified by the density ratio between the training and testing distributions. As our main result, we develop a new algorithm -- inspired by robust optimization techniques -- that avoids this undesirable behavior, resulting in no misspecification amplification while still obtaining optimal statistical rates. As applications, we use this regression procedure to obtain new guarantees in offline and online reinforcement learning with misspecification and establish new separations between previously studied structural conditions and notions of coverage.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12216"
    },
    {
        "doc_id": 201,
        "title": "Unsupervised Machine Learning for the Classification of Astrophysical X-ray Sources",
        "authors": [
            "V\u00edctor Samuel P\u00e9rez-D\u00edaz",
            "Juan Rafael Mart\u00ednez-Galarza",
            "Alexander Caicedo",
            "Raffaele D'Abrusco"
        ],
        "subjects": [
            "Instrumentation and Methods for Astrophysics",
            "Artificial Intelligence"
        ],
        "abstract": "The automatic classification of X-ray detections is a necessary step in extracting astrophysical information from compiled catalogs of astrophysical sources. Classification is useful for the study of individual objects, statistics for population studies, as well as for anomaly detection, i.e., the identification of new unexplored phenomena, including transients and spectrally extreme sources. Despite the importance of this task, classification remains challenging in X-ray astronomy due to the lack of optical counterparts and representative training sets. We develop an alternative methodology that employs an unsupervised machine learning approach to provide probabilistic classes to Chandra Source Catalog sources with a limited number of labeled sources, and without ancillary information from optical and infrared catalogs. We provide a catalog of probabilistic classes for 8,756 sources, comprising a total of 14,507 detections, and demonstrate the success of the method at identifying emission from young stellar objects, as well as distinguishing between small-scale and large-scale compact accretors with a significant level of confidence. We investigate the consistency between the distribution of features among classified objects and well-established astrophysical hypotheses such as the unified AGN model. This provides interpretability to the probabilistic classifier. Code and tables are available publicly through GitHub. We provide a web playground for readers to explore our final classification at https://umlcaxs-playground.streamlit.app.",
        "comments": "21 pages, 11 figures. Accepted in MNRAS",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12203"
    },
    {
        "doc_id": 202,
        "title": "Using spatial extreme-value theory with machine learning to model and understand spatially compounding extremes",
        "authors": [
            "Jonathan Koh",
            "Daniel Steinfeld",
            "Olivia Martius"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "When extreme weather events affect large areas, their regional to sub-continental spatial scale is important for their impacts. We propose a novel methodology that combines spatial extreme-value theory with a machine learning (ML) algorithm to model weather extremes and quantify probabilities associated with the occurrence, intensity and spatial extent of these events. The model is here applied to Western European summertime heat extremes. Using new loss functions adapted to extreme values, we fit a theoretically-motivated spatial model to extreme positive temperature anomaly fields from 1959-2022, using the daily 500-hpa geopotential height fields across the Euro-Atlantic region and the local soil moisture as predictors. Our generative model reveals the importance of individual circulation features in determining different facets of heat extremes, thereby enriching our process understanding of them from a data-driven perspective. The occurrence, intensity, and spatial extent of heat extremes are sensitive to the relative position of individual ridges and troughs that are part of a large-scale wave pattern. Heat extremes in Europe are thus the result of a complex interplay between local and remote physical processes. Our approach is able to extrapolate beyond the range of the data to make risk-related probabilistic statements, and applies more generally to other weather extremes. It also offers an attractive alternative to physical model-based techniques, or to ML approaches that optimise scores focusing on predicting well the bulk instead of the tail of the data distribution.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12195"
    },
    {
        "doc_id": 203,
        "title": "Concentration inequalities for the sample correlation coefficient",
        "authors": [
            "Daniel Salnikov"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "The sample correlation coefficient $R$ plays an important role in many statistical analyses. We study the moments of $R$ under the bivariate Gaussian model assumption, provide a novel approximation for its finite sample mean and connect it with known results for the variance. We exploit these approximations to present non-asymptotic concentration inequalities for $R$. Finally, we illustrate our results in a simulation experiment that further validates the approximations presented in this work.",
        "comments": "10 pages, preprint",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12190"
    },
    {
        "doc_id": 204,
        "title": "Semi-supervised segmentation of land cover images using nonlinear canonical correlation analysis with multiple features and t-SNE",
        "authors": [
            "Hong Wei",
            "James Xiao",
            "Yichao Zhang",
            "Xia Hong"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ],
        "abstract": "Image segmentation is a clustering task whereby each pixel is assigned a cluster label. Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available. This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple bands and a local patch as appropriate. Similarly, multiple types of texture features based on a pixel's local patch would also be beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount of ground truth, then training a supervised model, which is sometimes impractical. In this work, by resorting to label only a small quantity of pixels, a new semi-supervised segmentation approach is proposed. Initially, over all pixels, an image data matrix is created in high dimensional feature space. Then, t-SNE projects the high dimensional data onto 3D embedding. By using radial basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix via the small labelled data set. The associated canonical variables, obtained for the full image, are applied by k-means clustering algorithm. The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral images, demonstrating excellent segmentation results.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12164"
    },
    {
        "doc_id": 205,
        "title": "The accuracy of ALMA estimates of young disk radii and masses. Predicted observations from numerical simulations",
        "authors": [
            "Ngo-Duy Tung",
            "Leonardo Testi",
            "Ugo Lebreuilly",
            "Patrick Hennebelle",
            "Ana\u00eblle Maury",
            "Ralf S. Klessen",
            "Luca Cacciapuoti",
            "Matthias Gonz\u00e1lez",
            "Giovanni Rosotti",
            "Sergio Molinari"
        ],
        "subjects": [
            "Earth and Planetary Astrophysics",
            "Solar and Stellar Astrophysics"
        ],
        "abstract": "Protoplanetary disks, which are the natural consequence of the gravitational collapse of the dense molecular cloud cores, host the formation of the planetary systems known today in our universe. Numerous efforts have been dedicated to investigate the properties of these disks in the more mature Class II stage, either by using numerical simulations of disk evolution from a limited range of initial conditions or by observations of their dust continuum and line emission from specific molecular tracers, and to compare the results from the two standpoints. Yet few studies have investigated the main limitations at work when measuring the embedded Class 0/I disk properties from observations, especially in a statistical fashion. In this study, we provide a first attempt to compare the accuracy of some critical disk parameters in Class 0/I systems, as derived on real ALMA observational data, with the corresponding physical parameters that modellers can directly define in numerical simulations. The approach we follow is to provide full post-processing of the numerical simulations and apply on the synthetic observations the same techniques used by observers to derive the physical parameters. To that end, we performed 3D Monte Carlo radiative transfer and mock interferometric observations of the disk populations formed in an MHD simulation model of disk formation through the collapse of massive clumps with the tools RADMC-3D and CASA, respectively, to obtain their synthetic observations. With these observations, we re-employ the techniques commonly used in disk modelling from their continuum emissions to infer their properties that one would likely obtain if one observed them with real interferometers. We then demonstrate how their properties vary from the gas kinematics analyses to the dust continuum modelling.",
        "comments": "Accepted for publication in Astronomy & Astrophysics, 32 pages, 28 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12142"
    },
    {
        "doc_id": 206,
        "title": "Evaluation of QCNN-LSTM for Disability Forecasting in Multiple Sclerosis Using Sequential Multisequence MRI",
        "authors": [
            "John D. Mayfield",
            "Issam El Naqa"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Emerging Technologies",
            "Image and Video Processing"
        ],
        "abstract": "Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term Memory (LSTM) models were studied to provide sequential relationships for each timepoint in MRIs of patients with Multiple Sclerosis (MS). In this pilot study, we compared three QCNN-LSTM models for binary classification of MS disability benchmarked against classical neural network architectures. Our hypothesis is that quantum models will provide competitive performance. Methods Matrix Product State (MPS), reverse Multistate Entanglement Renormalization Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM layer to process near-annual MRI data of patients diagnosed with MS. These were benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision Transformer (ViViT). Predicted logits were measured against ground truth labels of each patient's Extended Disability Severity Score (EDSS) using binary cross-entropy loss. Training/validation/holdout testing was partitioned using 5-fold cross validation with a total split of 60:20:20. Levene's test of variance was used to measure statistical difference and Student's t-test for paired model differences in mean. Results The MPS-LSTM, reverse MERA-LSTM, and TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively (p-value 0.915). VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73 and 0.77, respectively (p-value 0.631). Overall variance and mean were not statistically significant (p-value 0.713), however, time to train was significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218, respectively, p-value <0.001). Conclusion QCNN-LSTM models perform competitively to their classical counterparts with greater efficiency in train time. Clinically, these can add value in terms of efficiency to time-dependent deep learning prediction of disease progression based upon medical imaging.",
        "comments": "ACM Class:          I.2.0; I.2.6",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12132"
    },
    {
        "doc_id": 207,
        "title": "Biological species delimitation based on genetic and spatial dissimilarity: a comparative study",
        "authors": [
            "Gabriele d'Angella",
            "Christian Hennig"
        ],
        "subjects": [
            "Populations and Evolution",
            "Applications",
            "Methodology"
        ],
        "abstract": "The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. This paper investigates the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Various approaches are compared (some of which already exist in the literature) based on simulated metapopulations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Approaches involve partial Mantel testing, maximum likelihood mixed-effects models with a population effect, and jackknife-based homogeneity tests. A key challenge is that most tests perform on genetic and geographical distance data, violating standard independence assumptions. Simulations showed that partial Mantel tests and mixed-effects models have larger power than jackknife-based methods, but tend to display type-I-error rates slightly above the significance level. Moreover, a multiple regression model neglecting the dependence in the dissimilarities did not show inflated type-I-error rate. An application on brassy ringlets concludes the paper.",
        "comments": "paper of 23 pages with 4 figures; appendix of 11 pages with 4 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12126"
    },
    {
        "doc_id": 208,
        "title": "Temporal Aggregation for the Synthetic Control Method",
        "authors": [
            "Liyang Sun",
            "Eli Ben-Michael",
            "Avi Feller"
        ],
        "subjects": [
            "Econometrics",
            "Methodology"
        ],
        "abstract": "The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit with panel data. Two challenges arise with higher frequency data (e.g., monthly versus yearly): (1) achieving excellent pre-treatment fit is typically more challenging; and (2) overfitting to noise is more likely. Aggregating data over time can mitigate these problems but can also destroy important signal. In this paper, we bound the bias for SCM with disaggregated and aggregated outcomes and give conditions under which aggregating tightens the bounds. We then propose finding weights that balance both disaggregated and aggregated series.",
        "comments": "9 pages, 3 figures, Prepared for 2024 AEA Papers and Proceedings \"Treatment Effects: Theory and Implementation\"",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12084"
    },
    {
        "doc_id": 209,
        "title": "The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization",
        "authors": [
            "Matan Schliserman",
            "Uri Sherman",
            "Tomer Koren"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "We study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence. First, for full-batch gradient descent (GD) we give a construction of a learning problem in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance of the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\u03a9(1)$ population excess risk. Our bound translates to a lower bound of $\u03a9(\\sqrt{d})$ on the number of training examples required for standard GD to reach a non-trivial test error, answering an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b) and showing that a non-trivial dimension dependence is unavoidable. Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that an application of the same construction technique provides a similar $\u03a9(\\sqrt{d})$ lower bound for the sample complexity of SGD to reach a non-trivial empirical error, despite achieving optimal test performance. This again provides an exponential improvement in the dimension dependence compared to previous work (Koren, Livni, Mansour, and Sherman, 2022), resolving an open question left therein.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12058"
    },
    {
        "doc_id": 210,
        "title": "A Bracketing Relationship for Long-Term Policy Evaluation with Combined Experimental and Observational Data",
        "authors": [
            "Yechan Park",
            "Yuya Sasaki"
        ],
        "subjects": [
            "Econometrics"
        ],
        "abstract": "Combining short-term experimental data with observational data enables credible long-term policy evaluation. The literature offers two key but non-nested assumptions, namely the latent unconfoundedness (LU; Athey et al., 2020) and equi-confounding bias (ECB; Ghassami et al., 2022) conditions, to correct observational selection. Committing to the wrong assumption leads to biased estimation. To mitigate such risks, we provide a novel bracketing relationship (cf. Angrist and Pischke, 2009) repurposed for the setting with data combination: the LU-based estimand and the ECB-based estimand serve as the lower and upper bounds, respectively, with the true causal effect lying in between if either assumption holds. For researchers further seeking point estimates, our Lalonde-style exercise suggests the conservatively more robust LU-based lower bounds align closely with the hold-out experimental estimates for educational policy evaluation. We investigate the economic substantives of these findings through the lens of a nonparametric class of selection mechanisms and sensitivity analysis. We uncover as key the sub-martingale property and sufficient-statistics role (Chetty, 2009) of the potential outcomes of student test scores (Chetty et al., 2011, 2014).",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12050"
    },
    {
        "doc_id": 211,
        "title": "Multi-objective optimisation using expected quantile improvement for decision making in disease outbreaks",
        "authors": [
            "Daria Semochkina",
            "Alexander I. J. Forrester",
            "David C Woods"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Optimization under uncertainty is important in many applications, particularly to inform policy and decision making in areas such as public health. A key source of uncertainty arises from the incorporation of environmental variables as inputs into computational models or simulators. Such variables represent uncontrollable features of the optimization problem and reliable decision making must account for the uncertainty they propagate to the simulator outputs. Often, multiple, competing objectives are defined from these outputs such that the final optimal decision is a compromise between different goals.\n  Here, we present emulation-based optimization methodology for such problems that extends expected quantile improvement (EQI) to address multi-objective optimization. Focusing on the practically important case of two objectives, we use a sequential design strategy to identify the Pareto front of optimal solutions. Uncertainty from the environmental variables is integrated out using Monte Carlo samples from the simulator. Interrogation of the expected output from the simulator is facilitated by use of (Gaussian process) emulators. The methodology is demonstrated on an optimization problem from public health involving the dispersion of anthrax spores across a spatial terrain. Environmental variables include meteorological features that impact the dispersion, and the methodology identifies the Pareto front even when there is considerable input uncertainty.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12031"
    },
    {
        "doc_id": 212,
        "title": "Four Gluon Vertex from Lattice QCD",
        "authors": [
            "Manuel Cola\u00e7o",
            "Orlando Oliveira",
            "Paulo J. Silva"
        ],
        "subjects": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Theory"
        ],
        "abstract": "A lattice QCD calculation for the four gluon one-particle irreducible Green function in the Landau gauge is discussed. Results for some of the associated form factors are reported for kinematical configurations with a single momentum scale. Our results show that the computation of this Green function requires large statistical ensembles with 10K or larger number of gauge configurations. The simulations considered herein have a clear Monte Carlo signal for momenta up to $\\sim 1$ GeV. The form factors show an hierarchy, with the form factor associated with the tree level Feynman rule being dominant and essentially constant for the range of momenta accessed. The remaining form factors seem to increase as the momentum decreases, suggesting that a possible $\\log$ divergence may occur. The computed form factors are, at least, in qualitative agreement with the results obtained with continuum approaches to this vertex, when available.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12008"
    },
    {
        "doc_id": 213,
        "title": "Integrating Statistical Significance and Discriminative Power in Pattern Discovery",
        "authors": [
            "Leonardo Alexandre",
            "Rafael S. Costa",
            "Rui Henriques"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains. Actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power. Our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality. We also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria. To test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\u03b4$-Trimax and TriGen, that use various pattern quality criteria, such as Mean Squared Residual (MSR), Least Squared Lines (LSL), and Multi Slope Measure (MSL). Results from three case studies show the role of the proposed methodology in discovering patterns with pronounced improvements of discriminative power and statistical significance without quality deterioration, highlighting its importance in supervisedly guiding the search. Although the proposed methodology is motivated over multivariate time series data, it can be straightforwardly extended to pattern discovery tasks involving multivariate, N-way (N>3), transactional, and sequential data structures.\n  Availability: The code is freely available at https://github.com/JupitersMight/MOF_Triclustering under the MIT license.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12000"
    },
    {
        "doc_id": 214,
        "title": "Elasticity of self-organized frustrated disordered spring networks",
        "authors": [
            "Tommaso Pettinari",
            "Gustavo D\u00fcring",
            "Edan Lerner"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Statistical Mechanics"
        ],
        "abstract": "There have been some interesting recent advances in understanding the notion of mechanical disorder in structural glasses and the statistical mechanics of these systems' low-energy excitations. Here we contribute to these advances by studying a minimal model for structural glasses' elasticity in which the degree of mechanical disorder -- as characterized by recently introduced dimensionless quantifiers -- is readily tunable over a very large range. We comprehensively investigate a number of scaling laws observed for various macro-, meso- and microscopic elastic properties, and rationalize them using scaling arguments. Interestingly, we demonstrate that the model features the universal quartic glassy vibrational density of states as seen in many atomistic and molecular models of structural glasses formed by cooling a melt. The emergence of this universal glassy spectrum highlights the role of self-organization (towards mechanical equilibrium) in its formation, and elucidates why models featuring structural frustration alone do not feature the same universal glassy spectrum. Finally, we discuss relations to existing work in the context of strain-stiffening of elastic networks and of low-energy excitations in structural glasses, in addition to future research directions.",
        "comments": "9 pages, 7 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11996"
    },
    {
        "doc_id": 215,
        "title": "1/f noise in quantum nanoscience",
        "authors": [
            "Giuseppe Falci",
            "Pertti J. Hakonen",
            "Elisabetta Paladino"
        ],
        "subjects": [
            "Mesoscale and Nanoscale Physics",
            "Superconductivity"
        ],
        "abstract": "Fundamental issues of 1/f noise in quantum nanoscience are reviewed starting from basic statistical noise processes. Fundamental noise models based on two-level systems (TLS) are described. We emphasize the importance of TLSs in materials parameter fluctuations, such as dielectric constant. The present understanding of 1/f noise in superconducting quantum interferometers and in single electron devices is summarized. For coherent quantum nanoscience, we introduce superconducting qubits and the relation between decoherence and 1/f noise using the filter function formulation. We also clarify the qubit noise spectroscopy and emphasize the importance of materials with reduced 1/f noise for future quantum coherent nanodevices.",
        "comments": "15 pages, 4 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11989"
    },
    {
        "doc_id": 216,
        "title": "Cross-Validation Conformal Risk Control",
        "authors": [
            "Kfir M. Cohen",
            "Sangwoo Park",
            "Osvaldo Simeone",
            "Shlomo Shamai"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate. The original CRC requires the available data set to be split between training and validation data sets. This can be problematic when data availability is limited, resulting in inefficient set predictors. In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC. The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions. CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor. Furthermore, numerical experiments show that CV-CRC can reduce the average set size with respect to CRC when the available data are limited.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11974"
    },
    {
        "doc_id": 217,
        "title": "RUMBoost: Gradient Boosted Random Utility Models",
        "authors": [
            "Nicolas Salvad\u00e9",
            "Tim Hillel"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "This paper introduces the RUMBoost model, a novel discrete choice modelling approach that combines the interpretability and behavioural robustness of Random Utility Models (RUMs) with the generalisation and predictive ability of deep learning methods. We obtain the full functional form of non-linear utility specifications by replacing each linear parameter in the utility functions of a RUM with an ensemble of gradient boosted regression trees. This enables piece-wise constant utility values to be imputed for all alternatives directly from the data for any possible combination of input variables. We introduce additional constraints on the ensembles to ensure three crucial features of the utility specifications: (i) dependency of the utilities of each alternative on only the attributes of that alternative, (ii) monotonicity of marginal utilities, and (iii) an intrinsically interpretable functional form, where the exact response of the model is known throughout the entire input space. Furthermore, we introduce an optimisation-based smoothing technique that replaces the piece-wise constant utility values of alternative attributes with monotonic piece-wise cubic splines to identify non-linear parameters with defined gradient. We demonstrate the potential of the RUMBoost model compared to various ML and Random Utility benchmark models for revealed preference mode choice data from London. The results highlight the great predictive performance and the direct interpretability of our proposed approach. Furthermore, the smoothed attribute utility functions allow for the calculation of various behavioural indicators and marginal utilities. Finally, we demonstrate the flexibility of our methodology by showing how the RUMBoost model can be extended to complex model specifications, including attribute interactions, correlation within alternative error terms and heterogeneity within the population.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11954"
    },
    {
        "doc_id": 218,
        "title": "The Ensemble Kalman Filter for Dynamic Inverse Problems",
        "authors": [
            "Simon Weissmann",
            "Neil K. Chada",
            "Xin T. Tong"
        ],
        "subjects": [
            "Numerical Analysis",
            "Methodology"
        ],
        "abstract": "In inverse problems, the goal is to estimate unknown model parameters from noisy observational data. Traditionally, inverse problems are solved under the assumption of a fixed forward operator describing the observation model. In this article, we consider the extension of this approach to situations where we have a dynamic forward model, motivated by applications in scientific computation and engineering. We specifically consider this extension for a derivative-free optimizer, the ensemble Kalman inversion (EKI). We introduce and justify a new methodology called dynamic-EKI, which is a particle-based method with a changing forward operator. We analyze our new method, presenting results related to the control of our particle system through its covariance structure. This analysis includes moment bounds and an ensemble collapse, which are essential for demonstrating a convergence result. We establish convergence in expectation and validate our theoretical findings through experiments with dynamic-EKI applied to a 2D Darcy flow partial differential equation.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11948"
    },
    {
        "doc_id": 219,
        "title": "Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent",
        "authors": [
            "Zhiyu Liu",
            "Zhi Han",
            "Yandong Tang",
            "Xi-Le Zhao",
            "Yao Wang"
        ],
        "subjects": [
            "Machine Learning",
            "Optimization and Control",
            "Machine Learning"
        ],
        "abstract": "This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it is worth noting that our method does not require the precise estimation of the tensor tubal-rank. Even in cases where the tubal-rank is slightly overestimated, our approach continues to demonstrate robust performance. A series of experiments have been carried out to demonstrate that, as compared to other popular ones, our approach exhibits superior performance in multiple scenarios, in terms of the faster computational speed and the smaller convergence error.",
        "comments": "13 pages, 4 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11940"
    },
    {
        "doc_id": 220,
        "title": "Large deviation full counting statistics in adiabatic open quantum dynamics",
        "authors": [
            "Paulo J. Paulino",
            "Igor Lesanovsky",
            "Federico Carollo"
        ],
        "subjects": [
            "Statistical Mechanics",
            "Quantum Physics"
        ],
        "abstract": "The state of an open quantum system undergoing an adiabatic process evolves by following the instantaneous stationary state of its time-dependent generator. This observation allows one to characterize, for a generic adiabatic evolution, the average dynamics of the open system. However, information about fluctuations of dynamical observables, such as the number of photons emitted or the time-integrated stochastic entropy production in single experimental runs, requires controlling the whole spectrum of the generator and not only the stationary state. Here, we show how such information can be obtained in adiabatic open quantum dynamics by exploiting tools from large deviation theory. We prove an adiabatic theorem for deformed generators, which allows us to encode, in a biased quantum state, the full counting statistics of generic time-integrated dynamical observables. We further compute the probability associated with an arbitrary \"rare\" time-history of the observable and derive a dynamics which realizes it in its typical behavior. Our results provide a way to characterize and engineer adiabatic open quantum dynamics and to control their fluctuations.",
        "comments": "7 + 8 pages, 3 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11933"
    },
    {
        "doc_id": 221,
        "title": "Combination of searches for pair-produced leptoquarks at $\\sqrt{s} = 13$ TeV with the ATLAS detector",
        "authors": [
            "ATLAS Collaboration"
        ],
        "subjects": [
            "High Energy Physics - Experiment"
        ],
        "abstract": "A statistical combination of various searches for pair-produced leptoquarks is presented, using the full LHC Run 2 (2015-2018) data set of $139$ fb$^{-1}$ collected with the ATLAS detector from proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}=13$ TeV. All possible decays of the leptoquarks into quarks of the third generation and charged or neutral leptons of any generation are investigated. Since no significant deviations from the Standard Model expectation are observed in any of the individual analyses, combined exclusion limits are set on the production cross-sections for scalar and vector leptoquarks. The resulting lower bounds on leptoquark masses exceed those from the individual analyses by up to 100 GeV, depending on the signal hypothesis.",
        "comments": "36 pages in total, authorlist starting on p19, 7 figures, 2 tables submitted to Phys. Lett. B. All figures are available at http://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/EXOT-2020-27",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11928"
    },
    {
        "doc_id": 222,
        "title": "Inertia drives concentration-wave turbulence in swimmer suspensions",
        "authors": [
            "Purnima Jain",
            "Navdeep Rana",
            "Sriram Ramaswamy",
            "Prasad Perlekar"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Statistical Mechanics",
            "Fluid Dynamics"
        ],
        "abstract": "We discover an instability mechanism in suspensions of self-propelled particles that does not involve active stress. Instead, it is driven by a subtle interplay of inertia, swimmer motility, and concentration fluctuations, through a crucial time lag between the velocity and the concentration field. The resulting time-persistent state seen in our high-resolution numerical simulations consists of self-sustained waves of concentration and orientation, transiting from regular oscillations to wave turbulence. We analyze the statistical features of this active turbulence, including an intriguing connection to the Batchelor spectrum of passive scalars.",
        "comments": "11 pages and 9 figures including supplementary material",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11927"
    },
    {
        "doc_id": 223,
        "title": "Comparison of Model Output Statistics and Neural Networks to Postprocess Wind Gusts",
        "authors": [
            "Cristina Primo Ramos",
            "Benedikt Schulz",
            "Sebastian Lerch",
            "Reinhold Hess"
        ],
        "subjects": [
            "Applications",
            "Atmospheric and Oceanic Physics"
        ],
        "abstract": "Wind gust prediction plays an important role in warning strategies of national meteorological services due to the high impact of its extreme values. However, forecasting wind gusts is challenging because they are influenced by small-scale processes and local characteristics. To account for the different sources of uncertainty, meteorological centers run ensembles of forecasts and derive probabilities of wind gusts exceeding a threshold. These probabilities often exhibit systematic errors and require postprocessing. Model Output Statistics (MOS) is a common operational postprocessing technique, although more modern methods such as neural network-bases approaches have shown promising results in research studies. The transition from research to operations requires an exhaustive comparison of both techniques. Taking a first step into this direction, our study presents a comparison of a postprocessing technique based on linear and logistic regression approaches with different neural network methods proposed in the literature to improve wind gust predictions, specifically distributional regression networks and Bernstein quantile networks. We further contribute to investigating optimal design choices for neural network-based postprocessing methods regarding changes of the numerical model in the training period, the use of persistence predictors, and the temporal composition of training datasets. The performance of the different techniques is compared in terms of calibration, accuracy, reliability and resolution based on case studies of wind gust forecasts from the operational weather model of the German weather service and observations from 170 weather stations.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11896"
    },
    {
        "doc_id": 224,
        "title": "Bootstrap prediction regions for daily curves of electricity demand and price using functional data",
        "authors": [
            "Rebeca Pel\u00e1ez",
            "Germ\u00e1n Aneiros",
            "Juan Vilar"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "The aim of this paper is to compute one-day-ahead prediction regions for daily curves of electricity demand and price. Three model-based procedures to construct general prediction regions are proposed, all of them using bootstrap algorithms. The first proposed method considers any $L_p$ norm for functional data to measure the distance between curves, the second one is designed to take different variabilities along the curve into account, and the third one takes advantage of the notion of depth of a functional data. The regression model with functional response on which our proposed prediction regions are based is rather general: it allows to include both endogenous and exogenous functional variables, as well as exogenous scalar variables; in addition, the effect of such variables on the response one is modeled in a parametric, nonparametric or semi-parametric way. A comparative study is carried out to analyse the performance of these prediction regions for the electricity market of mainland Spain, in year 2012. This work extends and complements the methods and results in Aneiros et al. (2016) (focused on curve prediction) and Vilar et al. (2018) (focused on prediction intervals), which use the same database as here.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11885"
    },
    {
        "doc_id": 225,
        "title": "A theoretical framework for BL Her stars -- II. New period-luminosity relations in the Gaia passbands",
        "authors": [
            "Susmita Das",
            "L\u00e1szl\u00f3 Moln\u00e1r",
            "Shashi M. Kanbur",
            "Meridith Joyce",
            "Anupam Bhardwaj",
            "Harinder P. Singh",
            "Marcella Marconi",
            "Vincenzo Ripepi",
            "Radoslaw Smolec"
        ],
        "subjects": [
            "Solar and Stellar Astrophysics"
        ],
        "abstract": "We present new theoretical period-luminosity (PL) and period-Wesenheit (PW) relations for a fine grid of convective BL Her, the shortest period T2Cs, models computed using MESA-RSP and compare our results with the empirical relations from Gaia DR3. We use the state-of-the-art 1D non-linear radial stellar pulsation tool MESA-RSP to compute models of BL Her stars over a wide range of input parameters - metallicity (-2.0 dex $\\leq$ [Fe/H] $\\leq$ 0.0 dex), stellar mass (0.5M$_{\\odot}$-0.8M$_{\\odot}$), stellar luminosity (50L$_{\\odot}$-300L$_{\\odot}$) and effective temperature (full extent of the instability strip; in steps of 50K). The BL Her stars in the All Sky region exhibit statistically different PL slopes compared to the theoretical PL slopes computed using the four sets of convection parameters. We find the empirical PL and PW slopes from BL Her stars in the Magellanic Clouds to be statistically consistent with the theoretical relations computed using the different convection parameter sets in the Gaia passbands. There is negligible effect of metallicity on the PL relations in the individual Gaia passbands. However, there exists a small but significant negative coefficient of metallicity in the PWZ relations for the BL Her models using the four sets of convection parameters. This could be attributed to the increased sensitivity of bolometric corrections to metallicities at wavelengths shorter than the V band. Our BL Her models also suggest a dependence of the mass-luminosity relation on metallicity. We found the observed Fourier parameter space to be covered well by our models. Higher mass models (> 0.6M$_{\\odot}$) may be needed to reliably model the observed light curves of BL Her stars in the All Sky region. We also found the theoretical light curve structures (especially the Fourier amplitude parameters) to be affected by the choice of convection parameters.",
        "comments": "19 pages, 8 figures, accepted in Astronomy & Astrophysics",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11869"
    },
    {
        "doc_id": 226,
        "title": "Fast measurement of group index variation with ultimate precision using Hong-Ou-Mandel interferometry",
        "authors": [
            "Sandeep Singh",
            "Vimlesh Kumar",
            "G. K. Samanta"
        ],
        "subjects": [
            "Quantum Physics",
            "Optics"
        ],
        "abstract": "Hong-Ou-Mandel (HOM) interferometry has emerged as a valuable tool for quantum sensing applications, particularly in measuring physical parameters that influence the relative optical delay between pair photons. Unlike classical techniques, HOM-based quantum sensors offer higher resolution due to their intrinsic dispersion cancellation property. Despite this advantage, achieving precise measurements of optical delay crucial for practical applications often involves time-consuming integration and post-processing with traditional statistical methods. To address this challenge, our recent work focused on optimizing optical delay measurements in a time-efficient manner. By carefully selecting the length of a 1 mm periodically-poled KTP (PPKTP) crystal for pair photon generation, we achieved a remarkable group index measurement precision of $\\sim 6.75\\times 10^{-6}$ per centimeter of sample length, surpassing the previous maximum precision by over 400$\\%$. These current measurements maintain fast detection and high photon counts, which are essential for practical quantum sensing applications. The HOM-based method, while limiting the measurement range, can be extended by compensating for photon delay using an optical delay stage. As a proof-of-principle, we measured the group index variation of PPKTP over a temperature range up to 200$^{\\circ}$C with a precision in the range of one part per million ($\\sim$10$^{-6}$). This advancement not only contributes to quantum sensing but also holds promising implications for high-precision and long-range measurements in quantum optical coherence tomography.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11853"
    },
    {
        "doc_id": 227,
        "title": "Subgroup analysis methods for time-to-event outcomes in heterogeneous randomized controlled trials",
        "authors": [
            "Valentine Perrin",
            "Nathan Noiry",
            "Nicolas Loiseau",
            "Alex Nowak"
        ],
        "subjects": [
            "Methodology",
            "Applications",
            "Machine Learning"
        ],
        "abstract": "Non-significant randomized control trials can hide subgroups of good responders to experimental drugs, thus hindering subsequent development. Identifying such heterogeneous treatment effects is key for precision medicine and many post-hoc analysis methods have been developed for that purpose. While several benchmarks have been carried out to identify the strengths and weaknesses of these methods, notably for binary and continuous endpoints, similar systematic empirical evaluation of subgroup analysis for time-to-event endpoints are lacking. This work aims to fill this gap by evaluating several subgroup analysis algorithms in the context of time-to-event outcomes, by means of three different research questions: Is there heterogeneity? What are the biomarkers responsible for such heterogeneity? Who are the good responders to treatment? In this context, we propose a new synthetic and semi-synthetic data generation process that allows one to explore a wide range of heterogeneity scenarios with precise control on the level of heterogeneity. We provide an open source Python package, available on Github, containing our generation process and our comprehensive benchmark framework. We hope this package will be useful to the research community for future investigations of heterogeneity of treatment effects and subgroup analysis methods benchmarking.",
        "comments": "9 pages, 8 figures, 2 tables. Code available at: https://github.com/owkin/hte. Comments are welcome!",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11842"
    },
    {
        "doc_id": 228,
        "title": "The NOSTRA model: coherent estimation of infection sources in the case of possible nosocomial transmission",
        "authors": [
            "David J Pascall",
            "Chris Jackson",
            "Stephanie Evans",
            "Theodore Gouliouris",
            "Chris Illingworth",
            "Stefan Piatek",
            "Julie V Robotham",
            "Oliver Stirrup",
            "Ben Warne",
            "Judith Breuer",
            "Daniela De Angelis"
        ],
        "subjects": [
            "Applications",
            "Quantitative Methods"
        ],
        "abstract": "Nosocomial infections have important consequences for patients and hospital staff: they worsen patient outcomes and their management stresses already overburdened health systems. Accurate judgements of whether an infection is nosocomial helps staff make appropriate choices to protect other patients within the hospital. Nosocomiality cannot be properly assessed without considering whether the infected patient came into contact with high risk potential infectors within the hospital. We developed a Bayesian model that integrates epidemiological, contact and pathogen genetic data to determine how likely an infection is to be nosocomial and the probability of given infection candidates being the source of the infection.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11837"
    },
    {
        "doc_id": 229,
        "title": "NSPT for $O(N)$ non-linear sigma model: the larger $N$ the better",
        "authors": [
            "Paolo Baglioni",
            "Francesco Di Renzo"
        ],
        "subjects": [
            "High Energy Physics - Lattice"
        ],
        "abstract": "The $O(N)$ non-linear sigma model (NLSM) is an example of field theory on a target space with nontrivial geometry. One interesting feature of NLSM is asymptotic freedom, which makes perturbative calculations interesting. Given the successes in Lattice Gauge Theories, Numerical Stochastic Perturbation Theory (NSPT) is a natural candidate for performing high-order computations also in the case of NLSM. However, in low-dimensional systems NSPT is known to display statistical fluctuations substantially increasing for increasing orders. In this work, we explore how for $O(N)$ NLSM this behaviour is strongly dependent on $N$. As largely expected on general grounds, the larger is $N$, the larger is the order at which a NSPT computation can be effectively performed.",
        "comments": "Proceedings of the 40th International Symposium on Lattice Field Theory (Lattice 2023), July 31st - August 4th, 2023, Fermilab, Batavia, Illinois, USA",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11833"
    },
    {
        "doc_id": 230,
        "title": "A Fair Evaluation of Various Deep Learning-Based Document Image Binarization Approaches",
        "authors": [
            "Richin Sukesh",
            "Mathias Seuret",
            "Anguelos Nicolaou",
            "Martin Mayr",
            "Vincent Christlein"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Binarization of document images is an important pre-processing step in the field of document analysis. Traditional image binarization techniques usually rely on histograms or local statistics to identify a valid threshold to differentiate between different aspects of the image. Deep learning techniques are able to generate binarized versions of the images by learning context-dependent features that are less error-prone to degradation typically occurring in document images. In recent years, many deep learning-based methods have been developed for document binarization. But which one to choose? There have been no studies that compare these methods rigorously. Therefore, this work focuses on the evaluation of different deep learning-based methods under the same evaluation protocol. We evaluate them on different Document Image Binarization Contest (DIBCO) datasets and obtain very heterogeneous results. We show that the DE-GAN model was able to perform better compared to other models when evaluated on the DIBCO2013 dataset while DP-LinkNet performed best on the DIBCO2017 dataset. The 2-StageGAN performed best on the DIBCO2018 dataset while SauvolaNet outperformed the others on the DIBCO2019 challenge. Finally, we make the code, all models and evaluation publicly available (https://github.com/RichSu95/Document_Binarization_Collection) to ensure reproducibility and simplify future binarization evaluations.",
        "comments": "DAS 2022",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11831"
    },
    {
        "doc_id": 231,
        "title": "Flexible Models for Simple Longitudinal Data",
        "authors": [
            "Helen Ogden"
        ],
        "subjects": [
            "Methodology",
            "Applications"
        ],
        "abstract": "We propose a new method for estimating subject-specific mean functions from longitudinal data. We aim to do this in a flexible manner (without restrictive assumptions about the shape of the subject-specific mean functions), while exploiting similarities in the mean functions between different subjects. Functional principal components analysis fulfils both requirements, and methods for functional principal components analysis have been developed for longitudinal data. However, we find that these existing methods sometimes give fitted mean functions which are more complex than needed to provide a good fit to the data. We develop a new penalised likelihood approach to flexibly model longitudinal data, with a penalty term to control the balance between fit to the data and smoothness of the subject-specific mean curves. We run simulation studies to demonstrate that the new method substantially improves the quality of inference relative to existing methods across a range of examples, and apply the method to data on changes in body composition in adolescent girls.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11827"
    },
    {
        "doc_id": 232,
        "title": "On the importance of factorization for fast binned likelihood inference",
        "authors": [
            "C\u00e9sar",
            "Jes\u00fas-Valls"
        ],
        "subjects": [
            "High Energy Physics - Experiment"
        ],
        "abstract": "Likelihood-based inference, central in modern particle physics data analysis requires the extensive evaluation of a likelihood function that depends on set of parameters defined by the statistical model under consideration. If an analytical expression for the likelihood can be defined from first principles the procedure is computationally straightforward. However, most experiments require approximating the likelihood numerically using large statistical samples of synthetic events generated using Monte Carlo methods. As a result, the likelihood consists of a comparison of the expected versus the observed event rates in a collection of histogram bins, defining binned likelihood functions. When this occurs, evaluating the likelihood function involves, on each occasion, recalculating the prediction in those bins, increasing the computational load of these analysis drastically. In this text, I highlight the importance of identifying which are the unique event configurations in the binned likelihood definition and I provide an exact formula to update the event rate predictions utilizing the minimum number of necessary calculations by means of factorization. The aim of the discussion is to decrease the computational load of widespread high-energy physics analyses, leading to substantial speed improvements and reduced carbon footprints.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11806"
    },
    {
        "doc_id": 233,
        "title": "Regression Copulas for Multivariate Responses",
        "authors": [
            "Nadja Klein",
            "Michael Stanley Smith",
            "David Nott",
            "Ryan Chrisholm"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "We propose a novel distributional regression model for a multivariate response vector based on a copula process over the covariate space. It uses the implicit copula of a Gaussian multivariate regression, which we call a ``regression copula''. To allow for large covariate vectors their coefficients are regularized using a novel multivariate extension of the horseshoe prior. Bayesian inference and distributional predictions are evaluated using efficient variational inference methods, allowing application to large datasets. An advantage of the approach is that the marginal distributions of the response vector can be estimated separately and accurately, resulting in predictive distributions that are marginally-calibrated. Two substantive applications of the methodology highlight its efficacy in multivariate modeling. The first is the econometric modeling and prediction of half-hourly regional Australian electricity prices. Here, our approach produces more accurate distributional forecasts than leading benchmark methods. The second is the evaluation of multivariate posteriors in likelihood-free inference (LFI) of a model for tree species abundance data, extending a previous univariate regression copula LFI method. In both applications, we demonstrate that our new approach exhibits a desirable marginal calibration property.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11804"
    },
    {
        "doc_id": 234,
        "title": "Stein EWMA Control Charts for Count Processes",
        "authors": [
            "Christian H. Wei\u00df"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "The monitoring of serially independent or autocorrelated count processes is considered, having a Poisson or (negative) binomial marginal distribution under in-control conditions. Utilizing the corresponding Stein identities, exponentially weighted moving-average (EWMA) control charts are constructed, which can be flexibly adapted to uncover zero inflation, over- or underdispersion. The proposed Stein EWMA charts' performance is investigated by simulations, and their usefulness is demonstrated by a real-world data example from health surveillance.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11789"
    },
    {
        "doc_id": 235,
        "title": "Very High-Energy ($>$50 GeV) Gamma-ray Flux Variability of Bright Fermi Blazars",
        "authors": [
            "Vaidehi S. Paliya"
        ],
        "subjects": [
            "High Energy Astrophysical Phenomena"
        ],
        "abstract": "Understanding the high-energy emission processes and variability patterns are two of the most challenging research problems associated with relativistic jets. In particular, the long-term (months-to-years) flux variability at very high energies (VHE, $>$50 GeV) has remained an unexplored domain so far. This is possibly due to the decreased sensitivity of the Fermi Large Area Telescope (LAT) above a few GeV, hence low photon statistics, and observing constraints associated with the ground-based Cherenkov telescopes. This paper reports the results obtained from the 0.05$-$2 TeV Fermi-LAT data analysis of a sample of 29 blazars with the primary objective to explore their months-to-year long VHE flux variability behavior. This systematic search has led to, for the first time, the detection of significant flux variations in 5 blazars at $>$99\\% confidence level, whereas, 8 of them exhibit variability albeit at a lower confidence level ($\\sim$95\\%-99\\%). A comparison of the 0.05$-$2 TeV flux variations with that observed at 0.1$-$50 GeV band has revealed similar variability behavior for most of the sources. However, complex variability patterns that are not reflected contemporaneously in both energy bands were also detected, thereby providing tantalizing clues about the underlying radiative mechanisms. These results open up a new dimension to unravel the VHE emission processes operating in relativistic jets, hence sowing the seeds for their future observations with the upcoming Cherenkov Telescope Array.",
        "comments": "ApJ, in press",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11762"
    },
    {
        "doc_id": 236,
        "title": "Texture Identification in Liquid Crystal-Protein Droplets using Evaporative Drying, Generalized Additive Modeling, and K-means Clustering",
        "authors": [
            "Anusuya Pal",
            "Amalesh Gope"
        ],
        "subjects": [
            "Soft Condensed Matter"
        ],
        "abstract": "Sessile drying droplets manifest distinct morphological patterns, encompassing diverse systems viz., DNA, proteins, blood, and protein-liquid crystal (LC) complexes. This study employs an integrated methodology that combines drying droplet, image texture analysis (features from First Order Statistics, Gray Level Co-occurrence Matrix, Gray Level Run Length Matrix, Gray Level Size Zone Matrix, and Gray Level Dependence Matrix), and statistical data analysis (Generalized Additive Modeling and K-means clustering). It provides a comprehensive qualitative and quantitative exploration by examining LC-protein droplets at varying initial phosphate buffered concentrations (0x, 0.25x, 0.5x, 0.75x, and 1x) during the drying process under optical microscopy with crossed polarizing configuration. Notably, it unveils distinct LC-protein textures across three drying stages: initial, middle, and final. The Generalized Additive Modeling (GAM) reveals that all the features significantly contribute to differentiating LC-protein droplets. Integrating the K-means clustering method with GAM analysis elucidates how textures evolve through the three drying stages compared to the entire drying process. Notably, the final drying stage stands out with well-defined, non-overlapping clusters, supporting the visual observations of unique LC textures. Furthermore, this paper contributes valuable insights, showcasing the efficacy of drying droplets as a rapid and straightforward tool for characterizing and classifying dynamic LC textures.",
        "comments": "23 pages, 5 figures, and 2 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11745"
    },
    {
        "doc_id": 237,
        "title": "Knowledge Navigation: Inferring the Interlocking Map of Knowledge from Research Trajectories",
        "authors": [
            "Shibing Xiang",
            "Bing Liu",
            "Yurui Huang",
            "Chaolin Tian",
            "Xin Jiang",
            "Yifang Ma"
        ],
        "subjects": [
            "Information Retrieval",
            "Digital Libraries",
            "Applications"
        ],
        "abstract": "\"If I have seen further, it is by standing on the shoulders of giants,\" Isaac Newton's renowned statement hints that new knowledge builds upon existing foundations, which means there exists an interdependent relationship between knowledge, which, yet uncovered, is implied in the historical development of scientific systems for hundreds of years. By leveraging natural language processing techniques, this study introduces an innovative embedding scheme designed to infer the \"knowledge interlocking map.\" This map, derived from the research trajectories of millions of scholars, reveals the intricate connections among knowledge. We validate that the inferred map effectively delineates disciplinary boundaries and captures the intricate relationships between diverse concepts. The utility of the interlocking map is showcased through multiple applications. Firstly, we demonstrated the multi-step analogy inferences within the knowledge space and the functional connectivity between concepts in different disciplines. Secondly, we trace the evolution of knowledge across domains, observing trends such as shifts from \"Theoretical\" to \"Applied\" or \"Chemistry\" to \"Biomedical\" along predefined functional directions. Lastly, by analyzing the high-dimensional knowledge network structure, we found that knowledge connects each other with shorter global pathways, and the interdisciplinary knowledge plays a critical role in accessibility of the global knowledge network. Our framework offers a novel approach to mining knowledge inheritance pathways in extensive scientific literature, which is of great significance for understanding scientific development patterns, tailoring scientific learning trajectories, and accelerating scientific progress.",
        "comments": "28 pages, 9 figures, 5 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11742"
    },
    {
        "doc_id": 238,
        "title": "The Bayes factor surface for searches for new physics",
        "authors": [
            "Andrew Fowlie"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "Cosmology and Nongalactic Astrophysics",
            "High Energy Physics - Experiment",
            "Data Analysis, Statistics and Probability"
        ],
        "abstract": "The Bayes factor surface is a new way to present results from experimental searches for new physics. Searches are regularly expressed in terms of phenomenological parameters - such as the mass and cross-section of a weakly interacting massive particle. Bayes factor surfaces indicate the strength of evidence for or against models relative to the background only model in terms of the phenomenological parameters that they predict. They provide a clear and direct measure of evidence, may be easily reinterpreted, but do not depend on choices of prior or parameterization. We demonstrate the Bayes factor surface with examples from dark matter, cosmology, and collider physics.",
        "comments": "15 pages, 5 figures, comments welcome",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11710"
    },
    {
        "doc_id": 239,
        "title": "Simulating Nighttime Visible Satellite Imagery of Tropical Cyclones Using Conditional Generative Adversarial Networks",
        "authors": [
            "Jinghuai Yao",
            "Puyuan Du",
            "Yucheng Zhao",
            "Yubo Wang"
        ],
        "subjects": [
            "Atmospheric and Oceanic Physics",
            "Machine Learning"
        ],
        "abstract": "Visible (VIS) imagery of satellites has various important applications in meteorology, including monitoring Tropical Cyclones (TCs). However, it is unavailable at night because of the lack of sunlight. This study presents a Conditional Generative Adversarial Networks (CGAN) model that generates highly accurate nighttime visible reflectance using infrared (IR) bands and sunlight direction parameters as input. The model was trained and validated using target area observations of the Advanced Himawari Imager (AHI) in the daytime. This study also presents the first nighttime model validation using the Day/Night Band (DNB) of the Visible/Infrared Imager Radiometer Suite (VIIRS). The daytime statistical results of the Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), Root Mean Square Error (RMSE), Correlation Coefficient (CC), and Bias are 0.885, 28.3, 0.0428, 0.984, and -0.0016 respectively, completely surpassing the model performance of previous studies. The nighttime statistical results of SSIM, PSNR, RMSE, and CC are 0.821, 24.4, 0.0643, and 0.969 respectively, which are slightly negatively impacted by the parallax between satellites. We performed full-disk model validation which proves our model could also be readily applied in the tropical ocean without TCs in the northern hemisphere. This model contributes to the nighttime monitoring of meteorological phenomena by providing accurate AI-generated visible imagery with adjustable virtual sunlight directions.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11679"
    },
    {
        "doc_id": 240,
        "title": "Asymptotic distribution of spiked eigenvalues in the large signal-plus-noise models",
        "authors": [
            "Zeqin Lin",
            "Guangming Pan",
            "Peng Zhao",
            "Jia Zhou"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "Consider large signal-plus-noise data matrices of the form $S + \u03a3^{1/2} X$, where $S$ is a low-rank deterministic signal matrix and the noise covariance matrix $\u03a3$ can be anisotropic. We establish the asymptotic joint distribution of its spiked singular values when the dimensionality and sample size are comparably large and the signals are supercritical under general assumptions concerning the structure of $(S, \u03a3)$ and the distribution of the random noise $X$. It turns out that the asymptotic distributions exhibit nonuniversality in the sense of dependence on the distributions of the entries of $X$, which contrasts with what has previously been established for the spiked sample eigenvalues in the context of spiked population models. Such a result yields the asymptotic distribution of the sample spiked eigenvalues associated with mixture models. We also explore the application of these findings in detecting mean heterogeneity of data matrices.",
        "comments": "59 pages",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11672"
    },
    {
        "doc_id": 241,
        "title": "Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo",
        "authors": [
            "Haoyang Zheng",
            "Wei Deng",
            "Christian Moya",
            "Guang Lin"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\\mathcal{\\tilde O}(d)$ to $\\mathcal{\\tilde O}(\\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.",
        "comments": "50 pages, 1 figure, to appear in AISTATS 2024",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11665"
    },
    {
        "doc_id": 242,
        "title": "Nonparametric Estimation via Variance-Reduced Sketching",
        "authors": [
            "Yuehaw Khoo",
            "Yifan Peng",
            "Daren Wang"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Numerical Analysis",
            "Methodology"
        ],
        "abstract": "Nonparametric models are of great interest in various scientific and engineering disciplines. Classical kernel methods, while numerically robust and statistically sound in low-dimensional settings, become inadequate in higher-dimensional settings due to the curse of dimensionality. In this paper, we introduce a new framework called Variance-Reduced Sketching (VRS), specifically designed to estimate density functions and nonparametric regression functions in higher dimensions with a reduced curse of dimensionality. Our framework conceptualizes multivariable functions as infinite-size matrices, and facilitates a new sketching technique motivated by numerical linear algebra literature to reduce the variance in estimation problems. We demonstrate the robust numerical performance of VRS through a series of simulated experiments and real-world data applications. Notably, VRS shows remarkable improvement over existing neural network estimators and classical kernel methods in numerous density estimation and nonparametric regression models. Additionally, we offer theoretical justifications for VRS to support its ability to deliver nonparametric estimation with a reduced curse of dimensionality.",
        "comments": "64 pages, 8 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11646"
    },
    {
        "doc_id": 243,
        "title": "Efficient PSF Modeling with ShOpt.jl: A PSF Benchmarking Study with JWST NIRCam Imaging",
        "authors": [
            "Edward Berman",
            "Jacqueline McCleary",
            "Anton M. Koekemoer",
            "Maximilien Franco",
            "Nicole E. Drakos",
            "Daizhong Liu",
            "James W. Nightingale",
            "Marko Shuntov",
            "Diana Scognamiglio",
            "Richard Massey",
            "Guillaume Mahler",
            "Henry Joy McCracken",
            "Brant E. Robertson",
            "Andreas L. Faisst Caitlin M. Casey",
            "Jeyhan S. Kartaltepe"
        ],
        "subjects": [
            "Instrumentation and Methods for Astrophysics"
        ],
        "abstract": "With their high angular resolutions of 30-100 mas, large fields of view, and complex optical systems, imagers on next-generation optical/near-infrared space observatories, such as the Near-Infrared Camera (NIRCam) on the James Webb Space Telescope (JWST), present both new opportunities for science and also new challenges for empirical point spread function (PSF) characterization. In this context, we introduce ShOpt, a new PSF fitting tool developed in Julia and designed to bridge the advanced features of PIFF (PSFs in the Full Field of View) with the computational efficiency of PSFEx (PSF Extractor). Along with ShOpt, we propose a suite of non-parametric statistics suitable for evaluating PSF fit quality in space-based imaging. Our study benchmarks ShOpt against the established PSF fitters PSFEx and PIFF using real and simulated COSMOS-Web Survey imaging. We assess their respective PSF model fidelity with our proposed diagnostic statistics and investigate their computational efficiencies, focusing on their processing speed relative to the complexity and size of the PSF models. Despite being in active development, we find that ShOpt can already achieve PSF model fidelity comparable to PSFEx and PIFF while maintaining competitive processing speeds, constructing PSF models for large NIRCam mosaics within minutes.",
        "comments": "53 pages, 27 figures, submitted to Astronomical Journal",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11625"
    },
    {
        "doc_id": 244,
        "title": "Radiative decays of X(3872) discriminate between the molecular and compact interpretations",
        "authors": [
            "B. Grinstein",
            "L. Maiani",
            "A. D. Polosa"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "Radiative decays X --> psi(1S) + gamma and X --> psi(2S) + gamma might be expected to have a ratio of branching fractions following the phase space volumes ratio. However data suggest the opposite, indicating a value for R=B(X --> psi^prime + gamma) / B(X --> psi +gamma) consistently larger than one. In this paper we present a calculation of R for both a compact Born-Oppenheimer cc-bar q-qbar state and a DD^* molecule. In the former case R~1 or larger is found, a value to be confronted with forthcoming high statistics data analyses. In the molecular picture, with D and D^* mesons described by the universal wave function used by Voloshin, Braaten and Kusunoki, we find that R would be of order 10^-2. A more precise experimental measure would be extremely helpful in clarifying the true nature of the X(3872).",
        "comments": "12 pages, 9 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11623"
    },
    {
        "doc_id": 245,
        "title": "Efficient local linearity regularization to overcome catastrophic overfitting",
        "authors": [
            "Elias Abad Rocamora",
            "Fanghui Liu",
            "Grigorios G. Chrysos",
            "Pablo M. Olmos",
            "Volkan Cevher"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Cryptography and Security",
            "Machine Learning"
        ],
        "abstract": "Catastrophic overfitting (CO) in single-step adversarial training (AT) results in abrupt drops in the adversarial test accuracy (even down to 0%). For models trained with multi-step AT, it has been observed that the loss function behaves locally linearly with respect to the input, this is however lost in single-step AT. To address CO in single-step AT, several methods have been proposed to enforce local linearity of the loss via regularization. However, these regularization terms considerably slow down training due to Double Backpropagation. Instead, in this work, we introduce a regularization term, called ELLE, to mitigate CO effectively and efficiently in classical AT evaluations, as well as some more difficult regimes, e.g., large adversarial perturbations and long training schedules. Our regularization term can be theoretically linked to curvature of the loss function and is computationally cheaper than previous methods by avoiding Double Backpropagation. Our thorough experimental validation demonstrates that our work does not suffer from CO, even in challenging settings where previous works suffer from it. We also notice that adapting our regularization parameter during training (ELLE-A) greatly improves the performance, specially in large $\u03b5$ setups. Our implementation is available in https://github.com/LIONS-EPFL/ELLE .",
        "comments": "Accepted in ICLR 2024",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11618"
    },
    {
        "doc_id": 246,
        "title": "An Interacting Wasserstein Gradient Flow Strategy to Robust Bayesian Inference",
        "authors": [
            "Felipe Igea",
            "Alice Cicirello"
        ],
        "subjects": [
            "Computation"
        ],
        "abstract": "Model Updating is frequently used in Structural Health Monitoring to determine structures' operating conditions and whether maintenance is required. Data collected by sensors are used to update the values of some initially unknown physics-based model's parameters. Bayesian Inference techniques for model updating require the assumption of a prior distribution. This choice of prior may affect posterior predictions and subsequent decisions on maintenance requirements, specially under the typical case in engineering applications of little informative data. Therefore, understanding how the choice of prior may affect the posterior prediction is of great interest. In this paper, a Robust Bayesian Inference technique evaluates the optimal and worst-case prior in the vicinity of a chosen nominal prior, and their corresponding posteriors. This technique employs an interacting Wasserstein gradient flow formulation. Two numerical case studies are used to showcase the proposed algorithm: a double-banana-posterior and a double beam structure. Optimal and worst-case prior are modelled by specifying an ambiguity set containing any distribution at a statistical distance to the nominal prior, less or equal to the radius. Examples show how particles flow from an initial assumed Gaussian distribution to the optimal worst-case prior distribution that lies inside the defined ambiguity set, and the resulting particles from the approximation to the posterior. The resulting posteriors may be used to yield the lower and upper bounds on subsequent calculations used for decision-making. If the metric used for decision-making is not sensitive to the resulting posteriors, it may be assumed that decisions taken are robust to prior uncertainty.",
        "comments": "Preprint submitted to Data-Centric Engineering",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11607"
    },
    {
        "doc_id": 247,
        "title": "Understanding the Generalization Benefits of Late Learning Rate Decay",
        "authors": [
            "Yinuo Ren",
            "Chao Ma",
            "Lexing Ying"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Why do neural networks trained with large learning rates for a longer time often lead to better generalization? In this paper, we delve into this question by examining the relation between training and testing loss in neural networks. Through visualization of these losses, we note that the training trajectory with a large learning rate navigates through the minima manifold of the training loss, finally nearing the neighborhood of the testing loss minimum. Motivated by these findings, we introduce a nonlinear model whose loss landscapes mirror those observed for real neural networks. Upon investigating the training process using SGD on our model, we demonstrate that an extended phase with a large learning rate steers our model towards the minimum norm solution of the training loss, which may achieve near-optimal generalization, thereby affirming the empirically observed benefits of late learning rate decay.",
        "comments": "Accepted by AISTATS 2024",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11600"
    },
    {
        "doc_id": 248,
        "title": "Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis",
        "authors": [
            "Sharu Theresa Jose",
            "Shana Moothedath"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "We explore a stochastic contextual linear bandit problem where the agent observes a noisy, corrupted version of the true context through a noise channel with an unknown noise parameter. Our objective is to design an action policy that can approximate\" that of an oracle, which has access to the reward model, the channel parameter, and the predictive distribution of the true context from the observed noisy context. In a Bayesian framework, we introduce a Thompson sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting an information-theoretic analysis, we demonstrate the Bayesian regret of our algorithm concerning the oracle's action policy. We also extend this problem to a scenario where the agent observes the true context with some delay after receiving the reward and show that delayed true contexts lead to lower Bayesian regret. Finally, we empirically demonstrate the performance of the proposed algorithms against baselines.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11565"
    },
    {
        "doc_id": 249,
        "title": "Enhancing selectivity using Wasserstein distance based reweighing",
        "authors": [
            "Pratik Worah"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "Given two labeled data-sets $\\mathcal{S}$ and $\\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\\mathcal{T}$.\n  On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.\n  Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1 (a highly similar protein). We tune the algorithm's parameter so that overall change in holdout loss is negligible, but the selectivity, i.e., the fraction of top 100 MNK2 binders that are MNK1 non-binders, increases from 54\\% to 95\\%, as a result of our reweighing. Of the 43 distinct small molecules predicted to be most selective from the enamine catalog, 2 small molecules were experimentally verified to be selective, i.e., they reduced the enzyme activity of MNK2 below 50\\% but not MNK1, at 10$\u03bc$M -- a 5\\% success rate.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11562"
    },
    {
        "doc_id": 250,
        "title": "Transfer Learning under Covariate Shift: Local $k$-Nearest Neighbours Regression with Heavy-Tailed Design",
        "authors": [
            "Petr Zamolodtchikov",
            "Hanyuan Hang"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "Covariate shift is a common transfer learning scenario where the marginal distributions of input variables vary between source and target data while the conditional distribution of the output variable remains consistent. The existing notions describing differences between marginal distributions face limitations in handling scenarios with unbounded support, particularly when the target distribution has a heavier tail. To overcome these challenges, we introduce a new concept called density ratio exponent to quantify the relative decay rates of marginal distributions' tails under covariate shift. Furthermore, we propose the local k-nearest neighbour regressor for transfer learning, which adapts the number of nearest neighbours based on the marginal likelihood of each test sample. From a theoretical perspective, convergence rates with and without supervision information on the target domain are established. Those rates indicate that our estimator achieves faster convergence rates when the density ratio exponent satisfies certain conditions, highlighting the benefits of using density estimation for determining different numbers of nearest neighbours for each test sample. Our contributions enhance the understanding and applicability of transfer learning under covariate shift, especially in scenarios with unbounded support and heavy-tailed distributions.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11554"
    },
    {
        "doc_id": 251,
        "title": "Investigation of triangle counts in graphs evolved by uniform clustering attachment",
        "authors": [
            "N. M. Markovich",
            "M. Vai\u010diulis"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "The clustering attachment model introduced in the paper Bagrow and Brockmann (2013) may be used as an evolution tool of random networks. We propose a new clustering attachment model which can be considered as the limit of the former clustering attachment model as model parameter $\u03b1$ tends to zero. We focus on the study of a total triangle count that is considered in the literature as an important characteristic of the network clustering. It is proved that total triangle count tends to infinity a.s. for the proposed model. Our simulation study is used for the modeling of sequences of triangle counts. It is based on the interpretation of the clustering attachment as a generalized P\u00f3lya-Eggenberger urn model that is introduced here at first time.",
        "comments": "16 pages, 2 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11548"
    },
    {
        "doc_id": 252,
        "title": "A new flexible class of kernel-based tests of independence",
        "authors": [
            "Marija Cupari\u0107",
            "Bruno Ebner",
            "Bojana Milo\u0161evi\u0107"
        ],
        "subjects": [
            "Methodology",
            "Statistics Theory"
        ],
        "abstract": "Spherical and hyperspherical data are commonly encountered in diverse applied research domains, underscoring the vital task of assessing independence within such data structures. In this context, we investigate the properties of test statistics relying on distance correlation measures originally introduced for the energy distance, and generalize the concept to strongly negative definite kernel-based distances. An important benefit of employing this method lies in its versatility across diverse forms of directional data, enabling the examination of independence among vectors of varying types. The applicability of tests is demonstrated on several real datasets.",
        "comments": "MSC Class:          62H11; 62H15; 62H20",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11540"
    },
    {
        "doc_id": 253,
        "title": "Addressing researcher degrees of freedom through minP adjustment",
        "authors": [
            "Maximilian M Mandl",
            "Andrea S Becker-Pennrich",
            "Ludwig C Hinske",
            "Sabine Hoffmann",
            "Anne-Laure Boulesteix"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "When different researchers study the same research question using the same dataset they may obtain different and potentially even conflicting results. This is because there is often substantial flexibility in researchers' analytical choices, an issue also referred to as ''researcher degrees of freedom''. Combined with selective reporting of the smallest p-value or largest effect, researcher degrees of freedom may lead to an increased rate of false positive and overoptimistic results. In this paper, we address this issue by formalizing the multiplicity of analysis strategies as a multiple testing problem. As the test statistics of different analysis strategies are usually highly dependent, a naive approach such as the Bonferroni correction is inappropriate because it leads to an unacceptable loss of power. Instead, we propose using the ''minP'' adjustment method, which takes potential test dependencies into account and approximates the underlying null distribution of the minimal p-value through a permutation-based procedure. This procedure is known to achieve more power than simpler approaches while ensuring a weak control of the family-wise error rate. We illustrate our approach for addressing researcher degrees of freedom by applying it to a study on the impact of perioperative paO2 on post-operative complications after neurosurgery. A total of 48 analysis strategies are considered and adjusted using the minP procedure. This approach allows to selectively report the result of the analysis strategy yielding the most convincing evidence, while controlling the type 1 error -- and thus the risk of publishing false positive results that may not be replicable.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11537"
    },
    {
        "doc_id": 254,
        "title": "Geometry-driven Bayesian Inference for Ultrametric Covariance Matrices",
        "authors": [
            "Tsung-Hung Yao",
            "Zhenke Wu",
            "Karthik Bharath",
            "Veerabhadran Baladandayuthapani"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Ultrametric matrices arise as covariance matrices in latent tree models for multivariate data with hierarchically correlated components. As a parameter space in a model, the set of ultrametric matrices is neither convex nor a smooth manifold, and focus in literature has hitherto mainly been restricted to estimation through projections and relaxation-based techniques. Leveraging the link between an ultrametric matrix and a rooted tree, we equip the set of ultrametric matrices with a convenient geometry based on the well-known geometry of phylogenetic trees, whose attractive properties (e.g. unique geodesics and Fr\u00e9chet means) the set of ultrametric matrices inherits. This results in a novel representation of an ultrametric matrix by coordinates of the tree space, which we then use to define a class of Markovian and consistent prior distributions on the set of ultrametric matrices in a Bayesian model, and develop an efficient algorithm to sample from the posterior distribution that generates updates by making intrinsic local moves along geodesics within the set of ultrametric matrices. In simulation studies, our proposed algorithm restores the underlying matrices with posterior samples that recover the tree topology with a high frequency of true topology and generate element-wise credible intervals with a high nominal coverage rate. We use the proposed algorithm on the pre-clinical cancer data to investigate the mechanism similarity by constructing the underlying treatment tree and identify treatments with high mechanism similarity also target correlated pathways in biological literature.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11515"
    },
    {
        "doc_id": 255,
        "title": "Topological superconductors in trapped-ion system and their Floquet engineering",
        "authors": [
            "Ming-Jian Gao",
            "Yu-Peng Ma",
            "Jun-Hong An"
        ],
        "subjects": [
            "Quantum Physics",
            "Superconductivity"
        ],
        "abstract": "Obeying non-Abelian statistics, Majorana fermion holds a promise to implement topological quantum computing. It was found that Majorana fermion can be simulated by the zero-energy excitation in a semiconducting nanowire with strong spin-orbit coupling interacting with a $s$-wave superconductor under a magnetic field. We here propose an alternative scheme to simulate the Majorana fermion in a trapped-ion system. Our dimitrized-ion configuration permits us to generate the Majorana modes not only at zero energy but also at the nonzero ones. We also investigate the controllability of the Majorana modes by Floquet engineering. It is found that a widely tunable number of Majorana modes are created on demand by applying a periodic driving on a topologically trivial trapped-ion system. Enriching the platforms for simulating Majorana fermion, our result would open another avenue for realizing topological quantum computing.",
        "comments": "8 pages and 4 figures in the main text and 3 pages in the supplentmental material",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11510"
    },
    {
        "doc_id": 256,
        "title": "Redundant multiple testing corrections: The fallacy of using family-based error rates to make inferences about individual hypotheses",
        "authors": [
            "Mark Rubin"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "During multiple testing, researchers often adjust their alpha level to control the familywise error rate for a statistical inference about a joint union alternative hypothesis (e.g., \"H1 or H2\"). However, in some cases, they do not make this inference and instead make separate inferences about each of the individual hypotheses that comprise the joint hypothesis (e.g., H1 and H2). For example, a researcher might use a Bonferroni correction to adjust their alpha level from the conventional level of 0.050 to 0.025 when testing H1 and H2, find a significant result for H1 (p < 0.025) and not for H2 (p > .0.025), and so claim support for H1 and not for H2. However, these separate individual inferences do not require an alpha adjustment. Only a statistical inference about the union alternative hypothesis \"H1 or H2\" requires an alpha adjustment because it is based on \"at least one\" significant result among the two tests, and so it depends on the familywise error rate. When a researcher corrects their alpha level during multiple testing but does not make an inference about the union alternative hypothesis, their correction is redundant. In the present article, I discuss this redundant correction problem, including its associated loss of statistical power and its potential causes vis-\u00e0-vis error rate confusions and the alpha adjustment ritual. I also provide three illustrations of redundant corrections from recent psychology studies. I conclude that redundant corrections represent a symptom of statisticism, and I call for a more nuanced and context-specific approach to multiple testing corrections.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11507"
    },
    {
        "doc_id": 257,
        "title": "Functional Limit Theorems for Hawkes Processes",
        "authors": [
            "Ulrich Horst",
            "Wei Xu"
        ],
        "subjects": [
            "Probability",
            "Statistics Theory",
            "Mathematical Finance"
        ],
        "abstract": "We prove that the long-run behavior of Hawkes processes is fully determined by the average number and the dispersion of child events. For subcritical processes we provide FLLNs and FCLTs under minimal conditions on the kernel of the process with the precise form of the limit theorems depending strongly on the dispersion of child events. For a critical Hawkes process with weakly dispersed child events, functional central limit theorems do not hold. Instead, we prove that the rescaled intensity processes and rescaled Hawkes processes behave like CIR-processes without mean-reversion, respectively integrated CIR-processes. We provide the rate of convergence by establishing an upper bound on the Wasserstein distance between the distributions of rescaled Hawkes process and the corresponding limit process. By contrast, critical Hawkes process with heavily dispersed child events share many properties of subcritical ones. In particular, functional limit theorems hold. However, unlike subcritical processes critical ones with heavily dispersed child events display long-range dependencies.",
        "comments": "59 pages; Keywords and phrases: Hawkes process, functional limit theorem, regular variation, convergence rate",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11495"
    },
    {
        "doc_id": 258,
        "title": "Local Identification in the Instrumental Variable Multivariate Quantile Regression Model",
        "authors": [
            "Haruki Kono"
        ],
        "subjects": [
            "Econometrics",
            "Statistics Theory"
        ],
        "abstract": "The instrumental variable (IV) quantile regression model introduced by Chernozhukov and Hansen (2005) is a useful tool for analyzing quantile treatment effects in the presence of endogeneity, but when outcome variables are multidimensional, it is silent on the joint distribution of different dimensions of each variable. To overcome this limitation, we propose an IV model built on the optimal-transport-based multivariate quantile that takes into account the correlation between the entries of the outcome variable. We then provide a local identification result for the model. Surprisingly, we find that the support size of the IV required for the identification is independent of the dimension of the outcome vector, as long as the IV is sufficiently informative. Our result follows from a general identification theorem that we establish, which has independent theoretical significance.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11422"
    },
    {
        "doc_id": 259,
        "title": "MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning",
        "authors": [
            "Mao Hong",
            "Zhiyue Zhang",
            "Yue Wu",
            "Yanxun Xu"
        ],
        "subjects": [
            "Machine Learning",
            "Statistics Theory",
            "Methodology",
            "Machine Learning"
        ],
        "abstract": "Model-based offline reinforcement learning methods (RL) have achieved state-of-the-art performance in many decision-making problems thanks to their sample efficiency and generalizability. Despite these advancements, existing model-based offline RL approaches either focus on theoretical studies without developing practical algorithms or rely on a restricted parametric policy space, thus not fully leveraging the advantages of an unrestricted policy space inherent to model-based methods. To address this limitation, we develop MoMA, a model-based mirror ascent algorithm with general function approximations under partial coverage of offline data. MoMA distinguishes itself from existing literature by employing an unrestricted policy class. In each iteration, MoMA conservatively estimates the value function by a minimization procedure within a confidence set of transition models in the policy evaluation step, then updates the policy with general function approximations instead of commonly-used parametric policy classes in the policy improvement step. Under some mild assumptions, we establish theoretical guarantees of MoMA by proving an upper bound on the suboptimality of the returned policy. We also provide a practically implementable, approximate version of the algorithm. The effectiveness of MoMA is demonstrated via numerical studies.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11380"
    },
    {
        "doc_id": 260,
        "title": "When exposure affects subgroup membership: Framing relevant causal questions in perinatal epidemiology and beyond",
        "authors": [
            "Shalika Gupta",
            "Laura B. Balzer",
            "Moses R. Kamya",
            "Diane V. Havlir",
            "Maya L. Petersen"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Perinatal epidemiology often aims to evaluate exposures on infant outcomes. When the exposure affects the composition of people who give birth to live infants (e.g., by affecting fertility, behavior, or birth outcomes), this \"live birth process\" mediates the exposure effect on infant outcomes. Causal estimands previously proposed for this setting include the total exposure effect on composite birth and infant outcomes, controlled direct effects (e.g., enforcing birth), and principal stratum direct effects. Using perinatal HIV transmission in the SEARCH Study as a motivating example, we present two alternative causal estimands: 1) conditional total effects; and 2) conditional stochastic direct effects, formulated under a hypothetical intervention to draw mediator values from some distribution (possibly conditional on covariates). The proposed conditional total effect includes impacts of an intervention that operate by changing the types of people who have a live birth and the timing of births. The proposed conditional stochastic direct effects isolate the effect of an exposure on infant outcomes excluding any impacts through this live birth process. In SEARCH, this approach quantifies the impact of a universal testing and treatment intervention on infant HIV-free survival absent any effect of the intervention on the live birth process, within a clearly defined target population of women of reproductive age with HIV at study baseline. Our approach has implications for the evaluation of intervention effects in perinatal epidemiology broadly, and whenever causal effects within a subgroup are of interest and exposure affects membership in the subgroup.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11368"
    },
    {
        "doc_id": 261,
        "title": "Folding Custom Gates with Verifier Input",
        "authors": [
            "Aard Vark",
            "Yan X Zhang"
        ],
        "subjects": [
            "Cryptography and Security",
            "Logic in Computer Science"
        ],
        "abstract": "In the context of interactive proofs, a \"folding scheme\" (popularized by Nova) is a way to combine multiple instances of a constraint system into a single instance, so the validity of the multiple instances can statistically be reduced to the validity of a single one. We show how Nova folding can be generalized to ``custom'' gates and extra rounds of verifier randomness. As an application of this extension, we present Origami, the first (to our knowledge) known example of a folding scheme for lookups.",
        "comments": "MSC Class:          94A60",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11364"
    },
    {
        "doc_id": 262,
        "title": "The Exact Risks of Reference Panel-based Regularized Estimators",
        "authors": [
            "Buxin Su",
            "Qiang Sun",
            "Xiaochen Yang",
            "Bingxin Zhao"
        ],
        "subjects": [
            "Methodology",
            "Statistics Theory"
        ],
        "abstract": "Reference panel-based estimators have become widely used in genetic prediction of complex traits due to their ability to address data privacy concerns and reduce computational and communication costs. These estimators estimate the covariance matrix of predictors using an external reference panel, instead of relying solely on the original training data. In this paper, we investigate the performance of reference panel-based $L_1$ and $L_2$ regularized estimators within a unified framework based on approximate message passing (AMP). We uncover several key factors that influence the accuracy of reference panel-based estimators, including the sample sizes of the training data and reference panels, the signal-to-noise ratio, the underlying sparsity of the signal, and the covariance matrix among predictors. Our findings reveal that, even when the sample size of the reference panel matches that of the training data, reference panel-based estimators tend to exhibit lower accuracy compared to traditional regularized estimators. Furthermore, we observe that this performance gap widens as the amount of training data increases, highlighting the importance of constructing large-scale reference panels to mitigate this issue. To support our theoretical analysis, we develop a novel non-separable matrix AMP framework capable of handling the complexities introduced by a general covariance matrix and the additional randomness associated with a reference panel. We validate our theoretical results through extensive simulation studies and real data analyses using the UK Biobank database.",
        "comments": "100 pages, 11 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11359"
    },
    {
        "doc_id": 263,
        "title": "Squared Wasserstein-2 Distance for Efficient Reconstruction of Stochastic Differential Equations",
        "authors": [
            "Mingtao Xia",
            "Xiangting Li",
            "Qijing Shen",
            "Tom Chou"
        ],
        "subjects": [
            "Probability",
            "Machine Learning",
            "Methodology"
        ],
        "abstract": "We provide an analysis of the squared Wasserstein-2 ($W_2$) distance between two probability distributions associated with two stochastic differential equations (SDEs). Based on this analysis, we propose the use of a squared $W_2$ distance-based loss functions in the \\textit{reconstruction} of SDEs from noisy data. To demonstrate the practicality of our Wasserstein distance-based loss functions, we performed numerical experiments that demonstrate the efficiency of our method in reconstructing SDEs that arise across a number of applications.",
        "comments": "37 pages, 5 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11354"
    },
    {
        "doc_id": 264,
        "title": "Geometric Insights and Empirical Observations on Covariate Adjustment and Stratified Randomization in Randomized Clinical Trials",
        "authors": [
            "Zhiwei Zhang"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "The statistical efficiency of randomized clinical trials can be improved by incorporating information from baseline covariates (i.e., pre-treatment patient characteristics). This can be done in the design stage using a covariate-adaptive randomization scheme such as stratified (permutated block) randomization, or in the analysis stage through covariate adjustment. This article provides a geometric perspective on covariate adjustment and stratified randomization in a unified framework where all regular, asymptotically linear estimators are identified as augmented estimators. From this perspective, covariate adjustment can be viewed as an effort to approximate the optimal augmentation function, and stratified randomization aims to improve a given approximation by projecting it into an affine subspace containing the optimal augmentation function. The efficiency benefit of stratified randomization is asymptotically equivalent to making full use of stratum information in covariate adjustment, which can be achieved using a simple calibration procedure. Simulation results indicate that stratified randomization is clearly beneficial to unadjusted estimators and much less so to adjusted ones and that calibration is an effective way to recover the efficiency benefit of stratified randomization without actually performing stratified randomization. These insights and observations are illustrated using real clinical trial data.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11352"
    },
    {
        "doc_id": 265,
        "title": "Quantum Machine Learning: from NISQ to Fault Tolerance",
        "authors": [
            "Yunfei Wang",
            "Junyu Liu"
        ],
        "subjects": [
            "Quantum Physics",
            "Artificial Intelligence",
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.",
        "comments": "28 pages. Invited review",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11351"
    },
    {
        "doc_id": 266,
        "title": "A study of experimental sensitivities to nucleon parton distributions with xFitter",
        "authors": [
            "Lucas Kotz"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "In collider physics, parton distribution functions (PDFs) play a crucial role in computing theoretical cross sections for scattering reactions. This study explores how different experimental data sets influence extracted PDFs in CTEQ-TEA and MSHT NNLO PDF analyses. To gauge the impact of experimental data, including the HERA and ZEUS combined charm and beauty production, LHCb 7 TeV charm and beauty production, and CMS 2.76 TeV W+c production, I utilize the $L_2$ sensitivity in the Hessian framework as a visual representation of their respective impacts. This sensitivity quantifies the statistical pulls on individual data sets against the best-fit PDFs, facilitating the identification of tensions among competing data sets. Using the QCD fitting framework xFitter, I extract the necessary values for plotting $L_2$ sensitivities for eight distinct data sets implemented in the program, employing recent PDF sets from the CTEQ-TEA and MSHT groups. The computed $L_2$ sensitivities estimate the potential impact of the examined data sets.",
        "comments": "11 pages, 11 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11350"
    },
    {
        "doc_id": 267,
        "title": "Optimization of random cost functions and statistical physics",
        "authors": [
            "Andrea Montanari"
        ],
        "subjects": [
            "Disordered Systems and Neural Networks"
        ],
        "abstract": "This is the text of my report presented at the 29th Solvay Conference on Physics on `The Structure and Dynamics of Disordered Systems' held in Bruxelles from October 19 to 21, 2023. I consider the problem of minimizing a random energy function $H(\u03c3)$, where $\u03c3$ is an $N$-dimensional vector, in the high-dimensional regime $N\\gg 1$. Using as a reference point a 1986 paper by Fu and Anderson, I take stock of the progress on this question over the last 40 years. In particular, I focus on the influence and ramifications of ideas originating from statistical physics. My own conclusion is that several of the most fundamental questions in this area (which in 1986 were barely formulated) have now received mathematically rigorous answers, at least in simple -- yet highly nontrivial -- settings. Instrumental to this spectacular progress was the dialogue between different research communities: physics, computer science, mathematics.",
        "comments": "21 pages; 5 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11348"
    },
    {
        "doc_id": 268,
        "title": "Estimating Default Probability and Correlation using Stan",
        "authors": [
            "Jesus A. Pinera-Esquivel"
        ],
        "subjects": [
            "Applications",
            "Methodology"
        ],
        "abstract": "This work has the objective of estimating default probabilities and correlations of credit portfolios given default rate information through a Bayesian framework using Stan. We use Vasicek's single factor credit model to establish the theoretical framework for the behavior of the default rates, and use NUTS Markov Chain Monte Carlo to estimate the parameters. We compare the Bayesian estimates with classical estimates such as moments estimators and maximum likelihood estimates. We apply the methodology both to simulated data and to corporate default rates, and perform inferences through Bayesian methods in order to exhibit the advantages of such a framework. We perform default forecasting and exhibit the importance of an adequate estimation of default correlations, and exhibit the advantage of using Stan to perform sampling regarding prior choice.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11346"
    },
    {
        "doc_id": 269,
        "title": "Mortality Modelling using Generalized Estimating Equations",
        "authors": [
            "Reza Dastranj",
            "Martin Kolar"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "This paper presents an application of Generalized Estimating Equations (GEE) for analyzing age-specific death rates (ASDRs), constituting a longitudinal dataset with repeated measurements over time. GEE models, known for their robustness in handling correlated data, offer a reliable solution when individual data records lack independence, thus violating the commonly assumed independence and identically distributed (iid) condition in traditional models. In the context of ASDRs, where correlations emerge among observations within age groups, two distinct GEE models for single and multipopulation ASDRs are introduced, providing robust estimates for regression parameters and their variances. We explore correlation structures, encompassing independence, AR(1), unstructured, and exchangeable structures, offering a comprehensive evaluation of GEE model efficiency in both single and multipopulation ASDRs. We critically examines the strengths and limitations of GEE models, shedding light on their applicability for mortality forecasting. Through detailed model specifications and empirical illustrations, the study contributes to an enhanced understanding of the nuanced capabilities of GEE models in predicting mortality rates.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11332"
    },
    {
        "doc_id": 270,
        "title": "A Hierarchical Decision-Based Maintenance for a Complex Modular System Driven by the { MoMA} Algorithm",
        "authors": [
            "M. L. Gamiz",
            "D. Montoro-Cazorla",
            "M. C. Segovia-Garcia"
        ],
        "subjects": [
            "Systems and Control",
            "Methodology"
        ],
        "abstract": "This paper presents a maintenance policy for a modular system formed by K independent modules (n-subsystems) subjected to environmental conditions (shocks). For the modeling of this complex system, the use of the Matrix-Analytical Method (MAM) is proposed under a layered approach according to its hierarchical structure. Thus, the operational state of the system (top layer) depends on the states of the modules (middle layer), which in turn depend on the states of their components (bottom layer). This allows a detailed description of the system operation to plan maintenance actions appropriately and optimally. We propose a hierarchical decision-based maintenance strategy with periodic inspections as follows: at the time of the inspection, the condition of the system is first evaluated. If intervention is necessary, the modules are then checked to make individual decisions based on their states, and so on. Replacement or repair will be carried out as appropriate. An optimization problem is formulated as a function of the length of the inspection period and the intervention cost incurred over the useful life of the system. Our method shows the advantages, providing compact and implementable expressions. The model is illustrated on a submarine Electrical Control Unit (ECU).",
        "comments": "43 pages, 6 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11328"
    },
    {
        "doc_id": 271,
        "title": "Measuring hierarchically-organized interactions in dynamic networks through spectral entropy rates: theory, estimation, and illustrative application to physiological networks",
        "authors": [
            "Laura Sparacino",
            "Yuri Antonacci",
            "Gorana Mijatovic",
            "Luca Faes"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Recent advances in signal processing and information theory are boosting the development of new approaches for the data-driven modelling of complex network systems. In the fields of Network Physiology and Network Neuroscience where the signals of interest are often rich of oscillatory content, the spectral representation of network systems is essential to ascribe the analyzed interactions to specific oscillations with physiological meaning. In this context, the present work formalizes a coherent framework which integrates several information dynamics approaches to quantify node-specific, pairwise and higher-order interactions in network systems. The framework establishes a hierarchical organization of interactions of different order using measures of entropy rate, mutual information rate and O-information rate, to quantify respectively the dynamics of individual nodes, the links between pairs of nodes, and the redundant/synergistic hyperlinks between groups of nodes. All measures are formulated in the time domain, and then expanded to the spectral domain to obtain frequency-specific information. The practical computation of all measures is favored presenting a toolbox that implements their parametric and non-parametric estimation, and includes approaches to assess their statistical significance. The framework is illustrated first using theoretical examples where the properties of the measures are displayed in benchmark simulated network systems, and then applied to representative examples of multivariate time series in the context of Network Neuroscience and Network Physiology.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11327"
    },
    {
        "doc_id": 272,
        "title": "PRILoRA: Pruned and Rank-Increasing Low-Rank Adaptation",
        "authors": [
            "Nadav Benedek",
            "Lior Wolf"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence"
        ],
        "abstract": "With the proliferation of large pre-trained language models (PLMs), fine-tuning all model parameters becomes increasingly inefficient, particularly when dealing with numerous downstream tasks that entail substantial training and storage costs. Several approaches aimed at achieving parameter-efficient fine-tuning (PEFT) have been proposed. Among them, Low-Rank Adaptation (LoRA) stands out as an archetypal method, incorporating trainable rank decomposition matrices into each target module. Nevertheless, LoRA does not consider the varying importance of each layer. To address these challenges, we introduce PRILoRA, which linearly allocates a different rank for each layer, in an increasing manner, and performs pruning throughout the training process, considering both the temporary magnitude of weights and the accumulated statistics of the input to any given layer. We validate the effectiveness of PRILoRA through extensive experiments on eight GLUE benchmarks, setting a new state of the art.",
        "comments": "EACL 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11316"
    },
    {
        "doc_id": 273,
        "title": "Weakly-Supervised Semantic Segmentation of Circular-Scan, Synthetic-Aperture-Sonar Imagery",
        "authors": [
            "Isaac J. Sledge",
            "Dominic M. Byrne",
            "Jonathan L. King",
            "Steven H. Ostertag",
            "Denton L. Woods",
            "James L. Prater",
            "Jermaine L. Kennedy",
            "Timothy M. Marston",
            "Jose C. Principe"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning",
            "Image and Video Processing"
        ],
        "abstract": "We propose a weakly-supervised framework for the semantic segmentation of circular-scan synthetic-aperture-sonar (CSAS) imagery. The first part of our framework is trained in a supervised manner, on image-level labels, to uncover a set of semi-sparse, spatially-discriminative regions in each image. The classification uncertainty of each region is then evaluated. Those areas with the lowest uncertainties are then chosen to be weakly labeled segmentation seeds, at the pixel level, for the second part of the framework. Each of the seed extents are progressively resized according to an unsupervised, information-theoretic loss with structured-prediction regularizers. This reshaping process uses multi-scale, adaptively-weighted features to delineate class-specific transitions in local image content. Content-addressable memories are inserted at various parts of our framework so that it can leverage features from previously seen images to improve segmentation performance for related images.\n  We evaluate our weakly-supervised framework using real-world CSAS imagery that contains over ten seafloor classes and ten target classes. We show that our framework performs comparably to nine fully-supervised deep networks. Our framework also outperforms eleven of the best weakly-supervised deep networks. We achieve state-of-the-art performance when pre-training on natural imagery. The average absolute performance gap to the next-best weakly-supervised network is well over ten percent for both natural imagery and sonar imagery. This gap is found to be statistically significant.",
        "comments": "Submitted to the IEEE Journal of Oceanic Engineering",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11313"
    },
    {
        "doc_id": 274,
        "title": "Handling incomplete outcomes and covariates in cluster-randomized trials: doubly-robust estimation, efficiency considerations, and sensitivity analysis",
        "authors": [
            "Bingkai Wang",
            "Fan Li",
            "Rui Wang"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "In cluster-randomized trials, missing data can occur in various ways, including missing values in outcomes and baseline covariates at the individual or cluster level, or completely missing information for non-participants. Among the various types of missing data in CRTs, missing outcomes have attracted the most attention. However, no existing method comprehensively addresses all the aforementioned types of missing data simultaneously due to their complexity. This gap in methodology may lead to confusion and potential pitfalls in the analysis of CRTs. In this article, we propose a doubly-robust estimator for a variety of estimands that simultaneously handles missing outcomes under a missing-at-random assumption, missing covariates with the missing-indicator method (with no constraint on missing covariate distributions), and missing cluster-population sizes via a uniform sampling framework. Furthermore, we provide three approaches to improve precision by choosing the optimal weights for intracluster correlation, leveraging machine learning, and modeling the propensity score for treatment assignment. To evaluate the impact of violated missing data assumptions, we additionally propose a sensitivity analysis that measures when missing data alter the conclusion of treatment effect estimation. Simulation studies and data applications both show that our proposed method is valid and superior to the existing methods.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11278"
    },
    {
        "doc_id": 275,
        "title": "Asymptotics for non-degenerate multivariate $U$-statistics with estimated nuisance parameters under the null and local alternative hypotheses",
        "authors": [
            "Alain Desgagn\u00e9",
            "Christian Genest",
            "Fr\u00e9d\u00e9ric Ouimet"
        ],
        "subjects": [
            "Statistics Theory",
            "Probability",
            "Applications",
            "Methodology"
        ],
        "abstract": "The large-sample behavior of non-degenerate multivariate $U$-statistics of arbitrary degree is investigated under the assumption that their kernel depends on parameters that can be estimated consistently. Mild regularity conditions are given which guarantee that once properly normalized, such statistics are asymptotically multivariate Gaussian both under the null hypothesis and sequences of local alternatives. The work of Randles (1982, Ann. Statist.) is extended in three ways: the data and the kernel values can be multivariate rather than univariate, the limiting behavior under local alternatives is studied for the first time, and the effect of knowing some of the nuisance parameters is quantified. These results can be applied to a broad range of goodness-of-fit testing contexts, as shown in one specific example.",
        "comments": "16 pages, 1 figure",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11272"
    },
    {
        "doc_id": 276,
        "title": "Assessing the Competitiveness of Matrix-Free Block Likelihood Estimation in Spatial Models",
        "authors": [
            "Alfredo Alegr\u00eda"
        ],
        "subjects": [
            "Methodology",
            "Statistics Theory"
        ],
        "abstract": "In geostatistics, block likelihood offers a balance between statistical accuracy and computational efficiency when estimating covariance functions. This balance is reached by dividing the sample into blocks and computing a weighted sum of (sub) log-likelihoods corresponding to pairs of blocks. Practitioners often choose block sizes ranging from hundreds to a few thousand observations, inherently involving matrix-based implementations. An alternative, residing at the opposite end of this methodological spectrum, treats each observation as a block, resulting in the matrix-free pairwise likelihood method. We propose an additional alternative within this broad methodological landscape, systematically constructing blocks of size two and merging pairs of blocks through conditioning. Importantly, our method strategically avoids large-sized blocks, facilitating explicit calculations that ultimately do not rely on matrix computations. Studies with both simulated and real data validate the effectiveness of our approach, on one hand demonstrating its superiority over pairwise likelihood, and on the other, challenging the intuitive notion that employing matrix-based versions universally lead to better statistical performance.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11265"
    },
    {
        "doc_id": 277,
        "title": "Adaptive Bayesian Optimization Algorithm for Unpredictable Business Environments",
        "authors": [
            "Sarit Maitra"
        ],
        "subjects": [
            "Optimization and Control"
        ],
        "abstract": "This paper presents an innovative optimization framework and algorithm based on the Bayes theorem, featuring adaptive conditioning and jitter. The adaptive conditioning function dynamically modifies the mean objective function in each iteration, enhancing its adaptability. The mean function, representing the model's best estimate of the optimal value for the true objective function, is adjusted based on observed data. The framework also incorporates an adaptive acquisition jitter function, enhancing adaptability by adjusting the jitter of the acquisition function. It also introduces a robust objective function with a penalty term, aiming to generate robust solutions under uncertainty. The evaluation of the framework includes single-objective, decoupled multi-objective, and combined multi-objective functions. Statistical analyses, including t-statistics, p-values, and effect size measures, highlight the superiority of the proposed framework over the original Bayes optimization. The adaptive nature of the conditioning function allows the algorithm to seamlessly incorporate new data, making it particularly beneficial in dynamic optimization scenarios.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11264"
    },
    {
        "doc_id": 278,
        "title": "Estimating heterogeneous treatment effect from survival outcomes via (orthogonal) censoring unbiased learning",
        "authors": [
            "Shenbo Xu",
            "Raluca Cobzaru",
            "Bang Zheng",
            "Stan N. Finkelstein",
            "Roy E. Welsch",
            "Kenney Ng",
            "Ioanna Tzoulaki",
            "Zach Shahn"
        ],
        "subjects": [
            "Methodology",
            "Machine Learning"
        ],
        "abstract": "Methods for estimating heterogeneous treatment effects (HTE) from observational data have largely focused on continuous or binary outcomes, with less attention paid to survival outcomes and almost none to settings with competing risks. In this work, we develop censoring unbiased transformations (CUTs) for survival outcomes both with and without competing risks.After converting time-to-event outcomes using these CUTs, direct application of HTE learners for continuous outcomes yields consistent estimates of heterogeneous cumulative incidence effects, total effects, and separable direct effects. Our CUTs enable application of a much larger set of state of the art HTE learners for censored outcomes than had previously been available, especially in competing risks settings. We provide generic model-free learner-specific oracle inequalities bounding the finite-sample excess risk. The oracle efficiency results depend on the oracle selector and estimated nuisance functions from all steps involved in the transformation. We demonstrate the empirical performance of the proposed methods in simulation studies.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11263"
    },
    {
        "doc_id": 279,
        "title": "Maximum Likelihood Estimators of Quantum Probabilities",
        "authors": [
            "Mirko Navara",
            "Jan \u0160evic"
        ],
        "subjects": [
            "Quantum Physics",
            "Statistics Theory"
        ],
        "abstract": "Classical probability theory is based on assumptions which are often violated in practice. Therefore quantum probability is a proposed alternative not only in quantum physics, but also in other sciences. However, so far it mostly criticizes the classical approach, but does not suggest a working alternative. Maximum likelihood estimators were given very low attention in this context. We show that they can be correctly defined and their computation in closed form is feasible at least in some cases.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11253"
    },
    {
        "doc_id": 280,
        "title": "AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking",
        "authors": [
            "Mehmet Y. Turali",
            "Mehmet E. Lorasdagi",
            "Ali T. Koc",
            "Suleyman S. Kozat"
        ],
        "subjects": [
            "Machine Learning",
            "Signal Processing",
            "Machine Learning"
        ],
        "abstract": "We study the problem of feature selection in general machine learning (ML) context, which is one of the most critical subjects in the field. Although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. To this end, we introduce the ``Adaptive Feature Selection with Binary Masking\" (AFS-BM) which remedies these problems. AFS-BM achieves this by joint optimization for simultaneous feature selection and model training. In particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. This approach leads to significant improvements in model accuracy and a reduction in computational requirements. We provide an extensive set of experiments where we compare AFS-BM with the established feature selection methods using well-known datasets from real-life competitions. Our results show that AFS-BM makes significant improvement in terms of accuracy and requires significantly less computational complexity. This is due to AFS-BM's ability to dynamically adjust to the changing importance of features during the training process, which an important contribution to the field. We openly share our code for the replicability of our results and to facilitate further research.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11250"
    },
    {
        "doc_id": 281,
        "title": "Evaluating if trust and personal information privacy concerns are barriers to using health insurance that explicitly utilizes AI",
        "authors": [
            "Alex Zarifis",
            "Peter Kawalek",
            "Aida Azadegan"
        ],
        "subjects": [
            "Computers and Society",
            "Artificial Intelligence"
        ],
        "abstract": "Trust and privacy have emerged as significant concerns in online transactions. Sharing information on health is especially sensitive but it is necessary for purchasing and utilizing health insurance. Evidence shows that consumers are increasingly comfortable with technology in place of humans, but the expanding use of AI potentially changes this. This research explores whether trust and privacy concern are barriers to the adoption of AI in health insurance. Two scenarios are compared: The first scenario has limited AI that is not in the interface and its presence is not explicitly revealed to the consumer. In the second scenario there is an AI interface and AI evaluation, and this is explicitly revealed to the consumer. The two scenarios were modeled and compared using SEM PLS-MGA. The findings show that trust is significantly lower in the second scenario where AI is visible. Privacy concerns are higher with AI but the difference is not statistically significant within the model.",
        "comments": "Journal of Internet Commerce (2021)",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11249"
    },
    {
        "doc_id": 282,
        "title": "Estimation with Pairwise Observations",
        "authors": [
            "Felix Chan",
            "Laszlo Matyas"
        ],
        "subjects": [
            "Econometrics",
            "Statistics Theory"
        ],
        "abstract": "The paper introduces a new estimation method for the standard linear regression model. The procedure is not driven by the optimisation of any objective function rather, it is a simple weighted average of slopes from observation pairs. The paper shows that such estimator is consistent for carefully selected weights. Other properties, such as asymptotic distributions, have also been derived to facilitate valid statistical inference. Unlike traditional methods, such as Least Squares and Maximum Likelihood, among others, the estimated residual of this estimator is not by construction orthogonal to the explanatory variables of the model. This property allows a wide range of practical applications, such as the testing of endogeneity, i.e.,the correlation between the explanatory variables and the disturbance terms, and potentially several others.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11229"
    },
    {
        "doc_id": 283,
        "title": "On the Information Leakage Performance of Secure Finite Blocklength Transmissions over Rayleigh Fading Channels",
        "authors": [
            "Milad Tatar Mamaghani",
            "Xiangyun Zhou",
            "Nan Yang",
            "A. Lee Swindlehurst",
            "H. Vincent Poor"
        ],
        "subjects": [
            "Information Theory"
        ],
        "abstract": "This paper presents a secrecy performance study of a wiretap communication system with finite blocklength (FBL) transmissions over Rayleigh fading channels, based on the definition of an average information leakage (AIL) metric. We evaluate the exact and closed-form approximate AIL performance, assuming that only statistical channel state information (CSI) of the eavesdropping link is available. Then, we reveal an inherent statistical relationship between the AIL metric in the FBL regime and the commonly-used secrecy outage probability in conventional infinite blocklength communications. Aiming to improve the secure communication performance of the considered system, we formulate a blocklength optimization problem and solve it via a low-complexity approach. Next, we present numerical results to verify our analytical findings and provide various important insights into the impacts of system parameters on the AIL. Specifically, our results indicate that i) compromising a small amount of AIL can lead to significant reliability improvements, and ii) the AIL experiences a secrecy floor in the high signal-to-noise ratio regime.",
        "comments": "6 pages, 5 figures. Accepted for presentation at the 2024 IEEE International Conference on Communications (CT Symposium), 9 - 13 June 2024, Denver, CO United States. Note: An extended version of this work is available as arXiv:2308.13184",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11219"
    },
    {
        "doc_id": 284,
        "title": "Bessel kernel determinants and integrable equations",
        "authors": [
            "Giulio Ruzza"
        ],
        "subjects": [
            "Exactly Solvable and Integrable Systems",
            "Mathematical Physics",
            "Probability"
        ],
        "abstract": "We derive differential equations for multiplicative statistics of the Bessel determinantal point process depending on two parameters. In particular, we prove that such statistics are solutions to an integrable nonlinear partial differential equation describing isospectral deformations of a Sturm--Liouville equation. We also derive identities relating solutions to the integrable partial differential equation and to the Sturm--Liouville equation which imply an analogue for Painlev\u00e9 V of Amir--Corwin--Quastel ``integro-differential Painlev\u00e9 II equation''. This equation reduces, in a degenerate limit, to the system of coupled Painlev\u00e9 V equations derived by Charlier and Doeraene for the generating function of the Bessel process, and to the Painlev\u00e9 V equation derived by Tracy and Widom for the gap probability of the Bessel process. Finally, we study an initial value problem for the integrable partial differential equation. The approach is based on Its--Izergin--Korepin--Slavnov theory of integrable operators and their associated Riemann--Hilbert problems.",
        "comments": "22 pages",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11213"
    },
    {
        "doc_id": 285,
        "title": "$\u03c0$- and $K$-meson properties for large $N_f$ and $N_c$",
        "authors": [
            "Aftab Ahmad",
            "Mumtaz Khan"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "Nuclear Theory"
        ],
        "abstract": "Dynamical chiral symmetry restoration for higher number of light quark flavors $N_f$ and breaking for higher number of colors $N_c$ implies the suppression and enhancement of the dynamically generated quark mass. The study of various larger values of number of colors and flavors may have greater impact on the internal structure of light hadrons. In this work, we study the properties of the pion and kaon, such as mass, condensate, and leptonic decay constant, for various $N_f$ and $N_c$. We use the symmetry-preserving vector-vector flavor-dependent contact interaction model of quark. The dynamical quark masses are calculated by using the Schwinger-Dyson equation (SDE). The masses of the pion and kaon for different values of $N_f$ and $N_c$ are determined using the homogeneous Bethe-Salpeter equation. For fixed $N_f=2$ and $N_c$ is increased, the dynamically generated quark mass ( mass of up and down quarks), strange quark mass, meson in-condensate, and decay constant, all increases. The pion mass remains approximately constant until $N_c$ reaches around 6.5, after which it grows rapidly. On the other hand, the kaon mass increases slowly with increasing $N_c$ until it reaches approximately $N_c=7.5$, beyond which it rises quickly. When $N_c=3$ is fixed at and various values of $N_f$ are considered, all the parameter values decrease as a function of $N_f$, except for the pion and kaon mass, which increase above a critical value of $N_f$ around $8$. This is the region where chiral symmetry is restored, and the pion and kaon behave as free particles, similar to their behavior in the presence of a heat bath. The results obtained for fixed $N_f=2$ and $N_c=3$ are fairly in decent agreement with experimentally calculated statistics and previous model calculations based on the Schwinger-Dyson equation (SDE) and Bethe-Salpeter equation (BSE).",
        "comments": "11 pages, 16 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11186"
    },
    {
        "doc_id": 286,
        "title": "Fermionic signal of vacuum polarization in strong laser fields",
        "authors": [
            "Ya-Nan Dai",
            "Karen Z. Hatsagortsyan",
            "Christoph H. Keitel",
            "Yue-Yue Chen"
        ],
        "subjects": [
            "High Energy Physics - Theory",
            "Plasma Physics"
        ],
        "abstract": "Vacuum polarization (VP) is investigated for the interaction of a polarized $\u03b3$-ray beam of GeV photons with a counterpropagating ultraintense laser pulse. In a conventional setup of a vacuum birefringence measurement, a VP signal is the emerging small circular (linear) polarization of the initially linearly (circularly) polarized probe photons. The pair production via the nonlinear Breit-Wheeler process in such a high-energy environment eliminates part of the $\u03b3$-photons in the outgoing $\u03b3$-beam, increasing the statistical error and decreasing the accuracy of this VP signal. In contrast, we investigate the conversion of the emerging circular polarization of $\u03b3$-photons into longitudinal polarization of the created positrons, considering the latter as the main VP signal. To study the VP effects in the highly nonlinear regime, where the Euler-Heisenberg effective Lagrangian method breaks down, we have developed a Monte-Carlo simulation method, incorporating vacuum birefringence and dichroism via the one-loop QED probabilities in the locally constant field approximation. Our Monte Carlo method will enable the study of VP effects in strong fields of arbitrary configuration. With 10~PW laser systems, we demonstrate the feasibility of detecting the fermionic signal of the VP effect at the 5$\u03c3$ confidence level with a few hours of measurement time.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11168"
    },
    {
        "doc_id": 287,
        "title": "Generalizing Speaker Verification for Spoof Awareness in the Embedding Space",
        "authors": [
            "Xuechen Liu",
            "Md Sahidullah",
            "Kong Aik Lee",
            "Tomi Kinnunen"
        ],
        "subjects": [
            "Cryptography and Security",
            "Artificial Intelligence",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "It is now well-known that automatic speaker verification (ASV) systems can be spoofed using various types of adversaries. The usual approach to counteract ASV systems against such attacks is to develop a separate spoofing countermeasure (CM) module to classify speech input either as a bonafide, or a spoofed utterance. Nevertheless, such a design requires additional computation and utilization efforts at the authentication stage. An alternative strategy involves a single monolithic ASV system designed to handle both zero-effort imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have the potential to provide stronger protections and more economic computations. To this end, we propose to generalize the standalone ASV (G-SASV) against spoofing attacks, where we leverage limited training data from CM to enhance a simple backend in the embedding space, without the involvement of a separate CM module during the test (authentication) phase. We propose a novel yet simple backend classifier based on deep neural networks and conduct the study via domain adaptation and multi-task integration of spoof embeddings at the training stage. Experiments are conducted on the ASVspoof 2019 logical access dataset, where we improve the performance of statistical ASV backends on the joint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and 49.8% in terms of equal error rates, respectively.",
        "comments": "To appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11156"
    },
    {
        "doc_id": 288,
        "title": "Weaving classical turbulence with quantum skeleton",
        "authors": [
            "Weiyu Shen",
            "Jie Yao",
            "Yue Yang"
        ],
        "subjects": [
            "Fluid Dynamics",
            "Quantum Gases",
            "Superconductivity"
        ],
        "abstract": "Matter entanglement is a common chaotic structure in both quantum and classical systems. Turbulence can be pictured as a tangle of vortex filaments in superfluids and viscous vortices in classical fluids. However, it is hard to explain how the statistical properties of turbulence arise from elemental structures. Here we use the quantum vortex tangle as a skeleton to generate an instantaneous classical turbulent field with intertwined vortex tubes. Combining the quantum skeleton and tunable vortex thickness makes the synthetic turbulence satisfy key statistical laws and provides valuable insights for elucidating energy cascade and extreme events. By manipulating the elemental structures, we customize turbulence with desired statistical features. This bottom-up approach of \"weaving\" turbulence provides a testbed for analyzing and modeling turbulence.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11149"
    },
    {
        "doc_id": 289,
        "title": "Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable",
        "authors": [
            "Yuta Kawakami",
            "Manabu Kuroki",
            "Jin Tian"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "There has been considerable recent interest in estimating heterogeneous causal effects. In this paper, we introduce conditional average partial causal effects (CAPCE) to reveal the heterogeneity of causal effects with continuous treatment. We provide conditions for identifying CAPCE in an instrumental variable setting. We develop three families of CAPCE estimators: sieve, parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze their statistical properties. We illustrate the proposed CAPCE estimators on synthetic and real-world data.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11130"
    },
    {
        "doc_id": 290,
        "title": "Regularized Estimation of Sparse Spectral Precision Matrices",
        "authors": [
            "Navonil Deb",
            "Amy Kuceyeski",
            "Sumanta Basu"
        ],
        "subjects": [
            "Methodology",
            "Computation"
        ],
        "abstract": "Spectral precision matrix, the inverse of a spectral density matrix, is an object of central interest in frequency-domain analysis of multivariate time series. Estimation of spectral precision matrix is a key step in calculating partial coherency and graphical model selection of stationary time series. When the dimension of a multivariate time series is moderate to large, traditional estimators of spectral density matrices such as averaged periodograms tend to be severely ill-conditioned, and one needs to resort to suitable regularization strategies involving optimization over complex variables.\n  In this work, we propose complex graphical Lasso (CGLASSO), an $\\ell_1$-penalized estimator of spectral precision matrix based on local Whittle likelihood maximization. We develop fast $\\textit{pathwise coordinate descent}$ algorithms for implementing CGLASSO on large dimensional time series data sets. At its core, our algorithmic development relies on a ring isomorphism between complex and real matrices that helps map a number of optimization problems over complex variables to similar optimization problems over real variables. This finding may be of independent interest and more broadly applicable for high-dimensional statistical analysis with complex-valued data. We also present a complete non-asymptotic theory of our proposed estimator which shows that consistent estimation is possible in high-dimensional regime as long as the underlying spectral precision matrix is suitably sparse. We compare the performance of CGLASSO with competing alternatives on simulated data sets, and use it to construct partial coherence network among brain regions from a real fMRI data set.",
        "comments": "55 pages, 8 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11128"
    },
    {
        "doc_id": 291,
        "title": "Measures determined by their values on balls and Gromov-Wasserstein convergence",
        "authors": [
            "Anne van Delft",
            "Andrew J. Blumberg"
        ],
        "subjects": [
            "Algebraic Topology",
            "Probability"
        ],
        "abstract": "A classical question about a metric space is whether Borel measures on the space are determined by their values on balls. We show that for any given measure this property is stable under Gromov-Wasserstein convergence of metric measure spaces. We then use this result to show that suitable bounded subspaces of the space of persistence diagrams have the property that any Borel measure is determined by its values on balls. This justifies the use of empirical ball volumes for statistical testing in topological data analysis (TDA). Our intended application is to deploy the statistical foundations of van Delft and Blumberg (2023) for time series of random geometric objects in the context of TDA invariants, specifically persistent homology and zigzag persistence.",
        "comments": "MSC Class:          62R40; 55N31",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11125"
    },
    {
        "doc_id": 292,
        "title": "Constraint-based measures of shift and relative shift for discrete frequency distributions",
        "authors": [
            "Kenneth J. Locey",
            "Brian D. Stein"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Comparisons of frequency distributions often invoke the concept of shift to describe directional changes in properties such as the mean. In the present study, we sought to define shift as a property in and of itself. Specifically, we define distributional shift (DS) as the concentration of frequencies away from the discrete class having the greatest value (e.g., the right-most bin of a histogram). We derive a measure of DS using the normalized sum of exponentiated cumulative frequencies. We then define relative distributional shift (RDS) as the difference in DS between two distributions, revealing the magnitude and direction by which one distribution is concentrated to lesser or greater discrete classes relative to another. We find that RDS is highly related to popular measures that, while based on the comparison of frequency distributions, do not explicitly consider shift. While RDS provides a useful complement to other comparative measures, DS allows shift to be quantified as a property of individual distributions, similar in concept to a statistical moment.",
        "comments": "21 pages, 1 table, 6 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11119"
    },
    {
        "doc_id": 293,
        "title": "A Finger on the Pulse of Cardiovascular Health: Smartphone Photoplethysmography-Based Pulse Waveform Analysis for Blood Pressure Measurement",
        "authors": [
            "Ivan Liu",
            "Fangyuan Liu",
            "Qi Zhong",
            "Shiguang Ni"
        ],
        "subjects": [
            "Signal Processing",
            "Computers and Society"
        ],
        "abstract": "Routine blood pressure (BP) monitoring, crucial for health assessment, faces challenges such as limited access to medical-grade equipment and expertise. Portable cuff BP devices, on the other hand, are cumbersome to carry all day and often cost-prohibitive in less developed countries. Besides, these sphygmomanometer-based devices can cause discomfort and disrupt blood flow during measurement. This study explores the use of smartphones for continuous BP monitoring, focusing on overcoming the trust barriers associated with the opacity of machine learning models in predicting BP from low-quality PPG signals. Our approach included developing models based on cardiovascular literature, using simple statistical methods to estimate BP from smartphone PPG signals with comprehensive data pre-processing, applying SHAP for enhanced interpretability and feature identification, and comparing our methods against standard references using Bland-Altman analysis. Validated with data from 125 participants, the study demonstrated significant correlations in waveform features between smartphone and reference BP monitoring devices. The cross-validation of linear regression [MAE=9.86 and 8.01 mmHg for systolic blood pressure (SBP) and diastolic blood pressure (DBP), respectively] and random forest model (MAE=8.91 and 6.68 mmHg for SBP and DBP) using waveform-only variables demonstrated the feasibility of using a smartphone to estimate BP. Although SHAP analysis identified key feature sets, Bland-Altman results did not fully meet established thresholds (84.64% and 94.69% of MAE<15 mmHg for SBP and DBP, respectively). The study suggests the potential of smartphone cameras to enhance the accuracy and interpretability of machine learning models for daily BP estimation, but also indicates that smartphone PPG-based BP prediction is not yet a replacement for traditional medical devices.",
        "comments": "33 pages, 9 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11117"
    },
    {
        "doc_id": 294,
        "title": "Efficient Data Shapley for Weighted Nearest Neighbor Algorithms",
        "authors": [
            "Jiachen T. Wang",
            "Prateek Mittal",
            "Ruoxi Jia"
        ],
        "subjects": [
            "Data Structures and Algorithms",
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "This work aims to address an open problem in data valuation literature concerning the efficient computation of Data Shapley for weighted $K$ nearest neighbor algorithm (WKNN-Shapley). By considering the accuracy of hard-label KNN with discretized weights as the utility function, we reframe the computation of WKNN-Shapley into a counting problem and introduce a quadratic-time algorithm, presenting a notable improvement from $O(N^K)$, the best result from existing literature. We develop a deterministic approximation algorithm that further improves computational efficiency while maintaining the key fairness properties of the Shapley value. Through extensive experiments, we demonstrate WKNN-Shapley's computational efficiency and its superior performance in discerning data quality compared to its unweighted counterpart.",
        "comments": "AISTATS 2024 Oral",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11103"
    },
    {
        "doc_id": 295,
        "title": "Asymptotic Normality of the Conditional Value-at-Risk based Pickands Estimator",
        "authors": [
            "Yizhou Li",
            "Pawel Polak"
        ],
        "subjects": [
            "Statistics Theory",
            "Other Statistics"
        ],
        "abstract": "The Pickands estimator for the extreme value index is beneficial due to its universal consistency, location, and scale invariance, which sets it apart from other types of estimators. However, similar to many extreme value index estimators, it is marked by poor asymptotic efficiency. Chen (2021) introduces a Conditional Value-at-Risk (CVaR)-based Pickands estimator, establishes its consistency, and demonstrates through simulations that this estimator significantly reduces mean squared error while preserving its location and scale invariance. The initial focus of this paper is on demonstrating the weak convergence of the empirical CVaR in functional space. Subsequently, based on the established weak convergence, the paper presents the asymptotic normality of the CVaR-based Pickands estimator. It further supports these theoretical findings with empirical evidence obtained through simulation studies.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11096"
    },
    {
        "doc_id": 296,
        "title": "Learned Image Compression with Dual-Branch Encoder and Conditional Information Coding",
        "authors": [
            "Haisheng Fu",
            "Feng Liang",
            "Jie Liang",
            "Zhenman Fang",
            "Guohe Zhang",
            "Jingning Han"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "Recent advancements in deep learning-based image compression are notable. However, prevalent schemes that employ a serial context-adaptive entropy model to enhance rate-distortion (R-D) performance are markedly slow. Furthermore, the complexities of the encoding and decoding networks are substantially high, rendering them unsuitable for some practical applications. In this paper, we propose two techniques to balance the trade-off between complexity and performance. First, we introduce two branching coding networks to independently learn a low-resolution latent representation and a high-resolution latent representation of the input image, discriminatively representing the global and local information therein. Second, we utilize the high-resolution latent representation as conditional information for the low-resolution latent representation, furnishing it with global information, thus aiding in the reduction of redundancy between low-resolution information. We do not utilize any serial entropy models. Instead, we employ a parallel channel-wise auto-regressive entropy model for encoding and decoding low-resolution and high-resolution latent representations. Experiments demonstrate that our method is approximately twice as fast in both encoding and decoding compared to the parallelizable checkerboard context model, and it also achieves a 1.2% improvement in R-D performance compared to state-of-the-art learned image compression schemes. Our method also outperforms classical image codecs including H.266/VVC-intra (4:4:4) and some recent learned methods in rate-distortion performance, as validated by both PSNR and MS-SSIM metrics on the Kodak dataset.",
        "comments": "Accepted by DCC2024",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11093"
    },
    {
        "doc_id": 297,
        "title": "Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions",
        "authors": [
            "Adel Javanmard",
            "Lin Chen",
            "Vahab Mirrokni",
            "Ashwinkumar Badanidiyuru",
            "Gang Fu"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Statistics Theory",
            "Machine Learning"
        ],
        "abstract": "Due to the rise of privacy concerns, in many practical applications the training data is aggregated before being shared with the learner, in order to protect privacy of users' sensitive responses. In an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is available only with an aggregate response, providing a summary of individuals' responses in that bag. In this paper, we study two natural loss functions for learning from aggregate responses: bag-level loss and the instance-level loss. In the former, the model is learnt by minimizing a loss between aggregate responses and aggregate model predictions, while in the latter the model aims to fit individual predictions to the aggregate responses. In this work, we show that the instance-level loss can be perceived as a regularized form of the bag-level loss. This observation lets us compare the two approaches with respect to bias and variance of the resulting estimators, and introduce a novel interpolating estimator which combines the two approaches. For linear regression tasks, we provide a precise characterization of the risk of the interpolating estimator in an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis allows us to theoretically understand the effect of different factors, such as bag size on the model prediction risk. In addition, we propose a mechanism for differentially private learning from aggregate responses and derive the optimal bag size in terms of prediction risk-privacy trade-off. We also carry out thorough experiments to corroborate our theory and show the efficacy of the interpolating estimator.",
        "comments": "To appear in the Twelfth International Conference on Learning Representations (ICLR 2024)",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11081"
    },
    {
        "doc_id": 298,
        "title": "Estimating the Hawkes process from a discretely observed sample path",
        "authors": [
            "Feng Chen",
            "Jeffrey Kwan",
            "Tom Stindl"
        ],
        "subjects": [
            "Methodology",
            "Computation"
        ],
        "abstract": "The Hawkes process is a widely used model in many areas, such as\n  finance, seismology, neuroscience, epidemiology, and social\n  sciences. Estimation of the Hawkes process from continuous\n  observations of a sample path is relatively straightforward using\n  either the maximum likelihood or other methods. However, estimating\n  the parameters of a Hawkes process from observations of a sample\n  path at discrete time points only is challenging due to the\n  intractability of the likelihood with such data. In this work, we\n  introduce a method to estimate the Hawkes process from a discretely\n  observed sample path. The method takes advantage of a state-space\n  representation of the incomplete data problem and use the sequential\n  Monte Carlo (aka particle filtering) to approximate the likelihood\n  function. As an estimator of the likelihood function the SMC\n  approximation is unbiased, and therefore it can be used together\n  with the Metropolis-Hastings algorithm to construct Markov Chains to\n  approximate the likelihood distribution, or more generally, the\n  posterior distribution of model parameters. The performance of the\n  methodology is assessed using simulation experiments and compared\n  with other recently published methods. The proposed estimator is\n  found to have a smaller mean square error than the two benchmark\n  estimators. The proposed method has the additional advantage that\n  confidence intervals for the parameters are easily available. We\n  apply the proposed estimator to the analysis of weekly count data on\n  measles cases in Tokyo Japan and compare the results to those by\n  one of the benchmark methods.",
        "comments": "23 page, 5 figures",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11075"
    },
    {
        "doc_id": 299,
        "title": "Efficient Data Reduction Strategies for Big Data and High-Dimensional LASSO Regressions",
        "authors": [
            "Xin Wang",
            "Min Yang",
            "William Li"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "The IBOSS approach proposed by Wang et al. (2019) selects the most informative subset of n points. It assumes that the ordinary least squares method is used and requires that the number of variables, p, is not large. However, in many practical problems, p is very large and penalty-based model fitting methods such as LASSO is used. We study the big data problems, in which both n and p are large. In the first part, we focus on reduction in data points. We develop theoretical results showing that the IBOSS type of approach can be applicable to penalty-based regressions such as LASSO. In the second part, we consider the situations where p is extremely large. We propose a two-step approach that involves first reducing the number of variables and then reducing the number of data points. Two separate algorithms are developed, whose performances are studied through extensive simulation studies. Compared to existing methods including well-known split-and-conquer approach, the proposed methods enjoy advantages in terms of estimation accuracy, prediction accuracy, and computation time.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11070"
    },
    {
        "doc_id": 300,
        "title": "Quality-Aware Hydraulic Control in Drinking Water Networks via Controllability Proxies",
        "authors": [
            "Salma M. Elsherif",
            "Mohamad H. Kazma",
            "Ahmad F. Taha"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "The operation of water distribution networks is a complex procedure aimed at efficiently delivering consumers with adequate water quantity while ensuring its safe quality. An added challenge is the dependency of the water quality dynamics on the system's hydraulics, which influences the performance of the water quality controller. Prior research has addressed either solving the optimum operational hydraulic setting problem or regulating the water quality dynamics as separate problems. Additionally, there have been efforts to couple these two problems and solve one compact problem resulting in trade-offs between the contradictory objectives. In contrast, this paper examines the dependency and influence from a control-theoretic standpoint. More specifically, we explore the influence of accountability for water quality controllability improvement when addressing the pump scheduling problem. We examine its effects on the cumulative cost of the interconnected systems as well as the subsequent performance of the water quality controller. To achieve this, we develop a framework that incorporates different controllability metrics within the operational hydraulic optimization problem; its aim is attaining an adequate level of water quality control across the system. We assess the aforementioned aspects' performance on various scaled networks with a wide range of numerical scenarios.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12214"
    },
    {
        "doc_id": 301,
        "title": "Programmable EM Sensor Array for Golden-Model Free Run-time Trojan Detection and Localization",
        "authors": [
            "Hanqiu Wang",
            "Max Panoff",
            "Zihao Zhan",
            "Shuo Wang",
            "Christophe Bobda",
            "Domenic Forte"
        ],
        "subjects": [
            "Cryptography and Security",
            "Signal Processing"
        ],
        "abstract": "Side-channel analysis has been proven effective at detecting hardware Trojans in integrated circuits (ICs). However, most detection techniques rely on large external probes and antennas for data collection and require a long measurement time to detect Trojans. Such limitations make these techniques impractical for run-time deployment and ineffective in detecting small Trojans with subtle side-channel signatures. To overcome these challenges, we propose a Programmable Sensor Array (PSA) for run-time hardware Trojan detection, localization, and identification. PSA is a tampering-resilient integrated on-chip magnetic field sensor array that can be re-programmed to change the sensors' shape, size, and location. Using PSA, EM side-channel measurement results collected from sensors at different locations on an IC can be analyzed to localize and identify the Trojan. The PSA has better performance than conventional external magnetic probes and state-of-the-art on-chip single-coil magnetic field sensors. We fabricated an AES-128 test chip with four AES Hardware Trojans. They were successfully detected, located, and identified with the proposed on-chip PSA within 10 milliseconds using our proposed cross-domain analysis.",
        "comments": "6 pages, 5 figures, Accepted at DATE2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12193"
    },
    {
        "doc_id": 302,
        "title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation",
        "authors": [
            "Zachary Novack",
            "Julian McAuley",
            "Taylor Berg-Kirkpatrick",
            "Nicholas J. Bryan"
        ],
        "subjects": [
            "Sound",
            "Artificial Intelligence",
            "Machine Learning",
            "Audio and Speech Processing"
        ],
        "abstract": "We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose frame-work for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents. Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency. We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control - all without ever fine-tuning the underlying model. When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models. Sound examples can be found at https://DITTO-Music.github.io/web/.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12179"
    },
    {
        "doc_id": 303,
        "title": "Waveform-Domain Complementary Signal Sets for Interrupted Sampling Repeater Jamming Suppression",
        "authors": [
            "Hanning Su",
            "Qinglong Bao",
            "Jiameng Pan",
            "Fucheng Guo",
            "Weidong Hu"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "The interrupted-sampling repeater jamming (ISRJ) is coherent and has the characteristic of suppression and deception to degrade the radar detection capabilities. The study focuses on anti-ISRJ techniques in the waveform domain, primarily capitalizing on waveform design and and anti-jamming signal processing methods in the waveform domain. By exploring the relationship between waveform-domain adaptive matched filtering (WD-AMF) output and waveform-domain signals, we demonstrate that ISRJ can be effectively suppressed when the transmitted waveform exhibits waveform-domain complementarity. We introduce a phase-coded (PC) waveform set with waveform-domain complementarity and propose a method for generating such waveform sets of arbitrary code lengths. The performance of WD-AMF are further developed due to the designed waveforms, and simulations affirm the superior adaptive anti-jamming capabilities of the designed waveforms compared to traditional ones. Remarkably, this improved performance is achieved without the need for prior knowledge of ISRJ interference parameters at either the transmitter or receiver stages.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12173"
    },
    {
        "doc_id": 304,
        "title": "Robust stability analysis of an energy-efficient control in a Networked Control System with application to Unmanned Ground Vehicles",
        "authors": [
            "Antonio Gonzalez",
            "Angel Cuenca",
            "Julian Salt",
            "Jelle Jacobs"
        ],
        "subjects": [
            "Systems and Control",
            "Optimization and Control"
        ],
        "abstract": "In this paper, the robust stability and disturbance rejection performance analysis of an energy-efficient control is addressed in the framework of Networked Control System (NCS). The control scheme under study integrates periodic event-triggered control, packet-based control, time-varying Kalman filter, dual-rate control and prediction techniques, whose design is aimed at reducing energy consumption and bandwidth usage. The robust stability against time-varying model uncertainties is analyzed by means of a suficient condition based on Linear Matrix Inequalities (LMI). Finally, the effectiveness of the proposed approach is experimentally validated in a tracking control for an Unmanned Ground Vehicle (UGV), which is a battery-constrained mobile device with limited computation capacities.",
        "comments": "38 pages, 12 figures, Information Sciences, 2021",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12172"
    },
    {
        "doc_id": 305,
        "title": "Dynamic Semantic Compression for CNN Inference in Multi-access Edge Computing: A Graph Reinforcement Learning-based Autoencoder",
        "authors": [
            "Nan Li",
            "Alexandros Iosifidis",
            "Qi Zhang"
        ],
        "subjects": [
            "Image and Video Processing",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "This paper studies the computational offloading of CNN inference in dynamic multi-access edge computing (MEC) networks. To address the uncertainties in communication time and computation resource availability, we propose a novel semantic compression method, autoencoder-based CNN architecture (AECNN), for effective semantic extraction and compression in partial offloading. In the semantic encoder, we introduce a feature compression module based on the channel attention mechanism in CNNs, to compress intermediate data by selecting the most informative features. In the semantic decoder, we design a lightweight decoder to reconstruct the intermediate data through learning from the received compressed data to improve accuracy. To effectively trade-off communication, computation, and inference accuracy, we design a reward function and formulate the offloading problem of CNN inference as a maximization problem with the goal of maximizing the average inference accuracy and throughput over the long term. To address this maximization problem, we propose a graph reinforcement learning-based AECNN (GRL-AECNN) method, which outperforms existing works DROO-AECNN, GRL-BottleNet++ and GRL-DeepJSCC under different dynamic scenarios. This highlights the advantages of GRL-AECNN in offloading decision-making in dynamic MEC.",
        "comments": "arXiv admin note: text overlap with arXiv:2211.13745",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12167"
    },
    {
        "doc_id": 306,
        "title": "ScoreDec: A Phase-preserving High-Fidelity Audio Codec with A Generalized Score-based Diffusion Post-filter",
        "authors": [
            "Yi-Chiao Wu",
            "Dejan Markovi\u0107",
            "Steven Krenn",
            "Israel D. Gebru",
            "Alexander Richard"
        ],
        "subjects": [
            "Audio and Speech Processing"
        ],
        "abstract": "Although recent mainstream waveform-domain end-to-end (E2E) neural audio codecs achieve impressive coded audio quality with a very low bitrate, the quality gap between the coded and natural audio is still significant. A generative adversarial network (GAN) training is usually required for these E2E neural codecs because of the difficulty of direct phase modeling. However, such adversarial learning hinders these codecs from preserving the original phase information. To achieve human-level naturalness with a reasonable bitrate, preserve the original phase, and get rid of the tricky and opaque GAN training, we develop a score-based diffusion post-filter (SPF) in the complex spectral domain and combine our previous AudioDec with the SPF to propose ScoreDec, which can be trained using only spectral and score-matching losses. Both the objective and subjective experimental results show that ScoreDec with a 24~kbps bitrate encodes and decodes full-band 48~kHz speech with human-level naturalness and well-preserved phase information.",
        "comments": "5 pages, 3 figures, 2 tables. Proc. ICASSP, 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12160"
    },
    {
        "doc_id": 307,
        "title": "Efficient Resource Allocation and User Association in NOMA-Enabled Vehicular-Aided HetNets with High Altitude Platforms",
        "authors": [
            "Ali Nauman",
            "Mashael Maashi",
            "Hend K. Alkahtani",
            "Fahd N. Al-Wesabi",
            "Nojood O Aljehane",
            "Mohammed Assiri",
            "Sara Saadeldeen Ibrahim",
            "Wali Ullah Khan"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "The increasing demand for massive connectivity and high data rates has made the efficient use of existing spectrum resources an increasingly challenging problem. Non-orthogonal multiple access (NOMA) is a potential solution for future heterogeneous networks (HetNets) due to its high capacity and spectrum efficiency. In this study, we analyze an uplink NOMA-enabled vehicular-aided HetNet, where multiple vehicular user equipment (VUEs) share the access link spectrum, and a high-altitude platform (HAP) communicates with roadside units (RSUs) through a backhaul communication link. We propose an improved algorithm for user association that selects VUEs for HAPs based on channel coefficient ratios and terrestrial VUEs based on a caching-state backhaul communication link. The joint optimization problems aim to maximize a utility function that considers VUE transmission rates and cross-tier interference while meeting the constraints of backhaul transmission rates and QoS requirements of each VUE. The joint resource allocation optimization problem consists of three sub-problems: bandwidth allocation, user association, and transmission power allocation. We derive a closed-form solution for bandwidth allocation and solve the transmission power allocation sub-problem iteratively using Taylor expansion to transform a non-convex term into a convex one. Our proposed three-stage iterative algorithm for resource allocation integrates all three sub-problems and is shown to be effective through simulation results. Specifically, the results demonstrate that our solution achieves performance improvements over existing approaches.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12141"
    },
    {
        "doc_id": 308,
        "title": "Evaluation of QCNN-LSTM for Disability Forecasting in Multiple Sclerosis Using Sequential Multisequence MRI",
        "authors": [
            "John D. Mayfield",
            "Issam El Naqa"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Emerging Technologies",
            "Image and Video Processing"
        ],
        "abstract": "Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term Memory (LSTM) models were studied to provide sequential relationships for each timepoint in MRIs of patients with Multiple Sclerosis (MS). In this pilot study, we compared three QCNN-LSTM models for binary classification of MS disability benchmarked against classical neural network architectures. Our hypothesis is that quantum models will provide competitive performance. Methods Matrix Product State (MPS), reverse Multistate Entanglement Renormalization Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM layer to process near-annual MRI data of patients diagnosed with MS. These were benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision Transformer (ViViT). Predicted logits were measured against ground truth labels of each patient's Extended Disability Severity Score (EDSS) using binary cross-entropy loss. Training/validation/holdout testing was partitioned using 5-fold cross validation with a total split of 60:20:20. Levene's test of variance was used to measure statistical difference and Student's t-test for paired model differences in mean. Results The MPS-LSTM, reverse MERA-LSTM, and TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively (p-value 0.915). VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73 and 0.77, respectively (p-value 0.631). Overall variance and mean were not statistically significant (p-value 0.713), however, time to train was significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218, respectively, p-value <0.001). Conclusion QCNN-LSTM models perform competitively to their classical counterparts with greater efficiency in train time. Clinically, these can add value in terms of efficiency to time-dependent deep learning prediction of disease progression based upon medical imaging.",
        "comments": "ACM Class:          I.2.0; I.2.6",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12132"
    },
    {
        "doc_id": 309,
        "title": "Energy-aware Trajectory Optimization for UAV-mounted RIS and Full-duplex Relay",
        "authors": [
            "Dimitrios Tyrovolas",
            "Nikos A. Mitsiou",
            "Thomas G. Boufikos",
            "Prodromos-Vasileios Mekikis",
            "Sotiris A. Tegos",
            "Panagiotis D. Diamantoulakis",
            "Sotiris Ioannidis",
            "Christos K. Liaskos",
            "George K. Karagiannidis"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "In the evolving landscape of sixth-generation (6G) wireless networks, unmanned aerial vehicles (UAVs) have emerged as transformative tools for dynamic and adaptive connectivity. However, dynamically adjusting their position to offer favorable communication channels introduces operational challenges in terms of energy consumption, especially when integrating advanced communication technologies like reconfigurable intelligent surfaces (RISs) and full-duplex relays (FDRs). To this end, by recognizing the pivotal role of UAV mobility, the paper introduces an energy-aware trajectory design for UAV-mounted RISs and UAV-mounted FDRs using the decode and forward (DF) protocol, aiming to maximize the network minimum rate and enhance user fairness, while taking into consideration the available on-board energy. Specifically, this work highlights their distinct energy consumption characteristics and their associated integration challenges by developing appropriate energy consumption models for both UAV-mounted RISs and FDRs that capture the intricate relationship between key factors such as weight, and their operational characteristics. Furthermore, a joint time-division multiple access (TDMA) user scheduling-UAV trajectory optimization problem is formulated, considering the power dynamics of both systems, while assuring that the UAV energy is not depleted mid-air. Finally, simulation results underscore the importance of energy considerations in determining the optimal trajectory and scheduling and provide insights into the performance comparison of UAV-mounted RISs and FDRs in UAV-assisted wireless networks.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12107"
    },
    {
        "doc_id": 310,
        "title": "Consistency Based Unsupervised Self-training For ASR Personalisation",
        "authors": [
            "Jisi Zhang",
            "Vandana Rajan",
            "Haaris Mehmood",
            "David Tuckey",
            "Pablo Peso Parada",
            "Md Asif Jalal",
            "Karthikeyan Saravanan",
            "Gil Ho Lee",
            "Jungin Lee",
            "Seokyeong Jung"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "On-device Automatic Speech Recognition (ASR) models trained on speech data of a large population might underperform for individuals unseen during training. This is due to a domain shift between user data and the original training data, differed by user's speaking characteristics and environmental acoustic conditions. ASR personalisation is a solution that aims to exploit user data to improve model robustness. The majority of ASR personalisation methods assume labelled user data for supervision. Personalisation without any labelled data is challenging due to limited data size and poor quality of recorded audio samples. This work addresses unsupervised personalisation by developing a novel consistency based training method via pseudo-labelling. Our method achieves a relative Word Error Rate Reduction (WERR) of 17.3% on unlabelled training data and 8.1% on held-out data compared to a pre-trained model, and outperforms the current state-of-the art methods.",
        "comments": "Accepted for IEEE ASRU 2023",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12085"
    },
    {
        "doc_id": 311,
        "title": "DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI",
        "authors": [
            "Sergio Morell-Ortega",
            "Marina Ruiz-Perez",
            "Marien Gadea",
            "Roberto Vivo-Hernando",
            "Gregorio Rubio",
            "Fernando Aparici",
            "Mariam de la Iglesia-Vaya",
            "Gwenaelle Catheline",
            "Pierrick Coup\u00e9",
            "Jos\u00e9 V. Manj\u00f3n"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Neurons and Cognition"
        ],
        "abstract": "This paper introduces a novel multimodal and high-resolution human brain cerebellum lobule segmentation method. Unlike current tools that operate at standard resolution ($1 \\text{ mm}^{3}$) or using mono-modal data, the proposed method improves cerebellum lobule segmentation through the use of a multimodal and ultra-high resolution ($0.125 \\text{ mm}^{3}$) training dataset. To develop the method, first, a database of semi-automatically labelled cerebellum lobules was created to train the proposed method with ultra-high resolution T1 and T2 MR images. Then, an ensemble of deep networks has been designed and developed, allowing the proposed method to excel in the complex cerebellum lobule segmentation task, improving precision while being memory efficient. Notably, our approach deviates from the traditional U-Net model by exploring alternative architectures. We have also integrated deep learning with classical machine learning methods incorporating a priori knowledge from multi-atlas segmentation, which improved precision and robustness. Finally, a new online pipeline, named DeepCERES, has been developed to make available the proposed method to the scientific community requiring as input only a single T1 MR image at standard resolution.",
        "comments": "20 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12074"
    },
    {
        "doc_id": 312,
        "title": "Resource-constrained stereo singing voice cancellation",
        "authors": [
            "Clara Borrelli",
            "James Rae",
            "Dogac Basaran",
            "Matt McVicar",
            "Mehrez Souden",
            "Matthias Mauch"
        ],
        "subjects": [
            "Sound",
            "Machine Learning",
            "Audio and Speech Processing"
        ],
        "abstract": "We study the problem of stereo singing voice cancellation, a subtask of music source separation, whose goal is to estimate an instrumental background from a stereo mix. We explore how to achieve performance similar to large state-of-the-art source separation networks starting from a small, efficient model for real-time speech separation. Such a model is useful when memory and compute are limited and singing voice processing has to run with limited look-ahead. In practice, this is realised by adapting an existing mono model to handle stereo input. Improvements in quality are obtained by tuning model parameters and expanding the training set. Moreover, we highlight the benefits a stereo model brings by introducing a new metric which detects attenuation inconsistencies between channels. Our approach is evaluated using objective offline metrics and a large-scale MUSHRA trial, confirming the effectiveness of our techniques in stringent listening tests.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12068"
    },
    {
        "doc_id": 313,
        "title": "NEUROSEC: FPGA-Based Neuromorphic Audio Security",
        "authors": [
            "Murat Isik",
            "Hiruna Vishwamith",
            "Yusuf Sur",
            "Kayode Inadagbo",
            "I. Can Dikmen"
        ],
        "subjects": [
            "Cryptography and Security",
            "Emerging Technologies",
            "Machine Learning",
            "Neural and Evolutionary Computing",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "Neuromorphic systems, inspired by the complexity and functionality of the human brain, have gained interest in academic and industrial attention due to their unparalleled potential across a wide range of applications. While their capabilities herald innovation, it is imperative to underscore that these computational paradigms, analogous to their traditional counterparts, are not impervious to security threats. Although the exploration of neuromorphic methodologies for image and video processing has been rigorously pursued, the realm of neuromorphic audio processing remains in its early stages. Our results highlight the robustness and precision of our FPGA-based neuromorphic system. Specifically, our system showcases a commendable balance between desired signal and background noise, efficient spike rate encoding, and unparalleled resilience against adversarial attacks such as FGSM and PGD. A standout feature of our framework is its detection rate of 94%, which, when compared to other methodologies, underscores its greater capability in identifying and mitigating threats within 5.39 dB, a commendable SNR ratio. Furthermore, neuromorphic computing and hardware security serve many sensor domains in mission-critical and privacy-preserving applications.",
        "comments": "Audio processing, FPGA, Hardware Security, Neuromorphic Computing",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12055"
    },
    {
        "doc_id": 314,
        "title": "Look, Listen and Recognise: Character-Aware Audio-Visual Subtitling",
        "authors": [
            "Bruno Korbar",
            "Jaesung Huh",
            "Andrew Zisserman"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "The goal of this paper is automatic character-aware subtitle generation. Given a video and a minimal amount of metadata, we propose an audio-visual method that generates a full transcript of the dialogue, with precise speech timestamps, and the character speaking identified. The key idea is to first use audio-visual cues to select a set of high-precision audio exemplars for each character, and then use these exemplars to classify all speech segments by speaker identity. Notably, the method does not require face detection or tracking. We evaluate the method over a variety of TV sitcoms, including Seinfeld, Fraiser and Scrubs. We envision this system being useful for the automatic generation of subtitles to improve the accessibility of the vast amount of videos available on modern streaming services. Project page : \\url{https://www.robots.ox.ac.uk/~vgg/research/look-listen-recognise/}",
        "comments": "Accepted for publication in ICASSP 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12039"
    },
    {
        "doc_id": 315,
        "title": "A Survey of Advances in Optimization Methods for Wireless Communication System Design",
        "authors": [
            "Ya-Feng Liu",
            "Tsung-Hui Chang",
            "Mingyi Hong",
            "Zheyu Wu",
            "Anthony Man-Cho So",
            "Eduard A. Jorswieck",
            "Wei Yu"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing",
            "Optimization and Control"
        ],
        "abstract": "Mathematical optimization is now widely regarded as an indispensable modeling and solution tool for the design of wireless communications systems. While optimization has played a significant role in the revolutionary progress in wireless communication and networking technologies from 1G to 5G and onto the future 6G, the innovations in wireless technologies have also substantially transformed the nature of the underlying mathematical optimization problems upon which the system designs are based and have sparked significant innovations in the development of methodologies to understand, to analyze, and to solve those problems. In this paper, we provide a comprehensive survey of recent advances in mathematical optimization theory and algorithms for wireless communication system design. We begin by illustrating common features of mathematical optimization problems arising in wireless communication system design. We discuss various scenarios and use cases and their associated mathematical structures from an optimization perspective. We then provide an overview of recent advances in mathematical optimization theory and algorithms, from nonconvex optimization, global optimization, and integer programming, to distributed optimization and learning-based optimization. The key to successful solution of mathematical optimization problems is in carefully choosing and/or developing suitable optimization algorithms (or neural network architectures) that can exploit the underlying problem structure. We conclude the paper by identifying several open research challenges and outlining future research directions.",
        "comments": "47 pages, 10 figures, submitted for possible publication",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12025"
    },
    {
        "doc_id": 316,
        "title": "NLCG-Net: A Model-Based Zero-Shot Learning Framework for Undersampled Quantitative MRI Reconstruction",
        "authors": [
            "Xinrui Jiang",
            "Yohan Jun",
            "Jaejin Cho",
            "Mengze Gao",
            "Xingwang Yong",
            "Berkin Bilgic"
        ],
        "subjects": [
            "Image and Video Processing",
            "Machine Learning",
            "Signal Processing"
        ],
        "abstract": "Typical quantitative MRI (qMRI) methods estimate parameter maps after image reconstructing, which is prone to biases and error propagation. We propose a Nonlinear Conjugate Gradient (NLCG) optimizer for model-based T2/T1 estimation, which incorporates U-Net regularization trained in a scan-specific manner. This end-to-end method directly estimates qMRI maps from undersampled k-space data using mono-exponential signal modeling with zero-shot scan-specific neural network regularization to enable high fidelity T1 and T2 mapping. T2 and T1 mapping results demonstrate the ability of the proposed NLCG-Net to improve estimation quality compared to subspace reconstruction at high accelerations.",
        "comments": "8 pages, 5 figures, submitted to International Society for Magnetic Resonance in Medicine 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12004"
    },
    {
        "doc_id": 317,
        "title": "Lightweight Protection for Privacy in Offloaded Speech Understanding",
        "authors": [
            "Dongqi Cai",
            "Shangguang Wang",
            "Zeling Zhang",
            "Felix Xiaozhu Lin",
            "Mengwei Xu"
        ],
        "subjects": [
            "Sound",
            "Cryptography and Security",
            "Audio and Speech Processing"
        ],
        "abstract": "Speech is a common input method for mobile embedded devices, but cloud-based speech recognition systems pose privacy risks. Disentanglement-based encoders, designed to safeguard user privacy by filtering sensitive information from speech signals, unfortunately require substantial memory and computational resources, which limits their use in less powerful devices. To overcome this, we introduce a novel system, XXX, optimized for such devices. XXX is built on the insight that speech understanding primarily relies on understanding the entire utterance's long-term dependencies, while privacy concerns are often linked to short-term details. Therefore, XXX focuses on selectively masking these short-term elements, preserving the quality of long-term speech understanding. The core of XXX is an innovative differential mask generator, grounded in interpretable learning, which fine-tunes the masking process. We tested XXX on the STM32H7 microcontroller, assessing its performance in various potential attack scenarios. The results show that XXX maintains speech understanding accuracy and privacy at levels comparable to existing encoders, but with a significant improvement in efficiency, achieving up to 53.3$\\times$ faster processing and a 134.1$\\times$ smaller memory footprint.",
        "comments": "under review",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11983"
    },
    {
        "doc_id": 318,
        "title": "Enhancing Safety in Nonlinear Systems: Design and Stability Analysis of Adaptive Cruise Control",
        "authors": [
            "Fan Yang",
            "Haoqi Li",
            "Maolong Lv",
            "Jiangping Hu",
            "Qingrui Zhou",
            "Bijoy K. Ghosh"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "The safety of autonomous driving systems, particularly self-driving vehicles, remains of paramount concern. These systems exhibit affine nonlinear dynamics and face the challenge of executing predefined control tasks while adhering to state and input constraints to mitigate risks. However, achieving safety control within the framework of control input constraints, such as collision avoidance and maintaining system states within secure boundaries, presents challenges due to limited options. In this study, we introduce a novel approach to address safety concerns by transforming safety conditions into control constraints with a relative degree of 1. This transformation is facilitated through the design of control barrier functions, enabling the creation of a safety control system for affine nonlinear networks. Subsequently, we formulate a robust control strategy that incorporates safety protocols and conduct a comprehensive analysis of its stability and reliability. To illustrate the effectiveness of our approach, we apply it to a specific problem involving adaptive cruise control. Through simulations, we validate the efficiency of our model in ensuring safety without compromising control performance. Our approach signifies significant progress in the field, providing a practical solution to enhance safety for autonomous driving systems operating within the context of affine nonlinear dynamics.",
        "comments": "11pages,9figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11961"
    },
    {
        "doc_id": 319,
        "title": "Observation-Guided Meteorological Field Downscaling at Station Scale: A Benchmark and a New Method",
        "authors": [
            "Zili Liu",
            "Hao Chen",
            "Lei Bai",
            "Wenyuan Li",
            "Keyan Chen",
            "Zhengyi Wang",
            "Wanli Ouyang",
            "Zhengxia Zou",
            "Zhenwei Shi"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Image and Video Processing"
        ],
        "abstract": "Downscaling (DS) of meteorological variables involves obtaining high-resolution states from low-resolution meteorological fields and is an important task in weather forecasting. Previous methods based on deep learning treat downscaling as a super-resolution task in computer vision and utilize high-resolution gridded meteorological fields as supervision to improve resolution at specific grid scales. However, this approach has struggled to align with the continuous distribution characteristics of meteorological fields, leading to an inherent systematic bias between the downscaled results and the actual observations at meteorological stations. In this paper, we extend meteorological downscaling to arbitrary scattered station scales, establish a brand new benchmark and dataset, and retrieve meteorological states at any given station location from a coarse-resolution meteorological field. Inspired by data assimilation techniques, we integrate observational data into the downscaling process, providing multi-scale observational priors. Building on this foundation, we propose a new downscaling model based on hypernetwork architecture, namely HyperDS, which efficiently integrates different observational information into the model training, achieving continuous scale modeling of the meteorological field. Through extensive experiments, our proposed method outperforms other specially designed baseline models on multiple surface variables. Notably, the mean squared error (MSE) for wind speed and surface pressure improved by 67% and 19.5% compared to other methods. We will release the dataset and code subsequently.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11960"
    },
    {
        "doc_id": 320,
        "title": "Maximizing Spectral and Energy Efficiency in Multi-user MIMO OFDM Systems with RIS and Hardware Impairment",
        "authors": [
            "Mohammad Soleymani",
            "Ignacio Santamaria",
            "Aydin Sezgin",
            "Eduard Jorswieck"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "An emerging technology to enhance the spectral efficiency (SE) and energy efficiency (EE) of wireless communication systems is reconfigurable intelligent surface (RIS), which is shown to be very powerful in single-carrier systems. However, in multi-user orthogonal frequency division multiplexing (OFDM) systems, RIS may not be as promising as in single-carrier systems since an independent optimization of RIS elements at each sub-carrier is impossible in multi-carrier systems. Thus, this paper investigates the performance of various RIS technologies like regular (reflective and passive), simultaneously transmit and reflect (STAR), and multi-sector beyond diagonal (BD) RIS in multi-user multiple-input multiple-output (MIMO) OFDM broadcast channels (BC). This requires to formulate and solve a joint MIMO precoding and RIS optimization problem. The obtained solution reveals that RIS can significantly improve the system performance even when the number of RIS elements is relatively low. Moreover, we develop resource allocation schemes for STAR-RIS and multi-sector BD-RIS in MIMO OFDM BCs, and show that these RIS technologies can outperform a regular RIS, especially when the regular RIS cannot assist the communications for all the users.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11921"
    },
    {
        "doc_id": 321,
        "title": "A Training-Free Defense Framework for Robust Learned Image Compression",
        "authors": [
            "Myungseo Song",
            "Jinyoung Choi",
            "Bohyung Han"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "We study the robustness of learned image compression models against adversarial attacks and present a training-free defense technique based on simple image transform functions. Recent learned image compression models are vulnerable to adversarial attacks that result in poor compression rate, low reconstruction quality, or weird artifacts. To address the limitations, we propose a simple but effective two-way compression algorithm with random input transforms, which is conveniently applicable to existing image compression models. Unlike the na\u00efve approaches, our approach preserves the original rate-distortion performance of the models on clean images. Moreover, the proposed algorithm requires no additional training or modification of existing models, making it more practical. We demonstrate the effectiveness of the proposed techniques through extensive experiments under multiple compression models, evaluation metrics, and attack scenarios.",
        "comments": "10 pages and 14 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11902"
    },
    {
        "doc_id": 322,
        "title": "Fully Differentiable Ray Tracing via Discontinuity Smoothing for Radio Network Optimization",
        "authors": [
            "Jerome Eertmans",
            "Laurent Jacques",
            "Claude Oestges"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "Recently, Differentiable Ray Tracing has been successfully applied in the field of wireless communications for learning radio materials or optimizing the transmitter orientation. However, in the frame of gradient based optimization, obstruction of the rays by objects can cause sudden variations in the related objective functions or create entire regions where the gradient is zero. As these issues can dramatically impact convergence, this paper presents a novel Ray Tracing framework that is fully differentiable with respect to any scene parameter, but also provides a loss function continuous everywhere, thanks to specific local smoothing techniques. Previously non-continuous functions are replaced by a smoothing function, that can be exchanged with any function having similar properties. This function is also configurable via a parameter that determines how smooth the approximation should be. The present method is applied on a basic one-transmitter-multi-receiver scenario, and shows that it can successfully find the optimal solution. As a complementary resource, a 2D Python library, DiffeRT2d, is provided in Open Access, with examples and a comprehensive documentation.",
        "comments": "5 pages, 5 figures, accepted at EuCAP 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11882"
    },
    {
        "doc_id": 323,
        "title": "A Review of Physics-Informed Machine Learning Methods with Applications to Condition Monitoring and Anomaly Detection",
        "authors": [
            "Yuandi Wu",
            "Brett Sicard",
            "Stephen Andrew Gadsden"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Systems and Control"
        ],
        "abstract": "This study presents a comprehensive overview of PIML techniques in the context of condition monitoring. The central concept driving PIML is the incorporation of known physical laws and constraints into machine learning algorithms, enabling them to learn from available data while remaining consistent with physical principles. Through fusing domain knowledge with data-driven learning, PIML methods offer enhanced accuracy and interpretability in comparison to purely data-driven approaches. In this comprehensive survey, detailed examinations are performed with regard to the methodology by which known physical principles are integrated within machine learning frameworks, as well as their suitability for specific tasks within condition monitoring. Incorporation of physical knowledge into the ML model may be realized in a variety of methods, with each having its unique advantages and drawbacks. The distinct advantages and limitations of each methodology for the integration of physics within data-driven models are detailed, considering factors such as computational efficiency, model interpretability, and generalizability to different systems in condition monitoring and fault detection. Several case studies and works of literature utilizing this emerging concept are presented to demonstrate the efficacy of PIML in condition monitoring applications. From the literature reviewed, the versatility and potential of PIML in condition monitoring may be demonstrated. Novel PIML methods offer an innovative solution for addressing the complexities of condition monitoring and associated challenges. This comprehensive survey helps form the foundation for future work in the field. As the technology continues to advance, PIML is expected to play a crucial role in enhancing maintenance strategies, system reliability, and overall operational efficiency in engineering systems.",
        "comments": "Paper has been submitted for review to the journal Expert Systems with Applications (December 31, 2023). 90 pages, 22 figures, 9 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11860"
    },
    {
        "doc_id": 324,
        "title": "LKFormer: Large Kernel Transformer for Infrared Image Super-Resolution",
        "authors": [
            "Feiwei Qin",
            "Kang Yan",
            "Changmiao Wang",
            "Ruiquan Ge",
            "Yong Peng",
            "Kai Zhang"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Given the broad application of infrared technology across diverse fields, there is an increasing emphasis on investigating super-resolution techniques for infrared images within the realm of deep learning. Despite the impressive results of current Transformer-based methods in image super-resolution tasks, their reliance on the self-attentive mechanism intrinsic to the Transformer architecture results in images being treated as one-dimensional sequences, thereby neglecting their inherent two-dimensional structure. Moreover, infrared images exhibit a uniform pixel distribution and a limited gradient range, posing challenges for the model to capture effective feature information. Consequently, we suggest a potent Transformer model, termed Large Kernel Transformer (LKFormer), to address this issue. Specifically, we have designed a Large Kernel Residual Depth-wise Convolutional Attention (LKRDA) module with linear complexity. This mainly employs depth-wise convolution with large kernels to execute non-local feature modeling, thereby substituting the standard self-attentive layer. Additionally, we have devised a novel feed-forward network structure called Gated-Pixel Feed-Forward Network (GPFN) to augment the LKFormer's capacity to manage the information flow within the network. Comprehensive experimental results reveal that our method surpasses the most advanced techniques available, using fewer parameters and yielding considerably superior performance.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11859"
    },
    {
        "doc_id": 325,
        "title": "Adversarial speech for voice privacy protection from Personalized Speech generation",
        "authors": [
            "Shihao Chen",
            "Liping Chen",
            "Jie Zhang",
            "KongAik Lee",
            "Zhenhua Ling",
            "Lirong Dai"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers' voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker's speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.",
        "comments": "Accepted by icassp 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11857"
    },
    {
        "doc_id": 326,
        "title": "MOSformer: Momentum encoder-based inter-slice fusion transformer for medical image segmentation",
        "authors": [
            "De-Xing Huang",
            "Xiao-Hu Zhou",
            "Xiao-Liang Xie",
            "Shi-Qi Liu",
            "Zhen-Qiu Feng",
            "Mei-Jiang Gui",
            "Hao Li",
            "Tian-Yu Xiang",
            "Xiu-Ling Liu",
            "Zeng-Guang Hou"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Medical image segmentation takes an important position in various clinical applications. Deep learning has emerged as the predominant solution for automated segmentation of volumetric medical images. 2.5D-based segmentation models bridge computational efficiency of 2D-based models and spatial perception capabilities of 3D-based models. However, prevailing 2.5D-based models often treat each slice equally, failing to effectively learn and exploit inter-slice information, resulting in suboptimal segmentation performances. In this paper, a novel Momentum encoder-based inter-slice fusion transformer (MOSformer) is proposed to overcome this issue by leveraging inter-slice information at multi-scale feature maps extracted by different encoders. Specifically, dual encoders are employed to enhance feature distinguishability among different slices. One of the encoders is moving-averaged to maintain the consistency of slice representations. Moreover, an IF-Swin transformer module is developed to fuse inter-slice multi-scale features. The MOSformer is evaluated on three benchmark datasets (Synapse, ACDC, and AMOS), establishing a new state-of-the-art with 85.63%, 92.19%, and 85.43% of DSC, respectively. These promising results indicate its competitiveness in medical image segmentation. Codes and models of MOSformer will be made publicly available upon acceptance.",
        "comments": "Under Review",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11856"
    },
    {
        "doc_id": 327,
        "title": "Privacy-Preserving Data Fusion for Traffic State Estimation: A Vertical Federated Learning Approach",
        "authors": [
            "Qiqing Wang",
            "Kaidi Yang"
        ],
        "subjects": [
            "Machine Learning",
            "Cryptography and Security",
            "Systems and Control"
        ],
        "abstract": "This paper proposes a privacy-preserving data fusion method for traffic state estimation (TSE). Unlike existing works that assume all data sources to be accessible by a single trusted party, we explicitly address data privacy concerns that arise in the collaboration and data sharing between multiple data owners, such as municipal authorities (MAs) and mobility providers (MPs). To this end, we propose a novel vertical federated learning (FL) approach, FedTSE, that enables multiple data owners to collaboratively train and apply a TSE model without having to exchange their private data. To enhance the applicability of the proposed FedTSE in common TSE scenarios with limited availability of ground-truth data, we further propose a privacy-preserving physics-informed FL approach, i.e., FedTSE-PI, that integrates traffic models into FL. Real-world data validation shows that the proposed methods can protect privacy while yielding similar accuracy to the oracle method without privacy considerations.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11836"
    },
    {
        "doc_id": 328,
        "title": "Intelligibility Enhancement of Acoustic Noisy Speech for Autism Spectrum Disorder Condition",
        "authors": [
            "M. Pillonetto",
            "A. Queiroz",
            "R. Coelho"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "This work introduces a time domain personalized method (pGTFF0) to achieve intelligibility improvement of noisy speech for Autism Spectrum Disorder (ASD) situation. For this proposal, harmonic features estimated from speech frames are considered as center frequencies of Gammatone auditory filterbanks. A gain factor is further applied to the output of the filtered samples. The key goal is the emulation of an external noise filtering tailored for individuals with ASD. A perceptual listening test demonstrates that ASD volunteers attained lower intelligibility rates than Neurotypical (NT). The proposed solution is compared to three competing approaches considering four acoustic noises at different signal-to-noise ratios. Two objective measures (ESTOI and PESQ) are also adopted for evaluation. The experimental results show that the personalized solution outperformed the competing approaches in terms of intelligibility and quality improvement.",
        "comments": "5 pages, 3 figues, 2 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11832"
    },
    {
        "doc_id": 329,
        "title": "Harmonic Detection from Noisy Speech with Auditory Frame Gain for Intelligibility Enhancement",
        "authors": [
            "A. Queiroz",
            "R. Coelho"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "This paper introduces a novel (HDAG - Harmonic Detection for Auditory Gain) method for speech intelligibility enhancement in noisy scenarios. In the proposed scheme, a series of selective Gammachirp filters are adopted to emphasize the harmonic components of speech reducing the masking effects of acoustic noises. The fundamental frequency are estimated by the HHT-Amp technique. Harmonic patterns estimated with low accuracy are detected and adjusted according the FSFFE low/high pitch separation. The central frequencies of the filterbank are defined considering the third octave subbands which are best suited to cover the regions most relevant to intelligibility. Before signal reconstruction, the gammachirp filtered components are amplified by gain factors regulated by FSFFE classification. The proposed HDAG solution and three baseline techniques are examined considering six background noises with four signal-to-noise ratios. Three objective measures are adopted for the evaluation of speech intelligibility and quality. Several experiments are conducted to demonstrate that the proposed scheme achieves better speech intelligibility improvement when compared to the competing approaches. A perceptual listening test is further considered and corroborates with the objective results.",
        "comments": "9 pages, 6 figures, 4 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11829"
    },
    {
        "doc_id": 330,
        "title": "Performance Analysis of Fluid Antenna-aided Backscatter Communications Systems",
        "authors": [
            "Farshad Rostami Ghadi",
            "Masoud Kaveh",
            "Kai-Kit Wong"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "This paper studies the performance of backscatter communications (BC) over emerging fluid antenna (FA) technology. In particular, a single-antenna source sends information to a FA reader through the wireless forward (i.e., source-to-tag) and backscatter (tag-to-reader) channels. For the considered BC, we first derive the cumulative distribution function (CDF) of the equivalent channel at the FA receiver, and then we obtain closed-form expressions of the outage probability (OP) and delay outage rate (DOR) under a correlated Rayleigh distribution. Moreover, in order to gain more insights into the system performance, we present analytical expressions of the OP and DOR at the high SNR regime. Numerical results indicate that considering the FA at the reader can significantly improve the performance of BC in terms of the OP and DOR compared with a single-antenna reader.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11820"
    },
    {
        "doc_id": 331,
        "title": "Analyzing the coupling process of distributed mixed real-virtual prototypes",
        "authors": [
            "Peter Baumann",
            "Lars Mikelsons",
            "Oliver Kotte",
            "Dieter Schramm"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "The ongoing connection and automation of vehicles leads to a closer interaction of the individual vehicle components, which demands for consideration throughout the entire development process. In the design phase, this is achieved through co-simulation of component models. However, complex co-simulation environments are rarely (re-)used in the verification and validation phases, in which mixed real-virtual prototypes (e.g. Hardware-in-the-Loop) are already available. One reason for this are coupling errors such as time-delays, which inevitably occur in co-simulation of virtual and real-time systems, and which influence system behavior in an unknown and generally detrimental way. This contribution introduces a novel, adaptive method to compensate for constant time-delays in potentially highly nonlinear, spatially distributed mixed real-virtual prototypes, using small feedforward neural networks. Their optimal initialization with respect to defined frequency domain features results from a-priori frequency domain analysis of the entire coupled system, including coupling faults and compensation methods. A linear and a nonlinear example demonstrate the method and emphasize its suitability for nonlinear systems due to online training and adaptation. As the compensation method requires knowledge only of the bandwidths, the proposed method is applicable to distributed mixed real-virtual prototypes in general.",
        "comments": "8 pages, 12 figures, published at 33rd Annual European Simulation and Modelling Conference, ESM 2019",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11779"
    },
    {
        "doc_id": 332,
        "title": "Advancing Accessibility: Voice Cloning and Speech Synthesis for Individuals with Speech Disorders",
        "authors": [
            "Vinotha R",
            "Hepsiba D",
            "L. D. Vijay Anand",
            "Deepak John Reji"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "Neural Text-to-speech (TTS) synthesis is a powerful technology that can generate speech using neural networks. One of the most remarkable features of TTS synthesis is its capability to produce speech in the voice of different speakers. This paper introduces voice cloning and speech synthesis https://pypi.org/project/voice-cloning/ an open-source python package for helping speech disorders to communicate more effectively as well as for professionals seeking to integrate voice cloning or speech synthesis capabilities into their projects. This package aims to generate synthetic speech that sounds like the natural voice of an individual, but it does not replace the natural human voice. The architecture of the system comprises a speaker verification system, a synthesizer, a vocoder, and noise reduction. Speaker verification system trained on a varied set of speakers to achieve optimal generalization performance without relying on transcriptions. Synthesizer is trained using both audio and transcriptions that generate Mel spectrogram from a text and vocoder which converts the generated Mel Spectrogram into corresponding audio signal. Then the audio signal is processed by a noise reduction algorithm to eliminate unwanted noise and enhance speech clarity. The performance of synthesized speech from seen and unseen speakers are then evaluated using subjective and objective evaluation such as Mean Opinion Score (MOS), Gross Pitch Error (GPE), and Spectral distortion (SD). The model can create speech in distinct voices by including speaker characteristics that are chosen randomly.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11771"
    },
    {
        "doc_id": 333,
        "title": "Massive Synchrony in Distributed Antenna Systems",
        "authors": [
            "Erik G. Larsson"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "Distributed antennas must be phase-calibrated (phase-synchronized) for certain operations, such as reciprocity-based joint coherent downlink beamforming, to work. We use rigorous signal processing tools to analyze the accuracy of calibration protocols that are based on over-the-air measurements between antennas, with a focus on scalability aspects for large systems. We show that (i) for some who-measures-on-whom topologies, the errors in the calibration process are unbounded when the network grows; and (ii) despite that conclusion, it is optimal -- irrespective of the topology -- to solve a single calibration problem for the entire system and use the result everywhere to support the beamforming. The analyses are exemplified by investigating specific topologies, including lines, rings, and two-dimensional surfaces.",
        "comments": "Journal ref:        IEEE Transactions on Signal Processing, 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11730"
    },
    {
        "doc_id": 334,
        "title": "Beyond the Manual Touch: Situational-aware Force Control for Increased Safety in Robot-assisted Skullbase Surgery",
        "authors": [
            "Hisashi Ishida",
            "Deepa Galaiya",
            "Nimesh Nagururu",
            "Francis Creighton",
            "Peter Kazanzides",
            "Russell Taylor",
            "Manish Sahu"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "Purpose - Skullbase surgery demands exceptional precision when removing bone in the lateral skull base. Robotic assistance can alleviate the effect of human sensory-motor limitations. However, the stiffness and inertia of the robot can significantly impact the surgeon's perception and control of the tool-to-tissue interaction forces. Methods - We present a situational-aware, force control technique aimed at regulating interaction forces during robot-assisted skullbase drilling. The contextual interaction information derived from the digital twin environment is used to enhance sensory perception and suppress undesired high forces. Results - To validate our approach, we conducted initial feasibility experiments involving a medical and two engineering students. The experiment focused on further drilling around critical structures following cortical mastoidectomy. The experiment results demonstrate that robotic assistance coupled with our proposed control scheme effectively limited undesired interaction forces when compared to robotic assistance without the proposed force control. Conclusions - The proposed force control techniques show promise in significantly reducing undesired interaction forces during robot-assisted skullbase surgery. These findings contribute to the ongoing efforts to enhance surgical precision and safety in complex procedures involving the lateral skull base.",
        "comments": "*These authors contributed equally to this work",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11721"
    },
    {
        "doc_id": 335,
        "title": "Integrating 3D Slicer with a Dynamic Simulator for Situational Aware Robotic Interventions",
        "authors": [
            "Manish Sahu",
            "Hisashi Ishida",
            "Laura Connolly",
            "Hongyi Fan",
            "Anton Deguet",
            "Peter Kazanzides",
            "Francis X. Creighton",
            "Russell H. Taylor",
            "Adnan Munawar"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "Image-guided robotic interventions represent a transformative frontier in surgery, blending advanced imaging and robotics for improved precision and outcomes. This paper addresses the critical need for integrating open-source platforms to enhance situational awareness in image-guided robotic research. We present an open-source toolset that seamlessly combines a physics-based constraint formulation framework, AMBF, with a state-of-the-art imaging platform application, 3D Slicer. Our toolset facilitates the creation of highly customizable interactive digital twins, that incorporates processing and visualization of medical imaging, robot kinematics, and scene dynamics for real-time robot control. Through a feasibility study, we showcase real-time synchronization of a physical robotic interventional environment in both 3D Slicer and AMBF, highlighting low-latency updates and improved visualization.",
        "comments": "*These authors contributed equally",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11715"
    },
    {
        "doc_id": 336,
        "title": "Haptic-Assisted Collaborative Robot Framework for Improved Situational Awareness in Skull Base Surgery",
        "authors": [
            "Hisashi Ishida",
            "Manish Sahu",
            "Adnan Munawar",
            "Nimesh Nagururu",
            "Deepa Galaiya",
            "Peter Kazanzides",
            "Francis X. Creighton",
            "Russell H. Taylor"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "Skull base surgery is a demanding field in which surgeons operate in and around the skull while avoiding critical anatomical structures including nerves and vasculature. While image-guided surgical navigation is the prevailing standard, limitation still exists requiring personalized planning and recognizing the irreplaceable role of a skilled surgeon. This paper presents a collaboratively controlled robotic system tailored for assisted drilling in skull base surgery. Our central hypothesis posits that this collaborative system, enriched with haptic assistive modes to enforce virtual fixtures, holds the potential to significantly enhance surgical safety, streamline efficiency, and alleviate the physical demands on the surgeon. The paper describes the intricate system development work required to enable these virtual fixtures through haptic assistive modes. To validate our system's performance and effectiveness, we conducted initial feasibility experiments involving a medical student and two experienced surgeons. The experiment focused on drilling around critical structures following cortical mastoidectomy, utilizing dental stone phantom and cadaveric models. Our experimental results demonstrate that our proposed haptic feedback mechanism enhances the safety of drilling around critical structures compared to systems lacking haptic assistance. With the aid of our system, surgeons were able to safely skeletonize the critical structures without breaching any critical structure even under obstructed view of the surgical site.",
        "comments": "*These authors contributed equally",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11709"
    },
    {
        "doc_id": 337,
        "title": "Keep Decoding Parallel with Effective Knowledge Distillation from Language Models to End-to-end Speech Recognisers",
        "authors": [
            "Michael Hentschel",
            "Yuta Nishikawa",
            "Tatsuya Komatsu",
            "Yusuke Fujita"
        ],
        "subjects": [
            "Computation and Language",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "This study presents a novel approach for knowledge distillation (KD) from a BERT teacher model to an automatic speech recognition (ASR) model using intermediate layers. To distil the teacher's knowledge, we use an attention decoder that learns from BERT's token probabilities. Our method shows that language model (LM) information can be more effectively distilled into an ASR model using both the intermediate layers and the final layer. By using the intermediate layers as distillation target, we can more effectively distil LM knowledge into the lower network layers. Using our method, we achieve better recognition accuracy than with shallow fusion of an external LM, allowing us to maintain fast parallel decoding. Experiments on the LibriSpeech dataset demonstrate the effectiveness of our approach in enhancing greedy decoding with connectionist temporal classification (CTC).",
        "comments": "Accepted at ICASSP 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11700"
    },
    {
        "doc_id": 338,
        "title": "Emulation-based Stabilization for Networked Control Systems with Stochastic Channels",
        "authors": [
            "Wei Ren",
            "Wei Wang",
            "Zhuo-Rui Pan",
            "Xi-Ming Sun",
            "Andrew R. Teel",
            "Dragan Nesic"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "This paper studies the stabilization problem of networked control systems (NCSs) with random packet dropouts caused by stochastic channels. To describe the effects of stochastic channels on the information transmission, the transmission times are assumed to be deterministic, whereas the packet transmission is assumed to be random. We first propose a stochastic scheduling protocol to model random packet dropouts, and address the properties of the proposed stochastic scheduling protocol. The proposed scheduling protocol provides a unified modelling framework for a general class of random packet dropouts due to different stochastic channels. Next, the proposed scheduling protocol is embedded into the closed-loop system, which leads to a stochastic hybrid model for NCSs with random packet dropouts. Based on this stochastic hybrid model, we follow the emulation approach to establish sufficient conditions to guarantee uniform global asymptotical stability in probability. In particular, an upper bound on the maximally allowable transmission interval is derived explicitly for all stochastic protocols satisfying Lyapunov conditions that guarantee uniform global asymptotic stability in probability. Finally, two numerical examples are presented to demonstrate the derived results.",
        "comments": "12 pages, 4 figures, accepted",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11677"
    },
    {
        "doc_id": 339,
        "title": "Rethinking Cross-Attention for Infrared and Visible Image Fusion",
        "authors": [
            "Lihua Jian",
            "Songlei Xiong",
            "Han Yan",
            "Xiaoguang Niu",
            "Shaowu Wu",
            "Di Zhang"
        ],
        "subjects": [
            "Image and Video Processing"
        ],
        "abstract": "The salient information of an infrared image and the abundant texture of a visible image can be fused to obtain a comprehensive image. As can be known, the current fusion methods based on Transformer techniques for infrared and visible (IV) images have exhibited promising performance. However, the attention mechanism of the previous Transformer-based methods was prone to extract common information from source images without considering the discrepancy information, which limited fusion performance. In this paper, by reevaluating the cross-attention mechanism, we propose an alternate Transformer fusion network (ATFuse) to fuse IV images. Our ATFuse consists of one discrepancy information injection module (DIIM) and two alternate common information injection modules (ACIIM). The DIIM is designed by modifying the vanilla cross-attention mechanism, which can promote the extraction of the discrepancy information of the source images. Meanwhile, the ACIIM is devised by alternately using the vanilla cross-attention mechanism, which can fully mine common information and integrate long dependencies. Moreover, the successful training of ATFuse is facilitated by a proposed segmented pixel loss function, which provides a good trade-off for texture detail and salient structure preservation. The qualitative and quantitative results on public datasets indicate our ATFFuse is effective and superior compared to other state-of-the-art methods.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11675"
    },
    {
        "doc_id": 340,
        "title": "RTA-Former: Reverse Transformer Attention for Polyp Segmentation",
        "authors": [
            "Zhikai Li",
            "Murong Yi",
            "Ali Uneri",
            "Sihan Niu",
            "Craig Jones"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "Polyp segmentation is a key aspect of colorectal cancer prevention, enabling early detection and guiding subsequent treatments. Intelligent diagnostic tools, including deep learning solutions, are widely explored to streamline and potentially automate this process. However, even with many powerful network architectures, there still comes the problem of producing accurate edge segmentation. In this paper, we introduce a novel network, namely RTA-Former, that employs a transformer model as the encoder backbone and innovatively adapts Reverse Attention (RA) with a transformer stage in the decoder for enhanced edge segmentation. The results of the experiments illustrate that RTA-Former achieves state-of-the-art (SOTA) performance in five polyp segmentation datasets. The strong capability of RTA-Former holds promise in improving the accuracy of Transformer-based polyp segmentation, potentially leading to better clinical decisions and patient outcomes. Our code will be publicly available on GitHub.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11671"
    },
    {
        "doc_id": 341,
        "title": "Streaming Bilingual End-to-End ASR model using Attention over Multiple Softmax",
        "authors": [
            "Aditya Patil",
            "Vikas Joshi",
            "Purvi Agrawal",
            "Rupesh Mehta"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Computation and Language",
            "Sound"
        ],
        "abstract": "Even with several advancements in multilingual modeling, it is challenging to recognize multiple languages using a single neural model, without knowing the input language and most multilingual models assume the availability of the input language. In this work, we propose a novel bilingual end-to-end (E2E) modeling approach, where a single neural model can recognize both languages and also support switching between the languages, without any language input from the user. The proposed model has shared encoder and prediction networks, with language-specific joint networks that are combined via a self-attention mechanism. As the language-specific posteriors are combined, it produces a single posterior probability over all the output symbols, enabling a single beam search decoding and also allowing dynamic switching between the languages. The proposed approach outperforms the conventional bilingual baseline with 13.3%, 8.23% and 1.3% word error rate relative reduction on Hindi, English and code-mixed test sets, respectively.",
        "comments": "Published in IEEE's Spoken Language Technology (SLT) 2022, 8 pages (6 + 2 for references), 5 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11645"
    },
    {
        "doc_id": 342,
        "title": "Optimizing performance in elastic optical networks using advanced reconfigurable optical add-drop multiplexers: A novel design approach and comprehensive analysis",
        "authors": [
            "Faranak Khosravi",
            "Mehdi Shadaram"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "Network operators diversify service offerings and enhance network efficiency by leveraging bandwidth-variable transceivers and colorless flexible-grid reconfigurable optical add-drop multiplexers (ROADMs). Nonetheless, the paradigm shift from rigid to elastic optical networks (EONs) has affected several key parameters, including bit rate, center frequency spacing, modulation format, and optical reach. This study investigated the transformative impact of emerging technologies on the design and structure of optical network architectures, including spectrally efficient multicarrier systems and bandwidth-variable wavelength-selective switches. A cost-effective ROADM architecture applying an order-based connecting approach was introduced, which presented a high connectivity level and a blockage probability of less than 10-4. When this architecture was implemented in the EON, the data transportation rate was 1 Tb/s. This outcome successfully accommodated a 20 % surge in traffic demand, while the optimized network architecture significantly improved fiber utilization by 3.4 %. Consequently, this study contributed a practical and efficient solution for implementing flexible optical networks, effectively addressing current concerns and propelling the optical communication system sector forward.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11640"
    },
    {
        "doc_id": 343,
        "title": "Reframing Offline Reinforcement Learning as a Regression Problem",
        "authors": [
            "Prajwal Koirala",
            "Cody Fleming"
        ],
        "subjects": [
            "Machine Learning",
            "Systems and Control"
        ],
        "abstract": "The study proposes the reformulation of offline reinforcement learning as a regression problem that can be solved with decision trees. Aiming to predict actions based on input states, return-to-go (RTG), and timestep information, we observe that with gradient-boosted trees, the agent training and inference are very fast, the former taking less than a minute. Despite the simplification inherent in this reformulated problem, our agent demonstrates performance that is at least on par with established methods. This assertion is validated by testing it across standard datasets associated with D4RL Gym-MuJoCo tasks. We further discuss the agent's ability to generalize by testing it on two extreme cases, how it learns to model the return distributions effectively even with highly skewed expert datasets, and how it exhibits robust performance in scenarios with sparse/delayed rewards.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11630"
    },
    {
        "doc_id": 344,
        "title": "Real-Time Systems Optimization with Black-box Constraints and Hybrid Variables",
        "authors": [
            "Sen Wang",
            "Dong Li",
            "Shao-Yu Huang",
            "Xuanliang Deng",
            "Ashrarul H. Sifat",
            "Changhee Jung",
            "Ryan Williams",
            "Haibo Zeng"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "When optimizing real-time systems, designers often face a challenging problem where the schedulability constraints are non-convex, non-continuous, or lack an analytical form to understand their properties. Although the optimization framework NORTH proposed in previous work is general (it works with arbitrary schedulability analysis) and scalable, it can only handle problems with continuous variables, which limits its application. In this paper, we extend the applications of the framework NORTH to problems with a hybrid of continuous and discrete variables. This is achieved in a coordinate-descent method, where the continuous and discrete variables are optimized separately during iterations. The new framework, NORTH+, improves around 20% solution quality than NORTH in experiments.",
        "comments": "Workshop on OPtimization for Embedded and ReAl-time systems (OPERA 2023) co-located with the 44th IEEE Real-Time Systems Symposium (RTSS)",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11620"
    },
    {
        "doc_id": 345,
        "title": "Another Way to the Top: Exploit Contextual Clustering in Learned Image Coding",
        "authors": [
            "Yichi Zhang",
            "Zhihao Duan",
            "Ming Lu",
            "Dandan Ding",
            "Fengqing Zhu",
            "Zhan Ma"
        ],
        "subjects": [
            "Image and Video Processing"
        ],
        "abstract": "While convolution and self-attention are extensively used in learned image compression (LIC) for transform coding, this paper proposes an alternative called Contextual Clustering based LIC (CLIC) which primarily relies on clustering operations and local attention for correlation characterization and compact representation of an image. As seen, CLIC expands the receptive field into the entire image for intra-cluster feature aggregation. Afterward, features are reordered to their original spatial positions to pass through the local attention units for inter-cluster embedding. Additionally, we introduce the Guided Post-Quantization Filtering (GuidedPQF) into CLIC, effectively mitigating the propagation and accumulation of quantization errors at the initial decoding stage. Extensive experiments demonstrate the superior performance of CLIC over state-of-the-art works: when optimized using MSE, it outperforms VVC by about 10% BD-Rate in three widely-used benchmark datasets; when optimized using MS-SSIM, it saves more than 50% BD-Rate over VVC. Our CLIC offers a new way to generate compact representations for image compression, which also provides a novel direction along the line of LIC development.",
        "comments": "The 38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024)",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11615"
    },
    {
        "doc_id": 346,
        "title": "$\\texttt{immrax}$: A Parallelizable and Differentiable Toolbox for Interval Analysis and Mixed Monotone Reachability in JAX",
        "authors": [
            "Akash Harapanahalli",
            "Saber Jafarpour",
            "Samuel Coogan"
        ],
        "subjects": [
            "Systems and Control",
            "Machine Learning",
            "Optimization and Control"
        ],
        "abstract": "We present an implementation of interval analysis and mixed monotone interval reachability analysis as function transforms in Python, fully composable with the computational framework JAX. The resulting toolbox inherits several key features from JAX, including computational efficiency through Just-In-Time Compilation, GPU acceleration for quick parallelized computations, and Automatic Differentiability. We demonstrate the toolbox's performance on several case studies, including a reachability problem on a vehicle model controlled by a neural network, and a robust closed-loop optimal control problem for a swinging pendulum.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11608"
    },
    {
        "doc_id": 347,
        "title": "Thermal Image Calibration and Correction using Unpaired Cycle-Consistent Adversarial Networks",
        "authors": [
            "Hossein Rajoli",
            "Pouya Afshin",
            "Fatemeh Afghah"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning",
            "Image and Video Processing"
        ],
        "abstract": "Unmanned aerial vehicles (UAVs) offer a flexible and cost-effective solution for wildfire monitoring. However, their widespread deployment during wildfires has been hindered by a lack of operational guidelines and concerns about potential interference with aircraft systems. Consequently, the progress in developing deep-learning models for wildfire detection and characterization using aerial images is constrained by the limited availability, size, and quality of existing datasets. This paper introduces a solution aimed at enhancing the quality of current aerial wildfire datasets to align with advancements in camera technology. The proposed approach offers a solution to create a comprehensive, standardized large-scale image dataset. This paper presents a pipeline based on CycleGAN to enhance wildfire datasets and a novel fusion method that integrates paired RGB images as attribute conditioning in the generators of both directions, improving the accuracy of the generated images.",
        "comments": "This paper has been accepted at the Asilomar 2023 Conference and will be published",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11582"
    },
    {
        "doc_id": 348,
        "title": "Deterministic Multi-stage Constellation Reconfiguration Using Integer Linear Programing and Sequential Decision-Making Methods",
        "authors": [
            "Hang Woon Lee",
            "David O. Williams Rogers",
            "Brycen D. Pearl",
            "Hao Chen",
            "Koki Ho"
        ],
        "subjects": [
            "Optimization and Control",
            "Systems and Control"
        ],
        "abstract": "In this paper, we address the problem of reconfiguring Earth observation satellite constellation systems through multiple stages. The Multi-stage Constellation Reconfiguration Problem (MCRP) aims to maximize the total observation rewards obtained by covering a set of targets of interest through the active manipulation of the orbits and relative phasing of constituent satellites. In this paper, we consider deterministic problem settings in which the targets of interest are known a priori. We propose a novel integer linear programming formulation for MCRP, capable of obtaining provably optimal solutions. To overcome computational intractability due to the combinatorial explosion in solving large-scale instances, we introduce two computationally efficient sequential decision-making methods based on the principles of a myopic policy and a rolling horizon procedure. The computational experiments demonstrate that the devised sequential decision-making approaches yield high-quality solutions with improved computational efficiency over the baseline MCRP. Finally, a case study using Hurricane Harvey data showcases the advantages of multi-stage constellation reconfiguration over single-stage and no-reconfiguration scenarios.",
        "comments": "37 pages, 13 figures, submitted to the Journal of Spacecraft and Rockets",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11567"
    },
    {
        "doc_id": 349,
        "title": "Nigel -- Mechatronic Design and Robust Sim2Real Control of an Over-Actuated Autonomous Vehicle",
        "authors": [
            "Chinmay Vilas Samak",
            "Tanmay Vilas Samak",
            "Javad Mohammadpour Velni",
            "Venkat Narayan Krovi"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "Simulation to reality (sim2real) transfer from a dynamics and controls perspective usually involves re-tuning or adapting the designed algorithms to suit real-world operating conditions, which often violates the performance guarantees established originally. This work presents a generalizable framework for achieving reliable sim2real transfer of autonomy-oriented control systems using multi-model multi-objective robust optimal control synthesis, which lends well to uncertainty handling and disturbance rejection with theoretical guarantees. Particularly, this work is centered around an actuation-redundant scaled autonomous vehicle called Nigel, with independent all-wheel drive and independent all-wheel steering architecture, whose enhanced configuration space bodes well for robust control applications. To this end, we present a systematic study on the complete mechatronic design, dynamics modeling, parameter identification, and robust stabilizing as well as steady-state tracking control of Nigel using the proposed framework, with experimental validation.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11542"
    },
    {
        "doc_id": 350,
        "title": "Model Predictive Approach for Detumbling an Underactuated Satellite",
        "authors": [
            "Kota Kondo",
            "Yasuhiro Yoshimura",
            "Mai Bando",
            "Shuji Nagasaki",
            "Toshiya Hanada"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "This research proposes an innovative approach to detumble satellites' triple-axis angular velocities with only one single-axis magnetic torquer. Since magnetic torque is generated perpendicularly to magnetorquers, no intended control torque along the magnetorquer can be produced, which makes systems underactuated. Our paper introduces a control method using Model Predictive Control (MPC) and compares it with B-dot control algorithm. By applying these control laws to Kyushu University Light Curve Inversion (Q-Li) Demonstration Satellite in numerical simulations, we describe the applicability of these control laws to underactuated systems.",
        "comments": "15 pages, 4 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11539"
    },
    {
        "doc_id": 351,
        "title": "Maintenance cost assessment for heterogeneous multi-component systems incorporating perfect inspections and waiting time to maintenance",
        "authors": [
            "Luc\u00eda Bautista",
            "Inma T. Castro",
            "Luis Landesa"
        ],
        "subjects": [
            "Systems and Control",
            "Probability"
        ],
        "abstract": "Most existing research about complex systems maintenance assumes they consist of the same type of components. However, systems can be assembled with heterogeneous components (for example degrading and non-degrading components) that require different maintenance actions. Since industrial systems become more and more complex, more research about the maintenance of systems with heterogeneous components is needed. For this reason, in this paper, a system consisting of two groups of components: degrading and non-degrading components is analyzed. The main novelty of this paper is the evaluation of a maintenance policy at system-level coordinating condition-based maintenance for the degrading components, delay time to the maintenance and an inspection strategy for this heterogeneous system. To that end, an analytic cost model is built using the semi-regenerative processes theory. Furthermore, a safety constraint related to the reliability of the degrading components is imposed. To find the optimal maintenance strategy, meta-heuristic algorithms are used.",
        "comments": "27 pages, 4 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11538"
    },
    {
        "doc_id": 352,
        "title": "Nonlinear Model Predictive Detumbling of Small Satellites with a Single-axis Magnetorquer",
        "authors": [
            "Kota Kondo",
            "Ilya Kolmanovsky",
            "Yasuhiro Yoshimura",
            "Mai Bando",
            "Shuji Nagasaki",
            "Toshiya Hanada"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "Various actuators are used in spacecraft to achieve attitude stabilization, including thrusters, momentum wheels, and control moment gyros. Small satellites, however, have stringent size, weight, and cost constraints, which makes many actuator choices prohibitive. Consequently, magnetic torquers have commonly been applied to spacecraft to attenuate angular rates. Approaches for dealing with under-actuation due to magnetic control torque's dependency on the magnetic field and required high magnetic flux densities have been previously considered. Generally speaking, control of a satellite that becomes under-actuated as a result of on-board failures has been a recurrent theme in the literature. Methods for controlling spacecraft with fewer actuators than degrees of freedom are increasingly in demand due to the increased number of small satellite launches. Magnetic torquers have been extensively investigated for momentum management of spacecraft with momentum wheels and for nutation damping of spin satellites, momentum-biased, and dual-spin satellites. Nonetheless, severely under-actuated small spacecraft that carry only a single-axis magnetic torquer have not been previously treated. This note considers the detumbling of a small spacecraft using only a single-axis magnetic torquer. Even with a three-axis magnetic torquer, the spacecraft is under-actuated, while, in the case of only a single axis magnetic torquer, the problem is considerably more demanding. Our note examines the feasibility of spacecraft attitude control with a single-axis magnetic torquer and possible control methods that can be used.",
        "comments": "20 pages, 6 figures. Journal of Guidance, Control, and Dynamics (2021)",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11536"
    },
    {
        "doc_id": 353,
        "title": "Pulse Width Modulation Method Applied to Nonlinear Model Predictive Control on an Under-actuated Small Satellite",
        "authors": [
            "Kota Kondo",
            "Yasuhiro Yoshimura",
            "Shiji Nagasaki",
            "Toshiya Hanada"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "Among various satellite actuators, magnetic torquers have been widely equipped for stabilization and attitude control of small satellites. Although magnetorquers are generally used with other actuators, such as momentum wheels, this paper explores a control method where only a magnetic actuation is available. We applied a nonlinear optimal control method, Nonlinear Model Predictive Control (NMPC), to small satellites, employing the generalized minimal residual (GMRES) method, which generates continuous control inputs. Onboard magnetic actuation systems often find it challenging to produce smooth magnetic moments as a control input; hence, we employ the Pulse Width Modulation (PWM) method, which discretizes a control input and reduces the burden on actuators. In our case, the PWM approach discretizes control torques generated by the NMPC scheme. This study's main contributions are investigating the NMPC and the GMRES method applied to small spacecraft and presenting the PWM control system's feasibility.",
        "comments": "19 pages, 10 figures. In AIAA Scitech 2021 Forum",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11533"
    },
    {
        "doc_id": 354,
        "title": "CaBuAr: California Burned Areas dataset for delineation",
        "authors": [
            "Daniele Rege Cambrin",
            "Luca Colomba",
            "Paolo Garza"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning",
            "Image and Video Processing"
        ],
        "abstract": "Forest wildfires represent one of the catastrophic events that, over the last decades, caused huge environmental and humanitarian damages. In addition to a significant amount of carbon dioxide emission, they are a source of risk to society in both short-term (e.g., temporary city evacuation due to fire) and long-term (e.g., higher risks of landslides) cases. Consequently, the availability of tools to support local authorities in automatically identifying burned areas plays an important role in the continuous monitoring requirement to alleviate the aftereffects of such catastrophic events. The great availability of satellite acquisitions coupled with computer vision techniques represents an important step in developing such tools. This paper introduces a novel open dataset that tackles the burned area delineation problem, a binary segmentation problem applied to satellite imagery. The presented resource consists of pre- and post-fire Sentinel-2 L2A acquisitions of California forest fires that took place starting in 2015. Raster annotations were generated from the data released by California's Department of Forestry and Fire Protection. Moreover, in conjunction with the dataset, we release three different baselines based on spectral indexes analyses, SegFormer, and U-Net models.",
        "comments": "Accepted at the IEEE Geoscience and Remote Sensing Magazine",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11519"
    },
    {
        "doc_id": 355,
        "title": "Integration of Large Language Models in Control of EHD Pumps for Precise Color Synthesis",
        "authors": [
            "Yanhong Peng",
            "Ceng Zhang",
            "Chenlong Hu",
            "Zebing Mao"
        ],
        "subjects": [
            "Robotics",
            "Artificial Intelligence",
            "Systems and Control"
        ],
        "abstract": "This paper presents an innovative approach to integrating Large Language Models (LLMs) with Arduino-controlled Electrohydrodynamic (EHD) pumps for precise color synthesis in automation systems. We propose a novel framework that employs fine-tuned LLMs to interpret natural language commands and convert them into specific operational instructions for EHD pump control. This approach aims to enhance user interaction with complex hardware systems, making it more intuitive and efficient. The methodology involves four key steps: fine-tuning the language model with a dataset of color specifications and corresponding Arduino code, developing a natural language processing interface, translating user inputs into executable Arduino code, and controlling EHD pumps for accurate color mixing. Conceptual experiment results, based on theoretical assumptions, indicate a high potential for accurate color synthesis, efficient language model interpretation, and reliable EHD pump operation. This research extends the application of LLMs beyond text-based tasks, demonstrating their potential in industrial automation and control systems. While highlighting the limitations and the need for real-world testing, this study opens new avenues for AI applications in physical system control and sets a foundation for future advancements in AI-driven automation technologies.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11500"
    },
    {
        "doc_id": 356,
        "title": "HARDCORE: H-field and power loss estimation for arbitrary waveforms with residual, dilated convolutional neural networks in ferrite cores",
        "authors": [
            "Nikolas F\u00f6rster",
            "Wilhelm Kirchg\u00e4ssner",
            "Till Piepenbrock",
            "Oliver Schweins",
            "Oliver Wallscheid"
        ],
        "subjects": [
            "Systems and Control",
            "Machine Learning",
            "Applied Physics"
        ],
        "abstract": "The MagNet Challenge 2023 calls upon competitors to develop data-driven models for the material-specific, waveform-agnostic estimation of steady-state power losses in toroidal ferrite cores. The following HARDCORE (H-field and power loss estimation for Arbitrary waveforms with Residual, Dilated convolutional neural networks in ferrite COREs) approach shows that a residual convolutional neural network with physics-informed extensions can serve this task efficiently when trained on observational data beforehand. One key solution element is an intermediate model layer which first reconstructs the bh curve and then estimates the power losses based on the curve's area rendering the proposed topology physically interpretable. In addition, emphasis was placed on expert-based feature engineering and information-rich inputs in order to enable a lean model architecture. A model is trained from scratch for each material, while the topology remains the same. A Pareto-style trade-off between model size and estimation accuracy is demonstrated, which yields an optimum at as low as 1755 parameters and down to below 8\\,\\% for the 95-th percentile of the relative error for the worst-case material with sufficient samples.",
        "comments": "Competition submission version",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11488"
    },
    {
        "doc_id": 357,
        "title": "ColorVideoVDP: A visual difference predictor for image, video and display distortions",
        "authors": [
            "Rafal K. Mantiuk",
            "Param Hanji",
            "Maliha Ashraf",
            "Yuta Asano",
            "Alexandre Chapiro"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Graphics",
            "Image and Video Processing"
        ],
        "abstract": "ColorVideoVDP is a video and image quality metric that models spatial and temporal aspects of vision, for both luminance and color. The metric is built on novel psychophysical models of chromatic spatiotemporal contrast sensitivity and cross-channel contrast masking. It accounts for the viewing conditions, geometric, and photometric characteristics of the display. It was trained to predict common video streaming distortions (e.g. video compression, rescaling, and transmission errors), and also 8 new distortion types related to AR/VR displays (e.g. light source and waveguide non-uniformities). To address the latter application, we collected our novel XR-Display-Artifact-Video quality dataset (XR-DAVID), comprised of 336 distorted videos. Extensive testing on XR-DAVID, as well as several datasets from the literature, indicate a significant gain in prediction performance compared to existing metrics. ColorVideoVDP opens the doors to many novel applications which require the joint automated spatiotemporal assessment of luminance and color distortions, including video streaming, display specification and design, visual comparison of results, and perceptually-guided quality optimization.",
        "comments": "28 pages",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11485"
    },
    {
        "doc_id": 358,
        "title": "Distributed Traffic Signal Control of Interconnected Intersections: A Two-Lane Traffic Network Model",
        "authors": [
            "Xinfeng Ru",
            "Weiguo Xia",
            "Ting Bai"
        ],
        "subjects": [
            "Systems and Control",
            "Adaptation and Self-Organizing Systems"
        ],
        "abstract": "Practical and accurate traffic models play an important role in capturing real traffic dynamics and then in achieving effective control performance. This paper studies traffic signal control in a traffic network with multiple interconnected intersections, where the target is to balance the vehicle density on each lane by controlling the green times of each phase at every intersection. Different from traditional road-based modeling schemes, a two-lane intersection model is first proposed to model the flow propagation in a more accurate way. A distributed model predictive control (MPC) method is then presented to assign the green times. To enable the real-time feasibility of the proposed approach, the alternating direction method of multipliers (ADMM) is incorporated with the distributed MPC scheme for solving the problem. Finally, the simulation studies performed in VISSIM for a six-intersection traffic network in Dalian, China, show the effectiveness and characteristics of the proposed method.",
        "comments": "journal paper",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11483"
    },
    {
        "doc_id": 359,
        "title": "Battery-Free Sensor Array for Wireless Multi-Depth In-Situ Sensing",
        "authors": [
            "Hongzhi Guo",
            "Adam Kamrath"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "Underground in-situ sensing plays a vital role in precision agriculture and infrastructure monitoring. While existing sensing systems utilize wires to connect an array of sensors at various depths for spatial-temporal data collection, wireless underground sensor networks offer a cable-free alternative. However, these wireless sensors are typically battery-powered, necessitating periodic recharging or replacement. This paper proposes a battery-free sensor array which can be used for wireless multi-depth in-situ sensing. Utilizing Near Field Communication (NFC)-which can penetrate soil with negligible signal power loss-this sensor array can form a virtual magnetic waveguide, achieving long communication ranges. An analytical model has been developed to offer insights and determine optimal design parameters. Moreover, a prototype, constructed using off-the-shelf NFC sensors, was tested to validate the proposed concept. While this system is primarily designed for underground applications, it holds potential for other multi-depth in-situ sensing scenarios, including underwater environments.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11479"
    },
    {
        "doc_id": 360,
        "title": "Task-specific regularization loss towards model calibration for reliable lung cancer detection",
        "authors": [
            "Mehar Prateek Kalra",
            "Mansi Singhal",
            "Rohan Raju Dhanakashirur"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "Lung cancer is one of the significant causes of cancer-related deaths globally. Early detection and treatment improve the chances of survival. Traditionally CT scans have been used to extract the most significant lung infection information and diagnose cancer. This process is carried out manually by an expert radiologist. The imbalance in the radiologists-to-population ratio in a country like India implies significant work pressure on them and thus raises the need to automate a few of their responsibilities. The tendency of modern-day Deep Neural networks to make overconfident mistakes limit their usage to detect cancer. In this paper, we propose a new task-specific loss function to calibrate the neural network to reduce the risk of overconfident mistakes. We use the state-of-the-art Multi-class Difference in Confidence and Accuracy (MDCA) loss in conjunction with the proposed task-specific loss function to achieve the same. We also integrate post-hoc calibration by performing temperature scaling on top of the train-time calibrated model. We demonstrate 5.98% improvement in the Expected Calibration Error (ECE) and a 17.9% improvement in Maximum Calibration Error (MCE) as compared to the best-performing SOTA algorithm.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11464"
    },
    {
        "doc_id": 361,
        "title": "Energy Consumption Analysis for Continuous Phase Modulation in Smart-Grid Internet of Things of beyond 5G",
        "authors": [
            "Hongjian Gao",
            "Yang Lu",
            "Shaoshi Yang",
            "Jingsheng Tan",
            "Longlong Nie",
            "Xinyi Qu"
        ],
        "subjects": [
            "Signal Processing",
            "Networking and Internet Architecture"
        ],
        "abstract": "Wireless sensor network (WSN) underpinning the smart-grid Internet of Things (SG-IoT) has been a popular research topic in recent years due to its great potential for enabling a wide range of important applications. However, the energy consumption (EC) characteristic of sensor nodes is a key factor that affects the operational performance (e.g., lifetime of sensors) and the total cost of ownership of WSNs. In this paper, to find the modulation techniques suitable for WSNs, we investigate the EC characteristic of continuous phase modulation (CPM), which is an attractive modulation scheme candidate for WSNs because of its constant envelope property. We first develop an EC model for the sensor nodes of WSNs by considering the circuits and a typical communication protocol that relies on automatic repeat request (ARQ)-based retransmissions to ensure successful data delivery. Then, we use this model to analyze the EC characteristic of CPM under various configurations of modulation parameters. Furthermore, we compare the EC characteristic of CPM with that of other representative modulation schemes, such as offset quadrature phase-shift keying (OQPSK) and quadrature amplitude modulation (QAM), which are commonly used in communication protocols of WSNs. Our analysis and simulation results provide insights into the EC characteristics of multiple modulation schemes in the context of WSNs; thus, they are beneficial for designing energy-efficient SG-IoT in the beyond-5G (B5G) and the 6G era.",
        "comments": "7 figures, 2 tables",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11449"
    },
    {
        "doc_id": 362,
        "title": "Towards Non-Robocentric Dynamic Landing of Quadrotor UAVs",
        "authors": [
            "Li-Yu Lo",
            "Boyang Li",
            "Chih-Yung Wen",
            "Ching-Wei Chang"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "In this work, we propose a dynamic landing solution without the need for onboard exteroceptive sensors and an expensive computation unit, where all localization and control modules are carried out on the ground in a non-inertial frame. Our system starts with a relative state estimator of the aerial robot from the perspective of the landing platform, where the state tracking of the UAV is done through a set of onboard LED markers and an on-ground camera; the state is expressed geometrically on manifold, and is returned by Iterated Extended Kalman filter (IEKF) algorithm. Subsequently, a motion planning module is developed to guide the landing process, formulating it as a minimum jerk trajectory by applying the differential flatness property. Considering visibility and dynamic constraints, the problem is solved using quadratic programming, and the final motion primitive is expressed through piecewise polynomials. Through a series of experiments, the applicability of this approach is validated by successfully landing 18 cm x 18 cm quadrotor on a 43 cm x 43 cm platform, exhibiting performance comparable to conventional methods. Finally, we provide comprehensive hardware and software details to the research community for future reference.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11445"
    },
    {
        "doc_id": 363,
        "title": "IoT Cloud RAN Testbed for Ultra-Precise TDoA-based Localization in LPWANs",
        "authors": [
            "Thomas Maul",
            "Joerg Robert",
            "Sebastian Klob"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "There have been many research efforts in the area of localization in recent years. Especially within the Internet of Things (IoT), the knowledge of position information for individual components is of great interest, for example, in asset tracking, to name just one. However, many of these use cases require a high energy efficiency, making a GNSS-based approach infeasible. One promising candidate can be found in Low Power Wide Area Networks (LPWAN), which enable battery lifetimes of up to 20 years. However, no gold standard for localization exists for these types of networks. Our work proposes a testbed architecture that allows the investigation and development of localization algorithms within LPWA Networks. The concept is built on a Cloud Radio Access Network (CRAN) architecture that allows the streaming of IQ from remote base stations to a central processing unit. Furthermore, the architecture is expanded by a synchronization concept based on Signals of Opportunity (SoO) to enable the testbed for runtime-based positioning. Therefore, we propose a hardware concept consisting of antennas and a low-cost off-the-shelf software-defined radio (SDR)-based frontend architecture and a software framework using a hypertext transfer protocol (HTTP)-based server and client architecture. The proposed system is installed in an urban environment. Initial measurements are conducted, where it can be shown that the proposed architecture can be used for highly precise Time Difference of Arrival (TDoA) measurements, offering the possibility of time synchronization down to approximately 200 ps and frequency synchronization of 3 mHz.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11435"
    },
    {
        "doc_id": 364,
        "title": "Joint Downlink and Uplink Optimization for RIS-Aided FDD MIMO Communication Systems",
        "authors": [
            "Gyoseung Lee",
            "Hyeongtaek Lee",
            "Donghwan Kim",
            "Jaehoon Chung",
            "A. Lee. Swindlehurst",
            "Junil Choi"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "This paper investigates reconfigurable intelligent surface (RIS)-aided frequency division duplexing (FDD) communication systems. Since the downlink and uplink signals are simultaneously transmitted in FDD, the phase shifts at the RIS should be designed to support both transmissions. Considering a single-user multiple-input multiple-output system, we formulate a weighted sum-rate maximization problem to jointly maximize the downlink and uplink system performance. To tackle the non-convex optimization problem, we adopt an alternating optimization (AO) algorithm, in which two phase shift optimization techniques are developed to handle the unit-modulus constraints induced by the reflection coefficients at the RIS. The first technique exploits the manifold optimization-based algorithm, while the second uses a lower-complexity AO approach. Numerical results verify that the proposed techniques rapidly converge to local optima and significantly improve the overall system performance compared to existing benchmark schemes.",
        "comments": "Accepted to IEEE Transactions on Wireless Communications",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11429"
    },
    {
        "doc_id": 365,
        "title": "Joint UAV Deployment and Resource Allocation in THz-Assisted MEC-Enabled Integrated Space-Air-Ground Networks",
        "authors": [
            "Yan Kyaw Tun",
            "Gy\u00f6rgy D\u00e1n",
            "Yu Min Park",
            "Choong Seon Hong"
        ],
        "subjects": [
            "Networking and Internet Architecture",
            "Signal Processing"
        ],
        "abstract": "Multi-access edge computing (MEC)-enabled integrated space-air-ground (SAG) networks have drawn much attention recently, as they can provide communication and computing services to wireless devices in areas that lack terrestrial base stations (TBSs). Leveraging the ample bandwidth in the terahertz (THz) spectrum, in this paper, we propose MEC-enabled integrated SAG networks with collaboration among unmanned aerial vehicles (UAVs). We then formulate the problem of minimizing the energy consumption of devices and UAVs in the proposed MEC-enabled integrated SAG networks by optimizing tasks offloading decisions, THz sub-bands assignment, transmit power control, and UAVs deployment. The formulated problem is a mixed-integer nonlinear programming (MILP) problem with a non-convex structure, which is challenging to solve. We thus propose a block coordinate descent (BCD) approach to decompose the problem into four sub-problems: 1) device task offloading decision problem, 2) THz sub-band assignment and power control problem, 3) UAV deployment problem, and 4) UAV task offloading decision problem. We then propose to use a matching game, concave-convex procedure (CCP) method, successive convex approximation (SCA), and block successive upper-bound minimization (BSUM) approaches for solving the individual subproblems. Finally, extensive simulations are performed to demonstrate the effectiveness of our proposed algorithm.",
        "comments": "36 pages, 8 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11419"
    },
    {
        "doc_id": 366,
        "title": "Optimal detection of non-overlapping images via combinatorial auction",
        "authors": [
            "Simon Anuk",
            "Tamir Bendory",
            "Amichai Painsky"
        ],
        "subjects": [
            "Image and Video Processing",
            "Signal Processing",
            "Optimization and Control"
        ],
        "abstract": "This paper studies the classical problem of detecting the location of multiple image occurrences in a two-dimensional, noisy measurement. Assuming the image occurrences do not overlap, we formulate this task as a constrained maximum likelihood optimization problem. We show that the maximum likelihood estimator is equivalent to an instance of the winner determination problem from the field of combinatorial auction, and that the solution can be obtained by searching over a binary tree. We then design a pruning mechanism that significantly accelerates the runtime of the search. We demonstrate on simulations and electron microscopy data sets that the proposed algorithm provides accurate detection in challenging regimes of high noise levels and densely packed image occurrences.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11413"
    },
    {
        "doc_id": 367,
        "title": "Robust Beamforming for Downlink Multi-Cell Systems: A Bilevel Optimization Perspective",
        "authors": [
            "Xingdi Chen",
            "Yu Xiong",
            "Kai Yang"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "Utilization of inter-base station cooperation for information processing has shown great potential in enhancing the overall quality of communication services (QoS) in wireless communication networks. Nevertheless, such cooperations require the knowledge of channel state information (CSI) at base stations (BSs), which is assumed to be perfectly known. However, CSI errors are inevitable in practice which necessitates beamforming techniques that can achieve robust performance in the presence of channel estimation errors. Existing approaches relax the robust beamforming design problems into semidefinite programming (SDP), which can only achieve a solution that is far from being optimal. To this end, this paper views robust beamforming design problems from a bilevel optimization perspective. In particular, we focus on maximizing the worst-case weighted sum-rate (WSR) in the downlink multi-cell multi-user multiple-input single-output (MISO) system considering bounded CSI errors. We first reformulate this problem into a bilevel optimization problem and then develop an efficient algorithm based on the cutting plane method. A distributed optimization algorithm has also been developed to facilitate the parallel processing in practical settings. Numerical results are provided to confirm the effectiveness of the proposed algorithm in terms of performance and complexity, particularly in the presence of CSI uncertainties.",
        "comments": "accepted at AAAI2024",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11409"
    },
    {
        "doc_id": 368,
        "title": "Application of a Novel Model Reduction Technique to the Assessment of Boundedness/Stability of Some Delay Time-Varying Vector Nonlinear Systems",
        "authors": [
            "Mark A. Pinsky"
        ],
        "subjects": [
            "Systems and Control",
            "Differential Geometry"
        ],
        "abstract": "This paper develops a new approach to the assessment of the boundedness/stability of some vector nonlinear systems with delays and variable coefficients. The approach rests on the development of scalar counterparts to the original vector systems. We show that the solutions to these scalar auxiliary nonlinear equations with delay and variable coefficients bound from the above the norms of solutions to the original equations with the matched history functions. This prompts the assessment of the boundedness/stability traits of the vector systems through the abridged evaluation of the dynamics of their scalar counterparts. The latter task is achieved in effortless simulations or through the application of simplified analytical inferences. Consequently, we convey some novel boundedness/ stability criteria and estimate the radiuses of the balls imbedded in the boundedness/stability regions. Lastly, we authenticate our inferences in representative simulations that also measure their accuracy.",
        "comments": "17 pages",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11398"
    },
    {
        "doc_id": 369,
        "title": "Joint User Scheduling and Computing Resource Allocation Optimization in Asynchronous Mobile Edge Computing Networks",
        "authors": [
            "Yihan Cang",
            "Ming Chen",
            "Yijin Pan",
            "Zhaohui Yang",
            "Ye Hu",
            "Haijian Sun",
            "Mingzhe Chen"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "In this paper, the problem of joint user scheduling and computing resource allocation in asynchronous mobile edge computing (MEC) networks is studied. In such networks, edge devices will offload their computational tasks to an MEC server, using the energy they harvest from this server. To get their tasks processed on time using the harvested energy, edge devices will strategically schedule their task offloading, and compete for the computational resource at the MEC server. Then, the MEC server will execute these tasks asynchronously based on the arrival of the tasks. This joint user scheduling, time and computation resource allocation problem is posed as an optimization framework whose goal is to find the optimal scheduling and allocation strategy that minimizes the energy consumption of these mobile computing tasks. To solve this mixed-integer non-linear programming problem, the general benders decomposition method is adopted which decomposes the original problem into a primal problem and a master problem. Specifically, the primal problem is related to computation resource and time slot allocation, of which the optimal closed-form solution is obtained. The master problem regarding discrete user scheduling variables is constructed by adding optimality cuts or feasibility cuts according to whether the primal problem is feasible, which is a standard mixed-integer linear programming problem and can be efficiently solved. By iteratively solving the primal problem and master problem, the optimal scheduling and resource allocation scheme is obtained. Simulation results demonstrate that the proposed asynchronous computing framework reduces 87.17% energy consumption compared with conventional synchronous computing counterpart.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11377"
    },
    {
        "doc_id": 370,
        "title": "Self-supervised Contrastive Learning for 6G UM-MIMO THz Communications: Improving Robustness Under Imperfect CSI",
        "authors": [
            "Rafid Umayer Murshed",
            "Md Saheed Ullah",
            "Mohammad Saquib",
            "Moe Z. Win"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "This paper investigates the potential of contrastive learning in 6G ultra-massive multiple-input multiple-output (UM-MIMO) communication systems, specifically focusing on hybrid beamforming under imperfect channel state information (CSI) conditions at THz. UM-MIMO systems are promising for future 6G wireless communication networks due to their high spectral efficiency and capacity. The accuracy of CSI significantly influences the performance of UM-MIMO systems. However, acquiring perfect CSI is challenging due to various practical constraints such as channel estimation errors, feedback delays, and hardware imperfections. To address this issue, we propose a novel self-supervised contrastive learning-based approach for hybrid beamforming, which is robust against imperfect CSI. We demonstrate the power of contrastive learning to tackle the challenges posed by imperfect CSI and show that our proposed method results in improved system performance in terms of achievable rate compared to traditional methods.",
        "comments": "6 pages, 7 figures, Submitted to IEEE International Conference on Communications, 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11376"
    },
    {
        "doc_id": 371,
        "title": "Modeling Considerations for Developing Deep Space Autonomous Spacecraft and Simulators",
        "authors": [
            "Christopher Agia",
            "Guillem Casadesus Vila",
            "Saptarshi Bandyopadhyay",
            "David S. Bayard",
            "Kar-Ming Cheung",
            "Charles H. Lee",
            "Eric Wood",
            "Ian Aenishanslin",
            "Steven Ardito",
            "Lorraine Fesq",
            "Marco Pavone",
            "Issa A. D. Nesnas"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "To extend the limited scope of autonomy used in prior missions for operation in distant and complex environments, there is a need to further develop and mature autonomy that jointly reasons over multiple subsystems, which we term system-level autonomy. System-level autonomy establishes situational awareness that resolves conflicting information across subsystems, which may necessitate the refinement and interconnection of the underlying spacecraft and environment onboard models. However, with a limited understanding of the assumptions and tradeoffs of modeling to arbitrary extents, designing onboard models to support system-level capabilities presents a significant challenge.\n  In this paper, we provide a detailed analysis of the increasing levels of model fidelity for several key spacecraft subsystems, with the goal of informing future spacecraft functional- and system-level autonomy algorithms and the physics-based simulators on which they are validated. We do not argue for the adoption of a particular fidelity class of models but, instead, highlight the potential tradeoffs and opportunities associated with the use of models for onboard autonomy and in physics-based simulators at various fidelity levels. We ground our analysis in the context of deep space exploration of small bodies, an emerging frontier for autonomous spacecraft operation in space, where the choice of models employed onboard the spacecraft may determine mission success. We conduct our experiments in the Multi-Spacecraft Concept and Autonomy Tool (MuSCAT), a software suite for developing spacecraft autonomy algorithms.",
        "comments": "Project page: https://sites.google.com/stanford.edu/spacecraft-models. 20 pages, 8 figures. Accepted to the IEEE Conference on Aerospace (AeroConf) 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11371"
    },
    {
        "doc_id": 372,
        "title": "Self-sustaining Software Systems (S4): Towards Improved Interpretability and Adaptation",
        "authors": [
            "Christian Cabrera",
            "Andrei Paleyes",
            "Neil D. Lawrence"
        ],
        "subjects": [
            "Software Engineering",
            "Artificial Intelligence",
            "Systems and Control"
        ],
        "abstract": "Software systems impact society at different levels as they pervasively solve real-world problems. Modern software systems are often so sophisticated that their complexity exceeds the limits of human comprehension. These systems must respond to changing goals, dynamic data, unexpected failures, and security threats, among other variable factors in real-world environments. Systems' complexity challenges their interpretability and requires autonomous responses to dynamic changes. Two main research areas explore autonomous systems' responses: evolutionary computing and autonomic computing. Evolutionary computing focuses on software improvement based on iterative modifications to the source code. Autonomic computing focuses on optimising systems' performance by changing their structure, behaviour, or environment variables. Approaches from both areas rely on feedback loops that accumulate knowledge from the system interactions to inform autonomous decision-making. However, this knowledge is often limited, constraining the systems' interpretability and adaptability. This paper proposes a new concept for interpretable and adaptable software systems: self-sustaining software systems (S4). S4 builds knowledge loops between all available knowledge sources that define modern software systems to improve their interpretability and adaptability. This paper introduces and discusses the S4 concept.",
        "comments": "Accepted at The 1st International Workshop New Trends in Software Architecture (SATrends) 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11370"
    },
    {
        "doc_id": 373,
        "title": "A Fast Effective Greedy Approach for MU-MIMO Beam Selection in mm-Wave and THz Communications",
        "authors": [
            "Rafid Umayer Murshed",
            "Md Saheed Ullah",
            "Mohammad Saquib"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "This paper addresses the beam-selection challenges in Multi-User Multiple Input Multiple Output (MU-MIMO) beamforming for mm-wave and THz channels, focusing on the pivotal aspect of spectral efficiency (SE) and computational efficiency. We introduce a novel approach, the Greedy Interference-Optimized Singular Vector Beam-selection (G-IOSVB) algorithm, which offers a strategic balance between high SE and low computational complexity. Our study embarks on a comparative analysis of G-IOSVB against the traditional IOSVB and the exhaustive Singular-Vector Beamspace Search (SVBS) algorithms. The findings reveal that while SVBS achieves the highest SE, it incurs significant computational costs, approximately 162 seconds per channel realization. In contrast, G-IOSVB aligns closely with IOSVB in SE performance yet is markedly more computationally efficient. Heatmaps vividly demonstrate this efficiency, highlighting G-IOSVB's reduced computation time without sacrificing SE. We also delve into the mathematical intricacies of G-IOSVB, demonstrating its theoretical and practical superiority through rigorous expressions and detailed algorithmic analysis. The numerical results illustrate that G-IOSVB stands out as an efficient, practical solution for MU-MIMO systems, making it a promising candidate for high-speed, high-efficiency wireless communication networks.",
        "comments": "Accepted for Lecture presentation at the 58th Annual Conference on Information Sciences and Systems, to be held at Princeton University from March 13-15, 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11369"
    },
    {
        "doc_id": 374,
        "title": "Asynchronous Parallel Reinforcement Learning for Optimizing Propulsive Performance in Fin Ray Control",
        "authors": [
            "Xin-Yang Liu",
            "Dariush Bodaghi",
            "Qian Xue",
            "Xudong Zheng",
            "Jian-Xun Wang"
        ],
        "subjects": [
            "Fluid Dynamics",
            "Machine Learning",
            "Systems and Control"
        ],
        "abstract": "Fish fin rays constitute a sophisticated control system for ray-finned fish, facilitating versatile locomotion within complex fluid environments. Despite extensive research on the kinematics and hydrodynamics of fish locomotion, the intricate control strategies in fin-ray actuation remain largely unexplored. While deep reinforcement learning (DRL) has demonstrated potential in managing complex nonlinear dynamics; its trial-and-error nature limits its application to problems involving computationally demanding environmental interactions. This study introduces a cutting-edge off-policy DRL algorithm, interacting with a fluid-structure interaction (FSI) environment to acquire intricate fin-ray control strategies tailored for various propulsive performance objectives. To enhance training efficiency and enable scalable parallelism, an innovative asynchronous parallel training (APT) strategy is proposed, which fully decouples FSI environment interactions and policy/value network optimization. The results demonstrated the success of the proposed method in discovering optimal complex policies for fin-ray actuation control, resulting in a superior propulsive performance compared to the optimal sinusoidal actuation function identified through a parametric grid search. The merit and effectiveness of the APT approach are also showcased through comprehensive comparison with conventional DRL training strategies in numerical experiments of controlling nonlinear dynamics.",
        "comments": "37 pages, 12 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11349"
    },
    {
        "doc_id": 375,
        "title": "Decentralized Optimization in Networks with Arbitrary Delays",
        "authors": [
            "Tomas Ortega",
            "Hamid Jafarkhani"
        ],
        "subjects": [
            "Optimization and Control",
            "Multiagent Systems",
            "Signal Processing",
            "Systems and Control"
        ],
        "abstract": "We consider the problem of decentralized optimization in networks with communication delays. To accommodate delays, we need decentralized optimization algorithms that work on directed graphs. Existing approaches require nodes to know their out-degree to achieve convergence. We propose a novel gossip-based algorithm that circumvents this requirement, allowing decentralized optimization in networks with communication delays. We prove that our algorithm converges on non-convex objectives, with the same main complexity order term as centralized Stochastic Gradient Descent (SGD), and show that the graph topology and the delays only affect the higher order terms. We provide numerical simulations that illustrate our theoretical results.",
        "comments": "Accepted to IEEE ICC 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11344"
    },
    {
        "doc_id": 376,
        "title": "Error bounds of constant gain least-mean-squares algorithms",
        "authors": [
            "Chang Liu",
            "Antwan D. Clark"
        ],
        "subjects": [
            "Signal Processing",
            "Systems and Control"
        ],
        "abstract": "Constant gain least-mean-squares (LMS) algorithms have a wide range of applications in trajectory tracking problems, but the formal convergence of LMS in mean square is not yet fully established. This work provides an upper bound on the constant gain that guarantees a bounded mean-squared error of LMS for a general design vector. These results highlight the role of the fourth-order moment of the design vector. Numerical examples demonstrate the applicability of this upper bound in setting a constant gain in LMS, while existing criteria may fail. We also provide the associated error bound, which can be applied to design vectors with linearly dependent elements.",
        "comments": "6 pages",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11333"
    },
    {
        "doc_id": 377,
        "title": "A Hierarchical Decision-Based Maintenance for a Complex Modular System Driven by the { MoMA} Algorithm",
        "authors": [
            "M. L. Gamiz",
            "D. Montoro-Cazorla",
            "M. C. Segovia-Garcia"
        ],
        "subjects": [
            "Systems and Control",
            "Methodology"
        ],
        "abstract": "This paper presents a maintenance policy for a modular system formed by K independent modules (n-subsystems) subjected to environmental conditions (shocks). For the modeling of this complex system, the use of the Matrix-Analytical Method (MAM) is proposed under a layered approach according to its hierarchical structure. Thus, the operational state of the system (top layer) depends on the states of the modules (middle layer), which in turn depend on the states of their components (bottom layer). This allows a detailed description of the system operation to plan maintenance actions appropriately and optimally. We propose a hierarchical decision-based maintenance strategy with periodic inspections as follows: at the time of the inspection, the condition of the system is first evaluated. If intervention is necessary, the modules are then checked to make individual decisions based on their states, and so on. Replacement or repair will be carried out as appropriate. An optimization problem is formulated as a function of the length of the inspection period and the intervention cost incurred over the useful life of the system. Our method shows the advantages, providing compact and implementable expressions. The model is illustrated on a submarine Electrical Control Unit (ECU).",
        "comments": "43 pages, 6 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11328"
    },
    {
        "doc_id": 378,
        "title": "Weakly-Supervised Semantic Segmentation of Circular-Scan, Synthetic-Aperture-Sonar Imagery",
        "authors": [
            "Isaac J. Sledge",
            "Dominic M. Byrne",
            "Jonathan L. King",
            "Steven H. Ostertag",
            "Denton L. Woods",
            "James L. Prater",
            "Jermaine L. Kennedy",
            "Timothy M. Marston",
            "Jose C. Principe"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning",
            "Image and Video Processing"
        ],
        "abstract": "We propose a weakly-supervised framework for the semantic segmentation of circular-scan synthetic-aperture-sonar (CSAS) imagery. The first part of our framework is trained in a supervised manner, on image-level labels, to uncover a set of semi-sparse, spatially-discriminative regions in each image. The classification uncertainty of each region is then evaluated. Those areas with the lowest uncertainties are then chosen to be weakly labeled segmentation seeds, at the pixel level, for the second part of the framework. Each of the seed extents are progressively resized according to an unsupervised, information-theoretic loss with structured-prediction regularizers. This reshaping process uses multi-scale, adaptively-weighted features to delineate class-specific transitions in local image content. Content-addressable memories are inserted at various parts of our framework so that it can leverage features from previously seen images to improve segmentation performance for related images.\n  We evaluate our weakly-supervised framework using real-world CSAS imagery that contains over ten seafloor classes and ten target classes. We show that our framework performs comparably to nine fully-supervised deep networks. Our framework also outperforms eleven of the best weakly-supervised deep networks. We achieve state-of-the-art performance when pre-training on natural imagery. The average absolute performance gap to the next-best weakly-supervised network is well over ten percent for both natural imagery and sonar imagery. This gap is found to be statistically significant.",
        "comments": "Submitted to the IEEE Journal of Oceanic Engineering",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11313"
    },
    {
        "doc_id": 379,
        "title": "RoTIR: Rotation-Equivariant Network and Transformers for Fish Scale Image Registration",
        "authors": [
            "Ruixiong Wang",
            "Alin Achim",
            "Renata Raele-Rolfe",
            "Qiao Tong",
            "Dylan Bergen",
            "Chrissy Hammond",
            "Stephen Cross"
        ],
        "subjects": [
            "Image and Video Processing"
        ],
        "abstract": "Image registration is an essential process for aligning features of interest from multiple images. With the recent development of deep learning techniques, image registration approaches have advanced to a new level. In this work, we present 'Rotation-Equivariant network and Transformers for Image Registration' (RoTIR), a deep-learning-based method for the alignment of fish scale images captured by light microscopy. This approach overcomes the challenge of arbitrary rotation and translation detection, as well as the absence of ground truth data. We employ feature-matching approaches based on Transformers and general E(2)-equivariant steerable CNNs for model creation. Besides, an artificial training dataset is employed for semi-supervised learning. Results show RoTIR successfully achieves the goal of fish scale image registration.",
        "comments": "7 pages, 4 figures, 2 tables",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11270"
    },
    {
        "doc_id": 380,
        "title": "Word-Level ASR Quality Estimation for Efficient Corpus Sampling and Post-Editing through Analyzing Attentions of a Reference-Free Metric",
        "authors": [
            "Golara Javadi",
            "Kamer Ali Yuksel",
            "Yunsu Kim",
            "Thiago Castro Ferreira",
            "Mohamed Al-Badrashiny"
        ],
        "subjects": [
            "Computation and Language",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "In the realm of automatic speech recognition (ASR), the quest for models that not only perform with high accuracy but also offer transparency in their decision-making processes is crucial. The potential of quality estimation (QE) metrics is introduced and evaluated as a novel tool to enhance explainable artificial intelligence (XAI) in ASR systems. Through experiments and analyses, the capabilities of the NoRefER (No Reference Error Rate) metric are explored in identifying word-level errors to aid post-editors in refining ASR hypotheses. The investigation also extends to the utility of NoRefER in the corpus-building process, demonstrating its effectiveness in augmenting datasets with insightful annotations. The diagnostic aspects of NoRefER are examined, revealing its ability to provide valuable insights into model behaviors and decision patterns. This has proven beneficial for prioritizing hypotheses in post-editing workflows and fine-tuning ASR models. The findings suggest that NoRefER is not merely a tool for error detection but also a comprehensive framework for enhancing ASR systems' transparency, efficiency, and effectiveness. To ensure the reproducibility of the results, all source codes of this study are made publicly available.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11268"
    },
    {
        "doc_id": 381,
        "title": "AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking",
        "authors": [
            "Mehmet Y. Turali",
            "Mehmet E. Lorasdagi",
            "Ali T. Koc",
            "Suleyman S. Kozat"
        ],
        "subjects": [
            "Machine Learning",
            "Signal Processing",
            "Machine Learning"
        ],
        "abstract": "We study the problem of feature selection in general machine learning (ML) context, which is one of the most critical subjects in the field. Although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. To this end, we introduce the ``Adaptive Feature Selection with Binary Masking\" (AFS-BM) which remedies these problems. AFS-BM achieves this by joint optimization for simultaneous feature selection and model training. In particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. This approach leads to significant improvements in model accuracy and a reduction in computational requirements. We provide an extensive set of experiments where we compare AFS-BM with the established feature selection methods using well-known datasets from real-life competitions. Our results show that AFS-BM makes significant improvement in terms of accuracy and requires significantly less computational complexity. This is due to AFS-BM's ability to dynamically adjust to the changing importance of features during the training process, which an important contribution to the field. We openly share our code for the replicability of our results and to facilitate further research.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11250"
    },
    {
        "doc_id": 382,
        "title": "Hierarchical Cell-Free Massive MIMO for High Capacity with Simple Implementation",
        "authors": [
            "Wei Jiang",
            "Hans D. Schotten"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "Cell-free massive multi-input multi-output (MIMO) has recently gained much attention for its potential in shaping the landscape of sixth-generation (6G) wireless systems. This paper proposes a hierarchical network architecture tailored for cell-free massive MIMO, seamlessly integrating co-located and distributed antennas. A central base station (CBS), equipped with an antenna array, positions itself near the center of the coverage area, complemented by distributed access points spanning the periphery. The proposed architecture remarkably outperforms conventional cell-free networks, demonstrating superior sum throughput while maintaining a comparable worst-case per-user spectral efficiency. Meanwhile, the implementation cost associated with the fronthaul network is substantially diminished.",
        "comments": "2024 IEEE International Conference on Communications (ICC-2024)",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11236"
    },
    {
        "doc_id": 383,
        "title": "Susceptibility of Adversarial Attack on Medical Image Segmentation Models",
        "authors": [
            "Zhongxuan Wang",
            "Leo Xu"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "The nature of deep neural networks has given rise to a variety of attacks, but little work has been done to address the effect of adversarial attacks on segmentation models trained on MRI datasets. In light of the grave consequences that such attacks could cause, we explore four models from the U-Net family and examine their responses to the Fast Gradient Sign Method (FGSM) attack. We conduct FGSM attacks on each of them and experiment with various schemes to conduct the attacks. In this paper, we find that medical imaging segmentation models are indeed vulnerable to adversarial attacks and that there is a negligible correlation between parameter size and adversarial attack success. Furthermore, we show that using a different loss function than the one used for training yields higher adversarial attack success, contrary to what the FGSM authors suggested. In future efforts, we will conduct the experiments detailed in this paper with more segmentation models and different attacks. We will also attempt to find ways to counteract the attacks by using model ensembles or special data augmentations. Our code is available at https://github.com/ZhongxuanWang/adv_attk",
        "comments": "6 pages, 8 figures, presented at 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI) conference",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11224"
    },
    {
        "doc_id": 384,
        "title": "3D Receiver for Molecular Communications in Internet of Organoids",
        "authors": [
            "Shaojie Zhang",
            "Ozgur B. Akan"
        ],
        "subjects": [
            "Emerging Technologies",
            "Systems and Control"
        ],
        "abstract": "Organoids have garnered attention due to their effectiveness in modeling the 3D structure of organ interactions. However, the communication engineering perspective has received relatively little attention. One way to achieve organoids communication is molecular communication (MC). Molecular communication is a bio-inspired communication paradigm that uses molecules as information carriers. It is considered one of the most promising methods for enabling the Internet of Nano-Things (IoNT) and nanonetworks. BioFETs are commonly used to implement practical MC receivers. However, most previous analyses have focused on a planar device, neglecting considerations like the threshold voltage and its potential 3D structure. This paper introduces the first FinFET-based MC receiver that covers both the top and side gates with receptors. Both binding noise and flicker noise are considered in the analysis. The performance, in terms of signal-to-noise ratio (SNR) and symbol error probability (SEP), is compared with that of the 2D receiver.",
        "comments": "10 pages, 11 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11214"
    },
    {
        "doc_id": 385,
        "title": "Joint Beamforming Optimization and Mode Selection for RDARS-aided MIMO Systems",
        "authors": [
            "Jintao Wang",
            "Chengzhi Ma",
            "Shiqi Gong",
            "Xi Yang",
            "Shaodan Ma"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "Considering the appealing distribution gains of distributed antenna systems (DAS) and passive gains of reconfigurable intelligent surface (RIS), a flexible reconfigurable architecture called reconfigurable distributed antenna and reflecting surface (RDARS) is proposed. RDARS encompasses DAS and RIS as two special cases and maintains the advantages of distributed antennas while reducing the hardware cost by replacing some active antennas with low-cost passive reflecting surfaces. In this paper, we present a RDARS-aided uplink multi-user communication system and investigate the system transmission reliability with the newly proposed architecture. Specifically, in addition to the distribution gain and the reflection gain provided by the connection and reflection modes, respectively, we also consider the dynamic mode switching of each element which introduces an additional degree of freedom (DoF) and thus results in a selection gain. As such, we aim to minimize the total sum mean-square-error (MSE) of all data streams by jointly optimizing the receive beamforming matrix, the reflection phase shifts and the channel-aware placement of elements in the connection mode. To tackle this nonconvex problem with intractable binary and cardinality constraints, we propose an inexact block coordinate descent (BCD) based penalty dual decomposition (PDD) algorithm with the guaranteed convergence. Since the PDD algorithm usually suffers from high computational complexity, a low-complexity greedy-search-based alternating optimization (AO) algorithm is developed to yield a semi-closed-form solution with acceptable performance. Numerical results demonstrate the superiority of the proposed architecture compared to the conventional fully passive RIS or DAS. Furthermore, some insights about the practical implementation of RDARS are provided.",
        "comments": "13 pages, 9 figures. This paper has been submitted to IEEE journal for possible publication",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11205"
    },
    {
        "doc_id": 386,
        "title": "Transversally exponentially stable Euclidean space extension technique for discrete time systems",
        "authors": [
            "Soham Shanbhag",
            "Dong Eui Chang"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "We propose a modification technique for discrete time systems for exponentially fast convergence to compact sets. The extension technique allows us to use tools defined on Euclidean spaces to systems evolving on manifolds by modifying the dynamics of the system such that the manifold is an attractor set. We show the stability properties of this technique using the simulation of the rigid body rotation system on the unit sphere $S^3$. We also show the improvement afforded due to this technique on a Luenberger like observer designed for the rigid body rotation system on $S^3$.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11200"
    },
    {
        "doc_id": 387,
        "title": "Projected Belief Networks With Discriminative Alignment for Acoustic Event Classification: Rivaling State of the Art CNNs",
        "authors": [
            "Paul M. Baggenstoss",
            "Kevin Wilkinghoff",
            "Felix Govaers",
            "Frank Kurth"
        ],
        "subjects": [
            "Machine Learning",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "The projected belief network (PBN) is a generative stochastic network with tractable likelihood function based on a feed-forward neural network (FFNN). The generative function operates by \"backing up\" through the FFNN. The PBN is two networks in one, a FFNN that operates in the forward direction, and a generative network that operates in the backward direction. Both networks co-exist based on the same parameter set, have their own cost functions, and can be separately or jointly trained. The PBN therefore has the potential to possess the best qualities of both discriminative and generative classifiers. To realize this potential, a separate PBN is trained on each class, maximizing the generative likelihood function for the given class, while minimizing the discriminative cost for the FFNN against \"all other classes\". This technique, called discriminative alignment (PBN-DA), aligns the contours of the likelihood function to the decision boundaries and attains vastly improved classification performance, rivaling that of state of the art discriminative networks. The method may be further improved using a hidden Markov model (HMM) as a component of the PBN, called PBN-DA-HMM. This paper provides a comprehensive treatment of PBN, PBN-DA, and PBN-DA-HMM. In addition, the results of two new classification experiments are provided. The first experiment uses air-acoustic events, and the second uses underwater acoustic data consisting of marine mammal calls. In both experiments, PBN-DA-HMM attains comparable or better performance as a state of the art CNN, and attain a factor of two error reduction when combined with the CNN.",
        "comments": "15 Pages. Submitted to IEEE-TNNLS",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11199"
    },
    {
        "doc_id": 388,
        "title": "Machine learning based state observer for discrete time systems evolving on Lie groups",
        "authors": [
            "Soham Shanbhag",
            "Dong Eui Chang"
        ],
        "subjects": [
            "Systems and Control",
            "Machine Learning"
        ],
        "abstract": "In this paper, a machine learning based observer for systems evolving on manifolds is designed such that the state of the observer is restricted to the Lie group on which the system evolves. Conventional techniques involving machine learning based observers on systems evolving on Lie groups involve designing charts for the Lie group, training a machine learning based observer for each chart, and switching between the trained models based on the state of the system. We propose a novel deep learning based technique whose predictions are restricted to a measure 0 subset of Euclidean space without using charts. Using this network, we design an observer ensuring that the state of the observer is restricted to the Lie group, and predicting the state using only one trained algorithm. The deep learning network predicts an ``error term'' on the Lie algebra of the Lie group, uses the map from the Lie algebra to the group, and uses the group action and the present state to estimate the state at the next epoch. This model being purely data driven does not require the model of the system. The proposed algorithm provides a novel framework for constraining the output of machine learning networks to a measure 0 subset of a Euclidean space without chart specific training and without requiring switching. We show the validity of this method using Monte Carlo simulations performed of the rigid body rotation and translation system.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11196"
    },
    {
        "doc_id": 389,
        "title": "Triple-Refined Hybrid-Field Beam Training for mmWave Extremely Large-Scale MIMO",
        "authors": [
            "Kangjian Chen",
            "Chenhao Qi",
            "Octavia A. Dobre",
            "Geoffrey Ye Li"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "This paper investigates beam training for extremely large-scale multiple-input multiple-output systems. By considering both the near field and far field, a triple-refined hybrid-field beam training scheme is proposed, where high-accuracy estimates of channel parameters are obtained through three steps of progressive beam refinement. First, the hybrid-field beam gain (HFBG)-based first refinement method is developed. Based on the analysis of the HFBG, the first-refinement codebook is designed and the beam training is performed accordingly to narrow down the potential region of the channel path. Then, the maximum likelihood (ML)-based and principle of stationary phase (PSP)-based second refinement methods are developed. By exploiting the measurements of the beam training, the ML is used to estimate the channel parameters. To avoid the high computational complexity of ML, closed-form estimates of the channel parameters are derived according to the PSP. Moreover, the Gaussian approximation (GA)-based third refinement method is developed. The hybrid-field neighboring search is first performed to identify the potential region of the main lobe of the channel steering vector. Afterwards, by applying the GA, a least-squares estimator is developed to obtain the high-accuracy channel parameter estimation. Simulation results verify the effectiveness of the proposed scheme.",
        "comments": "Journal ref:        IEEE Transactions on Wireless Communications, 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11195"
    },
    {
        "doc_id": 390,
        "title": "Angular velocity and linear acceleration measurement bias estimators for the rigid body system with global exponential convergence",
        "authors": [
            "Soham Shanbhag",
            "Dong Eui Chang"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "Rigid body systems usually consider measurements of the pose of the body using onboard cameras/LiDAR systems, that of linear acceleration using an accelerometer and of angular velocity using an IMU. However, the measurements of the linear acceleration and angular velocity are usually biased with an unknown constant or slowly varying bias. We propose a measurement bias estimator for such systems under assumption of boundedness of angular velocity. We also provide continuous estimates to the state of the system, i.e. the pose, linear velocity, and position of the body. These estimates are globally exponentially convergent to the state of the rigid body system. We propose two bias estimators designed with the estimate of the pose in the ambient Euclidean space of the Special Euclidean group and show global exponential convergence of the proposed observers to the state of the system. The first observer assumes knowledge of bounds of the angular velocity, while the second observer uses a Riccati observer to overcome this limitation. We show the convergence with an example of a rigid body rotation and translation system on the special Euclidean group. We show that the observer is able to estimate the bias using data collected from an Intel Realsense camera.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11191"
    },
    {
        "doc_id": 391,
        "title": "Globally exponentially convergent observer for systems evolving on matrix Lie groups",
        "authors": [
            "Soham Shanbhag",
            "Dong Eui Chang"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "We propose a globally exponentially convergent observer for the dynamical system evolving on matrix Lie groups with bounded velocity with unknown bound. We design the observer in the ambient Euclidean space and show exponential convergence of the observer to the state of the system. We show the convergence with an example of a rigid body rotation and translation system on the special Euclidean group. We compare the proposed observer with an observer present in the literature.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11189"
    },
    {
        "doc_id": 392,
        "title": "Predictive stability filters for nonlinear dynamical systems affected by disturbances",
        "authors": [
            "Alexandre Didier",
            "Andrea Zanelli",
            "Kim P. Wabersich",
            "Melanie N. Zeilinger"
        ],
        "subjects": [
            "Systems and Control",
            "Optimization and Control"
        ],
        "abstract": "Predictive safety filters provide a way of projecting potentially unsafe inputs onto the set of inputs that guarantee recursive state and input constraint satisfaction. Unsafe inputs, proposed, e.g. by a human or learning-based controller, can thereby be filtered by leveraging model predictive control techniques. In this paper, we extend this framework such that in addition, robust asymptotic stability of the closed-loop system can be guaranteed by enforcing a decrease of an implicit Lyapunov function which is constructed using a predicted system trajectory. Differently from previous results, we show robust asymptotic stability on an extended state consisting of the system state and a warmstart input sequence and establish robust asymptotic stability for the augmented dynamics with respect to a predefined disturbance. The proposed strategy is applied to an automotive lane keeping example in simulation.",
        "comments": "Submitted to NMPC'24",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11183"
    },
    {
        "doc_id": 393,
        "title": "Data-Driven Target Localization: Benchmarking Gradient Descent Using the Cram\u00e9r-Rao Bound",
        "authors": [
            "Shyam Venkatasubramanian",
            "Sandeep Gogineni",
            "Bosung Kang",
            "Ali Pezeshki",
            "Muralidhar Rangaswamy",
            "Vahid Tarokh"
        ],
        "subjects": [
            "Signal Processing",
            "Machine Learning"
        ],
        "abstract": "In modern radar systems, precise target localization using azimuth and velocity estimation is paramount. Traditional unbiased estimation methods have leveraged gradient descent algorithms to reach the theoretical limits of the Cram\u00e9r Rao Bound (CRB) for the error of the parameter estimates. In this study, we present a data-driven neural network approach that outperforms these traditional techniques, demonstrating improved accuracies in target azimuth and velocity estimation. Using a representative simulated scenario, we show that our proposed neural network model consistently achieves improved parameter estimates due to its inherently biased nature, yielding a diminished mean squared error (MSE). Our findings underscore the potential of employing deep learning methods in radar systems, paving the way for more accurate localization in cluttered and dynamic environments.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11176"
    },
    {
        "doc_id": 394,
        "title": "Generalizing Speaker Verification for Spoof Awareness in the Embedding Space",
        "authors": [
            "Xuechen Liu",
            "Md Sahidullah",
            "Kong Aik Lee",
            "Tomi Kinnunen"
        ],
        "subjects": [
            "Cryptography and Security",
            "Artificial Intelligence",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "It is now well-known that automatic speaker verification (ASV) systems can be spoofed using various types of adversaries. The usual approach to counteract ASV systems against such attacks is to develop a separate spoofing countermeasure (CM) module to classify speech input either as a bonafide, or a spoofed utterance. Nevertheless, such a design requires additional computation and utilization efforts at the authentication stage. An alternative strategy involves a single monolithic ASV system designed to handle both zero-effort imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have the potential to provide stronger protections and more economic computations. To this end, we propose to generalize the standalone ASV (G-SASV) against spoofing attacks, where we leverage limited training data from CM to enhance a simple backend in the embedding space, without the involvement of a separate CM module during the test (authentication) phase. We propose a novel yet simple backend classifier based on deep neural networks and conduct the study via domain adaptation and multi-task integration of spoof embeddings at the training stage. Experiments are conducted on the ASVspoof 2019 logical access dataset, where we improve the performance of statistical ASV backends on the joint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and 49.8% in terms of equal error rates, respectively.",
        "comments": "To appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11156"
    },
    {
        "doc_id": 395,
        "title": "Enhancing System-Level Safety in Mixed-Autonomy Platoon via Safe Reinforcement Learning",
        "authors": [
            "Jingyuan Zhou",
            "Longhao Yan",
            "Kaidi Yang"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "Connected and automated vehicles (CAVs) have recently gained prominence in traffic research, thanks to the advancements in communication technology and autonomous driving. A variety of longitudinal control strategies for CAVs have been developed to enhance traffic efficiency, stability, and safety in mixed-autonomy scenarios. Deep reinforcement learning (DRL) is one promising strategy for mixed-autonomy platoon control since it can tackle complex scenarios in real-time. However, there are three research gaps for DRL-based mixed-autonomy platoon control. First, incorporating safety considerations into DRL typically relies on designing collision avoidance-based reward functions, which lack collision-free guarantees. Second, current DRL-based-control approaches for mixed traffic only consider the safety of CAVs, with little attention paid to the surrounding HDVs. To address the research gaps, we introduce a differentiable safety layer that converts DRL actions into safe actions with collision-free guarantees. This process relies on solving a differentiable quadratic programming problem that incorporates control barrier function-based (CBF) safety constraints for both CAV and its following HDVs to achieve system-level safety. Moreover, constructing CBF constraints needs system dynamics for the following HDVs, and thus we employ an online system identification module to estimate the car-following dynamics of the surrounding HDVs. The proposed safe reinforcement learning approach explicitly integrates system-level safety constraints into the training process and enables our method to adapt to varying safety-critical scenarios. Simulation results demonstrate that our proposed method effectively ensures CAV safety and improves HDV safety in mixed platoon environments while simultaneously enhancing traffic capacity and string stability.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11148"
    },
    {
        "doc_id": 396,
        "title": "Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities",
        "authors": [
            "Georgios Ioannides",
            "Aman Chadha",
            "Aaron Elkins"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Sound",
            "Audio and Speech Processing",
            "Signal Processing"
        ],
        "abstract": "We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy across a diverse range of tasks, including emotion recognition in speech, image classification, and text classification, thereby establishing its robustness and versatility in handling multi-modal data. Furthermore, we introduce the Importance Factor (IF), a new learning-based metric that enhances the explainability of models trained with GAAM-based methods. Overall, GAAM represents an advancement towards development of better performing and more explainable attention models across multiple modalities.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11143"
    },
    {
        "doc_id": 397,
        "title": "Wideband Beamforming for RIS Assisted Near-Field Communications",
        "authors": [
            "Ji Wang",
            "Jian Xiao",
            "Yixuan Zou",
            "Wenwu Xie",
            "Yuanwei Liu"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "A near-field wideband beamforming scheme is investigated for reconfigurable intelligent surface (RIS) assisted multiple-input multiple-output (MIMO) systems, in which a deep learning-based end-to-end (E2E) optimization framework is proposed to maximize the system spectral efficiency. To deal with the near-field double beam split effect, the base station is equipped with frequency-dependent hybrid precoding architecture by introducing sub-connected true time delay (TTD) units, while two specific RIS architectures, namely true time delay-based RIS (TTD-RIS) and virtual subarray-based RIS (SA-RIS), are exploited to realize the frequency-dependent passive beamforming at the RIS. Furthermore, the efficient E2E beamforming models without explicit channel state information are proposed, which jointly exploits the uplink channel training module and the downlink wideband beamforming module. In the proposed network architecture of the E2E models, the classical communication signal processing methods, i.e., polarized filtering and sparsity transform, are leveraged to develop a signal-guided beamforming network. Numerical results show that the proposed E2E models have superior beamforming performance and robustness to conventional beamforming benchmarks. Furthermore, the tradeoff between the beamforming gain and the hardware complexity is investigated for different frequency-dependent RIS architectures, in which the TTD-RIS can achieve better spectral efficiency than the SA-RIS while requiring additional energy consumption and hardware cost.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11141"
    },
    {
        "doc_id": 398,
        "title": "Reconfigurable Intelligent Surface-Enabled Array Radar for Interference Mitigation",
        "authors": [
            "Shengyao Chen",
            "Qi Feng",
            "Longyao Ran",
            "Feng Xi",
            "Zhong Liu"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "Conventional active array radars often jointly design the transmit and receive beamforming for effectively suppressing interferences. To further promote the interference suppression performance, this paper introduces a reconfigurable intelligent surface (RIS) to assist the radar receiver because the RIS has the ability to bring plentiful additional degrees-of-freedom. To maximize the output signal-to-interference-plus-noise ratio (SINR) of receive array, we formulate the codesign of transmit beamforming and RIS-aided receive beamforming into a nonconvex constrained fractional programming problem, and then propose an alternating minimization-based algorithm to jointly optimize the transmit beamformer, receive beamformer and RIS reflection coefficients. Specifically, we offer the closed-form optimal solutions of transmit and receive beamformers according to the minimum variance distortionless response principle, and translate the RIS reflection coefficients design into a series of unimodular quadratic programming (UQP) subproblems by employing the Dinkelbach transform. To tackle the UQP subproblems efficiently, we propose a second-order Riemannian Newton method (RNM) with improved Riemannian Newton direction, which avoids the line search and has better convergence speed than typical first-order Riemannian manifold optimization methods. Moreover, we derive the convergence of the proposed codesign algorithm by deducing the explicit convergence condition of RNM. We also analyze the computational complexity. Numerical results demonstrate that the proposed RIS-aided array radar has superior performance of interference suppression to the RIS-free one, and the SINR improvement is proportional to the number of RIS elements.",
        "comments": "28 pages, 9 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11137"
    },
    {
        "doc_id": 399,
        "title": "A Finger on the Pulse of Cardiovascular Health: Smartphone Photoplethysmography-Based Pulse Waveform Analysis for Blood Pressure Measurement",
        "authors": [
            "Ivan Liu",
            "Fangyuan Liu",
            "Qi Zhong",
            "Shiguang Ni"
        ],
        "subjects": [
            "Signal Processing",
            "Computers and Society"
        ],
        "abstract": "Routine blood pressure (BP) monitoring, crucial for health assessment, faces challenges such as limited access to medical-grade equipment and expertise. Portable cuff BP devices, on the other hand, are cumbersome to carry all day and often cost-prohibitive in less developed countries. Besides, these sphygmomanometer-based devices can cause discomfort and disrupt blood flow during measurement. This study explores the use of smartphones for continuous BP monitoring, focusing on overcoming the trust barriers associated with the opacity of machine learning models in predicting BP from low-quality PPG signals. Our approach included developing models based on cardiovascular literature, using simple statistical methods to estimate BP from smartphone PPG signals with comprehensive data pre-processing, applying SHAP for enhanced interpretability and feature identification, and comparing our methods against standard references using Bland-Altman analysis. Validated with data from 125 participants, the study demonstrated significant correlations in waveform features between smartphone and reference BP monitoring devices. The cross-validation of linear regression [MAE=9.86 and 8.01 mmHg for systolic blood pressure (SBP) and diastolic blood pressure (DBP), respectively] and random forest model (MAE=8.91 and 6.68 mmHg for SBP and DBP) using waveform-only variables demonstrated the feasibility of using a smartphone to estimate BP. Although SHAP analysis identified key feature sets, Bland-Altman results did not fully meet established thresholds (84.64% and 94.69% of MAE<15 mmHg for SBP and DBP, respectively). The study suggests the potential of smartphone cameras to enhance the accuracy and interpretability of machine learning models for daily BP estimation, but also indicates that smartphone PPG-based BP prediction is not yet a replacement for traditional medical devices.",
        "comments": "33 pages, 9 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11117"
    }
]