[
    {
        "doc_id": 0,
        "title": "General duality and dual attainment for adapted transport",
        "authors": [
            "Daniel Kr\u0161ek",
            "Gudmund Pammer"
        ],
        "subjects": [
            "Probability",
            "Optimization and Control",
            "Mathematical Finance"
        ],
        "abstract": "We investigate duality and existence of dual optimizers for several adapted optimal transport problems under minimal assumptions. This includes the causal and bicausal transport, the barycenter problem, and a general multimarginal problem incorporating causality constraints. Moreover, we discuss applications of our results in robust finance. We consider a non-dominated model of several financial markets where stocks are traded dynamically, but the joint stock dynamics are unknown. We show that a no-arbitrage assumption in a quasi-sure sense naturally leads to sets of multicausal couplings. Consequently, computing the robust superhedging price is equivalent to solving an adapted transport problem, and finding a superhedging strategy means solving the corresponding dual.",
        "comments": "32 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11958"
    },
    {
        "doc_id": 1,
        "title": "Forecasting and Backtesting Gradient Allocations of Expected Shortfall",
        "authors": [
            "Takaaki Koike",
            "Cathy W. S. Chen",
            "Edward M. H. Lin"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "Capital allocation is a procedure for quantifying the contribution of each source of risk to aggregated risk. The gradient allocation rule, also known as the Euler principle, is a prevalent rule of capital allocation under which the allocated capital captures the diversification benefit of the marginal risk as a component of overall risk. This research concentrates on Expected Shortfall (ES) as a regulatory standard and focuses on the gradient allocations of ES, also called ES contributions. We achieve the comprehensive treatment of backtesting the tuple of ES contributions in the framework of the traditional and comparative backtests based on the concepts of joint identifiability and multi-objective elicitability. For robust forecast evaluation against the choice of scoring function, we further develop Murphy diagrams for ES contributions as graphical tools to check whether one forecast dominates another under a class of scoring functions. Finally, leveraging the recent concept of multi-objective elicitability, we propose a novel semiparametric model for forecasting dynamic ES contributions based on a compositional regression model. In an empirical analysis of stock returns we evaluate and compare a variety of models for forecasting dynamic ES contributions and demonstrate the outstanding performance of the proposed model.",
        "comments": "MSC Class:          62F07; 62P05; 91B30",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11701"
    },
    {
        "doc_id": 2,
        "title": "A Novel Decision Ensemble Framework: Customized Attention-BiLSTM and XGBoost for Speculative Stock Price Forecasting",
        "authors": [
            "Riaz Ud Din",
            "Salman Ahmed",
            "Saddam Hussain Khan"
        ],
        "subjects": [
            "Statistical Finance",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "Forecasting speculative stock prices is essential for effective investment risk management that drives the need for the development of innovative algorithms. However, the speculative nature, volatility, and complex sequential dependencies within financial markets present inherent challenges which necessitate advanced techniques. This paper proposes a novel framework, CAB-XDE (customized attention BiLSTM-XGB decision ensemble), for predicting the daily closing price of speculative stock Bitcoin-USD (BTC-USD). CAB-XDE framework integrates a customized bi-directional long short-term memory (BiLSTM) with the attention mechanism and the XGBoost algorithm. The customized BiLSTM leverages its learning capabilities to capture the complex sequential dependencies and speculative market trends. Additionally, the new attention mechanism dynamically assigns weights to influential features, thereby enhancing interpretability, and optimizing effective cost measures and volatility forecasting. Moreover, XGBoost handles nonlinear relationships and contributes to the proposed CAB-XDE framework robustness. Additionally, the weight determination theory-error reciprocal method further refines predictions. This refinement is achieved by iteratively adjusting model weights. It is based on discrepancies between theoretical expectations and actual errors in individual customized attention BiLSTM and XGBoost models to enhance performance. Finally, the predictions from both XGBoost and customized attention BiLSTM models are concatenated to achieve diverse prediction space and are provided to the ensemble classifier to enhance the generalization capabilities of CAB-XDE. The proposed CAB-XDE framework is empirically validated on volatile Bitcoin market, sourced from Yahoo Finance and outperforms state-of-the-art models with a MAPE of 0.0037, MAE of 84.40, and RMSE of 106.14.",
        "comments": "30 pages, 16 Figures, 4 Tables",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11621"
    },
    {
        "doc_id": 3,
        "title": "The geometry of multi-curve interest rate models",
        "authors": [
            "Claudio Fontana",
            "Giacomo Lanaro",
            "Agatha Murgoci"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "We study the problems of consistency and of the existence of finite-dimensional realizations for multi-curve interest rate models of Heath-Jarrow-Morton type, generalizing the geometric approach developed by T. Bj\u00f6rk and co-authors in the classical single-curve setting. We characterize when a multi-curve interest rate model is consistent with a given parameterized family of forward curves and spreads and when a model can be realized by a finite-dimensional state process. We illustrate the general theory in a number of model classes and examples, providing explicit constructions of finite-dimensional realizations. Based on these theoretical results, we perform the calibration of a three-curve Hull-White model to market data and analyse the stability of the estimated parameters.",
        "comments": "28 pages, 2 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11619"
    },
    {
        "doc_id": 4,
        "title": "Functional Limit Theorems for Hawkes Processes",
        "authors": [
            "Ulrich Horst",
            "Wei Xu"
        ],
        "subjects": [
            "Probability",
            "Statistics Theory",
            "Mathematical Finance"
        ],
        "abstract": "We prove that the long-run behavior of Hawkes processes is fully determined by the average number and the dispersion of child events. For subcritical processes we provide FLLNs and FCLTs under minimal conditions on the kernel of the process with the precise form of the limit theorems depending strongly on the dispersion of child events. For a critical Hawkes process with weakly dispersed child events, functional central limit theorems do not hold. Instead, we prove that the rescaled intensity processes and rescaled Hawkes processes behave like CIR-processes without mean-reversion, respectively integrated CIR-processes. We provide the rate of convergence by establishing an upper bound on the Wasserstein distance between the distributions of rescaled Hawkes process and the corresponding limit process. By contrast, critical Hawkes process with heavily dispersed child events share many properties of subcritical ones. In particular, functional limit theorems hold. However, unlike subcritical processes critical ones with heavily dispersed child events display long-range dependencies.",
        "comments": "59 pages; Keywords and phrases: Hawkes process, functional limit theorem, regular variation, convergence rate",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11495"
    },
    {
        "doc_id": 5,
        "title": "PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding",
        "authors": [
            "Ruochi Zhang",
            "Haoran Wu",
            "Chang Liu",
            "Huaping Li",
            "Yuqian Wu",
            "Kewei Li",
            "Yifan Wang",
            "Yifan Deng",
            "Jiahui Chen",
            "Fengfeng Zhou",
            "Xin Gao"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "Recent advances in protein language models have catalyzed significant progress in peptide sequence representation. Despite extensive exploration in this field, pre-trained models tailored for peptide-specific needs remain largely unaddressed due to the difficulty in capturing the complex and sometimes unstable structures of peptides. This study introduces a novel multi-view contrastive learning framework PepHarmony for the sequence-based peptide encoding task. PepHarmony innovatively combines both sequence- and structure-level information into a sequence-level encoding module through contrastive learning. We carefully select datasets from the Protein Data Bank (PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences and structures. The experimental data highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures compared with the baseline and fine-tuned models. The robustness of our model is confirmed through extensive ablation studies, which emphasize the crucial roles of contrastive loss and strategic data sorting in enhancing predictive performance. The proposed PepHarmony framework serves as a notable contribution to peptide representations, and offers valuable insights for future applications in peptide drug discovery and peptide engineering. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony or http://www.healthinformaticslab.org/supp/.",
        "comments": "25 pages, 5 figures, 3 tables",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11360"
    },
    {
        "doc_id": 6,
        "title": "Data-driven Option Pricing",
        "authors": [
            "Min Dai",
            "Hanqing Jin",
            "Xi Yang"
        ],
        "subjects": [
            "Pricing of Securities"
        ],
        "abstract": "We propose an innovative data-driven option pricing methodology that relies exclusively on the dataset of historical underlying asset prices. While the dataset is rooted in the objective world, option prices are commonly expressed as discounted expectations of their terminal payoffs in a risk-neutral world. Bridging this gap motivates us to identify a pricing kernel process, transforming option pricing into evaluating expectations in the objective world. We recover the pricing kernel by solving a utility maximization problem, and evaluate the expectations in terms of a functional optimization problem. Leveraging the deep learning technique, we design data-driven algorithms to solve both optimization problems over the dataset. Numerical experiments are presented to demonstrate the efficiency of our methodology.",
        "comments": "15 pages, 3 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11158"
    },
    {
        "doc_id": 7,
        "title": "BioFinBERT: Finetuning Large Language Models (LLMs) to Analyze Sentiment of Press Releases and Financial Text Around Inflection Points of Biotech Stocks",
        "authors": [
            "Valentina Aparicio",
            "Daniel Gordon",
            "Sebastian G. Huayamares",
            "Yuhuai Luo"
        ],
        "subjects": [
            "General Finance",
            "Computational Finance",
            "Trading and Market Microstructure"
        ],
        "abstract": "Large language models (LLMs) are deep learning algorithms being used to perform natural language processing tasks in various fields, from social sciences to finance and biomedical sciences. Developing and training a new LLM can be very computationally expensive, so it is becoming a common practice to take existing LLMs and finetune them with carefully curated datasets for desired applications in different fields. Here, we present BioFinBERT, a finetuned LLM to perform financial sentiment analysis of public text associated with stocks of companies in the biotechnology sector. The stocks of biotech companies developing highly innovative and risky therapeutic drugs tend to respond very positively or negatively upon a successful or failed clinical readout or regulatory approval of their drug, respectively. These clinical or regulatory results are disclosed by the biotech companies via press releases, which are followed by a significant stock response in many cases. In our attempt to design a LLM capable of analyzing the sentiment of these press releases,we first finetuned BioBERT, a biomedical language representation model designed for biomedical text mining, using financial textual databases. Our finetuned model, termed BioFinBERT, was then used to perform financial sentiment analysis of various biotech-related press releases and financial text around inflection points that significantly affected the price of biotech stocks.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11011"
    },
    {
        "doc_id": 8,
        "title": "Forecasting Cryptocurrency Staking Rewards",
        "authors": [
            "Sauren Gupta",
            "Apoorva Hathi Katharaki",
            "Yifan Xu",
            "Bhaskar Krishnamachari",
            "Rajarshi Gupta"
        ],
        "subjects": [
            "Statistical Finance",
            "Cryptography and Security",
            "Machine Learning"
        ],
        "abstract": "This research explores a relatively unexplored area of predicting cryptocurrency staking rewards, offering potential insights to researchers and investors. We investigate two predictive methodologies: a) a straightforward sliding-window average, and b) linear regression models predicated on historical data. The findings reveal that ETH staking rewards can be forecasted with an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day look-aheads respectively, using a 7-day sliding-window average approach. Additionally, we discern diverse prediction accuracies across various cryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is identified as superior to the moving-window average for perdicting in the short term for XTZ and ATOM. The results underscore the generally stable and predictable nature of staking rewards for most assets, with MATIC presenting a noteworthy exception.",
        "comments": "9 pages, 18 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10931"
    },
    {
        "doc_id": 9,
        "title": "Application of Machine Learning in Stock Market Forecasting: A Case Study of Disney Stock",
        "authors": [
            "Dengxin Huang"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning",
            "Applications"
        ],
        "abstract": "This document presents a stock market analysis conducted on a dataset consisting of 750 instances and 16 attributes donated in 2014-10-23. The analysis includes an exploratory data analysis (EDA) section, feature engineering, data preparation, model selection, and insights from the analysis. The Fama French 3-factor model is also utilized in the analysis. The results of the analysis are presented, with linear regression being the best-performing model.",
        "comments": "9 pages, 7 figures",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10903"
    },
    {
        "doc_id": 10,
        "title": "Stylized Facts and Market Microstructure: An In-Depth Exploration of German Bond Futures Market",
        "authors": [
            "Hamza Bodor",
            "Laurent Carlier"
        ],
        "subjects": [
            "Statistical Finance",
            "Trading and Market Microstructure"
        ],
        "abstract": "This paper presents an in-depth analysis of stylized facts in the context of futures on German bonds. The study examines four futures contracts on German bonds: Schatz, Bobl, Bund and Buxl, using tick-by-tick limit order book datasets. It uncovers a range of stylized facts and empirical observations, including the distribution of order sizes, patterns of order flow, and inter-arrival times of orders. The findings reveal both commonalities and unique characteristics across the different futures, thereby enriching our understanding of these markets. Furthermore, the paper introduces insightful realism metrics that can be used to benchmark market simulators. The study contributes to the literature on financial stylized facts by extending empirical observations to this class of assets, which has been relatively underexplored in existing research. This work provides valuable guidance for the development of more accurate and realistic market simulators.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10722"
    },
    {
        "doc_id": 11,
        "title": "Dynamic Programming: Finite States",
        "authors": [
            "Thomas J. Sargent",
            "John Stachurski"
        ],
        "subjects": [
            "General Economics",
            "Optimization and Control"
        ],
        "abstract": "This book is about dynamic programming and its applications in economics, finance, and adjacent fields. It brings together recent innovations in the theory of dynamic programming and provides applications and code that can help readers approach the research frontier. The book is aimed at graduate students and researchers, although most chapters are accessible to undergraduate students with solid quantitative backgrounds.",
        "comments": "MSC Class:          90C39",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10473"
    },
    {
        "doc_id": 12,
        "title": "Deep Generative Modeling for Financial Time Series with Application in VaR: A Comparative Review",
        "authors": [
            "Lars Ericson",
            "Xuejun Zhu",
            "Xusi Han",
            "Rao Fu",
            "Shuang Li",
            "Steve Guo",
            "Ping Hu"
        ],
        "subjects": [
            "Computational Finance",
            "Machine Learning",
            "Risk Management",
            "Statistical Finance"
        ],
        "abstract": "In the financial services industry, forecasting the risk factor distribution conditional on the history and the current market environment is the key to market risk modeling in general and value at risk (VaR) model in particular. As one of the most widely adopted VaR models in commercial banks, Historical simulation (HS) uses the empirical distribution of daily returns in a historical window as the forecast distribution of risk factor returns in the next day. The objectives for financial time series generation are to generate synthetic data paths with good variety, and similar distribution and dynamics to the original historical data. In this paper, we apply multiple existing deep generative methods (e.g., CGAN, CWGAN, Diffusion, and Signature WGAN) for conditional time series generation, and propose and test two new methods for conditional multi-step time series generation, namely Encoder-Decoder CGAN and Conditional TimeVAE. Furthermore, we introduce a comprehensive framework with a set of KPIs to measure the quality of the generated time series for financial modeling. The KPIs cover distribution distance, autocorrelation and backtesting. All models (HS, parametric and neural networks) are tested on both historical USD yield curve data and additional data simulated from GARCH and CIR processes. The study shows that top performing models are HS, GARCH and CWGAN models. Future research directions in this area are also discussed.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10370"
    },
    {
        "doc_id": 13,
        "title": "Interplay between Cryptocurrency Transactions and Online Financial Forums",
        "authors": [
            "Ana Fern\u00e1ndez Vilas",
            "Rebeca P. D\u00edaz Redondo",
            "Daniel Couto Cancela",
            "Alejandro Torrado Pazos"
        ],
        "subjects": [
            "General Finance",
            "Computers and Society",
            "Machine Learning"
        ],
        "abstract": "Cryptocurrencies are a type of digital money meant to provide security and anonymity while using cryptography techniques. Although cryptocurrencies represent a breakthrough and provide some important benefits, their usage poses some risks that are a result of the lack of supervising institutions and transparency. Because disinformation and volatility is discouraging for personal investors, cryptocurrencies emerged hand-in-hand with the proliferation of online users' communities and forums as places to share information that can alleviate users' mistrust. This research focuses on the study of the interplay between these cryptocurrency forums and fluctuations in cryptocurrency values. In particular, the most popular cryptocurrency Bitcoin (BTC) and a related active discussion community, Bitcointalk, are analyzed. This study shows that the activity of Bitcointalk forum keeps a direct relationship with the trend in the values of BTC, therefore analysis of this interaction would be a perfect base to support personal investments in a non-regulated market and, to confirm whether cryptocurrency forums show evidences to detect abnormal behaviors in BTC values as well as to predict or estimate these values. The experiment highlights that forum data can explain specific events in the financial field. It also underlines the relevance of quotes (regular mechanism to response a post) at periods: (1) when there is a high concentration of posts around certain topics; (2) when peaks in the BTC price are observed; and, (3) when the BTC price gradually shifts downwards and users intend to sell.",
        "comments": "Journal ref:        Mathematics 2021, 9(4), 411;",
        "date": "27 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10238"
    },
    {
        "doc_id": 14,
        "title": "An Exploration to the Correlation Structure and Clustering of Macroeconomic Variables (MEV)",
        "authors": [
            "Garvit Arora",
            "Shubhangi Shubhangi",
            "Ying Wu",
            "Xuan Mei"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "As a quantitative characterization of the complicated economy, Macroeconomic Variables (MEVs), including GDP, inflation, unemployment, income, spending, interest rate, etc., are playing a crucial role in banks' portfolio management and stress testing exercise. In recent years, especially during the COVID-19 period and the current high inflation environment, people are frequently talking about the changing \"correlation structure\" of MEVs. In this paper, we use a principal component based algorithm to better understand MEVs' correlation structure in a given period. We also demonstrate how this method can be used to visualize historical MEVs pattern changes between 2000 and 2022. Further, we use this method to compare different hypothetical or historical macroeconomic scenarios and present our key findings.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10162"
    },
    {
        "doc_id": 15,
        "title": "Cardiac Digital Twin Pipeline for Virtual Therapy Evaluation",
        "authors": [
            "Julia Camps",
            "Zhinuo Jenny Wang",
            "Ruben Doste",
            "Maxx Holmes",
            "Brodie Lawson",
            "Jakub Tomek",
            "Kevin Burrage",
            "Alfonso Bueno-Orovio",
            "Blanca Rodriguez"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Tissues and Organs"
        ],
        "abstract": "Cardiac digital twins are computational tools capturing key functional and anatomical characteristics of patient hearts for investigating disease phenotypes and predicting responses to therapy. When paired with large-scale computational resources and large clinical datasets, digital twin technology can enable virtual clinical trials on virtual cohorts to fast-track therapy development. Here, we present an automated pipeline for personalising ventricular anatomy and electrophysiological function based on routinely acquired cardiac magnetic resonance (CMR) imaging data and the standard 12-lead electrocardiogram (ECG). Using CMR-based anatomical models, a sequential Monte-Carlo approximate Bayesian computational inference method is extended to infer electrical activation and repolarisation characteristics from the ECG. Fast simulations are conducted with a reaction-Eikonal model, including the Purkinje network and biophysically-detailed subcellular ionic current dynamics for repolarisation. For each patient, parameter uncertainty is represented by inferring a population of ventricular models rather than a single one, which means that parameter uncertainty can be propagated to therapy evaluation. Furthermore, we have developed techniques for translating from reaction-Eikonal to monodomain simulations, which allows more realistic simulations of cardiac electrophysiology. The pipeline is demonstrated in a healthy female subject, where our inferred reaction-Eikonal models reproduced the patient's ECG with a Pearson's correlation coefficient of 0.93, and the translated monodomain simulations have a correlation coefficient of 0.89. We then apply the effect of Dofetilide to the monodomain population of models for this subject and show dose-dependent QT and T-peak to T-end prolongations that are in keeping with large population drug response data.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10029"
    },
    {
        "doc_id": 16,
        "title": "Consistent asset modelling with random coefficients and switches between regimes",
        "authors": [
            "Felix L. Wolf",
            "Griselda Deelstra",
            "Lech A. Grzelak"
        ],
        "subjects": [
            "Pricing of Securities",
            "Computational Finance",
            "Risk Management"
        ],
        "abstract": "We explore a stochastic model that enables capturing external influences in two specific ways. The model allows for the expression of uncertainty in the parametrisation of the stochastic dynamics and incorporates patterns to account for different behaviours across various times or regimes. To establish our framework, we initially construct a model with random parameters, where the switching between regimes can be dictated either by random variables or deterministically. Such a model is highly interpretable. We further ensure mathematical consistency by demonstrating that the framework can be elegantly expressed through local volatility models taking the form of standard jump diffusions. Additionally, we consider a Markov-modulated approach for the switching between regimes characterised by random parameters. For all considered models, we derive characteristic functions, providing a versatile tool with wide-ranging applications. In a numerical experiment, we apply the framework to the financial problem of option pricing. The impact of parameter uncertainty is analysed in a two-regime model, where the asset process switches between periods of high and low volatility imbued with high and low uncertainty, respectively.",
        "comments": "MSC Class:          91G20 91G30",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09955"
    },
    {
        "doc_id": 17,
        "title": "Cross-Domain Behavioral Credit Modeling: transferability from private to central data",
        "authors": [
            "O. Didkovskyi",
            "N. Jean",
            "G. Le Pera",
            "C. Nordio"
        ],
        "subjects": [
            "Risk Management",
            "Statistical Finance"
        ],
        "abstract": "This paper introduces a credit risk rating model for credit risk assessment in quantitative finance, aiming to categorize borrowers based on their behavioral data. The model is trained on data from Experian, a widely recognized credit bureau, to effectively identify instances of loan defaults among bank customers. Employing state-of-the-art statistical and machine learning techniques ensures the model's predictive accuracy. Furthermore, we assess the model's transferability by testing it on behavioral data from the Bank of Italy, demonstrating its potential applicability across diverse datasets during prediction. This study highlights the benefits of incorporating external behavioral data to improve credit risk assessment in financial institutions.",
        "comments": "25 pages, 15 figures",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09778"
    },
    {
        "doc_id": 18,
        "title": "Neural Hawkes: Non-Parametric Estimation in High Dimension and Causality Analysis in Cryptocurrency Markets",
        "authors": [
            "Timoth\u00e9e Fabre",
            "Ioane Muni Toke"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Mathematical Finance"
        ],
        "abstract": "We propose a novel approach to marked Hawkes kernel inference which we name the moment-based neural Hawkes estimation method. Hawkes processes are fully characterized by their first and second order statistics through a Fredholm integral equation of the second kind. Using recent advances in solving partial differential equations with physics-informed neural networks, we provide a numerical procedure to solve this integral equation in high dimension. Together with an adapted training pipeline, we give a generic set of hyperparameters that produces robust results across a wide range of kernel shapes. We conduct an extensive numerical validation on simulated data. We finally propose two applications of the method to the analysis of the microstructure of cryptocurrency markets. In a first application we extract the influence of volume on the arrival rate of BTC-USD trades and in a second application we analyze the causality relationships and their directions amongst a universe of 15 cryptocurrency pairs in a centralized exchange.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09361"
    },
    {
        "doc_id": 19,
        "title": "A closer look at the chemical potential of an ideal agent system",
        "authors": [
            "Christoph J. B\u00f6rner",
            "Ingo Hoffmann",
            "John H. Stiebel"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "Models for spin systems known from statistical physics are used in econometrics in the form of agent-based models. Econophysics research in econometrics is increasingly developing general market models that describe exchange phenomena and use the chemical potential $\u03bc$ known from physics in the context of particle number changes. In statistical physics, equations of state are known for the chemical potential, which take into account the respective model framework and the corresponding state variables. A simple transfer of these equations of state to problems in econophysics appears difficult. To the best of our knowledge, the equation of state for the chemical potential is currently missing even for the simplest conceivable model of an ideal agent system. In this paper, this research gap is closed and the equation of state for the chemical potential is derived from the econophysical model assumptions of the ideal agent system. An interpretation of the equation of state leads to fundamental relationships that could also have been guessed, but are shown here by the theory.",
        "comments": "11 Pages, 0 Figures, Working Paper, Theoretical Contribution",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09233"
    },
    {
        "doc_id": 20,
        "title": "Mean-Field SDEs driven by $G$-Brownian Motion",
        "authors": [
            "Karl-Wilhelm Georg Bollweg",
            "Thilo Meyer-Brandis"
        ],
        "subjects": [
            "Probability",
            "Mathematical Finance"
        ],
        "abstract": "We extend the notion of mean-field SDEs to SDEs driven by $G$-Brownian motion. More precisely, we consider a $G$-SDE where the coefficients depend not only on time and the current state but also on the solution as random variable.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09113"
    },
    {
        "doc_id": 21,
        "title": "AI Thrust: Ranking Emerging Powers for Tech Startup Investment in Latin America",
        "authors": [
            "Abraham Ramos Torres",
            "Laura N Montoya"
        ],
        "subjects": [
            "General Economics",
            "Risk Management"
        ],
        "abstract": "Artificial intelligence (AI) is rapidly transforming the global economy, and Latin America is no exception. In recent years, there has been a growing interest in AI development and implementation in the region. This paper presents a ranking of Latin American (LATAM) countries based on their potential to become emerging powers in AI. The ranking is based on three pillars: infrastructure, education, and finance. Infrastructure is measured by the availability of electricity, high-speed internet, the quality of telecommunications networks, and the availability of supercomputers. Education is measured by the quality of education and the research status. Finance is measured by the cost of investments, history of investments, economic metrics, and current implementation of AI.\n  While Brazil, Chile, and Mexico have established themselves as major players in the AI industry in Latin America, our ranking demonstrates the new emerging powers in the region. According to the results, Argentina, Colombia, Uruguay, Costa Rica, and Ecuador are leading as new emerging powers in AI in Latin America. These countries have strong education systems, well-developed infrastructure, and growing financial resources. The ranking provides a useful tool for policymakers, investors, and businesses interested in AI development in Latin America. It can help to identify emerging LATAM countries with the greatest potential for AI growth and success.",
        "comments": "9 pages, 4 tables, 9 figures",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09056"
    },
    {
        "doc_id": 22,
        "title": "On continuity of state-dependent utilities",
        "authors": [
            "Edoardo Berton",
            "Alessandro Doldi",
            "Marco Maggis"
        ],
        "subjects": [
            "Mathematical Finance",
            "Theoretical Economics"
        ],
        "abstract": "State-dependent preferences for a general Savage's state space were shown in Wakker and Zank (1999) to admit a numerical representation in the form of the integral of a state-dependent utility, as soon as pointwise continuity of the preference ordering is assumed. In this paper we prove that such a state-dependent function inherits pointwise continuity from the preference ordering, providing in this way a positive answer to a conjecture posed in the aforementioned seminal work. We further apply this result to obtain an explicit representation of conditional Chisini means in the form of a conditional certainty equivalent.",
        "comments": "MSC Class:          91B06; 91B08; 60A05",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09054"
    },
    {
        "doc_id": 23,
        "title": "Spurious Default Probability Projections in Credit Risk Stress Testing Models",
        "authors": [
            "Bernd Engelmann"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "Credit risk stress testing has become an important risk management device which is used both by banks internally and by regulators. Stress testing is complex because it essentially means projecting a bank's full balance sheet conditional on a macroeconomic scenario over multiple years. Part of the complexity stems from using a wide range of model parameters for, e.g., rating transition, write-off rules, prepayment, or origination of new loans. A typical parameterization of a credit risk stress test model specifies parameters linked to an average economic, the through-the-cycle, state. These parameters are transformed to a stressed state by utilizing a macroeconomic model. It will be shown that the model parameterization implies a unique through-the-cycle portfolio which is unrelated to a bank's current portfolio. Independent of the stress imposed to the model, the current portfolio will have a tendency to propagate towards the through-the-cycle portfolio. This could create unwanted spurious effects on projected portfolio default rates especially when a stress test model's parameterization is inconsistent with a bank's current portfolio.",
        "comments": "15 pages, 4 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08892"
    },
    {
        "doc_id": 24,
        "title": "Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities and Risks",
        "authors": [
            "Xihan Xiong",
            "Zhipeng Wang",
            "Xi Chen",
            "William Knottenbelt",
            "Michael Huth"
        ],
        "subjects": [
            "General Finance",
            "Cryptography and Security"
        ],
        "abstract": "Lido, the leading Liquid Staking Derivative (LSD) provider on Ethereum, allows users to stake an arbitrary amount of ETH to receive stETH, which can be integrated with Decentralized Finance (DeFi) protocols such as Aave. The composability between Lido and Aave enables a novel strategy called \"leverage staking\", where users stake ETH on Lido to acquire stETH, utilize stETH as collateral on Aave to borrow ETH, and then restake the borrowed ETH on Lido. Users can iteratively execute this process to optimize potential returns based on their risk profile.\n  This paper systematically studies the opportunities and risks associated with leverage staking. We are the first to formalize the leverage staking strategy within the Lido-Aave ecosystem. Our empirical study identifies 262 leverage staking positions on Ethereum, with an aggregated staking amount of 295,243 ETH (482M USD). We discover that 90.13% of leverage staking positions have achieved higher returns than conventional staking. Furthermore, we perform stress tests to evaluate the risk introduced by leverage staking under extreme conditions. We find that leverage staking significantly amplifies the risk of cascading liquidations. We hope this paper can inform and encourage the development of robust risk management approaches to protect the Lido-Aave LSD ecosystem.",
        "comments": " ",
        "date": "28 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08610"
    },
    {
        "doc_id": 25,
        "title": "Forking paths in financial economics",
        "authors": [
            "Guillaume Coqueret"
        ],
        "subjects": [
            "General Finance",
            "Methodology"
        ],
        "abstract": "We argue that spanning large numbers of degrees of freedom in empirical analysis allows better characterizations of effects and thus improves the trustworthiness of conclusions. Our ideas are illustrated in three studies: equity premium prediction, asset pricing anomalies and risk premia estimation. In the first, we find that each additional degree of freedom in the protocol expands the average range of $t$-statistics by at least 30%. In the second, we show that resorting to forking paths instead of bootstrapping in multiple testing raises the bar of significance for anomalies: at the 5% confidence level, the threshold for bootstrapped statistics is 4.5, whereas with paths, it is at least 8.2, a bar much higher than those currently used in the literature. In our third application, we reveal the importance of particular steps in the estimation of premia. In addition, we use paths to corroborate prior findings in the three topics. We document heterogeneity in our ability to replicate prior studies: some conclusions seem robust, others do not align with the paths we were able to generate.",
        "comments": " ",
        "date": "25 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08606"
    },
    {
        "doc_id": 26,
        "title": "Reinforcement Learning and Deep Stochastic Optimal Control for Final Quadratic Hedging",
        "authors": [
            "Bernhard Hientzsch"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "We consider two data driven approaches, Reinforcement Learning (RL) and Deep Trajectory-based Stochastic Optimal Control (DTSOC) for hedging a European call option without and with transaction cost according to a quadratic hedging P&L objective at maturity (\"variance-optimal hedging\" or \"final quadratic hedging\"). We study the performance of the two approaches under various market environments (modeled via the Black-Scholes and/or the log-normal SABR model) to understand their advantages and limitations. Without transaction costs and in the Black-Scholes model, both approaches match the performance of the variance-optimal Delta hedge. In the log-normal SABR model without transaction costs, they match the performance of the variance-optimal Barlett's Delta hedge. Agents trained on Black-Scholes trajectories with matching initial volatility but used on SABR trajectories match the performance of Bartlett's Delta hedge in average cost, but show substantially wider variance. To apply RL approaches to these problems, P&L at maturity is written as sum of step-wise contributions and variants of RL algorithms are implemented and used that minimize expectation of second moments of such sums.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2302.07996",
        "date": "20 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08600"
    },
    {
        "doc_id": 27,
        "title": "Fitting random cash management models to data",
        "authors": [
            "Francisco Salas-Molina"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "Organizations use cash management models to control balances to both avoid overdrafts and obtain a profit from short-term investments. Most management models are based on control bounds which are derived from the assumption of a particular cash flow probability distribution. In this paper, we relax this strong assumption to fit cash management models to data by means of stochastic and linear programming. We also introduce ensembles of random cash management models which are built by randomly selecting a subsequence of the original cash flow data set. We illustrate our approach by means of a real case study showing that a small random sample of data is enough to fit sufficiently good bound-based models.",
        "comments": "19 pages,6 figures, 1 table",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08548"
    },
    {
        "doc_id": 28,
        "title": "Dynamic portfolio selection under generalized disappointment aversion",
        "authors": [
            "Zongxia Liang",
            "Sheng Wang",
            "Jianming Xia",
            "Fengyi Yuan"
        ],
        "subjects": [
            "Mathematical Finance",
            "Portfolio Management"
        ],
        "abstract": "This paper addresses the continuous-time portfolio selection problem under generalized disappointment aversion (GDA). The implicit definition of the certainty equivalent within GDA preferences introduces time inconsistency to this problem. We provide the sufficient and necessary conditions for a strategy to be an equilibrium by a fully nonlinear ordinary differential equation (ODE). Through an exploration of the existence and uniqueness of solution to the ODE, we establish the existence and uniqueness of the equilibrium. Our findings indicate that under disappointment aversion (DA) preferences, non-participation in the stock market is the unique equilibrium. The numerical analysis reveals that, under GDA preferences, the investment proportion in the stock market consistently remains smaller than the investment proportion under the classical Expected Utility (EU) theory.",
        "comments": "27 pages, 4 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08323"
    },
    {
        "doc_id": 29,
        "title": "Do backrun auctions protect traders?",
        "authors": [
            "Andrew W. Macpherson"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Distributed, Parallel, and Cluster Computing",
            "Computer Science and Game Theory"
        ],
        "abstract": "We study a new \"laminated\" queueing model for orders on batched trading venues such as decentralised exchanges. The model aims to capture and generalise transaction queueing infrastructure that has arisen to organise MEV activity on public blockchains such as Ethereum, providing convenient channels for sophisticated agents to extract value by acting on end-user order flow by performing arbitrage and related HFT activities. In our model, market orders are interspersed with orders created by arbitrageurs that under idealised conditions reset the marginal price to a global equilibrium between each trade, improving predictability of execution for liquidity traders.\n  If an arbitrageur has a chance to land multiple opportunities in a row, he may attempt to manipulate the execution price of the intervening market order by a probabilistic blind sandwiching strategy. To study how bad this manipulation can get, we introduce and bound a price manipulation coefficient that measures the deviation from global equilibrium of local pricing quoted by a rational arbitrageur. We exhibit cases in which this coefficient is well approximated by a \"zeta value' with interpretable and empirically measurable parameters.",
        "comments": "Keywords: MEV, queue discipline, sandwich, CFMM, arbitrage, blockchain, Ethereum",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08302"
    },
    {
        "doc_id": 30,
        "title": "Optimal Insurance to Maximize Exponential Utility when Premium is Computed by a Convex Functional",
        "authors": [
            "Jingyi Cao",
            "Dongchen Li",
            "Virginia R. Young",
            "Bin Zou"
        ],
        "subjects": [
            "Mathematical Finance",
            "Optimization and Control",
            "Risk Management"
        ],
        "abstract": "We find the optimal indemnity to maximize the expected utility of terminal wealth of a buyer of insurance whose preferences are modeled by an exponential utility. The insurance premium is computed by a convex functional. We obtain a necessary condition for the optimal indemnity; then, because the candidate optimal indemnity is given implicitly, we use that necessary condition to develop a numerical algorithm to compute it. We prove that the numerical algorithm converges to a unique indemnity that, indeed, equals the optimal policy. We also illustrate our results with numerical examples.",
        "comments": "12 pages, 3 figures",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08094"
    },
    {
        "doc_id": 31,
        "title": "A Two-Step Longstaff Schwartz Monte Carlo Approach to Game Option Pricing",
        "authors": [
            "Ce Wang"
        ],
        "subjects": [
            "Computational Finance",
            "Pricing of Securities"
        ],
        "abstract": "We proposed a two-step Longstaff Schwartz Monte Carlo (LSMC) method with two regression models fitted at each time step to price game options. Although the original LSMC can be used to price game options with an enlarged range of path in regression and a modified cashflow updating rule, we identified a drawback of such approach, which motivated us to propose our approach. We implemented numerical examples with benchmarks using binomial tree and numerical PDE, and it showed that our method produces more reliable results comparing to the original LSMC.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08093"
    },
    {
        "doc_id": 32,
        "title": "Transformer-based approach for Ethereum Price Prediction Using Crosscurrency correlation and Sentiment Analysis",
        "authors": [
            "Shubham Singh",
            "Mayur Bhat"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Pricing of Securities"
        ],
        "abstract": "The research delves into the capabilities of a transformer-based neural network for Ethereum cryptocurrency price forecasting. The experiment runs around the hypothesis that cryptocurrency prices are strongly correlated with other cryptocurrencies and the sentiments around the cryptocurrency. The model employs a transformer architecture for several setups from single-feature scenarios to complex configurations incorporating volume, sentiment, and correlated cryptocurrency prices. Despite a smaller dataset and less complex architecture, the transformer model surpasses ANN and MLP counterparts on some parameters. The conclusion presents a hypothesis on the illusion of causality in cryptocurrency price movements driven by sentiments.",
        "comments": "12 pages",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08077"
    },
    {
        "doc_id": 33,
        "title": "Provisions and Economic Capital for Credit Losses",
        "authors": [
            "Dorinel Bastide",
            "St\u00e9phane Cr\u00e9pey"
        ],
        "subjects": [
            "Risk Management",
            "Probability",
            "General Finance"
        ],
        "abstract": "Based on supermodularity ordering properties, we show that convex risk measures of credit losses are nondecreasing  w.r.t. credit-credit and, in a wrong-way risk setup, credit-market, covariances of elliptically distributed latent factors. These results support the use of such setups for computing credit provisions and economic capital or for conducting stress test exercises and risk management analysis.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07728"
    },
    {
        "doc_id": 34,
        "title": "Cash and Card Acceptance in Retail Payments: Motivations and Factors",
        "authors": [
            "Samuel Vandak",
            "Geoffrey Goodell"
        ],
        "subjects": [
            "Computers and Society",
            "Computational Engineering, Finance, and Science",
            "General Finance"
        ],
        "abstract": "The landscape of payment methods in retail is a complex and evolving area. Vendors are motivated to conduct an appropriate analysis to decide what payment methods to accept out of a vast range of options. Many factors are included in this decision process, some qualitative and some quantitative. The following research project investigates vendors' acceptance of cards and cash from various viewpoints, all chosen to represent a novel perspective, including the barriers and preferences for each and correlations with external demographic factors. We observe that lower interchange fees, limited in this instance by the regulatory framework, play a crucial role in facilitating merchants' acceptance of card payments. The regulatory constraints on interchange fees create a favorable cost structure for merchants, making card payment adoption financially feasible. However, additional factors like technological readiness and consumer preferences might also play a significant role in their decision-making process. We also note that aggregate Merchant Service Providers (MSPs) have positively impacted the payment landscape by offering more competitive fee rates, particularly beneficial for small merchants and entrepreneurs. However, associated risks, such as account freezes or abrupt terminations, pose challenges and often lack transparency. Last, the quantitative analysis of the relationship between demographic variables and acceptance of payment types is presented. This analysis combines the current landscape of payment acceptance in the UK with data from the most recent census from 2021. We show that the unemployment rates shape card and cash acceptance, age affects contactless preference, and work-from-home impacts credit card preference.",
        "comments": "34 pages, 19 figures, 5 tables",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07682"
    },
    {
        "doc_id": 35,
        "title": "Empirical Evidence for the Fragment level Understanding on Drug Molecular Structure of LLMs",
        "authors": [
            "Xiuyuan Hu",
            "Guoqing Liu",
            "Yang Zhao",
            "Hao Zhang"
        ],
        "subjects": [
            "Machine Learning",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "AI for drug discovery has been a research hotspot in recent years, and SMILES-based language models has been increasingly applied in drug molecular design. However, no work has explored whether and how language models understand the chemical spatial structure from 1D sequences. In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments. The results indicate that language models can understand chemical structures from the perspective of molecular fragments, and the structural knowledge learned through fine-tuning is reflected in the high-frequency SMILES substrings generated by the model.",
        "comments": "Accepted by AAAI 2024 workshop: Large Language Models for Biological Discoveries (LLMs4Bio)",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07657"
    },
    {
        "doc_id": 36,
        "title": "Graph database while computationally efficient filters out quickly the ESG integrated equities in investment management",
        "authors": [
            "Partha Sen",
            "Sumana Sen"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "Design/methodology/approach This research evaluated the databases of SQL, No-SQL and graph databases to compare and contrast efficiency and performance. To perform this experiment the data were collected from multiple sources including stock price and financial news. Python is used as an interface to connect and query databases (to create database structures according to the feed file structure, to load data into tables, objects, to read data , to connect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM (Large language model) including RAG (Retrieval Augmented Generation) with Machine Learning, deep learning, NLP (natural language processing) or Decision Analytics are computationally expensive. Finding a better option to consume less resources and time to get the result. Findings The Graph database of ESG (Environmental, Social and Governance) is comparatively better and can be considered for extended analytics to integrate ESG in business and investment. Practical implications A graph ML with a RAG architecture model can be introduced as a new framework with less computationally expensive LLM application in the equity filtering process for portfolio management. Originality/value Filtering out selective stocks out of two thousand or more listed companies in any stock exchange for active investment, consuming less resource consumption especially memory and energy to integrate artificial intelligence and ESG in business and investment.",
        "comments": "10 pages, 17 figures",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07483"
    },
    {
        "doc_id": 37,
        "title": "Herd Behavior in Optimal Investment: A Dual-Agent Approach with Investment Opinion and Rational Decision Decomposition",
        "authors": [
            "Huisheng Wang",
            "H. Vicky Zhao"
        ],
        "subjects": [
            "Systems and Control",
            "Optimization and Control",
            "Mathematical Finance",
            "Portfolio Management"
        ],
        "abstract": "In this paper, we study the optimal investment problem involving two agents, where the decision of one agent is influenced by the other. To measure the distance between two agents' decisions, we introduce the average deviation. We formulate the stochastic optimal control problem considering herd behavior and derive the analytical solution through the variational method. We theoretically analyze the impact of users' herd behavior on the optimal decision by decomposing it into their rational decisions, which is called the rational decision decomposition. Furthermore, to quantify the preference for their rational decision over that of the other agent, we introduce the agent's investment opinion. Our study is validated through simulations on real stock data.",
        "comments": " ",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07183"
    },
    {
        "doc_id": 38,
        "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
        "authors": [
            "Varshini Reddy",
            "Rik Koncel-Kedziorski",
            "Viet Dac Lai",
            "Chris Tanner"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence"
        ],
        "abstract": "Research in quantitative reasoning within the financial domain indeed necessitates the use of realistic tasks and data, primarily because of the significant impact of decisions made in business and finance. Financial professionals often interact with documents hundreds of pages long, but most research datasets drastically reduce this context length. To address this, we introduce a long-document financial QA task. We augment 7,621 questions from the existing FinQA dataset with full-document context, extending the average context length for each question from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments of retrieval-based QA pipelines and long-context language models on the augmented data. Our results show that DocFinQA provides challenges for even the strongest, state-of-the-art systems.",
        "comments": "13 pages",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06915"
    },
    {
        "doc_id": 39,
        "title": "A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models",
        "authors": [
            "Emmanuil H. Georgoulis",
            "Antonis Papapantoleon",
            "Costas Smaragdakis"
        ],
        "subjects": [
            "Computational Finance",
            "Machine Learning",
            "Numerical Analysis",
            "Probability",
            "Machine Learning"
        ],
        "abstract": "We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assessed in a series of numerical experiments involving the Merton jump-diffusion model.",
        "comments": "16 pages, 11 figures",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06740"
    },
    {
        "doc_id": 40,
        "title": "Equity auction dynamics: latent liquidity models with activity acceleration",
        "authors": [
            "Mohammed Salek",
            "Damien Challet",
            "Ioane Muni Toke"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Statistical Finance"
        ],
        "abstract": "Equity auctions display several distinctive characteristics in contrast to continuous trading. As the auction time approaches, the rate of events accelerates causing a substantial liquidity buildup around the indicative price. This, in turn, results in a reduced price impact and decreased volatility of the indicative price. In this study, we adapt the latent/revealed order book framework to the specifics of equity auctions. We provide precise measurements of the model parameters, including order submissions, cancellations, and diffusion rates. Our setup allows us to describe the full dynamics of the average order book during closing auctions in Euronext Paris. These findings support the relevance of the latent liquidity framework in describing limit order book dynamics. Lastly, we analyze the factors contributing to a sub-diffusive indicative price and demonstrate the absence of indicative price predictability.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06724"
    },
    {
        "doc_id": 41,
        "title": "SpotV2Net: Multivariate Intraday Spot Volatility Forecasting via Vol-of-Vol-Informed Graph Attention Networks",
        "authors": [
            "Alessio Brini",
            "Giacomo Toscano"
        ],
        "subjects": [
            "Statistical Finance",
            "Computational Finance"
        ],
        "abstract": "This paper introduces SpotV2Net, a multivariate intraday spot volatility forecasting model based on a Graph Attention Network architecture. SpotV2Net represents financial assets as nodes within a graph and includes non-parametric high-frequency Fourier estimates of the spot volatility and co-volatility as node features. Further, it incorporates Fourier estimates of the spot volatility of volatility and co-volatility of volatility as features for node edges. We test the forecasting accuracy of SpotV2Net in an extensive empirical exercise, conducted with high-frequency prices of the components of the Dow Jones Industrial Average index. The results we obtain suggest that SpotV2Net shows improved accuracy, compared to alternative econometric and machine-learning-based models. Further, our results show that SpotV2Net maintains accuracy when performing intraday multi-step forecasts. To interpret the forecasts produced by SpotV2Net, we employ GNNExplainer, a model-agnostic interpretability tool and thereby uncover subgraphs that are critical to a node's predictions.",
        "comments": "34 pages, 9 figures",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06249"
    },
    {
        "doc_id": 42,
        "title": "CNN-DRL for Scalable Actions in Finance",
        "authors": [
            "Sina Montazeri",
            "Akram Mirzaeinia",
            "Haseebullah Jumakhan",
            "Amir Mirzaeinia"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "The published MLP-based DRL in finance has difficulties in learning the dynamics of the environment when the action scale increases. If the buying and selling increase to one thousand shares, the MLP agent will not be able to effectively adapt to the environment. To address this, we designed a CNN agent that concatenates the data from the last ninety days of the daily feature vector to create the CNN input matrix. Our extensive experiments demonstrate that the MLP-based agent experiences a loss corresponding to the initial environment setup, while our designed CNN remains stable, effectively learns the environment, and leads to an increase in rewards.",
        "comments": "10th Annual Conf. on Computational Science & Computational Intelligence",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06179"
    },
    {
        "doc_id": 43,
        "title": "CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine Learning Methods",
        "authors": [
            "Yue Chen",
            "Xingyi Andrew",
            "Salintip Supasanya"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "Historically, the economic recession often came abruptly and disastrously. For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from October 2007 to March 2009. If we could detect the signals of the crisis earlier, we could have taken preventive measures. Therefore, driven by such motivation, we use advanced machine learning techniques, including Random Forest and Extreme Gradient Boosting, to predict any potential market crashes mainly in the US market. Also, we would like to compare the performance of these methods and examine which model is better for forecasting US stock market crashes. We apply our models on the daily financial market data, which tend to be more responsive with higher reporting frequencies. We consider 75 explanatory variables, including general US stock market indexes, SP 500 sector indexes, as well as market indicators that can be used for the purpose of crisis prediction. Finally, we conclude, with selected classification metrics, that the Extreme Gradient Boosting method performs the best in predicting US stock market crisis events.",
        "comments": "14 pages, 9 figures",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06172"
    },
    {
        "doc_id": 44,
        "title": "Multimodal Gen-AI for Fundamental Investment Research",
        "authors": [
            "Lezhi Li",
            "Ting-Yu Chang",
            "Hai Wang"
        ],
        "subjects": [
            "General Finance",
            "Machine Learning"
        ],
        "abstract": "This report outlines a transformative initiative in the financial investment industry, where the conventional decision-making process, laden with labor-intensive tasks such as sifting through voluminous documents, is being reimagined. Leveraging language models, our experiments aim to automate information summarization and investment idea generation. We seek to evaluate the effectiveness of fine-tuning methods on a base model (Llama2) to achieve specific application-level goals, including providing insights into the impact of events on companies and sectors, understanding market condition relationships, generating investor-aligned investment ideas, and formatting results with stock recommendations and detailed explanations. Through state-of-the-art generative modeling techniques, the ultimate objective is to develop an AI agent prototype, liberating human investors from repetitive tasks and allowing a focus on high-level strategic thinking. The project encompasses a diverse corpus dataset, including research reports, investment memos, market news, and extensive time-series market data. We conducted three experiments applying unsupervised and supervised LoRA fine-tuning on the llama2_7b_hf_chat as the base model, as well as instruction fine-tuning on the GPT3.5 model. Statistical and human evaluations both show that the fine-tuned versions perform better in solving text modeling, summarization, reasoning, and finance domain questions, demonstrating a pivotal step towards enhancing decision-making processes in the financial domain. Code implementation for the project can be found on GitHub: https://github.com/Firenze11/finance_lm.",
        "comments": " ",
        "date": "23 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06164"
    },
    {
        "doc_id": 45,
        "title": "De novo Drug Design using Reinforcement Learning with Multiple GPT Agents",
        "authors": [
            "Xiuyuan Hu",
            "Guoqing Liu",
            "Yang Zhao",
            "Hao Zhang"
        ],
        "subjects": [
            "Biomolecules",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT.",
        "comments": "Accepted by NeurIPS 2023",
        "date": "21 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06155"
    },
    {
        "doc_id": 46,
        "title": "A Statistical Field Perspective on Capital Allocation and Accumulation: Individual dynamics",
        "authors": [
            "Pierre Gosselin",
            "A\u00efleen Lotz"
        ],
        "subjects": [
            "General Finance",
            "High Energy Physics - Theory"
        ],
        "abstract": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups. The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends to their investors. Financial capital is thus a function of dividends and stock valuations, whereas physical capital is a function of the total capital allocated by the financial sector. Whereas our previous work focused on the background fields that describe potential long-term equilibria, here we compute the transition functions of individual agents and study their probabilistic dynamics in the background field, as a function of their initial state. We show that capital accumulation depends on various factors. The probability associated with each firm's trajectories is the result of several contradictory effects: the firm tends to shift towards sectors with the greatest long-term return, but must take into account the impact of its shift on its attractiveness for investors throughout its trajectory. Since this trajectory depends largely on the average capital of transition sectors, a firm's attractiveness during its relocation depends on the relative level of capital in those sectors. Thus, an under-capitalized firm reaching a high-capital sector will experience a loss of attractiveness, and subsequently, in investors. Moreover, the firm must also consider the effects of competition in the intermediate sectors. An under-capitalized firm will tend to be ousted out towards sectors with lower average capital, while an over-capitalized firm will tend to shift towards higher averagecapital sectors. For investors, capital allocation depends on their short and long-term returns. These returns are not independent: in the short-term, returns are composed of both the firm's dividends and the increase in its stock prices. In the long-term, returns are based on the firm's growth expectations, but also, indirectly, on expectations of higher stock prices. Investors' capital allocation directly depends on the volatility of stock prices and {\\ldots}rms'dividends. Investors will tend to reallocate their capital to maximize their short and long-term returns. The higher their level of capital, the stronger the reallocation will be.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2312.16173, arXiv:2205.03087",
        "date": "30 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06142"
    },
    {
        "doc_id": 47,
        "title": "StockFormer: A Swing Trading Strategy Based on STL Decomposition and Self-Attention Networks",
        "authors": [
            "Bohan Ma",
            "Yiheng Wang",
            "Yuchao Lu",
            "Tianzixuan Hu",
            "Jinling Xu",
            "Patrick Houlihan"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Machine Learning"
        ],
        "abstract": "Amidst ongoing market recalibration and increasing investor optimism, the U.S. stock market is experiencing a resurgence, prompting the need for sophisticated tools to protect and grow portfolios. Addressing this, we introduce \"Stockformer,\" a cutting-edge deep learning framework optimized for swing trading, featuring the TopKDropout method for enhanced stock selection. By integrating STL decomposition and self-attention networks, Stockformer utilizes the S&P 500's complex data to refine stock return predictions. Our methodology entailed segmenting data for training and validation (January 2021 to January 2023) and testing (February to June 2023). During testing, Stockformer's predictions outperformed ten industry models, achieving superior precision in key predictive accuracy indicators (MAE, RMSE, MAPE), with a remarkable accuracy rate of 62.39% in detecting market trends. In our backtests, Stockformer's swing trading strategy yielded a cumulative return of 13.19% and an annualized return of 30.80%, significantly surpassing current state-of-the-art models. Stockformer has emerged as a beacon of innovation in these volatile times, offering investors a potent tool for market forecasting. To advance the field and foster community collaboration, we have open-sourced Stockformer, available at https://github.com/Eric991005/Stockformer.",
        "comments": "Currently under consideration for publication in the International Journal of Forecasting",
        "date": "22 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06139"
    },
    {
        "doc_id": 48,
        "title": "Quantum Probability Theoretic Asset Return Modeling: A Novel Schr\u00f6dinger-Like Trading Equation and Multimodal Distribution",
        "authors": [
            "Li Lin"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "Quantum theory provides a comprehensive framework for quantifying uncertainty, often applied in quantum finance to explore the stochastic nature of asset returns. This perspective likens returns to microscopic particle motion, governed by quantum probabilities akin to physical laws. However, such approaches presuppose specific microscopic quantum effects in return changes, a premise criticized for lack of guarantee. This paper diverges by asserting that quantum probability is a mathematical extension of classical probability to complex numbers. It isn't exclusively tied to microscopic quantum phenomena, bypassing the need for quantum effects in returns.By directly linking quantum probability's mathematical structure to traders' decisions and market behaviors, it avoids assuming quantum effects for returns and invoking the wave function. The complex phase of quantum probability, capturing transitions between long and short decisions while considering information interaction among traders, offers an inherent advantage over classical probability in characterizing the multimodal distribution of asset returns.Utilizing Fourier decomposition, we derive a Schr\u00f6dinger-like trading equation, where each term explicitly corresponds to implications of market trading. The equation indicates discrete energy levels in financial trading, with returns following a normal distribution at the lowest level. As the market transitions to higher trading levels, a phase shift occurs in the return distribution, leading to multimodality and fat tails. Empirical research on the Chinese stock market supports the existence of energy levels and multimodal distributions derived from this quantum probability asset returns model.",
        "comments": " ",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05823"
    },
    {
        "doc_id": 49,
        "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis",
        "authors": [
            "Frank Xing"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence",
            "Multiagent Systems",
            "General Finance"
        ],
        "abstract": "Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed.",
        "comments": "15 pages",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05799"
    },
    {
        "doc_id": 50,
        "title": "Super-hedging-pricing formulas and Immediate-Profit arbitrage for market models under random horizon",
        "authors": [
            "Tahir Choulli",
            "Emmanuel Lepinette"
        ],
        "subjects": [
            "Mathematical Finance",
            "Optimization and Control",
            "Probability",
            "Pricing of Securities"
        ],
        "abstract": "In this paper, we consider the discrete-time setting, and the market model described by (S,F,T)$. Herein F is the ``public\" flow of information which is available to all agents overtime, S is the discounted price process of d-tradable assets, and T is an arbitrary random time whose occurrence might not be observable via F. Thus, we consider the larger flow G which incorporates F and makes T an observable random time. This framework covers the credit risk theory setting, the life insurance setting and the setting of employee stock option valuation. For the stopped model (S^T,G) and for various vulnerable claims, based on this model, we address the super-hedging pricing valuation problem and its intrinsic Immediate-Profit arbitrage (IP hereafter for short). Our first main contribution lies in singling out the impact of change of prior and/or information on conditional essential supremum, which is a vital tool in super-hedging pricing. The second main contribution consists of describing as explicit as possible how the set of super-hedging prices expands under the stochasticity of T and its risks, and we address the IP arbitrage for (S^T,G) as well. The third main contribution resides in elaborating as explicit as possible pricing formulas for vulnerable claims, and singling out the various informational risks in the prices' dynamics.",
        "comments": " ",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05713"
    },
    {
        "doc_id": 51,
        "title": "Boundary conditions at infinity for Black-Scholes equations",
        "authors": [
            "Yukihiro Tsuzuki"
        ],
        "subjects": [
            "Mathematical Finance",
            "Computational Finance"
        ],
        "abstract": "We propose numerical procedures for computing the prices of forward contracts where the underlying asset price is a Markovian local martingale. If the underlying process is a strict local martingale, multiple solutions exist for the corresponding Black-Scholes equations, and the derivative prices are characterized as the minimal solutions. Our prices are upper and lower bounds obtained using numerical methods on a finite grid under the respective boundary conditions. These bounds and the boundary values converge to the exact value as the underlying price approaches infinity. The proposed procedures are demonstrated through numerical tests.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05549"
    },
    {
        "doc_id": 52,
        "title": "Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market Wraps?",
        "authors": [
            "Baptiste Lefort",
            "Eric Benhamou",
            "Jean-Jacques Ohana",
            "David Saltiel",
            "Beatrice Guez",
            "Damien Challet"
        ],
        "subjects": [
            "Statistical Finance",
            "Artificial Intelligence"
        ],
        "abstract": "We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to 2023, reposted on large financial media, to determine how global news headlines may affect stock market movements using ChatGPT and a two-stage prompt approach. We document a statistically significant positive correlation between the sentiment score and future equity market returns over short to medium term, which reverts to a negative correlation over longer horizons. Validation of this correlation pattern across multiple equity markets indicates its robustness across equity regions and resilience to non-linearity, evidenced by comparison of Pearson and Spearman correlations. Finally, we provide an estimate of the optimal horizon that strikes a balance between reactivity to new information and correlation.",
        "comments": " ",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05447"
    },
    {
        "doc_id": 53,
        "title": "An adaptive network-based approach for advanced forecasting of cryptocurrency values",
        "authors": [
            "Ali Mehrban",
            "Pegah Ahadian"
        ],
        "subjects": [
            "Statistical Finance",
            "Computational Engineering, Finance, and Science",
            "Cryptography and Security",
            "Machine Learning"
        ],
        "abstract": "This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time.",
        "comments": "11 pages",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05441"
    },
    {
        "doc_id": 54,
        "title": "Multi-relational Graph Diffusion Neural Network with Parallel Retention for Stock Trends Classification",
        "authors": [
            "Zinuo You",
            "Pengju Zhang",
            "Jin Zheng",
            "John Cartlidge"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning",
            "Neural and Evolutionary Computing"
        ],
        "abstract": "Stock trend classification remains a fundamental yet challenging task, owing to the intricate time-evolving dynamics between and within stocks. To tackle these two challenges, we propose a graph-based representation learning approach aimed at predicting the future movements of multiple stocks. Initially, we model the complex time-varying relationships between stocks by generating dynamic multi-relational stock graphs. This is achieved through a novel edge generation algorithm that leverages information entropy and signal energy to quantify the intensity and directionality of inter-stock relations on each trading day. Then, we further refine these initial graphs through a stochastic multi-relational diffusion process, adaptively learning task-optimal edges. Subsequently, we implement a decoupled representation learning scheme with parallel retention to obtain the final graph representation. This strategy better captures the unique temporal features within individual stocks while also capturing the overall structure of the stock graph. Comprehensive experiments conducted on real-world datasets from two US markets (NASDAQ and NYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the effectiveness of our method. Our approach consistently outperforms state-of-the-art baselines in forecasting next trading day stock trends across three test periods spanning seven years. Datasets and code have been released (https://github.com/pixelhero98/MGDPR).",
        "comments": "5 pages, 2 figures. Author manuscript accepted for ICASSP 2024 (IEEE International Conference on Acoustics, Speech and Signal Processing)",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05430"
    },
    {
        "doc_id": 55,
        "title": "Introduction of L0 norm and application of L1 and C1 norm in the study of time-series",
        "authors": [
            "Victor Ujaldon Garcia"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "Four markets are considered: Cryptocurrencies / South American exchange rate / Spanish Banking indices and European Indices and studied using TDA (Topological Data Analysis) tools. These tools are used to predict and showcase both strengths and weakness of the current TDA tools. In this paper a new tool $L0$ norm is defined and complemented with the already existing $C1$ norm.",
        "comments": "14 pages 8 figures",
        "date": "30 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05423"
    },
    {
        "doc_id": 56,
        "title": "Multiple-bubble testing in the cryptocurrency market: a case study of bitcoin",
        "authors": [
            "Sanaz Behzadi",
            "Mahmonir Bayanati",
            "Hamed Nozari"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "Economic periods and financial crises have highlighted the importance of evaluating financial markets to investors and researchers in recent decades.",
        "comments": " ",
        "date": "29 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05417"
    },
    {
        "doc_id": 57,
        "title": "On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors",
        "authors": [
            "Xinshuai Dong",
            "Haoyue Dai",
            "Yewen Fan",
            "Songyao Jin",
            "Sathyamoorthy Rajendran",
            "Kun Zhang"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning",
            "Methodology"
        ],
        "abstract": "Financial data is generally time series in essence and thus suffers from three fundamental issues: the mismatch in time resolution, the time-varying property of the distribution - nonstationarity, and causal factors that are important but unknown/unobserved. In this paper, we follow a causal perspective to systematically look into these three demons in finance. Specifically, we reexamine these issues in the context of causality, which gives rise to a novel and inspiring understanding of how the issues can be addressed. Following this perspective, we provide systematic solutions to these problems, which hopefully would serve as a foundation for future research in the area.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05414"
    },
    {
        "doc_id": 58,
        "title": "RIVCoin: an alternative, integrated, CeFi/DeFi-Vaulted Cryptocurrency",
        "authors": [
            "Roberto Rivera",
            "Guido Rocco",
            "Massimiliano Marzo",
            "Enrico Talin"
        ],
        "subjects": [
            "General Finance",
            "General Economics"
        ],
        "abstract": "This whitepaper introduces RIVCoin, a cryptocurrency built on Cosmos, fully stabilized by a diversified portfolio of both CeFi and DeFi assets, available in a digital, non-custodial wallet called RIV Wallet, that aims to provide Users an easy way to access the cryptocurrency markets, compliant to the strictest AML laws and regulations up to date. The token is a cryptocurrency at any time stabilized by a basket of assets: reserves are invested in a portfolio composed long term by 50% of CeFi assets, comprised of Fixed Income, Equity, Mutual and Hedge Funds and 50% of diversified strategies focused on digital assets, mainly staking and LP farming on the major, battle tested DeFi protocols. The cryptocurrency, as well as the dollar before Bretton Woods, is always fully stabilized by vaulted proof of assets: it is born and managed as a decentralized token, minted by a Decentralized Autonomous Organization, and entirely stabilized by assets evaluated by professional independent third parties. Users will trade, pool, and exchange the token without any intermediary, being able to merge them into a Liquidity Pool whose rewards will be composed by both the trading fees and the liquidity rewards derived from the reserve's seigniorage.\n  Users who wish and decide to pool RIVCoin in the Liquidity Pool will receive additional RIVCoin for themselves, and new RIVCoin are minted when the reserves increase in value or in case of purchase of new RIVCoin. The proposed model allows for alignment of incentives: decreasing the risk exposure by wealthier Users, but implicitly increasing that of smaller ones to a level perceived by them as still sustainable. Users indirectly benefit from the access to the rewards of sophisticated cryptocurrency portfolios hitherto precluded to them, without this turning into a disadvantage for the wealthy User.",
        "comments": " ",
        "date": "19 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05393"
    },
    {
        "doc_id": 59,
        "title": "Optimal Linear Signal: An Unsupervised Machine Learning Framework to Optimize PnL with Linear Signals",
        "authors": [
            "Pierre Renucci"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "This study presents an unsupervised machine learning approach for optimizing Profit and Loss (PnL) in quantitative finance. Our algorithm, akin to an unsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL generated from signals constructed linearly from exogenous variables. The methodology employs a linear relationship between exogenous variables and the trading signal, with the objective of maximizing the Sharpe Ratio through parameter optimization. Empirical application on an ETF representing U.S. Treasury bonds demonstrates the model's effectiveness, supported by regularization techniques to mitigate overfitting. The study concludes with potential avenues for further development, including generalized time steps and enhanced corrective terms.",
        "comments": "The code of the model and the empiric strategy are available on my GitHub: Cnernc/OptimalLinearSignal",
        "date": "22 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05337"
    },
    {
        "doc_id": 60,
        "title": "Comparison of Markowitz Model and Single-Index Model on Portfolio Selection of Malaysian Stocks",
        "authors": [
            "Zhang Chern Lee",
            "Wei Yun Tan",
            "Hoong Khen Koo",
            "Wilson Pang"
        ],
        "subjects": [
            "Portfolio Management"
        ],
        "abstract": "Our article is focused on the application of Markowitz Portfolio Theory and the Single Index Model on 10-year historical monthly return data for 10 stocks included in FTSE Bursa Malaysia KLCI, which is also our market index, as well as a risk-free asset which is the monthly fixed deposit rate. We will calculate the minimum variance portfolio and maximum Sharpe portfolio for both the Markowitz model and Single Index model subject to five different constraints, with the results presented in the form of tables and graphs such that comparisons between the different models and constraints can be made. We hope this article will help provide useful information for future investors who are interested in the Malaysian stock market and would like to construct an efficient investment portfolio. Keywords: Markowitz Portfolio Theory, Single Index Model, FTSE Bursa Malaysia KLCI, Efficient Portfolio",
        "comments": "19 pages, 5 figures",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05264"
    },
    {
        "doc_id": 61,
        "title": "A Mean Field Game between Informed Traders and a Broker",
        "authors": [
            "Philippe Bergault",
            "Leandro S\u00e1nchez-Betancourt"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Optimization and Control"
        ],
        "abstract": "We find closed-form solutions to the stochastic game between a broker and a mean-field of informed traders. In the finite player game, the informed traders observe a common signal and a private signal. The broker, on the other hand, observes the trading speed of each of his clients and provides liquidity to the informed traders. Each player in the game optimises wealth adjusted by inventory penalties. In the mean field version of the game, using a G\u00e2teaux derivative approach, we characterise the solution to the game with a system of forward-backward stochastic differential equations that we solve explicitly. We find that the optimal trading strategy of the broker is linear on his own inventory, on the average inventory among informed traders, and on the common signal or the average trading speed of the informed traders. The Nash equilibrium we find helps informed traders decide how to use private information, and helps brokers decide how much of the order flow they should externalise or internalise when facing a large number of clients.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05257"
    },
    {
        "doc_id": 62,
        "title": "On the Martingale Schr\u00f6dinger Bridge between Two Distributions",
        "authors": [
            "Marcel Nutz",
            "Johannes Wiesel"
        ],
        "subjects": [
            "Probability",
            "Mathematical Finance"
        ],
        "abstract": "We study a martingale Schr\u00f6dinger bridge problem: given two probability distributions, find their martingale coupling with minimal relative entropy. Our main result provides Schr\u00f6dinger potentials for this coupling. Namely, under certain conditions, the log-density of the optimal coupling is given by a triplet of real functions representing the marginal and martingale constraints. The potentials are also described as the solution of a dual problem.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05209"
    },
    {
        "doc_id": 63,
        "title": "Markowitz Portfolio Construction at Seventy",
        "authors": [
            "Stephen Boyd",
            "Kasper Johansson",
            "Ronald Kahn",
            "Philipp Schiele",
            "Thomas Schmelzer"
        ],
        "subjects": [
            "Portfolio Management",
            "Optimization and Control"
        ],
        "abstract": "More than seventy years ago Harry Markowitz formulated portfolio construction as an optimization problem that trades off expected return and risk, defined as the standard deviation of the portfolio returns. Since then the method has been extended to include many practical constraints and objective terms, such as transaction cost or leverage limits. Despite several criticisms of Markowitz's method, for example its sensitivity to poor forecasts of the return statistics, it has become the dominant quantitative method for portfolio construction in practice. In this article we describe an extension of Markowitz's method that addresses many practical effects and gracefully handles the uncertainty inherent in return statistics forecasting. Like Markowitz's original formulation, the extension is also a convex optimization problem, which can be solved with high reliability and speed.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05080"
    },
    {
        "doc_id": 64,
        "title": "Scaling Laws And Statistical Properties of The Transaction Flows And Holding Times of Bitcoin",
        "authors": [
            "Didier Sornette",
            "Yu Zhang"
        ],
        "subjects": [
            "Trading and Market Microstructure"
        ],
        "abstract": "We study the temporal evolution of the holding-time distribution of bitcoins and find that the average distribution of holding-time is a heavy-tailed power law extending from one day to over at least $200$ weeks with an exponent approximately equal to $0.9$, indicating very long memory effects. We also report significant sample-to-sample variations of the distribution of holding times, which can be best characterized as multiscaling, with power-law exponents varying between $0.3$ and $2.5$ depending on bitcoin price regimes. We document significant differences between the distributions of book-to-market and of realized returns, showing that traders obtain far from optimal performance. We also report strong direct qualitative and quantitative evidence of the disposition effect in the Bitcoin Blockchain data. Defining age-dependent transaction flows as the fraction of bitcoins that are traded at a given time and that were born (last traded) at some specific earlier time, we document that the time-averaged transaction flow fraction has a power law dependence as a function of age, with an exponent close to $-1.5$, a value compatible with priority queuing theory. We document the existence of multifractality on the measure defined as the normalized number of bitcoins exchanged at a given time.",
        "comments": " ",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04702"
    },
    {
        "doc_id": 65,
        "title": "Proof of Efficient Liquidity: A Staking Mechanism for Capital Efficient Liquidity",
        "authors": [
            "Arman Abgaryan",
            "Utkarsh Sharma",
            "Joshua Tobkin"
        ],
        "subjects": [
            "General Finance"
        ],
        "abstract": "The Proof of Efficient Liquidity (PoEL) protocol, designed for specialised Proof of Stake (PoS) consensus-based blockchain infrastructures that incorporate intrinsic DeFi applications, aims to support sustainable liquidity bootstrapping and network security. This innovative mechanism efficiently utilises budgeted staking rewards to attract and sustain liquidity through a risk structuring engine and incentive allocation strategy, both of which are designed to maximise capital efficiency. The proposed protocol seeks to serve the dual objective of - (i) capital creation, by efficiently attracting risk capital, and maximising its operational utility for intrinsic DeFi applications, thereby asserting sustainability; and (ii) enhancing the adopting blockchain network's economic security, by augmenting their staking (PoS) mechanism with a harmonious layer seeking to attract a diversity of digital assets. Finally, in the appendix, we seek to generalise the financial incentivisation protocol to the notion of service fee credits, such that it utilises the network's auxiliary services as a means to propagate incentives to attract liquidity and facilitate the network to achieve the critical mass of usage necessary for sustained operations and growth.",
        "comments": " ",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04521"
    },
    {
        "doc_id": 66,
        "title": "Computing the Gerber-Shiu function with interest and a constant dividend barrier by physics-informed neural networks",
        "authors": [
            "Zan Yu",
            "Lianzeng Zhang"
        ],
        "subjects": [
            "Numerical Analysis",
            "Probability",
            "Risk Management"
        ],
        "abstract": "In this paper, we propose a new efficient method for calculating the Gerber-Shiu discounted penalty function. Generally, the Gerber-Shiu function usually satisfies a class of integro-differential equation. We introduce the physics-informed neural networks (PINN) which embed a differential equation into the loss of the neural network using automatic differentiation. In addition, PINN is more free to set boundary conditions and does not rely on the determination of the initial value. This gives us an idea to calculate more general Gerber-Shiu functions. Numerical examples are provided to illustrate the very good performance of our approximation.",
        "comments": "23 pages; 5 figures",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04378"
    },
    {
        "doc_id": 67,
        "title": "Expiring Assets in Automated Market Makers",
        "authors": [
            "Kenan Wood",
            "Maurice Herlihy",
            "Hammurabi Mendes",
            "Jonad Pulaj"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Distributed, Parallel, and Cluster Computing",
            "Mathematical Finance",
            "Trading and Market Microstructure"
        ],
        "abstract": "An automated market maker (AMM) is a state machine that manages pools of assets, allowing parties to buy and sell those assets according to a fixed mathematical formula. AMMs are typically implemented as smart contracts on blockchains, and its prices are kept in line with the overall market price by arbitrage: if the AMM undervalues an asset with respect to the market, an \"arbitrageur\" can make a risk-free profit by buying just enough of that asset to bring the AMM's price back in line with the market.\n  AMMs, however, are not designed for assets that expire: that is, assets that cannot be produced or resold after a specified date. As assets approach expiration, arbitrage may not be able to reconcile supply and demand, and the liquidity providers that funded the AMM may have excessive exposure to risk due to rapid price variations.\n  This paper formally describes the design of a decentralized exchange (DEX) for assets that expire, combining aspects of AMMs and limit-order books. We ensure liveness and market clearance, providing mechanisms for liquidity providers to control their exposure to risk and adjust prices dynamically in response to situations where arbitrage may fail.",
        "comments": "33 pages",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04289"
    },
    {
        "doc_id": 68,
        "title": "Economic Forces in Stock Returns",
        "authors": [
            "Yue Chen",
            "Mohan Li"
        ],
        "subjects": [
            "General Economics",
            "Statistical Finance"
        ],
        "abstract": "When analyzing the components influencing the stock prices, it is commonly believed that economic activities play an important role. More specifically, asset prices are more sensitive to the systematic economic news that impose a pervasive effect on the whole market. Moreover, the investors will not be rewarded for bearing idiosyncratic risks as such risks are diversifiable. In the paper Economic Forces and the Stock Market 1986, the authors introduced an attribution model to identify the specific systematic economic forces influencing the market. They first defined and examined five classic factors from previous research papers: Industrial Production, Unanticipated Inflation, Change in Expected Inflation, Risk Premia, and The Term Structure. By adding in new factors, the Market Indices, Consumptions and Oil Prices, one by one, they examined the significant contribution of each factor to the stock return. The paper concluded that the stock returns are exposed to the systematic economic news, and they are priced with respect to their risk exposure. Also, the significant factors can be identified by simply adopting their model. Driven by such motivation, we conduct an attribution analysis based on the general framework of their model to further prove the importance of the economic factors and identify the specific identity of significant factors.",
        "comments": "11 pages, 10 figures",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04132"
    },
    {
        "doc_id": 69,
        "title": "Decomposing Smiles: A Time Change Approach",
        "authors": [
            "Liexin Cheng",
            "Xue Cheng"
        ],
        "subjects": [
            "Pricing of Securities",
            "Mathematical Finance"
        ],
        "abstract": "We develop a novel time-change approach to study the shape of implied volatility smiles. The method is applicable to common semimartingale models, including jump-diffusion, rough volatility and infinite activity models. We approximate the at-the-money skew and curvature with an improved moment-based formula. The moments are further explicitly computed under a time change framework. The limiting skew and curvature for several models are considered. We also test the accuracy of the short-term approximation results on models via numerical methods and on empirical data. Finally, we apply the method to the calibration problem.",
        "comments": " ",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03776"
    },
    {
        "doc_id": 70,
        "title": "Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection",
        "authors": [
            "Georgios Fatouros",
            "Konstantinos Metaxas",
            "John Soldatos",
            "Dimosthenis Kyriazis"
        ],
        "subjects": [
            "Computational Finance",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Computation and Language",
            "Machine Learning"
        ],
        "abstract": "In the dynamic and data-driven landscape of financial markets, this paper introduces MarketSenseAI, a novel AI-driven framework leveraging the advanced reasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI incorporates Chain of Thought and In-Context Learning methodologies to analyze a wide array of data sources, including market price dynamics, financial news, company fundamentals, and macroeconomic reports emulating the decision making process of prominent financial investment teams. The development, implementation, and empirical validation of MarketSenseAI are detailed, with a focus on its ability to provide actionable investment signals (buy, hold, sell) backed by cogent explanations. A notable aspect of this study is the use of GPT-4 not only as a predictive tool but also as an evaluator, revealing the significant impact of the AI-generated explanations on the reliability and acceptance of the suggested investment signals. In an extensive empirical evaluation with S&P 100 stocks, MarketSenseAI outperformed the benchmark index by 13%, achieving returns up to 40%, while maintaining a risk profile comparable to the market. These results demonstrate the efficacy of Large Language Models in complex financial decision-making and mark a significant advancement in the integration of AI into financial analysis and investment strategies. This research contributes to the financial AI field, presenting an innovative approach and underscoring the transformative potential of AI in revolutionizing traditional financial analysis investment methodologies.",
        "comments": "15 pages, 12 figures, 12 tables",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03737"
    },
    {
        "doc_id": 71,
        "title": "Structured factor copulas for modeling the systemic risk of European and United States banks",
        "authors": [
            "Hoang Nguyen",
            "Audron\u0117 Virbickait\u0117",
            "M. Concepci\u00f3n Aus\u00edn",
            "Pedro Galeano"
        ],
        "subjects": [
            "Statistical Finance",
            "Applications"
        ],
        "abstract": "In this paper, we employ Credit Default Swaps (CDS) to model the joint and conditional distress probabilities of banks in Europe and the U.S. using factor copulas. We propose multi-factor, structured factor, and factor-vine models where the banks in the sample are clustered according to their geographic location. We find that within each region, the co-dependence between banks is best described using both, systematic and idiosyncratic, financial contagion channels. However, if we consider the banking system as a whole, then the systematic contagion channel prevails, meaning that the distress probabilities are driven by a latent global factor and region-specific factors. In all cases, the co-dependence structure of bank CDS spreads is highly correlated in the tail. The out-of-sample forecasts of several measures of systematic risk allow us to identify the periods of distress in the banking sector over the recent years including the COVID-19 pandemic, the interest rate hikes in 2022, and the banking crisis in 2023.",
        "comments": " ",
        "date": "7 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03443"
    },
    {
        "doc_id": 72,
        "title": "Modelling and Predicting the Conditional Variance of Bitcoin Daily Returns: Comparsion of Markov Switching GARCH and SV Models",
        "authors": [
            "Dennis Koch",
            "Vahidin Jeleskovic",
            "Zahid I. Younas"
        ],
        "subjects": [
            "Statistical Finance",
            "Risk Management"
        ],
        "abstract": "This paper introduces a unique and valuable research design aimed at analyzing Bitcoin price volatility. To achieve this, a range of models from the Markov Switching-GARCH and Stochastic Autoregressive Volatility (SARV) model classes are considered and their out-of-sample forecasting performance is thoroughly examined. The paper provides insights into the rationale behind the recommendation for a two-stage estimation approach, emphasizing the separate estimation of coefficients in the mean and variance equations. The results presented in this paper indicate that Stochastic Volatility models, particularly SARV models, outperform MS-GARCH models in forecasting Bitcoin price volatility. Moreover, the study suggests that in certain situations, persistent simple GARCH models may even outperform Markov-Switching GARCH models in predicting the variance of Bitcoin log returns. These findings offer valuable guidance for risk management experts, highlighting the potential advantages of SARV models in managing and forecasting Bitcoin price volatility.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03393"
    },
    {
        "doc_id": 73,
        "title": "Volatility models in practice: Rough, Path-dependent or Markovian?",
        "authors": [
            "Eduardo Abi Jaber",
            "Shaun",
            "Li"
        ],
        "subjects": [
            "Mathematical Finance",
            "Computational Finance",
            "Pricing of Securities"
        ],
        "abstract": "An extensive empirical study of the class of Volterra Bergomi models using SPX options data between 2011 and 2022 reveals the following fact-check on two fundamental claims echoed in the rough volatility literature:\n  Do rough volatility models with Hurst index $H \\in (0,1/2)$ really capture well SPX implied volatility surface with very few parameters? No, rough volatility models are inconsistent with the global shape of SPX smiles. They suffer from severe structural limitations imposed by the roughness component, with the Hurst parameter $H \\in (0,1/2)$ controlling the smile in a poor way. In particular, the SPX at-the-money skew is incompatible with the power-law shape generated by rough volatility models. The skew of rough volatility models increases too fast on the short end, and decays too slow on the longer end where \"negative\" $H$ is sometimes needed.\n  Do rough volatility models really outperform consistently their classical Markovian counterparts? No, for short maturities they underperform their one-factor Markovian counterpart with the same number of parameters. For longer maturities, they do not systematically outperform the one-factor model and significantly underperform when compared to an under-parametrized two-factor Markovian model with only one additional calibratable parameter.\n  On the positive side: our study identifies a (non-rough) path-dependent Bergomi model and an under-parametrized two-factor Markovian Bergomi model that consistently outperform their rough counterpart in capturing SPX smiles between one week and three years with only 3 to 4 calibratable parameters. \\end{abstract}",
        "comments": " ",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03345"
    },
    {
        "doc_id": 74,
        "title": "Negatively dependent optimal risk sharing",
        "authors": [
            "Jean-Gabriel Lauzier",
            "Liyuan Lin",
            "Ruodu Wang"
        ],
        "subjects": [
            "Theoretical Economics",
            "Risk Management"
        ],
        "abstract": "We analyze the problem of optimally sharing risk using allocations that exhibit counter-monotonicity, the most extreme form of negative dependence. Counter-monotonic allocations take the form of either \"winner-takes-all\" lotteries or \"loser-loses-all\" lotteries, and we respectively refer to these (normalized) cases as jackpot or scapegoat allocations. Our main theorem, the counter-monotonic improvement theorem, states that for a given set of random variables that are either all bounded from below or all bounded from above, one can always find a set of counter-monotonic random variables such that each component is greater or equal than its counterpart in the convex order. We show that Pareto optimal allocations, if they exist, must be jackpot allocations when all agents are risk seeking. We essentially obtain the opposite when all agents have discontinuous Bernoulli utility functions, as scapegoat allocations maximize the probability of being above the discontinuity threshold. We also consider the case of rank-dependent expected utility (RDU) agents and find conditions which guarantee that RDU agents prefer jackpot allocations. We provide an application for the mining of cryptocurrencies and show that in contrast to risk-averse miners, RDU miners with small computing power never join a mining pool. Finally, we characterize the competitive equilibria with risk-seeking agents, providing a first and second fundamental theorem of welfare economics where all equilibrium allocations are jackpot allocations.",
        "comments": "35 pages, 1 figure, Keywords: Pareto optimality, Risk sharing, Counter-monotonicity, Risk seeking, Rank-dependent expected utility, Cryptocurrency mining pools",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03328"
    },
    {
        "doc_id": 75,
        "title": "Optimal Order Execution subject to Reservation Strategies under Execution Risk",
        "authors": [
            "Xue Cheng",
            "Peng Guo",
            "Tai-ho Wang"
        ],
        "subjects": [
            "Trading and Market Microstructure"
        ],
        "abstract": "The paper addresses the problem of meta order execution from a broker-dealer's point of view in Almgren-Chriss model under order fill uncertainty. A broker-dealer agency is authorized to execute an order of trading on client's behalf. The strategies that the agent is allowed to deploy is subject to a benchmark, referred to as the reservation strategy, regulated by the client. We formulate the broker's problem as a utility maximization problem in which the broker seeks to maximize his utility of excess profit-and-loss at the execution horizon. Optimal strategy in feedback form is obtained in closed form. In the absence of execution risk, the optimal strategies subject to reservation strategies are deterministic. We establish an affine structure among the trading trajectories under optimal strategies subject to general reservation strategies using implementation shortfall and target close orders as basis. We conclude the paper with numerical experiments illustrating the trading trajectories as well as histograms of terminal wealth and utility at investment horizon under optimal strategies versus those under TWAP strategies.",
        "comments": " ",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03305"
    },
    {
        "doc_id": 76,
        "title": "Synergistic Formulaic Alpha Generation for Quantitative Trading based on Reinforcement Learning",
        "authors": [
            "Hong-Gi Shin",
            "Sukhyun Jeong",
            "Eui-Yeon Kim",
            "Sungho Hong",
            "Young-Jin Cho",
            "Yong-Hoon Choi"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Artificial Intelligence"
        ],
        "abstract": "Mining of formulaic alpha factors refers to the process of discovering and developing specific factors or indicators (referred to as alpha factors) for quantitative trading in stock market. To efficiently discover alpha factors in vast search space, reinforcement learning (RL) is commonly employed. This paper proposes a method to enhance existing alpha factor mining approaches by expanding a search space and utilizing pretrained formulaic alpha set as initial seed values to generate synergistic formulaic alpha. We employ information coefficient (IC) and rank information coefficient (Rank IC) as performance evaluation metrics for the model. Using CSI300 market data, we conducted real investment simulations and observed significant performance improvement compared to existing techniques.",
        "comments": "Accepted by ICOIN 2024",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02710"
    },
    {
        "doc_id": 77,
        "title": "Displaying risk in mergers: a diagrammatic approach for exchange ratio determination",
        "authors": [
            "Alessandra Mainini",
            "Enrico Moretto",
            "Daniela Visetti"
        ],
        "subjects": [
            "General Finance"
        ],
        "abstract": "This article extends, in a stochastic setting, previous results in the determination of feasible exchange ratios for merging companies. A first outcome is that shareholders of the companies involved in the merging process face both an upper and a lower bounds for acceptable exchange ratios. Secondly, in order for the improved `bargaining region' to be intelligibly displayed, the diagrammatic approach developed by Kulpa is exploited.",
        "comments": " ",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02681"
    },
    {
        "doc_id": 78,
        "title": "Constrained Max Drawdown: a Fast and Robust Portfolio Optimization Approach",
        "authors": [
            "Albert Dorador"
        ],
        "subjects": [
            "Portfolio Management",
            "Optimization and Control"
        ],
        "abstract": "We propose an alternative linearization to the classical Markowitz quadratic portfolio optimization model, based on maximum drawdown. This model, which minimizes maximum portfolio drawdown, is particularly appealing during times of financial distress, like during the COVID-19 pandemic. In addition, we will present a Mixed-Integer Linear Programming variation of our new model that, based on our out-of-sample results and sensitivity analysis, delivers a more profitable and robust solution with a 200 times faster solving time compared to the standard Markowitz quadratic formulation.",
        "comments": " ",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02601"
    },
    {
        "doc_id": 79,
        "title": "Opinion formation in the world trade network",
        "authors": [
            "C\u00e9lestin Coquid\u00e9",
            "Jos\u00e9 Lages",
            "Dima L. Shepelyansky"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Statistical Mechanics",
            "Social and Information Networks",
            "Physics and Society"
        ],
        "abstract": "We extend the opinion formation approach to probe the world influence of economical organizations. Our opinion formation model mimics a battle between currencies within the international trade network. Based on the United Nations Comtrade database, we construct the world trade network for the years of the last decade from 2010 to 2020. We consider different core groups constituted by countries preferring to trade in a specific currency. We will consider principally two core groups, namely, 5 Anglo-Saxon countries which prefer to trade in US dollar and the 11 BRICS+ which prefer to trade in a hypothetical currency, hereafter called BRI, pegged to their economies. We determine the trade currency preference of the other countries via a Monte Carlo process depending on the direct transactions between the countries. The results obtained in the frame of this mathematical model show that starting from year 2014 the majority of the world countries would have preferred to trade in BRI than USD. The Monte Carlo process reaches a steady state with 3 distinct groups: two groups of countries preferring, whatever is the initial distribution of the trade currency preferences, to trade, one in BRI and the other in USD, and a third group of countries swinging as a whole between USD and BRI depending on the initial distribution of the trade currency preferences. We also analyze the battle between USD, EUR and BRI, and present the reduced Google matrix description of the trade relations between the Anglo-Saxon countries and the BRICS+.",
        "comments": "16 pages, 19 figures (including 9 figures present in Appendix section) and 1 table",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02378"
    },
    {
        "doc_id": 80,
        "title": "ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach",
        "authors": [
            "Zeynep Hilal Kilimci",
            "Mustafa Yalcin"
        ],
        "subjects": [
            "Biomolecules",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "Anticancer peptides (ACPs) are a class of molecules that have gained significant attention in the field of cancer research and therapy. ACPs are short chains of amino acids, the building blocks of proteins, and they possess the ability to selectively target and kill cancer cells. One of the key advantages of ACPs is their ability to selectively target cancer cells while sparing healthy cells to a greater extent. This selectivity is often attributed to differences in the surface properties of cancer cells compared to normal cells. That is why ACPs are being investigated as potential candidates for cancer therapy. ACPs may be used alone or in combination with other treatment modalities like chemotherapy and radiation therapy. While ACPs hold promise as a novel approach to cancer treatment, there are challenges to overcome, including optimizing their stability, improving selectivity, and enhancing their delivery to cancer cells, continuous increasing in number of peptide sequences, developing a reliable and precise prediction model. In this work, we propose an efficient transformer-based framework to identify anticancer peptides for by performing accurate a reliable and precise prediction model. For this purpose, four different transformer models, namely ESM, ProtBert, BioBERT, and SciBERT are employed to detect anticancer peptides from amino acid sequences. To demonstrate the contribution of the proposed framework, extensive experiments are carried on widely-used datasets in the literature, two versions of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of proposed model enhances classification accuracy when compared to the state-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of accuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and 88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.",
        "comments": " ",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02124"
    },
    {
        "doc_id": 81,
        "title": "Forecasting Bitcoin Volatility: A Comparative Analysis of Volatility Approaches",
        "authors": [
            "Cristina Chinazzo",
            "Vahidin Jeleskovic"
        ],
        "subjects": [
            "Trading and Market Microstructure"
        ],
        "abstract": "This paper conducts an extensive analysis of Bitcoin return series, with a primary focus on three volatility metrics: historical volatility (calculated as the sample standard deviation), forecasted volatility (derived from GARCH-type models), and implied volatility (computed from the emerging Bitcoin options market). These measures of volatility serve as indicators of market expectations for conditional volatility and are compared to elucidate their differences and similarities. The central finding of this study underscores a notably high expected level of volatility, both on a daily and annual basis, across all the methodologies employed. However, it's crucial to emphasize the potential challenges stemming from suboptimal liquidity in the Bitcoin options market. These liquidity constraints may lead to discrepancies in the computed values of implied volatility, particularly in scenarios involving extreme moneyness or maturity. This analysis provides valuable insights into Bitcoin's volatility landscape, shedding light on the unique characteristics and dynamics of this cryptocurrency within the context of financial markets.",
        "comments": " ",
        "date": "3 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02049"
    },
    {
        "doc_id": 82,
        "title": "Notes on the SWIFT method based on Shannon Wavelets for Option Pricing -- Revisited",
        "authors": [
            "Fabien Le Floc'h"
        ],
        "subjects": [
            "Computational Finance",
            "Numerical Analysis"
        ],
        "abstract": "This note revisits the SWIFT method based on Shannon wavelets to price European options under models with a known characteristic function in 2023. In particular, it discusses some possible improvements and exposes some concrete drawbacks of the method.",
        "comments": " ",
        "date": "7 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01758"
    },
    {
        "doc_id": 83,
        "title": "Text mining arXiv: a look through quantitative finance papers",
        "authors": [
            "Michele Leonardo Bianchi"
        ],
        "subjects": [
            "Digital Libraries",
            "Information Retrieval",
            "General Finance"
        ],
        "abstract": "This paper explores articles hosted on the arXiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research. Employing text mining techniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arXiv from 1997 to 2022. We extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain. Additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches.",
        "comments": " ",
        "date": "3 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01751"
    },
    {
        "doc_id": 84,
        "title": "Non-Atomic Arbitrage in Decentralized Finance",
        "authors": [
            "Lioba Heimbach",
            "Vabuk Pahari",
            "Eric Schertenleib"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "General Finance"
        ],
        "abstract": "The prevalence of maximal extractable value (MEV) in the Ethereum ecosystem has led to a characterization of the latter as a dark forest. Studies of MEV have thus far largely been restricted to purely on-chain MEV, i.e., sandwich attacks, cyclic arbitrage, and liquidations. In this work, we shed light on the prevalence of non-atomic arbitrage on decentralized exchanges (DEXes) on the Ethereum blockchain. Importantly, non-atomic arbitrage exploits price differences between DEXes on the Ethereum blockchain as well as exchanges outside the Ethereum blockchain (i.e., centralized exchanges or DEXes on other blockchains). Thus, non-atomic arbitrage is a type of MEV that involves actions on and off the Ethereum blockchain.\n  In our study of non-atomic arbitrage, we uncover that more than a fourth of the volume on Ethereum's biggest five DEXes from the merge until 31 October 2023 can likely be attributed to this type of MEV. We further highlight that only eleven searchers are responsible for more than 80% of the identified non-atomic arbitrage volume sitting at a staggering 137 billion US$ and draw a connection between the centralization of the block construction market and non-atomic arbitrage. Finally, we discuss the security implications of these high-value transactions that account for more than 10% of Ethereum's total block value and outline possible mitigations.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01622"
    },
    {
        "doc_id": 85,
        "title": "An arbitrage driven price dynamics of Automated Market Makers in the presence of fees",
        "authors": [
            "Joseph Najnudel",
            "Shen-Ning Tung",
            "Kazutoshi Yamazaki",
            "Ju-Yi Yen"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "We present a model for price dynamics in the Automated Market Makers (AMM) setting. Within this framework, we propose a reference market price following a geometric Brownian motion. The AMM price is constrained by upper and lower bounds, determined by constant multiplications of the reference price. Through the utilization of local times and excursion-theoretic approaches, we derive several analytical results, including its time-changed representation and limiting behavior.",
        "comments": " ",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01526"
    },
    {
        "doc_id": 86,
        "title": "Nash Equilibria in Greenhouse Gas Offset Credit Markets",
        "authors": [
            "Liam Welsh",
            "Sebastian Jaimungal"
        ],
        "subjects": [
            "General Finance",
            "Computational Finance",
            "Risk Management"
        ],
        "abstract": "In response to the global climate crisis, governments worldwide are introducing legislation to reduce greenhouse gas (GHG) emissions to help mitigate environmental catastrophes. One method to encourage emission reductions is to incentivize carbon capturing and carbon reducing projects while simultaneously penalising excess GHG output. Firms that invest in carbon capturing projects or reduce their emissions can receive offset credits (OCs) in return. These OCs can be used for regulatory purposes to offset their excess emissions in a compliance period. OCs may also be traded between firms. Thus, firms have the choice between investing in projects to generate OCs or to trade OCs. In this work, we present a novel market framework and characterise the optimal behaviour of GHG OC market participants in both single-player and two-player settings. We analyse both a single-period and multi-period setting. As the market model does not elicit a closed form solution, we develop a numerical methodology to estimate players' optimal behaviours in accordance to the Nash equilibria. Our findings indicate the actions players take are dependent on the scale of their project opportunities as well as their fellow market participants. We demonstrate the importance of behaving optimally via simulations in order to offset emission penalties and the importance of investing in GHG reducing or capturing projects from a financial perspective.",
        "comments": "MSC Class:          91G99; 35Q91; 91-08; 91A80; 91B74",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01427"
    },
    {
        "doc_id": 87,
        "title": "Accelerating Black-Box Molecular Property Optimization by Adaptively Learning Sparse Subspaces",
        "authors": [
            "Farshud Sorourifar",
            "Thomas Banker",
            "Joel A. Paulson"
        ],
        "subjects": [
            "Biomolecules",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "Molecular property optimization (MPO) problems are inherently challenging since they are formulated over discrete, unstructured spaces and the labeling process involves expensive simulations or experiments, which fundamentally limits the amount of available data. Bayesian optimization (BO) is a powerful and popular framework for efficient optimization of noisy, black-box objective functions (e.g., measured property values), thus is a potentially attractive framework for MPO. To apply BO to MPO problems, one must select a structured molecular representation that enables construction of a probabilistic surrogate model. Many molecular representations have been developed, however, they are all high-dimensional, which introduces important challenges in the BO process -- mainly because the curse of dimensionality makes it difficult to define and perform inference over a suitable class of surrogate models. This challenge has been recently addressed by learning a lower-dimensional encoding of a SMILE or graph representation of a molecule in an unsupervised manner and then performing BO in the encoded space. In this work, we show that such methods have a tendency to \"get stuck,\" which we hypothesize occurs since the mapping from the encoded space to property values is not necessarily well-modeled by a Gaussian process. We argue for an alternative approach that combines numerical molecular descriptors with a sparse axis-aligned Gaussian process model, which is capable of rapidly identifying sparse subspaces that are most relevant to modeling the unknown property function. We demonstrate that our proposed method substantially outperforms existing MPO methods on a variety of benchmark and real-world problems. Specifically, we show that our method can routinely find near-optimal molecules out of a set of more than $>100$k alternatives within 100 or fewer expensive queries.",
        "comments": "9 pages, 2 figures consisting of 6 and 4 plots, accepted to NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01398"
    },
    {
        "doc_id": 88,
        "title": "Almost Perfect Shadow Prices",
        "authors": [
            "Eberhard Mayerhofer"
        ],
        "subjects": [
            "Portfolio Management",
            "Optimization and Control",
            "Probability"
        ],
        "abstract": "Shadow prices simplify the derivation of optimal trading strategies in markets with transaction costs by transferring optimization into a more tractable, frictionless market. This paper establishes that a na\u00efve shadow price Ansatz for maximizing long term returns given average volatility yields a strategy that is, for small bid-ask-spreads, asymptotically optimal at third order. Considering the second-order impact of transaction costs, such a strategy is essentially optimal. However, for risk aversion different from one, we devise alternative strategies that outperform the shadow market at fourth order. Finally, it is shown that the risk-neutral objective rules out the existence of shadow prices.",
        "comments": "15 pages",
        "date": "1 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.00970"
    },
    {
        "doc_id": 89,
        "title": "A Portfolio's Common Causal Conditional Risk-neutral PDE",
        "authors": [
            "Alejandro Rodriguez Dominguez"
        ],
        "subjects": [
            "Portfolio Management",
            "Mathematical Finance"
        ],
        "abstract": "Portfolio's optimal drivers for diversification are common causes of the constituents' correlations. A closed-form formula for the conditional probability of the portfolio given its optimal common drivers is presented, with each pair constituent-common driver joint distribution modelled by Gaussian copulas. A conditional risk-neutral PDE is obtained for this conditional probability as a system of copulas' PDEs, allowing for dynamical risk management of a portfolio as shown in the experiments. Implied conditional portfolio volatilities and implied weights are new risk metrics that can be dynamically monitored from the PDEs or obtained from their solution.",
        "comments": "6 pages, 4 figures, Mathematical and Statistical Methods for Actuarial Sciences and Finance - MAF2024",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.00949"
    },
    {
        "doc_id": 90,
        "title": "Intraday Trading Algorithm for Predicting Cryptocurrency Price Movements Using Twitter Big Data Analysis",
        "authors": [
            "Vahidin Jeleskovic",
            "Stephen Mackay"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "Cryptocurrencies have emerged as a novel financial asset garnering significant attention in recent years. A defining characteristic of these digital currencies is their pronounced short-term market volatility, primarily influenced by widespread sentiment polarization, particularly on social media platforms such as Twitter. Recent research has underscored the correlation between sentiment expressed in various networks and the price dynamics of cryptocurrencies. This study delves into the 15-minute impact of informative tweets disseminated through foundation channels on trader behavior, with a focus on potential outcomes related to sentiment polarization. The primary objective is to identify factors that can predict positive price movements and potentially be leveraged through a trading algorithm. To accomplish this objective, we conduct a conditional examination of return and excess return rates within the 15 minutes following tweet publication. The empirical findings reveal statistically significant increases in return rates, particularly within the initial three minutes following tweet publication. Notably, adverse effects resulting from the messages were not observed. Surprisingly, sentiments were found to have no discerni-ble impact on cryptocurrency price movements. Our analysis further identifies that inves-tors are primarily influenced by the quality of tweet content, as reflected in the choice of words and tweet volume. While the basic trading algorithm presented in this study does yield some benefits within the 15-minute timeframe, these benefits are not statistically significant. Nevertheless, it serves as a foundational framework for potential enhance-ments and further investigations.",
        "comments": " ",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00603"
    },
    {
        "doc_id": 91,
        "title": "On the implied volatility of Inverse and Quanto Inverse options under stochastic volatility models",
        "authors": [
            "Elisa Al\u00f2s",
            "Eulalia Nualart",
            "Makar Pravosud"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "In this paper we study short-time behavior of the at-the-money implied volatility for Inverse and Quanto Inverse European options with fixed strike price. The asset price is assumed to follow a general stochastic volatility process. Using techniques of the Malliavin calculus such as the anticipating Ito's formula we first compute the level of the implied volatility of the option when the maturity converges to zero. Then, we find a short maturity asymptotic formula for the skew of the implied volatility that depends on the roughness of the volatility model. We apply our general results to the SABR and fractional Bergomi models, and provide some numerical simulations that confirm the accurateness of the asymptotic formula for the skew.",
        "comments": "arXiv admin note: text overlap with arXiv:2308.15341, arXiv:2208.01353",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00539"
    },
    {
        "doc_id": 92,
        "title": "Financial Time-Series Forecasting: Towards Synergizing Performance And Interpretability Within a Hybrid Machine Learning Approach",
        "authors": [
            "Shun Liu",
            "Kexin Wu",
            "Chufeng Jiang",
            "Bin Huang",
            "Danqing Ma"
        ],
        "subjects": [
            "Machine Learning",
            "Statistical Finance"
        ],
        "abstract": "In the realm of cryptocurrency, the prediction of Bitcoin prices has garnered substantial attention due to its potential impact on financial markets and investment strategies. This paper propose a comparative study on hybrid machine learning algorithms and leverage on enhancing model interpretability. Specifically, linear regression(OLS, LASSO), long-short term memory(LSTM), decision tree regressors are introduced. Through the grounded experiments, we observe linear regressor achieves the best performance among candidate models. For the interpretability, we carry out a systematic overview on the preprocessing techniques of time-series statistics, including decomposition, auto-correlational function, exponential triple forecasting, which aim to excavate latent relations and complex patterns appeared in the financial time-series forecasting. We believe this work may derive more attention and inspire more researches in the realm of time-series analysis and its realistic applications.",
        "comments": " ",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00534"
    },
    {
        "doc_id": 93,
        "title": "Optimization of portfolios with cryptocurrencies: Markowitz and GARCH-Copula model approach",
        "authors": [
            "Vahidin Jeleskovic",
            "Claudio Latini",
            "Zahid I. Younas",
            "Mamdouh A. S. Al-Faryan"
        ],
        "subjects": [
            "Portfolio Management",
            "Applications"
        ],
        "abstract": "The growing interest in cryptocurrencies has drawn the attention of the financial world to this innovative medium of exchange. This study aims to explore the impact of cryptocurrencies on portfolio performance. We conduct our analysis retrospectively, assessing the performance achieved within a specific time frame by three distinct portfolios: one consisting solely of equities, bonds, and commodities; another composed exclusively of cryptocurrencies; and a third, which combines both 'traditional' assets and the best-performing cryptocurrency from the second portfolio.To achieve this, we employ the classic variance-covariance approach, utilizing the GARCH-Copula and GARCH-Vine Copula methods to calculate the risk structure. The optimal asset weights within the optimized portfolios are determined through the Markowitz optimization problem. Our analysis predominantly reveals that the portfolio comprising both cryptocurrency and traditional assets exhibits a higher Sharpe ratio from a retrospective viewpoint and demonstrates more stable performances from a prospective perspective. We also provide an explanation for our choice of portfolio optimization based on the Markowitz approach rather than CVaR and ES.",
        "comments": " ",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00507"
    },
    {
        "doc_id": 94,
        "title": "A framework for the valuation of insurance liabilities by production cost",
        "authors": [
            "Christoph Moehr"
        ],
        "subjects": [
            "Pricing of Securities"
        ],
        "abstract": "This paper sets out a framework for the valuation of insurance liabilities that is intended to be economically realistic, elementary, reasonably practically applicable, and as a special case to provide a basis for the valuation in regulatory solvency systems such as Solvency II and the SST. The valuation framework is based on the cost of producing the liabilities to an insurance company that is subject to solvency regulation (regulatory solvency capital requirements) and insolvency laws (consequences of failure) in finite discrete time. Starting from the replication approach of classical no-arbitrage theory, the framework additionally considers the nature and cost of capital (expressed by a ``financiability condition\"), that the liabilities may be required to be fulfilled only ``in sufficiently many cases\" (expressed by a ``fulfillment condition\"), production using ``fully illiquid\" assets in addition to tradables, and the asymmetry between assets and liabilities. We identify necessary and sufficient conditions on the capital investment under which the framework recovers the market prices of tradables, investigate extending production to take account of insolvency, implications of using illiquid assets in the production, and show how Solvency II and SST valuation can be derived with specific assumptions.",
        "comments": "35 pages, no figures",
        "date": "30 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00263"
    },
    {
        "doc_id": 95,
        "title": "Enhancing CVaR portfolio optimisation performance with GAM factor models",
        "authors": [
            "Davide Lauria",
            "W. Brent Lindquist",
            "Svetlozar T. Rachev"
        ],
        "subjects": [
            "Portfolio Management"
        ],
        "abstract": "We propose a discrete-time econometric model that combines autoregressive filters with factor regressions to predict stock returns for portfolio optimisation purposes. In particular, we test both robust linear regressions and general additive models on two different investment universes composed of the Dow Jones Industrial Average and the Standard & Poor's 500 indexes, and we compare the out-of-sample performances of mean-CVaR optimal portfolios over a horizon of six years. The results show a substantial improvement in portfolio performances when the factor model is estimated with general additive models.",
        "comments": " ",
        "date": "30 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00188"
    },
    {
        "doc_id": 96,
        "title": "Representation of forward performance criteria with random endowment via FBSDE and application to forward optimized certainty equivalent",
        "authors": [
            "Gechun Liang",
            "Yifan Sun",
            "Thaleia Zariphopoulou"
        ],
        "subjects": [
            "Portfolio Management",
            "Probability"
        ],
        "abstract": "We extend the notion of forward performance criteria to settings with random endowment in incomplete markets. Building on these results, we introduce and develop the novel concept of forward optimized certainty equivalent (forward OCE), which offers a genuinely dynamic valuation mechanism that accommodates progressively adaptive market model updates, stochastic risk preferences, and incoming claims with arbitrary maturities.\n  In parallel, we develop a new methodology to analyze the emerging stochastic optimization problems by directly studying the candidate optimal control processes for both the primal and dual problems. Specifically, we derive two new systems of forward-backward stochastic differential equations (FBSDEs) and establish necessary and sufficient conditions for optimality, and various equivalences between the two problems. This new approach is general and complements the existing one based on backward stochastic partial differential equations (backward SPDEs) for the related value functions. We, also, consider representative examples for both forward performance criteria with random endowment and forward OCE, and for the case of exponential criteria, we investigate the connection between forward OCE and forward entropic risk measures.",
        "comments": "50 pages",
        "date": "29 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00103"
    },
    {
        "doc_id": 97,
        "title": "Synthetic Data Applications in Finance",
        "authors": [
            "Vamsi K. Potluru",
            "Daniel Borrajo",
            "Andrea Coletta",
            "Niccol\u00f2 Dalmasso",
            "Yousef El-Laham",
            "Elizabeth Fons",
            "Mohsen Ghassemi",
            "Sriram Gopalakrishnan",
            "Vikesh Gosai",
            "Eleonora Krea\u010di\u0107",
            "Ganapathy Mani",
            "Saheed Obitayo",
            "Deepak Paramanand",
            "Natraj Raman",
            "Mikhail Solonin",
            "Srijan Sood",
            "Svitlana Vyetrenko",
            "Haibei Zhu",
            "Manuela Veloso",
            "Tucker Balch"
        ],
        "subjects": [
            "Machine Learning",
            "General Finance"
        ],
        "abstract": "Synthetic data has made tremendous strides in various commercial settings including finance, healthcare, and virtual reality. We present a broad overview of prototypical applications of synthetic data in the financial sector and in particular provide richer details for a few select ones. These cover a wide variety of data modalities including tabular, time-series, event-series, and unstructured arising from both markets and retail financial applications. Since finance is a highly regulated industry, synthetic data is a potential approach for dealing with issues related to privacy, fairness, and explainability. Various metrics are utilized in evaluating the quality and effectiveness of our approaches in these applications. We conclude with open directions in synthetic data in the context of the financial domain.",
        "comments": "50 pages, journal submission",
        "date": "29 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00081"
    },
    {
        "doc_id": 98,
        "title": "Sector Rotation by Factor Model and Fundamental Analysis",
        "authors": [
            "Runjia Yang",
            "Beining Shi"
        ],
        "subjects": [
            "Portfolio Management"
        ],
        "abstract": "This study presents an analytical approach to sector rotation, leveraging both factor models and fundamental metrics. We initiate with a systematic classification of sectors, followed by an empirical investigation into their returns. Through factor analysis, the paper underscores the significance of momentum and short-term reversion in dictating sectoral shifts. A subsequent in-depth fundamental analysis evaluates metrics such as PE, PB, EV-to-EBITDA, Dividend Yield, among others. Our primary contribution lies in developing a predictive framework based on these fundamental indicators. The constructed models, post rigorous training, exhibit noteworthy predictive capabilities. The findings furnish a nuanced understanding of sector rotation strategies, with implications for asset management and portfolio construction in the financial domain.",
        "comments": " ",
        "date": "18 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.00001"
    },
    {
        "doc_id": 99,
        "title": "Causal Discovery in Financial Markets: A Framework for Nonstationary Time-Series Data",
        "authors": [
            "Agathe Sadeghi",
            "Achintya Gopal",
            "Mohammad Fesanghary"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "A deeper comprehension of financial markets necessitates understanding not only the statistical dependencies among various entities but also the causal dependencies. This paper extends the Constraint-based Causal Discovery from Heterogeneous Data algorithm to account for lagged relationships in time-series data (an algorithm we call CD-NOTS), shedding light on the complex causal relations between different financial assets and variables. We compare the performance of different algorithmic choices, such as the choice of conditional independence test, to give general advice on the effective way to use CD-NOTS. Using the results from the simulated data, we apply CD-NOTS to a broad range of indices and factors in order to identify causal connections among the entities, thereby showing how causal discovery can serve as a valuable tool for factor-based investing, portfolio diversification, and comprehension of market dynamics. Further, we show our algorithm is a more effective alternative to other causal discovery algorithms since the assumptions of our algorithm are more realistic in terms of financial data, a conclusion we find is statistically significant.",
        "comments": "24 pages, 25 figures",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2312.17375"
    },
    {
        "doc_id": 100,
        "title": "Biological species delimitation based on genetic and spatial dissimilarity: a comparative study",
        "authors": [
            "Gabriele d'Angella",
            "Christian Hennig"
        ],
        "subjects": [
            "Populations and Evolution",
            "Applications",
            "Methodology"
        ],
        "abstract": "The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. This paper investigates the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Various approaches are compared (some of which already exist in the literature) based on simulated metapopulations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Approaches involve partial Mantel testing, maximum likelihood mixed-effects models with a population effect, and jackknife-based homogeneity tests. A key challenge is that most tests perform on genetic and geographical distance data, violating standard independence assumptions. Simulations showed that partial Mantel tests and mixed-effects models have larger power than jackknife-based methods, but tend to display type-I-error rates slightly above the significance level. Moreover, a multiple regression model neglecting the dependence in the dissimilarities did not show inflated type-I-error rate. An application on brassy ringlets concludes the paper.",
        "comments": "paper of 23 pages with 4 figures; appendix of 11 pages with 4 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12126"
    },
    {
        "doc_id": 101,
        "title": "Matching biomolecular structures by registration of point clouds",
        "authors": [
            "Michael Habeck",
            "Andreas Kr\u00f6pelin",
            "Nima Vakili"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "Motivation: Assessing the match between two biomolecular structures is at the heart of structural analyses such as superposition, alignment and docking. These tasks are typically solved with specialized structure-matching techniques implemented in software for protein structural alignment, rigid-body docking, or rigid fitting into cryo-EM maps. Results: We present a unifying framework to compare biomolecular structures by applying ideas from computer vision. The structures are represented as three-dimensional point clouds and compared by quantifying their overlap. We use the kernel correlation to measure point cloud overlap, and discuss local and global optimization strategies for maximizing the kernel correlation over the space of rigid transformations. We derive a majorization-minimization procedure that can be used to register two point clouds without establishing a point-to-point correspondence. We demonstrate that the majorization-minimization algorithms outperform the commonly used Iterative Closest Point registration algorithm. Furthermore, we discuss and benchmark a randomization strategy for globally optimizing the kernel correlation. We illustrate the approach on various 3D fitting problems such as the comparison of circularly permuted structures and rigid fitting of cryo-EM maps or bead models from small-angle scattering.",
        "comments": "18 pages (main text), 7 figures (main text)",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12082"
    },
    {
        "doc_id": 102,
        "title": "DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI",
        "authors": [
            "Sergio Morell-Ortega",
            "Marina Ruiz-Perez",
            "Marien Gadea",
            "Roberto Vivo-Hernando",
            "Gregorio Rubio",
            "Fernando Aparici",
            "Mariam de la Iglesia-Vaya",
            "Gwenaelle Catheline",
            "Pierrick Coup\u00e9",
            "Jos\u00e9 V. Manj\u00f3n"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Neurons and Cognition"
        ],
        "abstract": "This paper introduces a novel multimodal and high-resolution human brain cerebellum lobule segmentation method. Unlike current tools that operate at standard resolution ($1 \\text{ mm}^{3}$) or using mono-modal data, the proposed method improves cerebellum lobule segmentation through the use of a multimodal and ultra-high resolution ($0.125 \\text{ mm}^{3}$) training dataset. To develop the method, first, a database of semi-automatically labelled cerebellum lobules was created to train the proposed method with ultra-high resolution T1 and T2 MR images. Then, an ensemble of deep networks has been designed and developed, allowing the proposed method to excel in the complex cerebellum lobule segmentation task, improving precision while being memory efficient. Notably, our approach deviates from the traditional U-Net model by exploring alternative architectures. We have also integrated deep learning with classical machine learning methods incorporating a priori knowledge from multi-atlas segmentation, which improved precision and robustness. Finally, a new online pipeline, named DeepCERES, has been developed to make available the proposed method to the scientific community requiring as input only a single T1 MR image at standard resolution.",
        "comments": "20 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12074"
    },
    {
        "doc_id": 103,
        "title": "Approximating a linear dynamical system from non-sequential data",
        "authors": [
            "Cliff Stein",
            "Pratik Worah"
        ],
        "subjects": [
            "Genomics"
        ],
        "abstract": "Given non-sequential snapshots from instances of a dynamical system, we design a compressed sensing based algorithm that reconstructs the dynamical system. We formally prove that successful reconstruction is possible under the assumption that we can construct an approximate clock from a subset of the coordinates of the underlying system.\n  As an application, we argue that our assumption is likely true for genomic datasets, and we recover the underlying nuclear receptor networks and predict pathways, as opposed to genes, that may differentiate phenotypes in some publicly available datasets.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11858"
    },
    {
        "doc_id": 104,
        "title": "The NOSTRA model: coherent estimation of infection sources in the case of possible nosocomial transmission",
        "authors": [
            "David J Pascall",
            "Chris Jackson",
            "Stephanie Evans",
            "Theodore Gouliouris",
            "Chris Illingworth",
            "Stefan Piatek",
            "Julie V Robotham",
            "Oliver Stirrup",
            "Ben Warne",
            "Judith Breuer",
            "Daniela De Angelis"
        ],
        "subjects": [
            "Applications",
            "Quantitative Methods"
        ],
        "abstract": "Nosocomial infections have important consequences for patients and hospital staff: they worsen patient outcomes and their management stresses already overburdened health systems. Accurate judgements of whether an infection is nosocomial helps staff make appropriate choices to protect other patients within the hospital. Nosocomiality cannot be properly assessed without considering whether the infected patient came into contact with high risk potential infectors within the hospital. We developed a Bayesian model that integrates epidemiological, contact and pathogen genetic data to determine how likely an infection is to be nosocomial and the probability of given infection candidates being the source of the infection.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11837"
    },
    {
        "doc_id": 105,
        "title": "Full-dimensional characterisation of time-warped spike-time stimulus-response distribution geometries",
        "authors": [
            "James B Isbister"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Characterising the representation of sensory stimuli in the brain is a fundamental scientific endeavor, which can illuminate principles of information coding. Most characterizations reduce the dimensionality of neural data by converting spike trains to firing rates or binned spike counts, applying explicitly named methods of ``dimensionality reduction'', or collapsing trial-to-trial variability. Characterisation of the full-dimensional geometry of timing-based representations may provide unexpected insights into how complex high-dimensional information is encoded. Recent research shows that the distribution of representations elicited over trials of a single stimulus can be geometrically characterized without the application of dimensionality reduction, maintaining the temporal spiking information of individual neurons in a cell assembly and illuminating rich geometric structure. We extend these results, showing that precise spike time patterns for larger cell assemblies are time-warped (i.e. stretched or compressed) on each trial. Moreover, by geometrically characterizing distributions of large spike time patterns, our analysis supports the hypothesis that the degree to which a spike time pattern is time-warped depends on the cortical area's background activity level on a single trial. Finally, we suggest that the proliferation of large electrophysiology datasets and the increasing concentration of ``neural geometrists'', creates ideal conditions for characterization of full-dimensional spike time representations, in complement to dimensionality reduction approaches.",
        "comments": "Accepted as an extended abstract at the NeurReps workshop at NeurIPS 2023. The workshop doesn't publish extended abstracts so submitting here",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11784"
    },
    {
        "doc_id": 106,
        "title": "Impact of temporal interaction on the evolution of cooperation",
        "authors": [
            "Yujie He",
            "Tianyu Ren",
            "Junjun Zheng",
            "Huawen Liang"
        ],
        "subjects": [
            "Physics and Society",
            "Social and Information Networks",
            "Populations and Evolution"
        ],
        "abstract": "This research investigates the impact of dynamic interactions with time-varying topologies on the evolution of cooperative behaviours in social dilemmas. Traditional research has focused on deterministic rules governing pairwise interactions, yet the impact of interaction frequency and synchronicity on cooperation remains underexplored. Addressing this gap, our work introduces two temporal interaction mechanisms to model the stochastic or periodic participation of individuals in these games, acknowledging real-life variances due to exogenous temporal factors and geographical time differences. We consider that the interaction state significantly influences both game payoff calculations and the strategy updating process, offering new insights into the emergence and sustainability of cooperation. Our results indicate that maximum game participation frequency is suboptimal under a stochastic interaction mechanism. Instead, an intermediate region of activation probability yields the highest cooperation level, especially under strong dilemma conditions. This suggests that a balance between inactivity security and interaction frequency is crucial. Furthermore, local synchronization of interactions within specific areas is shown to be beneficial, as time differences hinder the spread of cross-structures but promote the formation of dense cooperative clusters with smoother boundaries. Our findings provide an intuitive understanding of node-based temporality and probabilistic interactions, contributing to the broader discourse on resolving social dilemmas.",
        "comments": "7 pages, 6 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11782"
    },
    {
        "doc_id": 107,
        "title": "Combining oligo pools and Golden Gate cloning to create protein variant libraries or guide RNA libraries for CRISPR applications",
        "authors": [
            "Alicia Maci\u00e1 Valero",
            "Rianne C. Prins",
            "Thijs de Vroet",
            "Sonja Billerbeck"
        ],
        "subjects": [
            "Quantitative Methods",
            "Biomolecules"
        ],
        "abstract": "Oligo pools are array-synthesized, user-defined mixtures of single-stranded oligonucleotides that can be used as a source of synthetic DNA for library cloning. While currently offering the most affordable source of synthetic DNA, oligo pools also come with limitations such as a maximum synthesis length (approximately 350 bases), a higher error rate compared to alternative synthesis methods, and the presence of truncated molecules in the pool due to incomplete synthesis. Here, we provide users with a comprehensive protocol that details how oligo pools can be used in combination with Golden Gate cloning to create user-defined protein mutant libraries, as well as single guide RNA libraries for CRISPR applications. Our methods are optimized to work within the Yeast Toolkit Golden Gate scheme, but are in principle compatible with any other Golden Gate-based modular cloning toolkit and extendable to other restriction enzyme-based cloning methods beyond Golden Gate. Our methods yield high-quality, affordable, in-house variant libraries.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11746"
    },
    {
        "doc_id": 108,
        "title": "Evolutionary dynamics of any multiplayer game on regular graphs",
        "authors": [
            "Chaoqian Wang",
            "Matja\u017e Perc",
            "Attila Szolnoki"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Statistical Mechanics",
            "Computational Complexity",
            "Cellular Automata and Lattice Gases",
            "Populations and Evolution"
        ],
        "abstract": "Multiplayer games on graphs are at the heart of theoretical descriptions of key evolutionary processes that govern vital social and natural systems. However, a comprehensive theoretical framework for solving multiplayer games with an arbitrary number of strategies on graphs is still missing. Here, we solve this by drawing an analogy with the Ball-and-Box problem, based on which we show that the local configuration of multiplayer games on graphs is equivalent to distributing $k$ identical co-players among $n$ distinct strategies. We use this to derive the replicator equation for any $n$-strategy multiplayer game under weak selection, which can be solved in polynomial time. As an example, we revisit the second-order free-riding problem, where costly punishment cannot truly resolve social dilemmas in a well-mixed population. Yet, in structured populations, we derive an accurate threshold for the punishment strength, beyond which punishment can either lead to the extinction of defection or transform the system into a rock-paper-scissors-like cycle. The analytical solution also qualitatively agrees with the phase diagrams that were previously obtained for non-marginal selection strengths. Our framework thus allows an exploration of any multi-strategy multiplayer game on regular graphs.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11686"
    },
    {
        "doc_id": 109,
        "title": "Accelerating Seed Location Filtering in DNA Read Mapping Using a Commercial Compute-in-SRAM Architecture",
        "authors": [
            "Courtney Golden",
            "Dan Ilan",
            "Nicholas Cebry",
            "Christopher Batten"
        ],
        "subjects": [
            "Hardware Architecture",
            "Genomics"
        ],
        "abstract": "DNA sequence alignment is an important workload in computational genomics. Reference-guided DNA assembly involves aligning many read sequences against candidate locations in a long reference genome. To reduce the computational load of this alignment, candidate locations can be pre-filtered using simpler alignment algorithms like edit distance. Prior work has explored accelerating filtering on simulated compute-in-DRAM, due to the massive parallelism of compute-in-memory architectures. In this paper, we present work-in-progress on accelerating filtering using a commercial compute-in-SRAM accelerator. We leverage the recently released Gemini accelerator platform from GSI Technology, which is the first, to our knowledge, commercial-scale compute-in-SRAM system. We accelerate the Myers' bit-parallel edit distance algorithm, producing average speedups of 14.1x over single-core CPU performance. Individual query/candidate alignments produce speedups of up to 24.1x. These early results suggest this novel architecture is well-suited to accelerating the filtering step of sequence-to-sequence DNA alignment.",
        "comments": "Journal ref:        5th Workshop on Accelerator Architecture in Computational Biology and Bioinformatics (AACBB), June 2023",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11685"
    },
    {
        "doc_id": 110,
        "title": "Modern approaches to improving phase contrast electron microscopy",
        "authors": [
            "Jeremy J. Axelrod",
            "Jessie T. Zhang",
            "Petar N. Petrov",
            "Robert M. Glaeser",
            "Holger Mueller"
        ],
        "subjects": [
            "Quantitative Methods",
            "Optics",
            "Biomolecules"
        ],
        "abstract": "Although defocus can be used to generate partial phase contrast in transmission electron microscope images, cryo-electron microscopy (cryo-EM) can be further improved by the development of phase plates which increase contrast by applying a phase shift to the unscattered part of the electron beam. Many approaches have been investigated, including the ponderomotive interaction between light and electrons. We review the recent successes achieved with this method in high-resolution, single-particle cryo-EM. We also review the status of using pulsed or near-field enhanced laser light as alternatives, along with approaches that use scanning transmission electron microscopy (STEM) with a segmented detector rather than a phase plate.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11678"
    },
    {
        "doc_id": 111,
        "title": "Enhancing selectivity using Wasserstein distance based reweighing",
        "authors": [
            "Pratik Worah"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "Given two labeled data-sets $\\mathcal{S}$ and $\\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\\mathcal{T}$.\n  On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.\n  Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1 (a highly similar protein). We tune the algorithm's parameter so that overall change in holdout loss is negligible, but the selectivity, i.e., the fraction of top 100 MNK2 binders that are MNK1 non-binders, increases from 54\\% to 95\\%, as a result of our reweighing. Of the 43 distinct small molecules predicted to be most selective from the enamine catalog, 2 small molecules were experimentally verified to be selective, i.e., they reduced the enzyme activity of MNK2 below 50\\% but not MNK1, at 10$\u03bc$M -- a 5\\% success rate.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11562"
    },
    {
        "doc_id": 112,
        "title": "Understanding Hepatitis B Virus Infection through Hepatocyte Proliferation and Capsid Recycling",
        "authors": [
            "Rupchand Sutradhar",
            "D C Dalal"
        ],
        "subjects": [
            "Populations and Evolution",
            "Dynamical Systems"
        ],
        "abstract": "Proliferation of uninfected as well as infected hepatocytes and recycling of DNA-containing\n  capsids are two major mechanisms playing significant roles in the clearance of hepatitis B\n  virus (HBV) infection. In this study, the temporal dynamics of this infection are investigated\n  through two in silico bio-mathematical models considering both proliferation of hepatocytes\n  and the recycling of capsids. Both models are formulated on the basis of a key finding in the existing literature: mitosis of infected yields in two uninfected progenies. In the first model,\n  we examine regular proliferation (occurs continuously), while the second model deals with the\n  irregular proliferation (happens when the total number of liver cells decreases to less than 70%\n  of its initial volume). The models are calibrated with the experimental data obtained from\n  an adult chimpanzee. Results of this study suggest that when both hepatocytes proliferate\n  with equal rate, proliferation aids the individual in a rapid recovery from the acute infection\n  whereas in the case of chronic infection, the severity of the infection increases if the proliferation\n  occurs frequently. On the other hand, if the infected cells proliferate at a slower rate than uninfected cells, the proliferation of uninfected hepatocytes contributes to increase the infection,\n  but the proliferation of infected hepatocytes acts to reduce the infection from the long-term\n  perspective. Furthermore, it is also observed that the differences between the outcomes of\n  regular and irregular proliferations are substantial and noteworthy.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11481"
    },
    {
        "doc_id": 113,
        "title": "Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis",
        "authors": [
            "Li Yin",
            "Xiong Yu",
            "Fan Wenxin",
            "Wang Kai",
            "Yu Qingqing",
            "Si Liping",
            "van der Smagt Patrick",
            "Tang Jun",
            "Chen Nutan"
        ],
        "subjects": [
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT.\n  Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities.\n  Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\\,\\%$ to $72\\%$, and for LSTM models, it is $66\\,\\%$ to $84\\,\\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for LSTM models it is between $1.09$ and $1.77$. Notably, these RMSEs are significantly lower than the random prediction error of $4.55$.\n  Conclusion: We creatively apply sequential models in the long-term management of SCIT with promising accuracy in the prediction of SCIT nonadherence in Allergic Rhinitis (AR) patients. While LSTM outperforms SLAC in adherence prediction, SLAC excels in score prediction for patients undergoing SCIT for AR. The state-action-based SLAC adds flexibility, presenting a novel and effective approach for managing long-term AIT.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11447"
    },
    {
        "doc_id": 114,
        "title": "MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks via Text Prompts",
        "authors": [
            "Haoqiang Guo",
            "Sendong Zhao",
            "Haochun Wang",
            "Yanrui Du",
            "Bing Qin"
        ],
        "subjects": [
            "Machine Learning",
            "Computation and Language",
            "Biomolecules"
        ],
        "abstract": "Deep learning is now widely used in drug discovery, providing significant acceleration and cost reduction. As the most fundamental building block, molecular representation is essential for predicting molecular properties to enable various downstream applications. Most existing methods attempt to incorporate more information to learn better representations. However, not all features are equally important for a specific task. Ignoring this would potentially compromise the training efficiency and predictive accuracy. To address this issue, we propose a novel approach, which treats language models as an agent and molecular pretraining models as a knowledge base. The agent accentuates task-relevant features in the molecular representation by understanding the natural language description of the task, just as a tailor customizes clothes for clients. Thus, we call this approach MolTailor. Evaluations demonstrate MolTailor's superior performance over baselines, validating the efficacy of enhancing relevance for molecular representation learning. This illustrates the potential of language model guided optimization to better exploit and unleash the capabilities of existing powerful molecular representation methods. Our codes and appendix are available at https://github.com/SCIR-HI/MolTailor.",
        "comments": "Accepted by AAAI 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11403"
    },
    {
        "doc_id": 115,
        "title": "PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding",
        "authors": [
            "Ruochi Zhang",
            "Haoran Wu",
            "Chang Liu",
            "Huaping Li",
            "Yuqian Wu",
            "Kewei Li",
            "Yifan Wang",
            "Yifan Deng",
            "Jiahui Chen",
            "Fengfeng Zhou",
            "Xin Gao"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "Recent advances in protein language models have catalyzed significant progress in peptide sequence representation. Despite extensive exploration in this field, pre-trained models tailored for peptide-specific needs remain largely unaddressed due to the difficulty in capturing the complex and sometimes unstable structures of peptides. This study introduces a novel multi-view contrastive learning framework PepHarmony for the sequence-based peptide encoding task. PepHarmony innovatively combines both sequence- and structure-level information into a sequence-level encoding module through contrastive learning. We carefully select datasets from the Protein Data Bank (PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences and structures. The experimental data highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures compared with the baseline and fine-tuned models. The robustness of our model is confirmed through extensive ablation studies, which emphasize the crucial roles of contrastive loss and strategic data sorting in enhancing predictive performance. The proposed PepHarmony framework serves as a notable contribution to peptide representations, and offers valuable insights for future applications in peptide drug discovery and peptide engineering. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony or http://www.healthinformaticslab.org/supp/.",
        "comments": "25 pages, 5 figures, 3 tables",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11360"
    },
    {
        "doc_id": 116,
        "title": "Sensory adaptation in a continuum model of bacterial chemotaxis -- working range, cost-accuracy relation, and coupled systems",
        "authors": [
            "Vansh Kharbanda",
            "Benedikt Sabass"
        ],
        "subjects": [
            "Cell Behavior",
            "Soft Condensed Matter"
        ],
        "abstract": "Sensory adaptation enables organisms to adjust their perception in a changing environment. A paradigm is bacterial chemotaxis, where the output activity of chemoreceptors is adapted to different baseline concentrations via receptor methylation. The range of internal receptor states limits the stimulus magnitude to which these systems can adapt. Here, we employ a highly idealized, Langevin-equation based model to study how the finite range of state variables affects the adaptation accuracy and the energy dissipation in individual and coupled systems. Maintaining an adaptive state requires constant energy dissipation. We show that the steady-state dissipation rate increases approximately linearly with the adaptation accuracy for varying stimulus magnitudes in the so-called perfect adaptation limit. This result complements the well-known logarithmic cost-accuracy relationship for varying chemical driving. Next, we study linearly coupled pairs of sensory units. We find that the interaction reduces the dissipation rate per unit and affects the overall cost-accuracy relationship. A coupling of the slow methylation variables results in a better accuracy than a coupling of activities. Overall, the findings highlight the significance of both the working range and collective operation mode as crucial design factors that impact the accuracy and energy expenditure of molecular adaptation networks.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11341"
    },
    {
        "doc_id": 117,
        "title": "Uncertainty quantification of receptor ligand binding sites prediction",
        "authors": [
            "Nanjie Chen",
            "Dongliang Yu",
            "Dmitri Beglov",
            "Mark Kon",
            "Julio Enrique Castrillon Candas"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "Recent advancements in protein docking site prediction have highlighted the limitations of traditional rigid docking algorithms, like PIPER, which often neglect critical stochastic elements such as solvent-induced fluctuations. These oversights can lead to inaccuracies in identifying viable docking sites due to the complexity of high-dimensional, stochastic energy manifolds with low regularity. To address this issue, our research introduces a novel model where the molecular shapes of ligands and receptors are represented using multi-variate Karhunen-Lo `eve (KL) expansions. This method effectively captures the stochastic nature of energy manifolds, allowing for a more accurate representation of molecular interactions.Developed as a plugin for PIPER, our scientific computing software enhances the platform, delivering robust uncertainty measures for the energy manifolds of ranked binding sites. Our results demonstrate that top-ranked binding sites, characterized by lower uncertainty in the stochastic energy manifold, align closely with actual docking sites. Conversely, sites with higher uncertainty correlate with less optimal docking positions. This distinction not only validates our approach but also sets a new standard in protein docking predictions, offering substantial implications for future molecular interaction research and drug development.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11312"
    },
    {
        "doc_id": 118,
        "title": "Seasonality of primary productivity affects coastal species more than its magnitude",
        "authors": [
            "Carlota Muniz",
            "Christopher McQuaid",
            "Nicolas Weidberg"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "While the importance of extreme conditions is recognised, patterns in species abundances are often interpreted through average environmental conditions within their distributional range. For marine species with pelagic larvae, temperature and phytoplankton concentration are key variables. Along the south coast of South Africa, conspicuous spatial patterns in recruitment rates and the abundances of different mussel species exist, with focal areas characterized by large populations. We studied 15 years of sea surface temperature (SST) and chlorophyll-a (chl-a) satellite data, using spectral analyses to partition their temporal variability over ecologically relevant time periods, including seasonal (101 to 365 days) and intra-seasonal cycles (20 to 100 days). Adult cover and mussel recruitment were measured at 10 sites along the south coast and regression models showed that about 70 percent of the variability in recruitment and adult cover was explained by seasonal variability in chl-a, while mean annual chl-a and SST only explained 30 percent of the recruitment, with no significant effect for adult cover. SST and chl-a at two upwelling centres showed less predictable seasonal cycles during the second half of the study period with a significant cooling trend during austral autumn, coinciding with one of the mussel reproductive peaks. This likely reflects recent changes in the Agulhas Current, the world largest western boundary current, which affects coastal ecosystems by driving upwelling.",
        "comments": "Journal ref:        Science of the Total Environment, 757:143740, 2021",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11289"
    },
    {
        "doc_id": 119,
        "title": "Smart Drug-Delivery Systems for Cancer Nanotherapy",
        "authors": [
            "Paola Sanchez-Moreno",
            "Juan Luis Ortega-Vinuesa",
            "Jose Manuel Peula-Garcia",
            "Juan Antonio Marchal",
            "Houria Boulaiz"
        ],
        "subjects": [
            "Tissues and Organs",
            "Mesoscale and Nanoscale Physics",
            "Biological Physics"
        ],
        "abstract": "Despite all the advances achieved in the field of tumor-biology research, in most cases conventional therapies including chemotherapy are still the leading choices. The main disadvantage of these treatments, in addition to the low solubility of many antitumor drugs, is their lack of specificity, which explains the frequent occurrence of serious side effects due to nonspecific drug uptake by healthy cells. Progress in nanotechnology and its application in medicine have provided new opportunities and different smart systems. Such systems can improve the intracellular delivery of the drugs due to their multifunctionality and targeting potential. The purpose of this manuscript is to review and analyze the recent progress made in nanotherapy applied to cancer treatment. First, we provide a global overview of cancer and different smart nanoparticles currently used in oncology. Then, we analyze in detail the development of drug-delivery strategies in cancer therapy, focusing mainly on the intravenously administered smart nanoparticles with protein corona to avoid immune-system clearance. Finally, we discuss the challenges, clinical trials, and future directions of the nanoparticle-based therapy in cancer.",
        "comments": "Preprint version, 25 pages, 7 figures, 3 tables. Authors thank to Bentham Science the posibility of deposit the ACCEPTED VERSION of the peer-reviewed article after 12 months of publication on journal web site on arXiv repository. The published manuscript is available at EurekaSelect via https://www.eurekaselect.com/openurl/content.php?genre=article&doi=10.2174/1389450117666160527142544",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11192"
    },
    {
        "doc_id": 120,
        "title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
        "authors": [
            "Chih-Hsuan Wei",
            "Alexis Allot",
            "Po-Ting Lai",
            "Robert Leaman",
            "Shubo Tian",
            "Ling Luo",
            "Qiao Jin",
            "Zhizheng Wang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "subjects": [
            "Computation and Language",
            "Quantitative Methods"
        ],
        "abstract": "PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art AI techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11048"
    },
    {
        "doc_id": 121,
        "title": "Equivariant Graph Neural Operator for Modeling 3D Dynamics",
        "authors": [
            "Minkai Xu",
            "Jiaqi Han",
            "Aaron Lou",
            "Jean Kossaifi",
            "Arvind Ramanathan",
            "Kamyar Azizzadenesheli",
            "Jure Leskovec",
            "Stefano Ermon",
            "Anima Anandkumar"
        ],
        "subjects": [
            "Machine Learning",
            "Numerical Analysis",
            "Quantitative Methods"
        ],
        "abstract": "Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stacking the Fourier layers over equivariant networks. EGNO is the first operator learning framework that is capable of modeling solution dynamics functions over time while retaining 3D equivariance. Comprehensive experiments in multiple domains, including particle simulations, human motion capture, and molecular dynamics, demonstrate the significantly superior performance of EGNO against existing methods, thanks to the equivariant temporal modeling.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11037"
    },
    {
        "doc_id": 122,
        "title": "Clustering Molecular Energy Landscapes by Adaptive Network Embedding",
        "authors": [
            "Paula Mercurio",
            "Di Liu"
        ],
        "subjects": [
            "Biomolecules",
            "Statistical Mechanics",
            "Machine Learning"
        ],
        "abstract": "In order to efficiently explore the chemical space of all possible small molecules, a common approach is to compress the dimension of the system to facilitate downstream machine learning tasks. Towards this end, we present a data driven approach for clustering potential energy landscapes of molecular structures by applying recently developed Network Embedding techniques, to obtain latent variables defined through the embedding function. To scale up the method, we also incorporate an entropy sensitive adaptive scheme for hierarchical sampling of the energy landscape, based on Metadynamics and Transition Path Theory. By taking into account the kinetic information implied by a system's energy landscape, we are able to interpret dynamical node-node relationships in reduced dimensions. We demonstrate the framework through Lennard-Jones (LJ) clusters and a human DNA sequence.",
        "comments": "19 pages, 10 figures",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10972"
    },
    {
        "doc_id": 123,
        "title": "Homogenisation of nonlinear blood flow in periodic networks: the limit of small haematocrit heterogeneity",
        "authors": [
            "Y. Ben-Ami",
            "B. D. Wood",
            "J. M. Pitt-Francis",
            "P. K. Maini",
            "H. M. Byrne"
        ],
        "subjects": [
            "Tissues and Organs",
            "Soft Condensed Matter",
            "Biological Physics"
        ],
        "abstract": "In this work we develop a homogenisation methodology to upscale mathematical descriptions of microcirculatory blood flow from the microscale (where individual vessels are resolved) to the macroscopic (or tissue) scale. Due to the assumed two-phase nature of blood and specific features of red blood cells (RBCs), mathematical models for blood flow in the microcirculation are highly nonlinear, coupling the flow and RBC concentrations (haematocrit). In contrast to previous works which accomplished blood-flow homogenisation by assuming that the haematocrit level remains constant, here we allow for spatial heterogeneity in the haematocrit concentration and thus begin with a nonlinear microscale model. We simplify the analysis by considering the limit of small haematocrit heterogeneity which prevails when variations in haematocrit concentration between neighbouring vessels are small. Homogenisation results in a system of coupled, nonlinear partial differential equations describing the flow and haematocrit transport at the macroscale, in which a nonlinear Darcy-type model relates the flow and pressure gradient via a haematocrit-dependent permeability tensor. During the analysis we obtain further that haematocrit transport at the macroscale is governed by a purely advective equation. Applying the theory to particular examples of two- and three-dimensional geometries of periodic networks, we calculate the effective permeability tensor associated with blood flow in these vascular networks. We demonstrate how the statistical distribution of vessel lengths and diameters, together with the average haematocrit level, affect the statistical properties of the macroscopic permeability tensor. These data can be used to simulate blood flow and haematocrit transport at the macroscale.",
        "comments": "34 pages, 8 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10932"
    },
    {
        "doc_id": 124,
        "title": "A Chaotic Associative Memory",
        "authors": [
            "Nurani Rajagopal Rohan",
            "Sayan Gupta",
            "V. Srinivasa Chakravarthy"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Chaotic Dynamics"
        ],
        "abstract": "We propose a novel Chaotic Associative Memory model using a network of chaotic Rossler systems and investigate the storage capacity and retrieval capabilities of this model as a function of increasing periodicity and chaos. In early models of associate memory networks, memories were modeled as fixed points, which may be mathematically convenient but has poor neurobiological plausibility. Since brain dynamics is inherently oscillatory, attempts have been made to construct associative memories using nonlinear oscillatory networks. However, oscillatory associative memories are plagued by the problem of poor storage capacity, though efforts have been made to improve capacity by adding higher order oscillatory modes. The chaotic associative memory proposed here exploits the continuous spectrum of chaotic elements and has higher storage capacity than previously described oscillatory associate memories.",
        "comments": "10 pages, 8 Figures, Submitted to \"Chaos: An Interdisciplinary Journal of Nonlinear Science\"",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10922"
    },
    {
        "doc_id": 125,
        "title": "Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior",
        "authors": [
            "Jason Toy",
            "Josh MacAdam",
            "Phil Tabor"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Artificial Intelligence"
        ],
        "abstract": "Recent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face challenges such as limited context windows and difficulties in generalization. In this paper, we introduce a metacognition module for generative agents, enabling them to observe their own thought processes and actions. This metacognitive approach, designed to emulate System 1 and System 2 cognitive processes, allows agents to significantly enhance their performance by modifying their strategy. We tested the metacognition module on a variety of scenarios, including a situation where generative agents must survive a zombie apocalypse, and observe that our system outperform others, while agents adapt and improve their strategies to complete tasks over time.",
        "comments": "9 pages, 4 figures",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10910"
    },
    {
        "doc_id": 126,
        "title": "Novel community data in ecology -- properties and prospects",
        "authors": [
            "Florian Hartig",
            "Nerea Abrego",
            "Alex Bush",
            "Jonathan M. Chase",
            "Gurutzeta Guillera-Arroita",
            "Mathew A. Leibold",
            "Otso Ovaskainen",
            "Lo\u00efc Pellissier",
            "Maximilian Pichler",
            "Giovanni Poggiato",
            "Laura Pollock",
            "Sara Si-Moussi",
            "Wilfried Thuiller",
            "Duarte S. Viana",
            "David I. Warton",
            "Damaris Zurell",
            "Douglas W. Yu"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "New technologies for acquiring biological information such as eDNA, acoustic or optical sensors, make it possible to generate spatial community observations at unprecedented scales. The potential of these novel community data to standardize community observations at high spatial, temporal, and taxonomic resolution and at large spatial scale ('many rows and many columns') has been widely discussed, but so far, there has been little integration of these data with ecological models and theory. Here, we review these developments and highlight emerging solutions, focusing on statistical methods for analyzing novel community data, in particular joint species distribution models; the new ecological questions that can be answered with these data; and the potential implications of these developments for policy and conservation.",
        "comments": "Journal ref:        Trends in Ecology & Evolution, 2024",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10860"
    },
    {
        "doc_id": 127,
        "title": "Exploring the role of structure in a time constrained decision task",
        "authors": [
            "Naomi Chaix-Eichel",
            "Gautham Venugopal",
            "Thomas Boraud",
            "Nicolas P. Rougier"
        ],
        "subjects": [
            "Neural and Evolutionary Computing",
            "Neurons and Cognition"
        ],
        "abstract": "The structure of the basal ganglia is remarkably similar across a number of species (often described in terms of direct, indirect and hyperdirect pathways) and is deeply involved in decision making and action selection. In this article, we are interested in exploring the role of structure when solving a decision task while avoiding to make any strong assumption regarding the actual structure. To do so, we exploit the echo state network paradigm that allows to solve complex task based on a random architecture. Considering a temporal decision task, the question is whether a specific structure allows for better performance and if so, whether this structure shares some similarity with the basal ganglia. Our results highlight the advantage of having a slow (direct) and a fast (hyperdirect) pathway that allows to deal with late information during a decision making task.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10849"
    },
    {
        "doc_id": 128,
        "title": "Neural Population Decoding and Imbalanced Multi-Omic Datasets For Cancer Subtype Diagnosis",
        "authors": [
            "Charles Theodore Kent",
            "Leila Bagheriye",
            "Johan Kwisthout"
        ],
        "subjects": [
            "Neural and Evolutionary Computing",
            "Machine Learning",
            "Genomics",
            "Quantitative Methods",
            "Applications"
        ],
        "abstract": "Recent strides in the field of neural computation has seen the adoption of Winner Take All (WTA) circuits to facilitate the unification of hierarchical Bayesian inference and spiking neural networks as a neurobiologically plausible model of information processing. Current research commonly validates the performance of these networks via classification tasks, particularly of the MNIST dataset. However, researchers have not yet reached consensus about how best to translate the stochastic responses from these networks into discrete decisions, a process known as population decoding. Despite being an often underexamined part of SNNs, in this work we show that population decoding has a significanct impact on the classification performance of WTA networks. For this purpose, we apply a WTA network to the problem of cancer subtype diagnosis from multi omic data, using datasets from The Cancer Genome Atlas (TCGA). In doing so we utilise a novel implementation of gene similarity networks, a feature encoding technique based on Kohoens self organising map algorithm. We further show that the impact of selecting certain population decoding methods is amplified when facing imbalanced datasets.",
        "comments": "This paper has been accepted in BIOINFORMATICS 2024 (BIOSTEC 2024)",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10844"
    },
    {
        "doc_id": 129,
        "title": "DeepRLI: A Multi-objective Framework for Universal Protein--Ligand Interaction Prediction",
        "authors": [
            "Haoyu Lin",
            "Shiwei Wang",
            "Jintao Zhu",
            "Yibo Li",
            "Jianfeng Pei",
            "Luhua Lai"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "Protein (receptor)--ligand interaction prediction is a critical component in computer-aided drug design, significantly influencing molecular docking and virtual screening processes. Despite the development of numerous scoring functions in recent years, particularly those employing machine learning, accurately and efficiently predicting binding affinities for protein--ligand complexes remains a formidable challenge. Most contemporary methods are tailored for specific tasks, such as binding affinity prediction, binding pose prediction, or virtual screening, often failing to encompass all aspects. In this study, we put forward DeepRLI, a novel protein--ligand interaction prediction architecture. It encodes each protein--ligand complex into a fully connected graph, retaining the integrity of the topological and spatial structure, and leverages the improved graph transformer layers with cosine envelope as the central module of the neural network, thus exhibiting superior scoring power. In order to equip the model to generalize to conformations beyond the confines of crystal structures and to adapt to molecular docking and virtual screening tasks, we propose a multi-objective strategy, that is, the model outputs three scores for scoring and ranking, docking, and screening, and the training process optimizes these three objectives simultaneously. For the latter two objectives, we augment the dataset through a docking procedure, incorporate suitable physics-informed blocks and employ an effective contrastive learning approach. Eventually, our model manifests a balanced performance across scoring, ranking, docking, and screening, thereby demonstrating its ability to handle a range of tasks. Overall, this research contributes a multi-objective framework for universal protein--ligand interaction prediction, augmenting the landscape of structure-based drug design.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10806"
    },
    {
        "doc_id": 130,
        "title": "FIMBA: Evaluating the Robustness of AI in Genomics via Feature Importance Adversarial Attacks",
        "authors": [
            "Heorhii Skovorodnikov",
            "Hoda Alkhzaimi"
        ],
        "subjects": [
            "Machine Learning",
            "Cryptography and Security",
            "Genomics"
        ],
        "abstract": "With the steady rise of the use of AI in bio-technical applications and the widespread adoption of genomics sequencing, an increasing amount of AI-based algorithms and tools is entering the research and production stage affecting critical decision-making streams like drug discovery and clinical outcomes. This paper demonstrates the vulnerability of AI models often utilized downstream tasks on recognized public genomics datasets. We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance. Further, we enhance our approach by generating poisoned data using a variational autoencoder-based model. Our empirical findings unequivocally demonstrate a decline in model performance, underscored by diminished accuracy and an upswing in false positives and false negatives. Furthermore, we analyze the resulting adversarial samples via spectral analysis yielding conclusions for countermeasures against such attacks.",
        "comments": "15 pages, core code available at: https://github.com/HeorhiiS/fimba-attack",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10657"
    },
    {
        "doc_id": 131,
        "title": "Exact analytical algorithm for solvent accessible surface area and derivatives in implicit solvent molecular simulations on GPUs",
        "authors": [
            "Xin Cao",
            "Michelle H. Hummel",
            "Yuzhang Wang",
            "Carlos Simmerling",
            "Evangelos A. Coutsias"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "In this paper, we present dSASA (differentiable SASA), an exact geometric method to calculate solvent accessible surface area (SASA) analytically along with atomic derivatives on GPUs. The atoms in a molecule are first assigned to tetrahedra in groups of four atoms by Delaunay tetrahedrization adapted for efficient GPU implementation and the SASA values for atoms and molecules are calculated based on the tetrahedrization information and inclusion-exclusion method. The SASA values from the numerical icosahedral-based method can be reproduced with more than 98% accuracy for both proteins and RNAs. Having been implemented on GPUs and incorporated into the software Amber, we can apply dSASA to implicit solvent molecular dynamics simulations with inclusion of this nonpolar term. The current GPU version of GB/SA simulations has been accelerated up to nearly 20-fold compared to the CPU version and it outperforms LCPO as the system size increases. The performance and importance of the nonpolar part in implicit solvent modeling are demonstrated in GB/SA simulations of proteins and accurate SASA calculation of nucleic acids.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10462"
    },
    {
        "doc_id": 132,
        "title": "Ecosystem models cannot predict the consequences of conservation decisions",
        "authors": [
            "Larissa Lubiana Botelho",
            "Cailan Jeynes-Smith",
            "Sarah Vollert",
            "Michael Bode"
        ],
        "subjects": [
            "Populations and Evolution",
            "Quantitative Methods"
        ],
        "abstract": "Ecosystem models are often used to predict the consequences of management decisions in applied ecology, including fisheries management and threatened species conservation. These models are high-dimensional, parameter-rich, and nonlinear, yet limited data is available to calibrate them, and they are rarely tested or validated. Consequently, the accuracy of their forecasts, and their utility as decision-support tools is a matter of debate. In this paper, we calibrate ecosystem models to time-series data from 110 different experimental microcosm ecosystems, each containing between three and five interacting species. We then assess how often these calibrated models offer accurate and useful predictions about how the ecosystem will respond to a set of standard management interventions. Our results show that for each timeseries dataset, a large number of very different parameter sets offer equivalent, good fits. However, these calibrated ecosystem models have poor predictive accuracy when forecasting future dynamics and offer ambiguous predictions about how species in the ecosystem will respond to management interventions. Closer inspection reveals that the ecosystem models fail because calibration cannot determine the types of interactions that occur within the ecosystem. Our findings call into question claims that ecosystem modelling can support applied ecological decision-making when they are calibrated against real-world datasets.",
        "comments": "23 pages (main text + supplementary material) 9 figures (main text + supplementary material)",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10439"
    },
    {
        "doc_id": 133,
        "title": "Diffusion of intrinsically disordered proteins within viscoelastic membraneless droplets",
        "authors": [
            "Fuga Watanabe",
            "Takuma Akimoto",
            "Robert B. Best",
            "Kresten Lindorff-Larsen",
            "Ralf Metzler",
            "Eiji Yamamoto"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Statistical Mechanics",
            "Biological Physics",
            "Computational Physics",
            "Biomolecules"
        ],
        "abstract": "In living cells, intrinsically disordered proteins (IDPs), such as FUS and DDX4, undergo phase separation, forming biomolecular condensates. Using molecular dynamics simulations, we investigate their behavior in their respective homogenous droplets. We find that the proteins exhibit transient subdiffusion due to the viscoelastic nature and confinement effects in the droplets. The conformation and the instantaneous diffusivity of the proteins significantly vary between the interior and the interface of the droplet, resulting in non-Gaussianity in the displacement distributions. This study highlights key aspects of IDP behavior in biomolecular condensates.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10438"
    },
    {
        "doc_id": 134,
        "title": "Exploring General Intelligence via Gated Graph Transformer in Functional Connectivity Studies",
        "authors": [
            "Gang Qu",
            "Anton Orlichenko",
            "Junqi Wang",
            "Gemeng Zhang",
            "Li Xiao",
            "Aiying Zhang",
            "Zhengming Ding",
            "Yu-Ping Wang"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Artificial Intelligence"
        ],
        "abstract": "Functional connectivity (FC) as derived from fMRI has emerged as a pivotal tool in elucidating the intricacies of various psychiatric disorders and delineating the neural pathways that underpin cognitive and behavioral dynamics inherent to the human brain. While Graph Neural Networks (GNNs) offer a structured approach to represent neuroimaging data, they are limited by their need for a predefined graph structure to depict associations between brain regions, a detail not solely provided by FCs. To bridge this gap, we introduce the Gated Graph Transformer (GGT) framework, designed to predict cognitive metrics based on FCs. Empirical validation on the Philadelphia Neurodevelopmental Cohort (PNC) underscores the superior predictive prowess of our model, further accentuating its potential in identifying pivotal neural connectivities that correlate with human cognitive processes.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10348"
    },
    {
        "doc_id": 135,
        "title": "DrugAssist: A Large Language Model for Molecule Optimization",
        "authors": [
            "Geyan Ye",
            "Xibao Cai",
            "Houtim Lai",
            "Xing Wang",
            "Junhong Huang",
            "Longyue Wang",
            "Wei Liu",
            "Xiangxiang Zeng"
        ],
        "subjects": [
            "Quantitative Methods",
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning"
        ],
        "abstract": "Recently, the impressive performance of large language models (LLMs) on a wide range of tasks has attracted an increasing number of attempts to apply LLMs in drug discovery. However, molecule optimization, a critical task in the drug discovery pipeline, is currently an area that has seen little involvement from LLMs. Most of existing approaches focus solely on capturing the underlying patterns in chemical structures provided by the data, without taking advantage of expert feedback. These non-interactive approaches overlook the fact that the drug discovery process is actually one that requires the integration of expert experience and iterative refinement. To address this gap, we propose DrugAssist, an interactive molecule optimization model which performs optimization through human-machine dialogue by leveraging LLM's strong interactivity and generalizability. DrugAssist has achieved leading results in both single and multiple property optimization, simultaneously showcasing immense potential in transferability and iterative optimization. In addition, we publicly release a large instruction-based dataset called MolOpt-Instructions for fine-tuning language models on molecule optimization tasks. We have made our code and data publicly available at https://github.com/blazerye/DrugAssist, which we hope to pave the way for future research in LLMs' application for drug discovery.",
        "comments": "Geyan Ye and Xibao Cai are equal contributors; Longyue Wang is corresponding author",
        "date": "28 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10334"
    },
    {
        "doc_id": 136,
        "title": "Fine scale depth regulation of invertebrate larvae around coastal fronts",
        "authors": [
            "Nicolas Weidberg",
            "Wayne Goschen",
            "Jennifer M. Jackson",
            "Paula Pattrick",
            "Christopher D. McQuaid",
            "Francesca Porri"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "Vertical migrations of zooplankters have been widely described, but their active movements through shallow, highly dynamic water columns within the inner shelf may be more complex and difficult to characterize. In this study, invertebrate larvae, currents, and hydrographic variables were sampled at different depths during and after the presence of fronts on three different cruises off the southern coast of South Africa. Internal wave dynamics were observed in the hydrographic data set but also through satellite imagery, although strong surface convergent currents were absent and thermal stratification was weak. During the first two cruises, fronts were more conspicuous and they preceded strong onshore currents at depth which developed with the rising tide. Vertical distributions of larvae changed accordingly, with higher abundances at these deep layers once the front disappeared. The third cruise was carried out during slack tides, the front was not conspicuous, deep strong onshore currents did not occur afterward and larval distributions did not change consistently through time. Overall, the vertical distributions of many larval taxa matched the vertical profiles of shoreward currents and multivariate analyses revealed that these flows structured the larval community, which was neither influenced by temperature nor chlorophyll. Thus, the ability to regulate active vertical positioning may enhance shoreward advection and determine nearshore larval distributions.",
        "comments": "Journal ref:        Limnology and Oceanography. 64 - 2, pp. 785 - 802, 2019",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10303"
    },
    {
        "doc_id": 137,
        "title": "Forecasting dengue outbreaks with uncertainty using seasonal weather patterns",
        "authors": [
            "Piumi Chathurangika",
            "Sanjeewa Perera",
            "Kushani De Silva"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "Dengue is a vector-borne disease transmitted to humans by vectors of genus Aedes and is a global threat with health, social, and economic impact in many of the tropical countries including Sri Lanka. The virus transmission is significantly impacted by environmental conditions, with a notable contribution from elevated per-capita vector density. These conditions are dynamic in nature and specially having the tropical climate, Sri Lanka experiences seasonal weather patterns dominated by monsoons. In this work, we investigate the dynamic influence of environmental conditions on dengue emergence in Colombo district where dengue is extremely prevalent in Sri Lanka. A novel approach leveraging the Markov chain Monte Carlo simulations has been employed to identify seasonal patterns of dengue disease emergence, utilizing the dynamics of weather patterns governing in the region. The newly developed algorithm allows us to estimate the timing of dengue outbreaks with uncertainty, enabling accurate forecasts of upcoming disease emergence patterns for better preparedness.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10295"
    },
    {
        "doc_id": 138,
        "title": "Mechanisms of nearshore retention and offshore export of mussel larvae over the Agulhas Bank",
        "authors": [
            "Nicolas Weidberg",
            "Francesca Porri",
            "Charles von der Meden",
            "Jennifer M. Jackson",
            "Wayne Goschen",
            "Christopher McQuaid"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "Ecological connectivity is critical for population dynamics but in many benthic species it is complicated by a planktonic larval phase, whose dispersal remains poorly understood. Using a plankton pump, we examine the distribution of intertidal mussel larvae along three axes: alongshore, cross-shelf and by depth during a large scale (600 km) cruise over the Agulhas Bank off southern Africa in August/September 2010. As a general pattern, higher veliger abundances were found close to the coast. Our analyses of the nearshore flow, estimated from ADCP data and the vertical distribution of larvae, show that onshore larval retention may be mediated by active vertical swimming through the water column guided by light and wind-induced turbulence. A massive offshore export of larvae off St Francis Bay was, however, observed during an Agulhas Current meander which influenced inner shelf waters. We hypothesize that, by increasing and homogenizing flow, the Agulhas Current may erase the effects of larval vertical positioning on onshore retention and transport larvae offshore. Our study highlights the need to integrate the effects of complex, region-specific physical dynamics with the swimming behaviour of larvae in order to explain their spatial distribution, population connectivity and the consequences for population dynamics.",
        "comments": "Journal ref:        Journal of Plankton Research. 37 - 6, pp. 1166 - 1180. Oxford Journals, 11/2015",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10292"
    },
    {
        "doc_id": 139,
        "title": "Analyzing Brain Activity During Learning Tasks with EEG and Machine Learning",
        "authors": [
            "Ryan Cho",
            "Mobasshira Zaman",
            "Kyu Taek Cho",
            "Jaejin Hwang"
        ],
        "subjects": [
            "Signal Processing",
            "Machine Learning",
            "Neurons and Cognition"
        ],
        "abstract": "This study aimed to analyze brain activity during various STEM activities, exploring the feasibility of classifying between different tasks. EEG brain data from twenty subjects engaged in five cognitive tasks were collected and segmented into 4-second clips. Power spectral densities of brain frequency waves were then analyzed. Testing different k-intervals with XGBoost, Random Forest, and Bagging Classifier revealed that Random Forest performed best, achieving a testing accuracy of 91.07% at an interval size of two. When utilizing all four EEG channels, cognitive flexibility was most recognizable. Task-specific classification accuracy showed the right frontal lobe excelled in mathematical processing and planning, the left frontal lobe in cognitive flexibility and mental flexibility, and the left temporoparietal lobe in connections. Notably, numerous connections between frontal and temporoparietal lobes were observed during STEM activities. This study contributes to a deeper understanding of implementing machine learning in analyzing brain activity and sheds light on the brain's mechanisms.",
        "comments": "20 pages, 7 figures",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10285"
    },
    {
        "doc_id": 140,
        "title": "EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model",
        "authors": [
            "Yuqi Chen",
            "Kan Ren",
            "Kaitao Song",
            "Yansen Wang",
            "Yifan Wang",
            "Dongsheng Li",
            "Lili Qiu"
        ],
        "subjects": [
            "Signal Processing",
            "Artificial Intelligence",
            "Machine Learning",
            "Multimedia",
            "Neurons and Cognition"
        ],
        "abstract": "Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance on various downstream tasks but also provide interpretable outcomes of the useful patterns within the data. To validate the effectiveness of our model, we extensively evaluate it on various downstream tasks and assess the performance under different transfer settings. Furthermore, we demonstrate how the learned model exhibits transferable anomaly detection performance and provides valuable interpretability of the acquired patterns via self-supervised learning.",
        "comments": "A preprint version of an ongoing work",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10278"
    },
    {
        "doc_id": 141,
        "title": "Evolving Diploid Boolean and Multi-Valued Gene Networks",
        "authors": [
            "Larry Bull"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "Boolean networks have been widely used to explore aspects of gene regulation, traditionally with a single network. A modified form of the model to explore the effects of increasing the number of gene states has also recently been introduced. In this paper, these discrete dynamical networks are evolved as diploids within rugged fitness landscapes to explore their behaviour. Results suggest the general properties of haploid networks in similar circumstances remain for diploids. The previously proposed inherent fitness landscape smoothing properties of eukaryotic sex are shown to be exhibited in these dynamical systems, as is their propensity to change in size based upon the characteristics of the network and fitness landscape.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2302.01694",
        "date": "19 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10237"
    },
    {
        "doc_id": 142,
        "title": "Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products",
        "authors": [
            "Shengjie Luo",
            "Tianlang Chen",
            "Aditi S. Krishnapriyan"
        ],
        "subjects": [
            "Machine Learning",
            "Materials Science",
            "Group Theory",
            "Chemical Physics",
            "Biomolecules"
        ],
        "abstract": "Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations increases significantly as higher-order tensors are used. In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps. We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics. Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics. This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis. Consequently, the multiplication between spherical functions represented by a 2D Fourier basis can be efficiently computed via the convolution theorem and Fast Fourier Transforms. This transformation reduces the complexity of full tensor products of irreps from $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^3)$, where $L$ is the max degree of irreps. Leveraging this approach, we introduce the Gaunt Tensor Product, which serves as a new method to construct efficient equivariant operations across different model architectures. Our experiments on the Open Catalyst Project and 3BPA datasets demonstrate both the increased efficiency and improved performance of our approach.",
        "comments": "36 pages; ICLR 2024 (Spotlight Presentation); Code: https://github.com/lsj2408/Gaunt-Tensor-Product",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10216"
    },
    {
        "doc_id": 143,
        "title": "Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation",
        "authors": [
            "Zhengyi Li",
            "Menglu Li",
            "Lida Zhu",
            "Wen Zhang"
        ],
        "subjects": [
            "Quantitative Methods",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context sequence information, and motif generated by aligning all context sequences of PTM sites assists the prediction. Extensive experiments on three datasets show that PTM-CMGMS outperforms the state-of-the-art methods.",
        "comments": " ",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10211"
    },
    {
        "doc_id": 144,
        "title": "Exploiting Hierarchical Interactions for Protein Surface Learning",
        "authors": [
            "Yiqun Lin",
            "Liang Pan",
            "Yi Li",
            "Ziwei Liu",
            "Xiaomeng Li"
        ],
        "subjects": [
            "Biomolecules",
            "Machine Learning"
        ],
        "abstract": "Predicting interactions between proteins is one of the most important yet challenging problems in structural bioinformatics. Intrinsically, potential function sites in protein surfaces are determined by both geometric and chemical features. However, existing works only consider handcrafted or individually learned chemical features from the atom type and extract geometric features independently. Here, we identify two key properties of effective protein surface learning: 1) relationship among atoms: atoms are linked with each other by covalent bonds to form biomolecules instead of appearing alone, leading to the significance of modeling the relationship among atoms in chemical feature learning. 2) hierarchical feature interaction: the neighboring residue effect validates the significance of hierarchical feature interaction among atoms and between surface points and atoms (or residues). In this paper, we present a principled framework based on deep learning techniques, namely Hierarchical Chemical and Geometric Feature Interaction Network (HCGNet), for protein surface analysis by bridging chemical and geometric features with hierarchical interactions. Extensive experiments demonstrate that our method outperforms the prior state-of-the-art method by 2.3% in site prediction task and 3.2% in interaction matching task, respectively. Our code is available at https://github.com/xmed-lab/HCGNet.",
        "comments": "Accepted to J-BHI",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10144"
    },
    {
        "doc_id": 145,
        "title": "Correlating fluorescence microscopy, optical and magnetic tweezers to study single chiral biopolymers, tested on DNA plectoneme formation dynamics",
        "authors": [
            "Jack W Shepherd",
            "Sebastien Guilbaud",
            "Zhaokun Zhou",
            "Jamieson Howard",
            "Matthew Burman",
            "Charley Schaefer",
            "Adam Kerrigan",
            "Clare Steele-King",
            "Agnes Noy",
            "Mark C Leake"
        ],
        "subjects": [
            "Biological Physics",
            "Biomolecules"
        ],
        "abstract": "Biopolymer topology is critical for determining interactions inside cell environments, exemplified by DNA where its response to mechanical perturbation is as important as biochemical properties to its cellular roles. The dynamic structures of chiral biopolymers exhibit complex dependence with extension and torsion, however the physical mechanisms underpinning the emergence of structural motifs upon physiological twisting and stretching are poorly understood due to technological limitations in correlating force, torque and spatial localization information. We present COMBI-Tweez (Combined Optical and Magnetic BIomolecule TWEEZers), a transformative tool that overcomes these challenges by integrating optical trapping, time-resolved electromagnetic tweezers, and fluorescence microscopy, demonstrated on single DNA molecules, that can controllably form and visualise higher order structural motifs including plectonemes. This technology combined with cutting-edge MD simulations provides quantitative insight into complex dynamic structures relevant to DNA cellular processes and can be adapted to study a range of filamentous biopolymers.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10087"
    },
    {
        "doc_id": 146,
        "title": "GPU Acceleration of a Conjugate Exponential Model for Cancer Tissue Heterogeneity",
        "authors": [
            "Anik Chaudhuri",
            "Anwoy Mohanty",
            "Manoranjan Satpathy"
        ],
        "subjects": [
            "Distributed, Parallel, and Cluster Computing",
            "Quantitative Methods"
        ],
        "abstract": "Heterogeneity in the cell population of cancer tissues poses many challenges in cancer diagnosis and treatment. Studying the heterogeneity in cell populations from gene expression measurement data in the context of cancer research is a problem of paramount importance. In addition, reducing the computation time of the algorithms that deal with high volumes of data has its obvious merits. Parallelizable models using Markov chain Monte Carlo methods are typically slow. This paper shows a novel, computationally efficient, and parallelizable model to analyze heterogeneity in cancer tissues using GPUs. Because our model is parallelizable, the input data size does not affect the computation time much, provided the hardware resources are not exhausted. Our model uses qPCR (quantitative polymerase chain reaction) gene expression measurements to study heterogeneity in cancer tissue. We compute the cell proportion breakup by accelerating variational methods on a GPU. We test this model on synthetic and real-world gene expression data collected from fibroblasts and compare the performance of our algorithm with those of MCMC and Expectation Maximization. Our new model is computationally less complex and faster than existing Bayesian models for cancer tissue heterogeneity.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10068"
    },
    {
        "doc_id": 147,
        "title": "Cardiac Digital Twin Pipeline for Virtual Therapy Evaluation",
        "authors": [
            "Julia Camps",
            "Zhinuo Jenny Wang",
            "Ruben Doste",
            "Maxx Holmes",
            "Brodie Lawson",
            "Jakub Tomek",
            "Kevin Burrage",
            "Alfonso Bueno-Orovio",
            "Blanca Rodriguez"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Tissues and Organs"
        ],
        "abstract": "Cardiac digital twins are computational tools capturing key functional and anatomical characteristics of patient hearts for investigating disease phenotypes and predicting responses to therapy. When paired with large-scale computational resources and large clinical datasets, digital twin technology can enable virtual clinical trials on virtual cohorts to fast-track therapy development. Here, we present an automated pipeline for personalising ventricular anatomy and electrophysiological function based on routinely acquired cardiac magnetic resonance (CMR) imaging data and the standard 12-lead electrocardiogram (ECG). Using CMR-based anatomical models, a sequential Monte-Carlo approximate Bayesian computational inference method is extended to infer electrical activation and repolarisation characteristics from the ECG. Fast simulations are conducted with a reaction-Eikonal model, including the Purkinje network and biophysically-detailed subcellular ionic current dynamics for repolarisation. For each patient, parameter uncertainty is represented by inferring a population of ventricular models rather than a single one, which means that parameter uncertainty can be propagated to therapy evaluation. Furthermore, we have developed techniques for translating from reaction-Eikonal to monodomain simulations, which allows more realistic simulations of cardiac electrophysiology. The pipeline is demonstrated in a healthy female subject, where our inferred reaction-Eikonal models reproduced the patient's ECG with a Pearson's correlation coefficient of 0.93, and the translated monodomain simulations have a correlation coefficient of 0.89. We then apply the effect of Dofetilide to the monodomain population of models for this subject and show dose-dependent QT and T-peak to T-end prolongations that are in keeping with large population drug response data.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10029"
    },
    {
        "doc_id": 148,
        "title": "An optimization-based equilibrium measure describes non-equilibrium steady state dynamics: application to edge of chaos",
        "authors": [
            "Junbin Qiu",
            "Haiping Huang"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Statistical Mechanics",
            "Neural and Evolutionary Computing"
        ],
        "abstract": "Understanding neural dynamics is a central topic in machine learning, non-linear physics and neuroscience. However, the dynamics is non-linear, stochastic and particularly non-gradient, i.e., the driving force can not be written as gradient of a potential. These features make analytic studies very challenging. The common tool is to use path integral approach or dynamical mean-field theory, but the drawback is one has to solve the integro-differential or dynamical mean-field equations, which is computationally expensive and has no closed form solutions in general. From the aspect of associated Fokker-Planck equation, the steady state solution is generally unknown. Here, we treat searching for the steady state as an optimization problem, and construct an approximate potential closely related to the speed of the dynamics, and find that searching for the ground state of this potential is equivalent to running a stochastic gradient dynamics. The resultant stationary state follows exactly the canonical Boltzmann measure. Within this framework, the quenched disorder intrinsic in the neural networks can be averaged out by applying the replica method. Our theory reproduces the well-known result of edge-of-chaos, and further the order parameters characterizing the continuous transition are derived, and different scaling behavior with respect to inverse temperature in both sides of the transition is also revealed. Our method opens the door to analytically study the steady state landscape of the deterministic or stochastic high dimensional dynamics.",
        "comments": "16 pages, 7 figures",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10009"
    },
    {
        "doc_id": 149,
        "title": "Artificial Intelligence-based algorithms in medical image scan seg-mentation and intelligent visual-content generation -- a concise overview",
        "authors": [
            "Zofia Rudnicka",
            "Janusz Szczepanski",
            "Agnieszka Pregowska"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Recently, Artificial Intelligence (AI)-based algorithms have revolutionized the medical image segmentation processes. Thus, the precise segmentation of organs and their lesions may contribute to an efficient diagnostics process and a more effective selection of targeted therapies as well as increasing the effectiveness of the training process. In this context, AI may contribute to the automatization of the image scan segmentation process and increase the quality of the resulting 3D objects, which may lead to the generation of more realistic virtual objects. In this paper, we focus on the AI-based solutions applied in the medical image scan segmentation, and intelligent visual-content generation, i.e. computer-generated three-dimensional (3D) images in the context of Extended Reality (XR). We consider different types of neural networks used with a special emphasis on the learning rules applied, taking into account algorithm accuracy and performance, as well as open data availability. This paper attempts to summarize the current development of AI-based segmentation methods in medical imaging and intelligent visual content generation that are applied in XR. It concludes also with possible developments and open challenges in AI application in Extended Reality-based solutions. Finally, the future lines of research and development directions of Artificial Intelligence applications both in medical image segmentation and Extended Reality-based medical solutions are discussed",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09857"
    },
    {
        "doc_id": 150,
        "title": "FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction",
        "authors": [
            "Alexander Telepov",
            "Artem Tsypin",
            "Kuzma Khrabrov",
            "Sergey Yakukhnov",
            "Pavel Strashnov",
            "Petr Zhilyaev",
            "Egor Rumiantsev",
            "Daniel Ezhov",
            "Manvel Avetisian",
            "Olga Popova",
            "Artur Kadurin"
        ],
        "subjects": [
            "Biomolecules",
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (arXiv:2110.01219). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches.",
        "comments": "37 pages, 10 figures, to be published in TMLR journal (https://www.jmlr.org/tmlr/)",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09840"
    },
    {
        "doc_id": 151,
        "title": "The impact of Covid-19 vaccination in Aotearoa New Zealand: a modelling study",
        "authors": [
            "Samik Datta",
            "Giorgia Vattiato",
            "Oliver J Maclaren",
            "Ning Hua",
            "Andrew Sporle",
            "Michael J Plank"
        ],
        "subjects": [
            "Populations and Evolution",
            "Physics and Society"
        ],
        "abstract": "Aotearoa New Zealand implemented a Covid-19 elimination strategy in 2020 and 2021, which enabled a large majority of the population to be vaccinated before being exposed to the virus. This strategy delivered one of the lowest pandemic mortality rates in the world. However, quantitative estimates of the population-level health benefits of vaccination are lacking. Here, we use a validated mathematical model to investigate counterfactual scenarios with differing levels of vaccine coverage in different age and ethnicity groups. The model builds on earlier research by adding age- and time-dependent case ascertainment, the effect of antiviral medications, improved hospitalisation rate estimates, and the impact of relaxing control measures. The model was used for scenario analysis and policy advice for the New Zealand Government in 2022 and 2023. We compare the number of Covid-19 hospitalisations, deaths, and years of life lost in each counterfactual scenario to a baseline scenario that is fitted to epidemiological data between January 2022 and June 2023. Our results estimate that vaccines saved 6650 (95% credible interval [4424, 10180]) lives, and prevented 74500 [51000, 115400] years of life lost and 45100 [34400, 55600] hospitalisations during this 18-month period. Making the same comparison before the benefit of antiviral medications is accounted for, the estimated number of lives saved by vaccines increases to 7604 [5080, 11942]. Due to inequities in the vaccine rollout, vaccination rates among M\u0101ori were lower than in people of European ethnicity. Our results show that, if vaccination rates had been equitable, an estimated 11-26% of the 292 M\u0101ori Covid-19 deaths that were recorded in this time period could have been prevented. We conclude that Covid-19 vaccination greatly reduced health burden in New Zealand and that equity needs to be a key focus of future vaccination programmes.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09679"
    },
    {
        "doc_id": 152,
        "title": "Functional Linear Non-Gaussian Acyclic Model for Causal Discovery",
        "authors": [
            "Tian-Le Yang",
            "Kuang-Yao Lee",
            "Kun Zhang",
            "Joe Suzuki"
        ],
        "subjects": [
            "Machine Learning",
            "Statistics Theory",
            "Neurons and Cognition",
            "Methodology"
        ],
        "abstract": "In causal discovery, non-Gaussianity has been used to characterize the complete configuration of a Linear Non-Gaussian Acyclic Model (LiNGAM), encompassing both the causal ordering of variables and their respective connection strengths. However, LiNGAM can only deal with the finite-dimensional case. To expand this concept, we extend the notion of variables to encompass vectors and even functions, leading to the Functional Linear Non-Gaussian Acyclic Model (Func-LiNGAM). Our motivation stems from the desire to identify causal relationships in brain-effective connectivity tasks involving, for example, fMRI and EEG datasets. We demonstrate why the original LiNGAM fails to handle these inherently infinite-dimensional datasets and explain the availability of functional data analysis from both empirical and theoretical perspectives. {We establish theoretical guarantees of the identifiability of the causal relationship among non-Gaussian random vectors and even random functions in infinite-dimensional Hilbert spaces.} To address the issue of sparsity in discrete time points within intrinsic infinite-dimensional functional data, we propose optimizing the coordinates of the vectors using functional principal component analysis. Experimental results on synthetic data verify the ability of the proposed framework to identify causal relationships among multivariate functions using the observed samples. For real data, we focus on analyzing the brain connectivity patterns derived from fMRI data.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09641"
    },
    {
        "doc_id": 153,
        "title": "Molecular causality in the advent of foundation models",
        "authors": [
            "Sebastian Lobentanzer",
            "Pablo Rodriguez-Mier",
            "Stefan Bauer",
            "Julio Saez-Rodriguez"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "Correlation is not causation. As simple as this widely agreed-upon statement may seem, scientifically defining causality and using it to drive our modern biomedical research is immensely challenging. In this perspective, we attempt to synergise the partly disparate fields of systems biology, causal reasoning, and machine learning, to inform future approaches in the field of systems biology and molecular networks.",
        "comments": "22 pages, 0 figures, 87 references; submitted to MSB",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09558"
    },
    {
        "doc_id": 154,
        "title": "Dimensional Neuroimaging Endophenotypes: Neurobiological Representations of Disease Heterogeneity Through Machine Learning",
        "authors": [
            "Junhao Wen",
            "Mathilde Antoniades",
            "Zhijian Yang",
            "Gyujoon Hwang",
            "Ioanna Skampardoni",
            "Rongguang Wang",
            "Christos Davatzikos"
        ],
        "subjects": [
            "Machine Learning",
            "Image and Video Processing",
            "Quantitative Methods"
        ],
        "abstract": "Machine learning has been increasingly used to obtain individualized neuroimaging signatures for disease diagnosis, prognosis, and response to treatment in neuropsychiatric and neurodegenerative disorders. Therefore, it has contributed to a better understanding of disease heterogeneity by identifying disease subtypes that present significant differences in various brain phenotypic measures. In this review, we first present a systematic literature overview of studies using machine learning and multimodal MRI to unravel disease heterogeneity in various neuropsychiatric and neurodegenerative disorders, including Alzheimer disease, schizophrenia, major depressive disorder, autism spectrum disorder, multiple sclerosis, as well as their potential in transdiagnostic settings. Subsequently, we summarize relevant machine learning methodologies and discuss an emerging paradigm which we call dimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological heterogeneity of neuropsychiatric and neurodegenerative disorders into a low dimensional yet informative, quantitative brain phenotypic representation, serving as a robust intermediate phenotype (i.e., endophenotype) largely reflecting underlying genetics and etiology. Finally, we discuss the potential clinical implications of the current findings and envision future research avenues.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09517"
    },
    {
        "doc_id": 155,
        "title": "Is the Emergence of Life an Expected Phase Transition in the Evolving Universe?",
        "authors": [
            "Stuart Kauffman",
            "Andrea Roli"
        ],
        "subjects": [
            "Populations and Evolution",
            "Biological Physics"
        ],
        "abstract": "We propose a novel definition of life in terms of which its emergence in the universe is expected, and its ever-creative open-ended evolution is entailed by no law. Living organisms are Kantian Wholes that achieve Catalytic Closure, Constraint Closure, and Spatial Closure. We here unite for the first time two established mathematical theories, namely Collectively Autocatalytic Sets and the Theory of the Adjacent Possible. The former establishes that a first-order phase transition to molecular reproduction is expected in the chemical evolution of the universe where the diversity and complexity of molecules increases; the latter posits that, under loose hypotheses, if the system starts with a small number of beginning molecules, each of which can combine with copies of itself or other molecules to make new molecules, over time the number of kinds of molecules increases slowly but then explodes upward hyperbolically. Together these theories imply that life is expected as a phase transition in the evolving universe. The familiar distinction between software and hardware loses its meaning in living cells. We propose new ways to study the phylogeny of metabolisms, new astronomical ways to search for life on exoplanets, new experiments to seek the emergence of the most rudimentary life, and the hint of a coherent testable pathway to prokaryotes with template replication and coding.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09514"
    },
    {
        "doc_id": 156,
        "title": "Role of Upwelling on Larval Dispersal and Productivity of Gooseneck Barnacle Populations in the Cantabrian Sea: Management Implications",
        "authors": [
            "Antonella Rivera",
            "Nicolas Weidberg",
            "Antonio F. Pardi\u00f1as",
            "Ricardo Gonzalez-Gil",
            "Luc\u0131a Garc\u0131a- Florez",
            "Jose Luis Acu\u00f1a"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "The effect of coastal upwelling on the recruitment and connectivity of coastal marine populations has rarely been characterized to a level of detail to be included into sound fishery management strategies. The gooseneck barnacle (Pollicipes pollicipes) fishery at the Cantabrian Coast (Northern Spain) is located at the fringes of the NW Spanish Upwelling system. This fishery is being co-managed through a fine-scale, interspersed set of protected rocks where each rock receives a distinct level of protection. Such interspersion is potentially beneficial, but the extent to which such spacing is consistent with mean larval dispersal distances is as yet unknown. We have simulated the spread of gooseneck barnacle larvae in the Central Cantabrian Coast using a high-resolution time-series of current profiles measured at a nearshore location. During a year of high upwelling activity (2009), theoretical recruitment success was 94% with peak recruitment predicted 56 km west of the emission point. However, for a year of low upwelling activity (2011) theoretical recruitment success dropped to 15.4% and peak recruitment was expected 13 km east of the emission point. This is consistent with a positive correlation between catch rates and the Integrated Upwelling Index, using a 4-year lag to allow recruits to reach commercial size. Furthermore, a net long-term westward larval transport was estimated by means of mitochondrial cytochrome c oxidase subunit I (COI) sequences for five populations in the Cantabrian Sea. Our results call into question the role of long distance dispersal, driven by the mesoscale processes in the area, in gooseneck barnacle populations and point to the prevalent role of small-scale, asymmetric connectivity more consistent with the typical scale of the co-management process in this fishery.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09513"
    },
    {
        "doc_id": 157,
        "title": "A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation",
        "authors": [
            "Nianzu Yang",
            "Kaipeng Zeng",
            "Haotian Lu",
            "Yexin Wu",
            "Zexin Yuan",
            "Shengdian Jiang",
            "Jiaxiang Wu",
            "Yimin Wang",
            "Junchi Yan"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Neuronal morphology is essential for studying brain functioning and understanding neurodegenerative disorders. As the acquiring of real-world morphology data is expensive, computational approaches especially learning-based ones e.g. MorphVAE for morphology generation were recently studied, which are often conducted in a way of randomly augmenting a given authentic morphology to achieve plausibility. Under such a setting, this paper proposes \\textbf{MorphGrower} which aims to generate more plausible morphology samples by mimicking the natural growth mechanism instead of a one-shot treatment as done in MorphVAE. Specifically, MorphGrower generates morphologies layer by layer synchronously and chooses a pair of sibling branches as the basic generation block, and the generation of each layer is conditioned on the morphological structure of previous layers and then generate morphologies via a conditional variational autoencoder with spherical latent space. Extensive experimental results on four real-world datasets demonstrate that MorphGrower outperforms MorphVAE by a notable margin. Our code will be publicly available to facilitate future research.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09500"
    },
    {
        "doc_id": 158,
        "title": "Gene-associated Disease Discovery Powered by Large Language Models",
        "authors": [
            "Jiayu Chang",
            "Shiyu Wang",
            "Chen Ling",
            "Zhaohui Qin",
            "Liang Zhao"
        ],
        "subjects": [
            "Quantitative Methods",
            "Information Retrieval"
        ],
        "abstract": "The intricate relationship between genetic variation and human diseases has been a focal point of medical research, evidenced by the identification of risk genes regarding specific diseases. The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment. To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes. This framework aims to automate the labor-intensive process of sifting through medical literature for evidence linking genetic variations to diseases, thereby enhancing the efficiency of disease identification. Our approach involves using LLMs to conduct literature searches, summarize relevant findings, and pinpoint diseases related to specific genes. This paper details the development and application of our LLM-powered framework, demonstrating its potential in streamlining the complex process of literature retrieval and summarization to identify diseases associated with specific genetic variations.",
        "comments": "This is the official paper accepted by AAAI 2024 Workshop on Large Language Models for Biological Discoveries",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09490"
    },
    {
        "doc_id": 159,
        "title": "The Interplay Between Logical Phenomena and the Cognitive System of the Mind",
        "authors": [
            "Kazem Haghnejad Azar"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "In this article, we employ mathematical concepts as a tool to examine the phenomenon of consciousness experience and logical phenomena. Through our investigation, we aim to demonstrate that our experiences, while not confined to limitations, cannot be neatly encapsulated within a singular collection. Our conscious experience emerges as a result of the developmental and augmentative trajectory of our cognitive system. As our cognitive abilities undergo refinement and advancement, our capacity for logical thinking likewise evolves, thereby manifesting a heightened level of conscious experience. The primary objective of this article is to embark upon a profound exploration of the concept of logical experience, delving into the intricate process by which these experiences are derived from our mind.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09465"
    },
    {
        "doc_id": 160,
        "title": "Diffusion-Driven Generative Framework for Molecular Conformation Prediction",
        "authors": [
            "Bobin Yang",
            "Jie Deng",
            "Zhenghan Chen",
            "Ruoxue Wu"
        ],
        "subjects": [
            "Biomolecules",
            "Artificial Intelligence",
            "Machine Learning",
            "Chemical Physics"
        ],
        "abstract": "The task of deducing three-dimensional molecular configurations from their two-dimensional graph representations holds paramount importance in the fields of computational chemistry and pharmaceutical development. The rapid advancement of machine learning, particularly within the domain of deep generative networks, has revolutionized the precision of predictive modeling in this context. Traditional approaches often adopt a two-step strategy: initially estimating interatomic distances and subsequently refining the spatial molecular structure by solving a distance geometry problem. However, this sequential approach occasionally falls short in accurately capturing the intricacies of local atomic arrangements, thereby compromising the fidelity of the resulting structural models. Addressing these limitations, this research introduces a cutting-edge generative framework named \\method{}. This framework is grounded in the principles of diffusion observed in classical non-equilibrium thermodynamics. \\method{} views atoms as discrete entities and excels in guiding the reversal of diffusion, transforming a distribution of stochastic noise back into coherent molecular structures through a process akin to a Markov chain. This transformation commences with the initial representation of a molecular graph in an abstract latent space, culminating in the realization of three-dimensional structures via a sophisticated bilevel optimization scheme meticulously tailored to meet the specific requirements of the task. One of the formidable challenges in this modeling endeavor involves preserving roto-translational invariance to ensure that the generated molecular conformations adhere to the laws of physics. Extensive experimental evaluations confirm the efficacy of the proposed \\method{} in comparison to state-of-the-art methods.",
        "comments": "arXiv admin note: text overlap with arXiv:2105.07246 by other authors",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09451"
    },
    {
        "doc_id": 161,
        "title": "Regenerative Medicine for Tendon/Ligament Injuries: De Novo Equine Tendon/Ligament Neotissue Generation and Application",
        "authors": [
            "Takashi Taguchi"
        ],
        "subjects": [
            "Tissues and Organs"
        ],
        "abstract": "Tendon and ligament injuries are debilitating conditions across species. Poor regenerative capacities of these tissues limit restoration of original functions. The first study of this dissertation evaluated the effect of cellular administration on tendon/ligament injuries in horses using meta-analysis. The findings led to the second study that engineered implantable de novo tendon neotissue using equine adipose-derived multipotent stromal cells and collagen type I. The neotendon was evaluated for its biocompatibility and therapeutic potential in the third study using immunocompetent and immunocompromised rat bilateral calcaneal tendon elongation model. The fourth study investigated the therapeutic effects of neotendon in surgically-induced non-terminal equine accessory ligament of deep digital flexor tendon injury model.",
        "comments": " ",
        "date": "24 October, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.09423"
    },
    {
        "doc_id": 162,
        "title": "PERMUTOOLS: A MATLAB Package for Multivariate Permutation Testing",
        "authors": [
            "Michael J. Crosse",
            "John J. Foxe",
            "Sophie Molholm"
        ],
        "subjects": [
            "Methodology",
            "Quantitative Methods",
            "Computation"
        ],
        "abstract": "Statistical hypothesis testing and effect size measurement are routine parts of quantitative research. Advancements in computer processing power have greatly improved the capability of statistical inference through the availability of resampling methods. However, many of the statistical practices used today are based on traditional, parametric methods that rely on assumptions about the underlying population. These assumptions may not always be valid, leading to inaccurate results and misleading interpretations. Permutation testing, on the other hand, generates the sampling distribution empirically by permuting the observed data, providing distribution-free hypothesis testing. Furthermore, this approach lends itself to a powerful method for multiple comparison correction - known as max correction - which is less prone to type II errors than conventional correction methods. Parametric methods have also traditionally been utilized for estimating the confidence interval of various test statistics and effect size measures. However, these too can be estimated empirically using permutation or bootstrapping techniques. Whilst resampling methods are generally considered preferable, many popular programming languages and statistical software packages lack efficient implementations. Here, we introduce PERMUTOOLS, a MATLAB package for multivariate permutation testing and effect size measurement.",
        "comments": "7 pages, 2 figures, for PERMUTOOLS toolbox, see https://github.com/mickcrosse/PERMUTOOLS",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09401"
    },
    {
        "doc_id": 163,
        "title": "Graph-based vulnerability assessment of resting-state functional brain networks in full-term neonates",
        "authors": [
            "Mahshid Fouladivanda",
            "Kamran Kazemi",
            "Habibollah Danyali",
            "Ardalan Aarabi"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Quantitative Methods"
        ],
        "abstract": "Network disruption during early brain development can result in long-term cognitive impairments. In this study, we investigated rich-club organization in resting-state functional brain networks in full-term neonates using a multiscale connectivity analysis. We further identified the most influential nodes, also called spreaders, having higher impacts on the flow of information throughout the network. The network vulnerability to damage to rich-club (RC) connectivity within and between resting-state networks was also assessed using a graph-based vulnerability analysis. Our results revealed a rich club organization and small-world topology for resting-state functional brain networks in full term neonates, regardless of the network size. Interconnected mostly through short-range connections, functional rich-club hubs were confined to sensory-motor, cognitive-attention-salience (CAS), default mode, and language-auditory networks with an average cross-scale overlap of 36%, 20%, 15% and 12%, respectively. The majority of the functional hubs also showed high spreading potential, except for several non-RC spreaders within CAS and temporal networks. The functional networks exhibited high vulnerability to loss of RC nodes within sensorimotor cortices, resulting in a significant increase and decrease in network segregation and integration, respectively. The network vulnerability to damage to RC nodes within the language-auditory, cognitive-attention-salience, and default mode networks was also significant but relatively less prominent. Our findings suggest that the network integration in neonates can be highly compromised by damage to RC connectivity due to brain immaturity.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09255"
    },
    {
        "doc_id": 164,
        "title": "Reproducibility via neural fields of visual illusions induced by localized stimuli",
        "authors": [
            "Cyprien Tamekue",
            "Dario Prandi",
            "Yacine Chitour"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Analysis of PDEs",
            "Numerical Analysis",
            "Pattern Formation and Solitons"
        ],
        "abstract": "This paper investigates the replication of experiments by Billock and Tsou [PNAS, 2007] using the controllability of neural fields of Amari-type modelling the cortical activity in the primary visual cortex (V1), focusing on a regular funnel pattern localised in the fovea or the peripheral visual field. The aim is to understand and model the visual phenomena observed in these experiments, emphasising their nonlinear nature. The study involves designing sensory inputs simulating the visual stimuli from Billock and Tsou's experiments. The after-images induced by these inputs are then theoretically and numerically studied to determine their capacity to replicate the experimentally observed visual effects. A key aspect of this research is investigating the effects induced by the nonlinear nature of neural responses. In particular, by highlighting the importance of both excitatory and inhibitory neurons in the emergence of certain visual phenomena, this study suggests that an interplay of both types of neuronal activities plays an essential role in visual processes, challenging the assumption that the latter is mainly driven by excitatory activities alone.",
        "comments": "MSC Class:          92C20; 35B36; 45A05; 45G15; 45K05; 65R20",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09108"
    },
    {
        "doc_id": 165,
        "title": "A hybrid tau-leap for simulating chemical kinetics with applications to parameter estimation",
        "authors": [
            "Thomas Trigo Trindade",
            "Konstantinos C. Zygalakis"
        ],
        "subjects": [
            "Molecular Networks",
            "Numerical Analysis",
            "Computation"
        ],
        "abstract": "We consider the problem of efficiently simulating stochastic models of chemical kinetics. The Gillespie Stochastic Simulation algorithm (SSA) is often used to simulate these models, however, in many scenarios of interest, the computational cost quickly becomes prohibitive. This is further exasperated in the Bayesian inference context when estimating parameters of chemical models, as the intractability of the likelihood requires multiple simulations of the underlying system. To deal with issues of computational complexity in this paper, we propose a novel hybrid $\u03c4$-leap algorithm for simulating well-mixed chemical systems. In particular, the algorithm uses $\u03c4$-leap when appropriate (high population densities), and SSA when necessary (low population densities, when discrete effects become non-negligible). In the intermediate regime, a combination of the two methods, which leverages the properties of the underlying Poisson formulation, is employed. As illustrated through a number of numerical experiments the hybrid $\u03c4$ offers significant computational savings when compared to SSA without however sacrificing the overall accuracy. This feature is particularly welcomed in the Bayesian inference context, as it allows for parameter estimation of stochastic chemical kinetics at reduced computational cost.",
        "comments": "25 pages, 8 figures",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09097"
    },
    {
        "doc_id": 166,
        "title": "Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction",
        "authors": [
            "Ziyang Yu",
            "Wenbing Huang",
            "Yang Liu"
        ],
        "subjects": [
            "Machine Learning",
            "Biomolecules"
        ],
        "abstract": "The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we propose a novel learning-based method called ElliDock, which predicts an elliptic paraboloid to represent the protein-protein docking interface. To be specific, our model estimates elliptic paraboloid interfaces for the two input proteins respectively, and obtains the roto-translation transformation for docking by making two interfaces coincide. By its design, ElliDock is independently equivariant with respect to arbitrary rotations/translations of the proteins, which is an indispensable property to ensure the generalization of the docking process. Experimental evaluations show that ElliDock achieves the fastest inference time among all compared methods and is strongly competitive with current state-of-the-art learning-based models such as DiffDock-PP and Multimer particularly for antibody-antigen docking.",
        "comments": "ICLR 2024",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08986"
    },
    {
        "doc_id": 167,
        "title": "From Physics to Sentience: Deciphering the Semantics of the Free-Energy Principle and Evaluating its Claims",
        "authors": [
            "Zahra Sheikhbahaee",
            "Adam Safron",
            "Casper Hesp",
            "Guillaume Dumas"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "The Free-Energy Principle (FEP) [1-3] has been adopted in a variety of ambitious proposals that aim to characterize all adaptive, sentient, and cognitive systems within a unifying framework. Judging by the amount of attention it has received from the scientific community, the FEP has gained significant traction in these pursuits. The current target article represents an important iteration of this research paradigm in formally describing emergent dynamics rather than merely (quasi-)steady states. This affords more in-depth considerations of the spatio-temporal complexities of cross-scale causality - as we have encouraged and built towards in previous publications (e.g., [4-9]). In this spirit of constructive feedback, we submit a few technical comments on some of the matters that appear to require further attention, in order to improve the clarity, rigour, and applicability of this framework.",
        "comments": " ",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08873"
    },
    {
        "doc_id": 168,
        "title": "Using i-vectors for subject-independent cross-session EEG transfer learning",
        "authors": [
            "Jonathan Lasko",
            "Jeff Ma",
            "Mike Nicoletti",
            "Jonathan Sussman-Fort",
            "Sooyoung Jeong",
            "William Hartmann"
        ],
        "subjects": [
            "Machine Learning",
            "Computation and Language",
            "Sound",
            "Audio and Speech Processing",
            "Neurons and Cognition"
        ],
        "abstract": "Cognitive load classification is the task of automatically determining an individual's utilization of working memory resources during performance of a task based on physiologic measures such as electroencephalography (EEG). In this paper, we follow a cross-disciplinary approach, where tools and methodologies from speech processing are used to tackle this problem. The corpus we use was released publicly in 2021 as part of the first passive brain-computer interface competition on cross-session workload estimation. We present our approach which used i-vector-based neural network classifiers to accomplish inter-subject cross-session EEG transfer learning, achieving 18% relative improvement over equivalent subject-dependent models. We also report experiments showing how our subject-independent models perform competitively on held-out subjects and improve with additional subject data, suggesting that subject-dependent training is not required for effective cognitive load determination.",
        "comments": "11 pages",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08851"
    },
    {
        "doc_id": 169,
        "title": "On the maximum value of the stairs2 index",
        "authors": [
            "Bryan Currie",
            "Kristina Wicke"
        ],
        "subjects": [
            "Combinatorics",
            "Populations and Evolution"
        ],
        "abstract": "Measures of tree balance play an important role in different research areas such as mathematical phylogenetics or theoretical computer science. The balance of a tree is usually quantified in a single number, called a balance or imbalance index, and several such indices exist in the literature. Here, we focus on the stairs2 balance index for rooted binary trees, which was first introduced in the context of viral phylogenetics but has not been fully analyzed from a mathematical viewpoint yet. While it is known that the caterpillar tree uniquely minimizes the stairs2 index for all leaf numbers and the fully balanced tree uniquely maximizes the stairs2 index for leaf numbers that are powers of two, understanding the maximum value and maximal trees for arbitrary leaf numbers is an open problem in the literature. In this note, we fill this gap by showing that for all leaf numbers, there is a unique rooted binary tree maximizing the stairs2 index. Additionally, we obtain recursive and closed expressions for the maximum value of the stairs2 index of a rooted binary tree with $n$ leaves.",
        "comments": "12 pages, 1 figure",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08838"
    },
    {
        "doc_id": 170,
        "title": "Mechanical constraints and cell cycle regulation in models of collective cell migration",
        "authors": [
            "Carles Falc\u00f3",
            "Daniel J. Cohen",
            "Jos\u00e9 A. Carrillo",
            "Ruth E. Baker"
        ],
        "subjects": [
            "Quantitative Methods",
            "Biological Physics"
        ],
        "abstract": "The spatiotemporal coordination and regulation of cell proliferation is fundamental in many aspects of development and tissue maintenance. Cells have the ability to adapt their division rates in response to mechanical checkpoints, yet we do not fully understand how cell proliferation regulation impacts cell migration phenomena. Here, we present a minimal continuum model of cell migration with cell cycle dynamics, which includes mechanical constraints and hence can account for cell proliferation regulation. By combining minimal mathematical modelling, Bayesian inference, and recent experimental data, we quantify the impact of mechanical constraints across different cell cycle stages in epithelial tissue expansion experiments. Our model suggests that cells sense local density and adapt cell cycle progression in response, during G1 and the combined S/G2/M phases, providing an explicit relationship between each cell cycle stage duration and local tissue density, which is consistent with several experimental observations. Finally, we compare our mathematical model predictions to different experiments studying cell cycle regulation and present a quantitative analysis on the impact of mechanical constraints on cell migration patterns. Our work presents a systematic approach for investigating and analysing cell cycle data, providing mechanistic insights into how individual cells regulate proliferation, based on population-based experimental measurements.",
        "comments": " ",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08805"
    },
    {
        "doc_id": 171,
        "title": "Machine Learning-Based Analysis of Ebola Virus' Impact on Gene Expression in Nonhuman Primates",
        "authors": [
            "Mostafa Rezapour",
            "Muhammad Khalid Khan Niazi",
            "Hao Lu",
            "Aarthi Narayanan",
            "Metin Nafi Gurcan"
        ],
        "subjects": [
            "Genomics",
            "Machine Learning"
        ],
        "abstract": "This study introduces the Supervised Magnitude-Altitude Scoring (SMAS) methodology, a machine learning-based approach, for analyzing gene expression data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV). We utilize a comprehensive dataset of NanoString gene expression profiles from Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen interaction analysis. SMAS effectively combines gene selection based on statistical significance and expression changes, employing linear classifiers such as logistic regression to accurately differentiate between RT-qPCR positive and negative NHP samples. A key finding of our research is the identification of IFI6 and IFI27 as critical biomarkers, demonstrating exceptional predictive performance with 100% accuracy and Area Under the Curve (AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6 and IFI27, genes, including MX1, OAS1, and ISG15, were significantly upregulated, highlighting their essential roles in the immune response to EBOV. Our results underscore the efficacy of the SMAS method in revealing complex genetic interactions and response mechanisms during EBOV infection. This research provides valuable insights into EBOV pathogenesis and aids in developing more precise diagnostic tools and therapeutic strategies to address EBOV infection in particular and viral infection in general.",
        "comments": "28 pages, 8 figures, 2 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08738"
    },
    {
        "doc_id": 172,
        "title": "Survival Analysis of Young Triple-Negative Breast Cancer Patients",
        "authors": [
            "M. Mehdi Owrang O",
            "Fariba Jafari Horestani",
            "Ginger Schwarz"
        ],
        "subjects": [
            "Quantitative Methods",
            "Machine Learning",
            "Applications"
        ],
        "abstract": "Breast cancer prognosis is crucial for effective treatment, with the disease more common in women over 40 years old but rare under 40 years old, where less than 5 percent of cases occur in the U.S. Studies indicate a worse prognosis in younger women, which varies by ethnicity. Breast cancers are classified based on receptors like estrogen, progesterone, and HER2. Triple-negative breast cancer (TNBC), lacking these receptors, accounts for about 15 percent of cases and is more prevalent in younger patients, often resulting in poorer outcomes. Nevertheless, the impact of age on TNBC prognosis remains unclear. Factors like age, race, tumor grade, size, and lymph node status are studied for their role in TNBC's clinical outcomes, but current research is inconclusive about age-related differences. This study uses SEER data set to examine the influence of younger age on survivability in TNBC patients, aiming to determine if age is a significant prognostic factor. Our experimental results on SEER dataset confirm the existing research reports that TNBC patients have worse prognosis compared to non-TNBC based on age. Our main goal was to investigate whether younger age has any significance on the survivability of TNBC patients. Experimental results do not show that younger age has any significance on the prognosis and survival rate of the TNBC patients",
        "comments": "31 Pages, 11 Figures, 7 Tables, Peer-reviewed article",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08712"
    },
    {
        "doc_id": 173,
        "title": "On Image Search in Histopathology",
        "authors": [
            "H. R. Tizhoosh",
            "Liron Pantanowitz"
        ],
        "subjects": [
            "Image and Video Processing",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition",
            "Information Retrieval",
            "Quantitative Methods"
        ],
        "abstract": "Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.",
        "comments": " ",
        "date": "14 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08699"
    },
    {
        "doc_id": 174,
        "title": "Concept Alignment",
        "authors": [
            "Sunayana Rane",
            "Polyphony J. Bruna",
            "Ilia Sucholutsky",
            "Christopher Kello",
            "Thomas L. Griffiths"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Neurons and Cognition"
        ],
        "abstract": "Discussion of AI alignment (alignment between humans and AI systems) has focused on value alignment, broadly referring to creating AI systems that share human values. We argue that before we can even attempt to align values, it is imperative that AI systems and humans align the concepts they use to understand the world. We integrate ideas from philosophy, cognitive science, and deep learning to explain the need for concept alignment, not just value alignment, between humans and machines. We summarize existing accounts of how humans and machines currently learn concepts, and we outline opportunities and challenges in the path towards shared concepts. Finally, we explain how we can leverage the tools already being developed in cognitive science and AI research to accelerate progress towards concept alignment.",
        "comments": "NeurIPS MP2 Workshop 2023",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08672"
    },
    {
        "doc_id": 175,
        "title": "Validation and Comparison of Non-Stationary Cognitive Models: A Diffusion Model Application",
        "authors": [
            "Lukas Schumacher",
            "Martin Schnuerch",
            "Andreas Voss",
            "Stefan T. Radev"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Methodology"
        ],
        "abstract": "Cognitive processes undergo various fluctuations and transient states across different temporal scales. Superstatistics are emerging as a flexible framework for incorporating such non-stationary dynamics into existing cognitive model classes. In this work, we provide the first experimental validation of superstatistics and formal comparison of four non-stationary diffusion decision models in a specifically designed perceptual decision-making task. Task difficulty and speed-accuracy trade-off were systematically manipulated to induce expected changes in model parameters. To validate our models, we assess whether the inferred parameter trajectories align with the patterns and sequences of the experimental manipulations. To address computational challenges, we present novel deep learning techniques for amortized Bayesian estimation and comparison of models with time-varying parameters. Our findings indicate that transition models incorporating both gradual and abrupt parameter shifts provide the best fit to the empirical data. Moreover, we find that the inferred parameter trajectories closely mirror the sequence of experimental manipulations. Posterior re-simulations further underscore the ability of the models to faithfully reproduce critical data patterns. Accordingly, our results suggest that the inferred non-stationary dynamics may reflect actual changes in the targeted psychological constructs. We argue that our initial experimental validation paves the way for the widespread application of superstatistics in cognitive modeling and beyond.",
        "comments": " ",
        "date": "7 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08626"
    },
    {
        "doc_id": 176,
        "title": "Dynamic Brain Behaviours in Stroke: A Longitudinal Investigation Based on fMRI Analysis",
        "authors": [
            "Kaichao Wu",
            "Beth Jelfs",
            "Katrina Neville",
            "Qiang Fang"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Background: The brain's functional network constantly adapts to external changes. However, the mechanisms underlying this dynamic adaptive behavior in stroke patients with motor injuries and its role in post-stroke motor recovery remain poorly understood.\n  Method: This study conducted a long-term investigation involving 15 first-stroke patients. Each participant underwent five fMRI scans distributed equally over a six-month period. Using functional neuroimaging data, time-varying functional modularity in post-stroke patients was detected, and subsequently, the dynamic brain behaviors, including recruitment, integration, and flexibility, along with their longitudinal changes, were assessed.\n  Results: Our findings reveal that stroke lesions lead to significant and enduring alterations in all three dynamic behaviors within functional brain networks. Furthermore, during the six-month recovery period, patients who exhibited good and poor recovery showed notable differences in recruitment and flexibility, indicating distinct recovery trajectories for these groups. Notably, when predicting post-stroke recovery status, whole-brain recruitment emerged as a robust and reliable feature, achieving an AUC of 85.93\n  Significance: Our study offers a comprehensive depiction of dynamic brain behavior in the post-ischemic-stroke brain, with a focus on longitudinal changes concurrent with functional recovery. These dynamic patterns hold promise as valuable tools for evaluating and predicting motor recovery following stroke.",
        "comments": " ",
        "date": "28 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08607"
    },
    {
        "doc_id": 177,
        "title": "Long cycles in linear thresholding systems",
        "authors": [
            "Anna Laddach",
            "Michael Shapiro"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Linear thresholding systems have been used as a model of neural activation and more recently proposed as a model of gene regulation. Here we exhibit linear thresholding systems whose dynamics produce surprisingly long cycles.",
        "comments": "3 pages",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08605"
    },
    {
        "doc_id": 178,
        "title": "From Conceptual Spaces to Quantum Concepts: Formalising and Learning Structured Conceptual Models",
        "authors": [
            "Sean Tull",
            "Razin A. Shaikh",
            "Sara Sabrina Zemljic",
            "Stephen Clark"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Artificial Intelligence",
            "Quantum Physics"
        ],
        "abstract": "In this article we present a new modelling framework for structured concepts using a category-theoretic generalisation of conceptual spaces, and show how the conceptual representations can be learned automatically from data, using two very different instantiations: one classical and one quantum. A contribution of the work is a thorough category-theoretic formalisation of our framework. We claim that the use of category theory, and in particular the use of string diagrams to describe quantum processes, helps elucidate some of the most important features of our approach. We build upon Gardenfors' classical framework of conceptual spaces, in which cognition is modelled geometrically through the use of convex spaces, which in turn factorise in terms of simpler spaces called domains. We show how concepts from the domains of shape, colour, size and position can be learned from images of simple shapes, where concepts are represented as Gaussians in the classical implementation, and quantum effects in the quantum one. In the classical case we develop a new model which is inspired by the Beta-VAE model of concepts, but is designed to be more closely connected with language, so that the names of concepts form part of the graphical model. In the quantum case, concepts are learned by a hybrid classical-quantum network trained to perform concept classification, where the classical image processing is carried out by a convolutional neural network and the quantum representations are produced by a parameterised quantum circuit. Finally, we consider the question of whether our quantum models of concepts can be considered conceptual spaces in the Gardenfors sense.",
        "comments": "This article consolidates our previous reports on concept formalisation and learning: arXiv:2302.14822 and arXiv:2203.11216",
        "date": "6 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08585"
    },
    {
        "doc_id": 179,
        "title": "How cytoskeletal crosstalk makes cells move: bridging cell-free and cell studies",
        "authors": [
            "James P. Conboy",
            "Irene Ist\u00fariz Petitjean",
            "Anouk van der Net",
            "Gijsje H. Koenderink"
        ],
        "subjects": [
            "Biological Physics",
            "Cell Behavior"
        ],
        "abstract": "Cell migration is a fundamental process for life and is highly dependent on the dynamical and mechanical properties of the cytoskeleton. Intensive physical and biochemical crosstalk between actin, microtubules, and intermediate filaments ensures their coordination to facilitate and enable migration. In this review we discuss the different mechanical aspects that govern cell migration and provide, for each mechanical aspect, a novel perspective by juxtaposing two complementary approaches to the biophysical study of cytoskeletal crosstalk: live-cell studies (often referred to as top-down studies) and cell-free studies (often referred to as bottom-up studies). We summarize the main findings from both experimental approaches, and we provide our perspective on bridging the two perspectives to address the open questions of how cytoskeletal crosstalk governs cell migration and makes cells move.",
        "comments": "4 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08368"
    },
    {
        "doc_id": 180,
        "title": "dabih -- encrypted data storage and sharing platform",
        "authors": [
            "Michael Huttner",
            "Jakob Simeth",
            "Renato Liguori",
            "Fulvia Ferrazzi",
            "Rainer Spang"
        ],
        "subjects": [
            "Cryptography and Security",
            "Software Engineering",
            "Genomics"
        ],
        "abstract": "Background: The secure management of sensitive clinical data, particularly human genomics data, has become a critical requirement in modern biomedical research. Although the necessary software and algorithms are readily available, their use by non-IT experts poses significant challenges.\n  Methods: We developed dabih, an open-source web application specifically designed to facilitate user-friendly encrypted data management. dabih enables web-based uploading, storing, sharing, and downloading of sensitive data in any format. Its approach to data security involves a two-stage envelope encryption process. We combine symmetric-key encryption for data and public-key encryption as key encapsulation mechanism. The private key necessary for decrypting the data remains exclusively on the owner's device. Thus, accessing data is impossible without explicit permission from the keyholder.\n  Results: dabih is available open-source on GitHub https://github.com/spang-lab/dabih, as ready to use containers on docker hub and includes a command line interface and a graphical bulk upload tool as pre-built binaries. Documentation is available as part of the web application.\n  Conclusions: dabih enables everyone to use strong cryptography for their data, while being just as simple to use as other, non-encrypted, data storage solutions. All the cryptography occurs seamlessly in the background as users interact with a secure web portal, simply by dragging and dropping files.",
        "comments": "16 pages including 4 figures and 5 appendices",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08333"
    },
    {
        "doc_id": 181,
        "title": "Multifractal organization of EEG signals in Multiple Sclerosis",
        "authors": [
            "Marcin W\u0105torek",
            "Wojciech Tomczyk",
            "Magda Gaw\u0142owska",
            "Natalia Golonka-Afek",
            "Aleksandra \u017byrkowska",
            "Monika Marona",
            "Marcin Wnuk",
            "Agnieszka S\u0142owik",
            "Jeremi K. Ochab",
            "Magdalena Fafrowicz",
            "Tadeusz Marek",
            "Pawe\u0142 O\u015bwi\u0119cimka"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Disordered Systems and Neural Networks",
            "Adaptation and Self-Organizing Systems",
            "Quantitative Methods"
        ],
        "abstract": "Quantifying the complex/multifractal organization of the brain signals is crucial to fully understanding the brain processes and structure. In this contribution, we performed the multifractal analysis of the electroencephalographic (EEG) data obtained from a controlled multiple sclerosis (MS) study, focusing on the correlation between the degree of multifractality, disease duration, and disability level. Our results reveal a significant correspondence between the complexity of the time series and multiple sclerosis development, quantified respectively by scaling exponents and the Expanded Disability Status Scale (EDSS). Namely, for some brain regions, a well-developed multifractality and little persistence of the time series were identified in patients with a high level of disability, whereas the control group and patients with low EDSS were characterised by persistence and monofractality of the signals. The analysis of the cross-correlations between EEG signals supported these results, with the most significant differences identified for patients with EDSS $> 1$ and the combined group of patients with EDSS $\\leq 1$ and controls. No association between the multifractality and disease duration was observed, indicating that the multifractal organisation of the data is a hallmark of developing the disease. The observed complexity/multifractality of EEG signals is hypothetically a result of neuronal compensation -- i.e., of optimizing neural processes in the presence of structural brain degeneration. The presented study is highly relevant due to the multifractal formalism used to quantify complexity and due to scarce resting-state EEG evidence for cortical reorganization associated with compensation.",
        "comments": "39 pages, including supplementary materials (11 figures, 4 tables)",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08321"
    },
    {
        "doc_id": 182,
        "title": "Sources of HIV infections among MSM with a migration background: a viral phylogenetic case study in Amsterdam, the Netherlands",
        "authors": [
            "Alexandra Blenkinsop",
            "Nikos Pantazis",
            "Evangelia Georgia Kostaki",
            "Lysandros Sofocleous",
            "Ard van Sighem",
            "Daniela Bezemer",
            "Thijs van de Laar",
            "Marc van der Valk",
            "Peter Reiss",
            "Godelieve de Bree",
            "Oliver Ratmann"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "Background: Men and women with a migration background comprise an increasing proportion of incident HIV cases across Western Europe. Several studies indicate a substantial proportion acquire HIV post-migration.\n  Methods: We used partial HIV consensus sequences with linked demographic and clinical data from the opt-out ATHENA cohort of people with HIV in the Netherlands to quantify population-level sources of transmission to Dutch-born and foreign-born Amsterdam men who have sex with men (MSM) between 2010-2021. We identified phylogenetically and epidemiologically possible transmission pairs in local transmission chains and interpreted these in the context of estimated infection dates, quantifying transmission dynamics between sub-populations by world region of birth.\n  Results: We estimate the majority of Amsterdam MSM who acquired their infection locally had a Dutch-born Amsterdam MSM source (56% [53-58%]). Dutch-born MSM were the predominant source population of infections among almost all foreign-born Amsterdam MSM sub-populations. Stratifying by two-year intervals indicated shifts in transmission dynamics, with a majority of infections originating from foreign-born MSM since 2018, although uncertainty ranges remained wide.\n  Conclusions: In the context of declining HIV incidence among Amsterdam MSM, our data suggest whilst native-born MSM have predominantly driven transmissions in 2010-2021, the contribution from foreign-born MSM living in Amsterdam is increasing.",
        "comments": " ",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08308"
    },
    {
        "doc_id": 183,
        "title": "Attention-Based CNN-BiLSTM for Sleep State Classification of Spatiotemporal Wide-Field Calcium Imaging Data",
        "authors": [
            "Xiaohui Zhang",
            "Eric C. Landsness",
            "Hanyang Miao",
            "Wei Chen",
            "Michelle Tang",
            "Lindsey M. Brier",
            "Joseph P. Culver",
            "Jin-Moo Lee",
            "Mark A. Anastasio"
        ],
        "subjects": [
            "Image and Video Processing",
            "Neurons and Cognition"
        ],
        "abstract": "Background: Wide-field calcium imaging (WFCI) with genetically encoded calcium indicators allows for spatiotemporal recordings of neuronal activity in mice. When applied to the study of sleep, WFCI data are manually scored into the sleep states of wakefulness, non-REM (NREM) and REM by use of adjunct EEG and EMG recordings. However, this process is time-consuming, invasive and often suffers from low inter- and intra-rater reliability. Therefore, an automated sleep state classification method that operates on spatiotemporal WFCI data is desired. New Method: A hybrid network architecture consisting of a convolutional neural network (CNN) to extract spatial features of image frames and a bidirectional long short-term memory network (BiLSTM) with attention mechanism to identify temporal dependencies among different time points was proposed to classify WFCI data into states of wakefulness, NREM and REM sleep. Results: Sleep states were classified with an accuracy of 84% and Cohen's kappa of 0.64. Gradient-weighted class activation maps revealed that the frontal region of the cortex carries more importance when classifying WFCI data into NREM sleep while posterior area contributes most to the identification of wakefulness. The attention scores indicated that the proposed network focuses on short- and long-range temporal dependency in a state-specific manner. Comparison with Existing Method: On a 3-hour WFCI recording, the CNN-BiLSTM achieved a kappa of 0.67, comparable to a kappa of 0.65 corresponding to the human EEG/EMG-based scoring. Conclusions: The CNN-BiLSTM effectively classifies sleep states from spatiotemporal WFCI data and will enable broader application of WFCI in sleep.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08098"
    },
    {
        "doc_id": 184,
        "title": "A new model of trust based on neural information processing",
        "authors": [
            "Scott E. Allen",
            "Ren\u00e9 F. Kizilcec",
            "A. David Redish"
        ],
        "subjects": [
            "General Economics",
            "Human-Computer Interaction",
            "Neurons and Cognition"
        ],
        "abstract": "More than 30 years of research has firmly established the vital role of trust in human organizations and relationships, but the underlying mechanisms by which people build, lose, and rebuild trust remains incompletely understood. We propose a mechanistic model of trust that is grounded in the modern neuroscience of decision making. Since trust requires anticipating the future actions of others, any mechanistic model must be built upon up-to-date theories on how the brain learns, represents, and processes information about the future within its decision-making systems. Contemporary neuroscience has revealed that decision making arises from multiple parallel systems that perform distinct, complementary information processing. Each system represents information in different forms, and therefore learns via different mechanisms. When an act of trust is reciprocated or violated, this provides new information that can be used to anticipate future actions. The taxonomy of neural information representations that is the basis for the system boundaries between neural decision-making systems provides a taxonomy for categorizing different forms of trust and generating mechanistic predictions about how these forms of trust are learned and manifested in human behavior. Three key predictions arising from our model are (1) strategic risk-taking can reveal how to best proceed in a relationship, (2) human organizations and environments can be intentionally designed to encourage trust among their members, and (3) violations of trust need not always degrade trust, but can also provide opportunities to build trust.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08064"
    },
    {
        "doc_id": 185,
        "title": "Understanding YTHDF2-mediated mRNA Degradation By m6A-BERT-Deg",
        "authors": [
            "Ting-He Zhang",
            "Sumin Jo",
            "Michelle Zhang",
            "Kai Wang",
            "Shou-Jiang Gao",
            "Yufei Huang"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "N6-methyladenosine (m6A) is the most abundant mRNA modification within mammalian cells, holding pivotal significance in the regulation of mRNA stability, translation, and splicing. Furthermore, it plays a critical role in the regulation of RNA degradation by primarily recruiting the YTHDF2 reader protein. However, the selective regulation of mRNA decay of the m6A-methylated mRNA through YTHDF2 binding is poorly understood. To improve our understanding, we developed m6A-BERT-Deg, a BERT model adapted for predicting YTHDF2-mediated degradation of m6A-methylated mRNAs. We meticulously assembled a high-quality training dataset by integrating multiple data sources for the HeLa cell line. To overcome the limitation of small training samples, we employed a pre-training-fine-tuning strategy by first performing a self-supervised pre-training of the model on 427,760 unlabeled m6A site sequences. The test results demonstrated the importance of this pre-training strategy in enabling m6A-BERT-Deg to outperform other benchmark models. We further conducted a comprehensive model interpretation and revealed a surprising finding that the presence of co-factors in proximity to m6A sites may disrupt YTHDF2-mediated mRNA degradation, subsequently enhancing mRNA stability. We also extended our analyses to the HEK293 cell line, shedding light on the context-dependent YTHDF2-mediated mRNA degradation.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08004"
    },
    {
        "doc_id": 186,
        "title": "Discovery of Generalizable TBI Phenotypes Using Multivariate Time-Series Clustering",
        "authors": [
            "Hamid Ghaderi",
            "Brandon Foreman",
            "Chandan K. Reddy",
            "Vignesh Subbian"
        ],
        "subjects": [
            "Machine Learning",
            "Quantitative Methods",
            "Applications"
        ],
        "abstract": "Traumatic Brain Injury (TBI) presents a broad spectrum of clinical presentations and outcomes due to its inherent heterogeneity, leading to diverse recovery trajectories and varied therapeutic responses. While many studies have delved into TBI phenotyping for distinct patient populations, identifying TBI phenotypes that consistently generalize across various settings and populations remains a critical research gap. Our research addresses this by employing multivariate time-series clustering to unveil TBI's dynamic intricates. Utilizing a self-supervised learning-based approach to clustering multivariate time-Series data with missing values (SLAC-Time), we analyzed both the research-centric TRACK-TBI and the real-world MIMIC-IV datasets. Remarkably, the optimal hyperparameters of SLAC-Time and the ideal number of clusters remained consistent across these datasets, underscoring SLAC-Time's stability across heterogeneous datasets. Our analysis revealed three generalizable TBI phenotypes (\u03b1, \\b{eta}, and \u03b3), each exhibiting distinct non-temporal features during emergency department visits, and temporal feature profiles throughout ICU stays. Specifically, phenotype \u03b1 represents mild TBI with a remarkably consistent clinical presentation. In contrast, phenotype \\b{eta} signifies severe TBI with diverse clinical manifestations, and phenotype \u03b3 represents a moderate TBI profile in terms of severity and clinical diversity. Age is a significant determinant of TBI outcomes, with older cohorts recording higher mortality rates. Importantly, while certain features varied by age, the core characteristics of TBI manifestations tied to each phenotype remain consistent across diverse populations.",
        "comments": "25 pages, 10 figures, 4 tables, submitted to Computers in Biology and Medicine",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08002"
    },
    {
        "doc_id": 187,
        "title": "Integrate Any Omics: Towards genome-wide data integration for patient stratification",
        "authors": [
            "Shihao Ma",
            "Andy G. X. Zeng",
            "Benjamin Haibe-Kains",
            "Anna Goldenberg",
            "John E Dick",
            "Bo Wang"
        ],
        "subjects": [
            "Genomics",
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "High-throughput omics profiling advancements have greatly enhanced cancer patient stratification. However, incomplete data in multi-omics integration presents a significant challenge, as traditional methods like sample exclusion or imputation often compromise biological diversity and dependencies. Furthermore, the critical task of accurately classifying new patients with partial omics data into existing subtypes is commonly overlooked. To address these issues, we introduce IntegrAO (Integrate Any Omics), an unsupervised framework for integrating incomplete multi-omics data and classifying new samples. IntegrAO first combines partially overlapping patient graphs from diverse omics sources and utilizes graph neural networks to produce unified patient embeddings. Our systematic evaluation across five cancer cohorts involving six omics modalities demonstrates IntegrAO's robustness to missing data and its accuracy in classifying new samples with partial profiles. An acute myeloid leukemia case study further validates its capability to uncover biological and clinical heterogeneity in incomplete datasets. IntegrAO's ability to handle heterogeneous and incomplete data makes it an essential tool for precision oncology, offering a holistic approach to patient characterization.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07937"
    },
    {
        "doc_id": 188,
        "title": "Predicting heteropolymer interactions: demixing and hypermixing of disordered protein sequences",
        "authors": [
            "Kyosuke Adachi",
            "Kyogo Kawaguchi"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Biological Physics",
            "Biomolecules"
        ],
        "abstract": "Cells contain multiple condensates which spontaneously form due to the heterotypic interactions between their components. Although the proteins and disordered region sequences that are responsible for condensate formation have been extensively studied, the rule of interactions between the components that allow demixing, i.e., the coexistence of multiple condensates, is yet to be elucidated. Here we construct an effective theory of the interaction between heteropolymers by fitting it to the molecular dynamics simulation results obtained for more than 200 sequences sampled from the disordered regions of human proteins. We find that the sum of amino acid pair interactions across two heteropolymers predicts the Boyle temperature qualitatively well, which can be quantitatively improved by the dimer pair approximation, where we incorporate the effect of neighboring amino acids in the sequences. The improved theory, combined with the finding of a metric that captures the effective interaction strength between distinct sequences, allowed the selection of up to three disordered region sequences that demix with each other in multicomponent simulations, as well as the generation of artificial sequences that demix with a given sequence. The theory points to a generic sequence design strategy to demix or hypermix thanks to the low dimensional nature of the space of the interactions that we identify. As a consequence of the geometric arguments in the space of interactions, we find that the number of distinct sequences that can demix with each other is strongly constrained, irrespective of the choice of the coarse-grained model. Altogether, we construct a theoretical basis for methods to estimate the effective interaction between heteropolymers, which can be utilized in predicting phase separation properties as well as rules of assignment in the localization and functions of disordered proteins.",
        "comments": "20 pages, 21 figures",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07826"
    },
    {
        "doc_id": 189,
        "title": "Phenotyping calcification in vascular tissues using artificial intelligence",
        "authors": [
            "Mehdi Ramezanpour",
            "Anne M. Robertson",
            "Yasutaka Tobe",
            "Xiaowei Jia",
            "Juan R. Cebral"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Data Analysis, Statistics and Probability",
            "Quantitative Methods",
            "Tissues and Organs"
        ],
        "abstract": "Vascular calcification is implicated as an important factor in major adverse cardiovascular events (MACE), including heart attack and stroke. A controversy remains over how to integrate the diverse forms of vascular calcification into clinical risk assessment tools. Even the commonly used calcium score for coronary arteries, which assumes risk scales positively with total calcification, has important inconsistencies. Fundamental studies are needed to determine how risk is influenced by the diverse calcification phenotypes. However, studies of these kinds are hindered by the lack of high-throughput, objective, and non-destructive tools for classifying calcification in imaging data sets. Here, we introduce a new classification system for phenotyping calcification along with a semi-automated, non-destructive pipeline that can distinguish these phenotypes in even atherosclerotic tissues. The pipeline includes a deep-learning-based framework for segmenting lipid pools in noisy micro-CT images and an unsupervised clustering framework for categorizing calcification based on size, clustering, and topology. This approach is illustrated for five vascular specimens, providing phenotyping for thousands of calcification particles across as many as 3200 images in less than seven hours. Average Dice Similarity Coefficients of 0.96 and 0.87 could be achieved for tissue and lipid pool, respectively, with training and validation needed on only 13 images despite the high heterogeneity in these tissues. By introducing an efficient and comprehensive approach to phenotyping calcification, this work enables large-scale studies to identify a more reliable indicator of the risk of cardiovascular events, a leading cause of global mortality and morbidity.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07825"
    },
    {
        "doc_id": 190,
        "title": "Animal-associated marine Acidobacteria with a rich natural product repertoire",
        "authors": [
            "Stefan Leopold-Messer",
            "Clara Chepkirui",
            "Mathijs F. J. Mabesoone",
            "Joshua Mayer",
            "Lucas Paoli",
            "Shinichi Sunagawa",
            "Agustinus R. Uria",
            "Toshiyuki Wakimoto",
            "J\u00f6rn Piel"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "Sponges have long been recognized as a rich source of bioactive natural products. Various studies suggest that many of these compounds are produced by symbiotic bacteria. However, substance supplies and functional insights about the producers remain limited because cultivation remains unsuccessful. To identify alternative, sustainable sources of sponge-derived polyketides, we computationally analyzed 5289 characterized and orphan trans-acyltransferase polyketide synthases, enzymes with widespread roles in polyketide biosynthesis by bacterial symbionts. The analytical workflow predicted marine animal-derived Acidobacteria of the family Acanthopleuribacteraceae with large sets of biosynthetic gene clusters to be enriched in sponge-type chemistry. Targeted compound isolation from a chiton-associated strain yielded new congeners of the phorboxazoles and calyculins, potent and scarce cytotoxins exclusively known from sponges. These first natural products of Acidobacteria and new coral metagenomic data on a third family member suggest animal-associated Acanthopleuribacteraceae as a rich source of sponge-type as well as novel metabolites",
        "comments": "Journal ref:        Chem, 9 (12), 2023, pp. 3696-3713",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07730"
    },
    {
        "doc_id": 191,
        "title": "Comprehensive Joint Modeling of First-Line Therapeutics in Non-Small Cell Lung Cancer",
        "authors": [
            "Benjamin Schneider",
            "S\u00e9bastien Benzekry",
            "Jonathan Mochel"
        ],
        "subjects": [
            "Cell Behavior",
            "Tissues and Organs"
        ],
        "abstract": "First-line antiproliferatives for non-small cell lung cancer (NSCLC) have a relatively high failure rate due to high intrinsic resistance rates and acquired resistance rates to therapy. 57% patients are diagnosed in late-stage disease due to the tendency of early-stage NSCLC to be asymptomatic. For patients first diagnosed with metastatic disease the 5-year survival rate is approximately 5%. To help accelerate the development of novel therapeutics and computer-based tools for optimizing individual therapy, we have collated data from 11 different clinical trials in NSCLC and developed a semi-mechanistic, clinical model of NSCLC growth and pharmacodynamics relative to the various therapeutics represented in the study. In this study, we have produced extremely precise estimates of clinical parameters fundamental to cancer modeling such as the rate of acquired resistance to various pharmaceuticals, the relationship between drug concentration and rate of cancer cell death, as well as the fine temporal dynamics of anti-VEGF therapy. In the simulation sets documented in this study, we have used the model to make meaningful descriptions of efficacy gain in making bevacizumab-antiproliferative combination therapy sequential, over a series of days, rather than concurrent.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07719"
    },
    {
        "doc_id": 192,
        "title": "Ion channels in critical membranes: clustering, cooperativity, and memory effects",
        "authors": [
            "Antonio Suma",
            "Daniel Sigg",
            "Seamus Gallagher",
            "Giuseppe Gonnella",
            "Vincenzo Carnevale"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Biological Physics",
            "Biomolecules"
        ],
        "abstract": "Much progress has been made in elucidating the inner workings of voltage-gated ion channels, but less understood is the influence of lipid rafts on gating kinetics. Here we propose that state-dependent channel affinity for different lipid species provides a unified explanation for the experimentally observed behaviors of clustering, cooperativity, and hysteresis. We develop models of diffusing lipids and channels engaged in Ising-like interactions to investigate the collective behaviors driven by raft formation in critical membranes close to the demixing transition. The model channels demonstrate lipid-mediated long-range interactions, activation curve steepening, and long-term memory in ionic currents. These behaviors likely play a role in channel-mediated cellular signaling and suggest a universal mechanism for self-organization of biomolecular assemblies.",
        "comments": "14 pages, 6 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07660"
    },
    {
        "doc_id": 193,
        "title": "Empirical Evidence for the Fragment level Understanding on Drug Molecular Structure of LLMs",
        "authors": [
            "Xiuyuan Hu",
            "Guoqing Liu",
            "Yang Zhao",
            "Hao Zhang"
        ],
        "subjects": [
            "Machine Learning",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "AI for drug discovery has been a research hotspot in recent years, and SMILES-based language models has been increasingly applied in drug molecular design. However, no work has explored whether and how language models understand the chemical spatial structure from 1D sequences. In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments. The results indicate that language models can understand chemical structures from the perspective of molecular fragments, and the structural knowledge learned through fine-tuning is reflected in the high-frequency SMILES substrings generated by the model.",
        "comments": "Accepted by AAAI 2024 workshop: Large Language Models for Biological Discoveries (LLMs4Bio)",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07657"
    },
    {
        "doc_id": 194,
        "title": "Measuring multisensory integration in reaction time: the relative entropy approach",
        "authors": [
            "Hans Colonius",
            "Adele Diederich"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "A classic definition of multisensory integration (MI) has been proposed as ``the presence of a (statistically) significant change in the response to a cross-modal stimulus complex compared to unimodal stimuli''. However, this general definition did not result in a broad consensus on how to quantify the amount of MI in the context of reaction time (RT). In this brief note, we argue that numeric measures of reaction times that only involve mean or median RTs do not uncover the information required to fully assess the effect of multisensory integration. We suggest instead novel measures that include the entire RT distributions functions. The central role is played by relative entropy (aka Kullback-Leibler divergence), a statistical concept in information theory, statistics, and machine learning to measure the (non-symmetric) distance between probability distributions. We provide a number of theoretical examples, but empirical applications and statistical testing are postponed to later study.",
        "comments": "9 pages, 1 figure",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07568"
    },
    {
        "doc_id": 195,
        "title": "Vitamin K content of Australian-grown horticultural commodities",
        "authors": [
            "Eleanor Dunlop",
            "Judy Cunningham",
            "Paul Adorno",
            "Georgios Dabos",
            "Stuart K Johnson",
            "Lucinda J Black"
        ],
        "subjects": [
            "Other Quantitative Biology"
        ],
        "abstract": "Vitamin K is emerging as a multi-function vitamin that plays a role in bone, brain and vascular health. Vitamin K composition data remain limited globally and Australia has lacked nationally representative data for vitamin K1 (phylloquinone, PK) in horticultural commodities. Primary samples (n = 927) of 90 different Australian-grown fruit, vegetable and nut commodities were purchased in three Australian cities. We measured PK in duplicate in 95 composite samples using liquid chromatography with electrospray ionisation-tandem mass spectrometry. The greatest mean concentrations of PK were found in kale (565 ug/100 g), baby spinach (255 ug/100 g) and Brussels sprouts (195 ug/100 g). The data contribute to the global collection of vitamin K food composition data. They add to the evidence that PK concentrations vary markedly between geographic regions, supporting development of region-specific datasets for national food composition databases that do not yet contain data for vitamin K.",
        "comments": "22 pages, 2 tables",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07473"
    },
    {
        "doc_id": 196,
        "title": "Inference of dynamical gene regulatory networks from single-cell data with physics informed neural networks",
        "authors": [
            "Maria Mircea",
            "Diego Garlaschelli",
            "Stefan Semrau"
        ],
        "subjects": [
            "Quantitative Methods",
            "Artificial Intelligence",
            "Machine Learning",
            "Biological Physics",
            "Molecular Networks"
        ],
        "abstract": "One of the main goals of developmental biology is to reveal the gene regulatory networks (GRNs) underlying the robust differentiation of multipotent progenitors into precisely specified cell types. Most existing methods to infer GRNs from experimental data have limited predictive power as the inferred GRNs merely reflect gene expression similarity or correlation. Here, we demonstrate, how physics-informed neural networks (PINNs) can be used to infer the parameters of predictive, dynamical GRNs that provide mechanistic understanding of biological processes. Specifically we study GRNs that exhibit bifurcation behavior and can therefore model cell differentiation. We show that PINNs outperform regular feed-forward neural networks on the parameter inference task and analyze two relevant experimental scenarios: 1. a system with cell communication for which gene expression trajectories are available and 2. snapshot measurements of a cell population in which cell communication is absent. Our analysis will inform the design of future experiments to be analyzed with PINNs and provides a starting point to explore this powerful class of neural network models further.",
        "comments": "25 pages, 8 figures",
        "date": "14 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07379"
    },
    {
        "doc_id": 197,
        "title": "Robust Genomic Prediction and Heritability Estimation using Density Power Divergence",
        "authors": [
            "Upama Paul Chowdhury",
            "Susmita Das",
            "Abhik Ghosh"
        ],
        "subjects": [
            "Methodology",
            "Genomics",
            "Applications"
        ],
        "abstract": "This manuscript delves into the intersection of genomics and phenotypic prediction, focusing on the statistical innovation required to navigate the complexities introduced by noisy covariates and confounders. The primary emphasis is on the development of advanced robust statistical models tailored for genomic prediction from single nucleotide polymorphism (SNP) data collected from genome-wide association studies (GWAS) in plant and animal breeding and multi-field trials. The manuscript explores the limitations of traditional marker-assisted recurrent selection, highlighting the significance of incorporating all estimated effects of marker loci into the statistical framework and aiming to reduce the high dimensionality of GWAS data while preserving critical information. This paper introduces a new robust statistical framework for genomic prediction, employing one-stage and two-stage linear mixed model analyses along with utilizing the popular robust minimum density power divergence estimator (MDPDE) to estimate genetic effects on phenotypic traits. The study illustrates the superior performance of the proposed MDPDE-based genomic prediction and associated heritability estimation procedures over existing competitors through extensive empirical experiments on artificial datasets and application to a real-life maize breeding dataset. The results showcase the robustness and accuracy of the proposed MDPDE-based approaches, especially in the presence of data contamination, emphasizing their potential applications in improving breeding programs and advancing genomic prediction of phenotyping traits.",
        "comments": "Under Review",
        "date": "14 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07344"
    },
    {
        "doc_id": 198,
        "title": "Phenotypic switching mechanisms determine the structure of cell migration into extracellular matrix under the `go-or-grow' hypothesis",
        "authors": [
            "Rebecca M. Crossley",
            "Kevin J. Painter",
            "Tommaso Lorenzi",
            "Philip K. Maini",
            "Ruth E. Baker"
        ],
        "subjects": [
            "Cell Behavior"
        ],
        "abstract": "A fundamental feature of collective cell migration is phenotypic heterogeneity which, for example, influences tumour progression and relapse. While current mathematical models often consider discrete phenotypic structuring of the cell population, in-line with the `go-or-grow' hypothesis \\cite{hatzikirou2012go, stepien2018traveling}, they regularly overlook the role that the environment may play in determining the cells' phenotype during migration. Comparing a previously studied volume-filling model for a homogeneous population of generalist cells that can proliferate, move and degrade extracellular matrix (ECM) \\cite{crossley2023travelling} to a novel model for a heterogeneous population comprising two distinct sub-populations of specialist cells that can either move and degrade ECM or proliferate, this study explores how different hypothetical phenotypic switching mechanisms affect the speed and structure of the invading cell populations. Through a continuum model derived from its individual-based counterpart, insights into the influence of the ECM and the impact of phenotypic switching on migrating cell populations emerge. Notably, specialist cell populations that cannot switch phenotype show reduced invasiveness compared to generalist cell populations, while implementing different forms of switching significantly alters the structure of migrating cell fronts. This key result suggests that the structure of an invading cell population could be used to infer the underlying mechanisms governing phenotypic switching.",
        "comments": "34 pages, 11 figures",
        "date": "14 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07279"
    },
    {
        "doc_id": 199,
        "title": "Balancing reaction-diffusion network for cell polarization pattern with stability and asymmetry",
        "authors": [
            "Yixuan Chen",
            "Guoye Guan",
            "Lei-Han Tang",
            "Chao Tang"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "Cell polarization is a critical process that separates molecules into two distinct regions in prokaryotic and eukaryotic cells, guiding biological processes such as cell division and cell differentiation. Although several underlying antagonistic reaction-diffusion networks capable of setting up cell polarization have been identified experimentally and theoretically, our understanding of how to manipulate pattern stability and asymmetry remains incomplete, especially when only a subset of network components are known. Here we present numerical results to show that the polarized pattern of an antagonistic 2-node network collapses into a homogeneous state when subjected to single-sided self-regulation, single-sided additional regulation, or unequal system parameters. However, polarity can be restored through a combination of two modifications that have opposing effects. Additionally, spatially inhomogeneous parameters favoring respective domains stabilize their interface at designated locations. To connect our findings to cell polarity studies of the nematode Caenorhabditis elegans zygote, we reconstituted a 5-node network where a 4-node circuit with full mutual inhibitions between anterior and posterior is modified by a mutual activation in the anterior and an additional mutual inhibition between the anterior and the posterior. Once again, a generic set of kinetic parameters moves the interface towards either the anterior or posterior end, yet a polarized pattern can be stabilized through spatial tuning of one or more parameters coupled to intracellular or extracellular cues. A user-friendly software, PolarSim, is introduced to facilitate the exploration of networks with alternative node numbers, parameter values, and regulatory pathways.",
        "comments": " ",
        "date": "14 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07227"
    },
    {
        "doc_id": 200,
        "title": "Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses",
        "authors": [
            "Tahereh Zarrat Ehsan",
            "Seyed Mehdi Mohtavipour"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ],
        "abstract": "Detecting anomalies in poultry houses is crucial for maintaining optimal chicken health conditions, minimizing economic losses and bolstering profitability. This paper presents a novel real-time framework for analyzing chicken behavior in cage-free poultry houses to detect abnormal behaviors. Specifically, two significant abnormalities, namely inactive broiler and huddling behavior, are investigated in this study. The proposed framework comprises three key steps: (1) chicken detection utilizing a state-of-the-art deep learning model, (2) tracking individual chickens across consecutive frames with a fast tracker module, and (3) detecting abnormal behaviors within the video stream. Experimental studies are conducted to evaluate the efficacy of the proposed algorithm in accurately assessing chicken behavior. The results illustrate that our framework provides a precise and efficient solution for real-time anomaly detection, facilitating timely interventions to maintain chicken health and enhance overall productivity on poultry farms. Github: https://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis",
        "comments": "11 pages, 7 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12176"
    },
    {
        "doc_id": 201,
        "title": "Centralization in Block Building and Proposer-Builder Separation",
        "authors": [
            "Maryam Bahrani",
            "Pranav Garimidi",
            "Tim Roughgarden"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Cryptography and Security",
            "Distributed, Parallel, and Cluster Computing",
            "Theoretical Economics"
        ],
        "abstract": "The goal of this paper is to rigorously interrogate conventional wisdom about centralization in block-building (due to, e.g., MEV and private order flow) and the outsourcing of block-building by validators to specialists (i.e., proposer-builder separation):\n  1. Does heterogeneity in skills and knowledge across block producers inevitably lead to centralization?\n  2. Does proposer-builder separation eliminate heterogeneity and preserve decentralization among proposers?\n  This paper develops mathematical models and results that offer answers to these questions:\n  1. In a game-theoretic model with endogenous staking, heterogeneous block producer rewards, and staking costs, we quantify the extent to which heterogeneous rewards lead to concentration in the equilibrium staking distribution.\n  2. In a stochastic model in which heterogeneous block producers repeatedly reinvest rewards into staking, we quantify, as a function of the block producer heterogeneity, the rate at which stake concentrates on the most sophisticated block producers.\n  3. In a model with heterogeneous proposers and specialized builders, we quantify, as a function of the competitiveness of the builder ecosystem, the extent to which proposer-builder separation reduces the heterogeneity in rewards across different proposers.\n  Our models and results take advantage of connections to contest design, P\u00f3lya urn processes, and auction theory.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12120"
    },
    {
        "doc_id": 202,
        "title": "Measures of the Capital Network of the U.S. Economy",
        "authors": [
            "Ben Klemens"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "About two million U.S. corporations and partnerships are linked to each other and human investors by about 15 million owner-subsidiary links. Comparable social networks such as corporate board memberships and socially-built systems such as the network of Internet links are \"small worlds,\" meaning a network with a small diameter and link densities with a power-law distribution, but these properties had not yet been measured for the business entity network. This article shows that both inbound links and outbound links display a power-law distribution with a coefficient of concentration estimable to within a generally narrow confidence interval, overall, for subnetworks including only business entities, only for the great connected component of the network, and in subnetworks with edges associated with certain industries, for all years 2009-2021. In contrast to other networks with power-law distributed link densities, the network is mostly a tree, and has a diameter an order of magnitude larger than a small-world network with the same link distribution. The regularity of the power-law distribution indicates that its coefficient can be used as a new, well-defined macroeconomic metric for the concentration of capital flows in an economy. Economists might use it as a new measure of market concentration which is more comprehensive than measures based only on the few biggest firms. Comparing capital link concentrations across countries would facilitate modeling the relationship between business network characteristics and other macroeconomic indicators.",
        "comments": "18 pages. JEL classifications: L14; C81; M42; G34",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12118"
    },
    {
        "doc_id": 203,
        "title": "Metrics matter, a Formal comment on Ward et al Plos-One 2016 paper : Is decoupling GDP growth from environmental impact possible?",
        "authors": [
            "Herv\u00e9 Bercegol",
            "Paul E. Brockway"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "The Ward et al. (2016) Plos-One paper is an important, heavily-cited paper in the decoupling literature. The authors present evidence of 1990-2015 growth in material and energy consumption and GDP at a world level, and for selected countries. They find only relative decoupling has occurred, leading to their central claim that future absolute decoupling is implausible. However, the authors have made two key errors in their collected data: GDP data is in current prices which includes inflation, and their global material use data is the total mass of fossil energy materials. Strictly, GDP data should be in constant prices to allow for its comparison over time, and material inputs to an economy should be the sum of mineral raw materials. Amending for these errors, we find much smaller levels of energy-GDP relative decoupling, and no materials-GDP decoupling at all at a global level. We check these new results by adding data for 1900-1990 to provide a longer time series, and find consistently low (and even no) levels of global relative decoupling of material use. The central claim for materials over the implausibility of future absolute decoupling therefore not only remains valid but is reinforced by the corrected datasets.",
        "comments": "6 pages, 4 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12100"
    },
    {
        "doc_id": 204,
        "title": "Temporal Aggregation for the Synthetic Control Method",
        "authors": [
            "Liyang Sun",
            "Eli Ben-Michael",
            "Avi Feller"
        ],
        "subjects": [
            "Econometrics",
            "Methodology"
        ],
        "abstract": "The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit with panel data. Two challenges arise with higher frequency data (e.g., monthly versus yearly): (1) achieving excellent pre-treatment fit is typically more challenging; and (2) overfitting to noise is more likely. Aggregating data over time can mitigate these problems but can also destroy important signal. In this paper, we bound the bias for SCM with disaggregated and aggregated outcomes and give conditions under which aggregating tightens the bounds. We then propose finding weights that balance both disaggregated and aggregated series.",
        "comments": "9 pages, 3 figures, Prepared for 2024 AEA Papers and Proceedings \"Treatment Effects: Theory and Implementation\"",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12084"
    },
    {
        "doc_id": 205,
        "title": "Market Responses to Genuine Versus Strategic Generosity: An Empirical Examination of NFT Charity Fundraisers",
        "authors": [
            "Chen Liang",
            "Murat Tunc",
            "Gordon Burtch"
        ],
        "subjects": [
            "General Economics",
            "Human-Computer Interaction"
        ],
        "abstract": "Crypto donations now represent a significant fraction of charitable giving worldwide. Nonfungible token (NFT) charity fundraisers, which involve the sale of NFTs of artistic works with the proceeds donated to philanthropic causes, have emerged as a novel development in this space. A unique aspect of NFT charity fundraisers is the significant potential for donors to reap financial gains from the rising value of purchased NFTs. Questions may arise about the motivations of donors in these charity fundraisers, resulting in a negative social image. NFT charity fundraisers thus offer a unique opportunity to understand the economic consequences of a donor's social image. We investigate these effects in the context of a large NFT charity fundraiser. We identify the causal effect of purchasing an NFT within the charity fundraiser on a donor's later market outcomes by leveraging random variation in transaction processing times on the blockchain. Further, we demonstrate a clear pattern of heterogeneity, based on an individual's decision to relist (versus hold) the purchased charity NFTs (a sign of strategic generosity), and based on an individual's degree of social exposure within the NFT marketplace. We show that charity-NFT \"relisters\" experience significant penalties in the market, in terms of the prices they are able to command on other NFT listings, particularly among those who relist quickly and those who are more socially exposed. Our study underscores the growing importance of digital visibility and traceability, features that characterize crypto-philanthropy, and online philanthropy more broadly.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12064"
    },
    {
        "doc_id": 206,
        "title": "A Bracketing Relationship for Long-Term Policy Evaluation with Combined Experimental and Observational Data",
        "authors": [
            "Yechan Park",
            "Yuya Sasaki"
        ],
        "subjects": [
            "Econometrics"
        ],
        "abstract": "Combining short-term experimental data with observational data enables credible long-term policy evaluation. The literature offers two key but non-nested assumptions, namely the latent unconfoundedness (LU; Athey et al., 2020) and equi-confounding bias (ECB; Ghassami et al., 2022) conditions, to correct observational selection. Committing to the wrong assumption leads to biased estimation. To mitigate such risks, we provide a novel bracketing relationship (cf. Angrist and Pischke, 2009) repurposed for the setting with data combination: the LU-based estimand and the ECB-based estimand serve as the lower and upper bounds, respectively, with the true causal effect lying in between if either assumption holds. For researchers further seeking point estimates, our Lalonde-style exercise suggests the conservatively more robust LU-based lower bounds align closely with the hold-out experimental estimates for educational policy evaluation. We investigate the economic substantives of these findings through the lens of a nonparametric class of selection mechanisms and sensitivity analysis. We uncover as key the sub-martingale property and sufficient-statistics role (Chetty, 2009) of the potential outcomes of student test scores (Chetty et al., 2011, 2014).",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12050"
    },
    {
        "doc_id": 207,
        "title": "Local Diversity of Condorcet Domains",
        "authors": [
            "Alexander Karpov",
            "Klas Markstr\u00f6m",
            "S\u00f8ren Riis",
            "Bei Zhou"
        ],
        "subjects": [
            "Theoretical Economics",
            "Discrete Mathematics"
        ],
        "abstract": "Several of the classical results in social choice theory demonstrate that in order for many voting systems to be well-behaved the set domain of individual preferences must satisfy some kind of restriction, such as being single-peaked on a political axis. As a consequence it becomes interesting to measure how diverse the preferences in a well-behaved domain can be.\n  In this paper we introduce an egalitarian approach to measuring preference diversity, focusing on the abundance of distinct suborders one subsets of the alternative. We provide a common generalisation of the frequently used concepts of ampleness and copiousness.\n  We give a detailed investigation of the abundance for Condorcet domains. Our theorems imply a ceiling for the local diversity in domains on large sets of alternatives, which show that in this measure Black's single-peaked domain is in fact optimal. We also demonstrate that for some numbers of alternatives, there are Condorcet domains which have largest local diversity without having maximum order.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11912"
    },
    {
        "doc_id": 208,
        "title": "Efficiency in random allocation with ordinal rules",
        "authors": [
            "Samson Alva",
            "Eun Jeong Heo",
            "Vikram Manjunath"
        ],
        "subjects": [
            "Theoretical Economics"
        ],
        "abstract": "We study ordinal rules for allocating indivisible goods via lottery. Ordinality requires a rule to consider only how agents rank degenerate lotteries and may be necessitated by cognitive, informational, or as we show, incentive constraints. The limited responsiveness of ordinal rules to agents' preferences means that they can only satisfy welfare properties based on first order stochastic dominance, which is incomplete.\n  We define a new efficiency concept for ordinal rules. While ordinality and efficiency together are incompatible with the usual notions of fairness and somewhat limit randomization, they does leave room for a rich class of rules. We demonstrate this through a characterization of all ordinal, efficient, strategy-proof, non-bossy, boundedly invariant, and neutral rules.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11899"
    },
    {
        "doc_id": 209,
        "title": "Finite horizon optimal control of reaction-diffusion SIV epidemic system with stochastic environment",
        "authors": [
            "Zong Wang"
        ],
        "subjects": [
            "Optimization and Control",
            "Dynamical Systems"
        ],
        "abstract": "This contribution mainly focuses on the finite horizon optimal control problems of a susceptible-infected-vaccinated(SIV) epidemic system governed by reaction-diffusion equations and Markov switching. Stochastic dynamic programming is employed to find the optimal vaccination effort and economic return for a stochastic reaction diffusion SIV epidemic model. To achieve this, a key step is to show the existence and uniqueness of invariant measure for the model. Then, we obtained the necessary and sufficient conditions for the near-optimal control. Furthermore, we give an algorithm to approximate the Hamilton-Jacobi Bellman (HJB) equation. Finally, some numerical simulations are presented to confirm our analytic results.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11744"
    },
    {
        "doc_id": 210,
        "title": "Memory-Efficient Prompt Tuning for Incremental Histopathology Classification",
        "authors": [
            "Yu Zhu",
            "Kang Li",
            "Lequan Yu",
            "Pheng-Ann Heng"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ],
        "abstract": "Recent studies have made remarkable progress in histopathology classification. Based on current successes, contemporary works proposed to further upgrade the model towards a more generalizable and robust direction through incrementally learning from the sequentially delivered domains. Unlike previous parameter isolation based approaches that usually demand massive computation resources during model updating, we present a memory-efficient prompt tuning framework to cultivate model generalization potential in economical memory cost. For each incoming domain, we reuse the existing parameters of the initial classification model and attach lightweight trainable prompts into it for customized tuning. Considering the domain heterogeneity, we perform decoupled prompt tuning, where we adopt a domain-specific prompt for each domain to independently investigate its distinctive characteristics, and one domain-invariant prompt shared across all domains to continually explore the common content embedding throughout time. All domain-specific prompts will be appended to the prompt bank and isolated from further changes to prevent forgetting the distinctive features of early-seen domains. While the domain-invariant prompt will be passed on and iteratively evolve by style-augmented prompt refining to improve model generalization capability over time. In specific, we construct a graph with existing prompts and build a style-augmented graph attention network to guide the domain-invariant prompt exploring the overlapped latent embedding among all delivered domains for more domain generic representations. We have extensively evaluated our framework with two histopathology tasks, i.e., breast cancer metastasis classification and epithelium-stroma tissue classification, where our approach yielded superior performance and memory efficiency over the competing methods.",
        "comments": "Accepted by AAAI 2024",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11674"
    },
    {
        "doc_id": 211,
        "title": "Analyzing the Impact of Financial Inclusion on Economic Growth in Bangladesh",
        "authors": [
            "Ganapati Kumar Biswas"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Financial inclusion is touted one of the principal drivers for economic growth for an economy. The study aims to explore the impact of financial inclusion on economic growth in Bangladesh. In my study, I used the number of loan accounts as the proxy for financial inclusion. Using time series data from spans from 2004-2021, the study revealed that there exists a long-run relationship between GDP, financial inclusion, and other macroeconomic variables in Bangladesh. The study also found that financial inclusion had a positive impact on economic growth of Bangladesh during the study period. Therefore, the policymakers and the central bank of Bangladesh as the apex authority of financial system should promote financial inclusion activities to achieve sustainable economic growth.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11585"
    },
    {
        "doc_id": 212,
        "title": "A note on the stability of Monotone Markov Chains",
        "authors": [
            "Bar Light"
        ],
        "subjects": [
            "Probability",
            "Theoretical Economics"
        ],
        "abstract": "This note studies monotone Markov chains a subclass of Markov chains with extensive applications in operations research and economics. While the properties that ensure the global stability of these chains are well studied, their establishment often relies on the fulfillment of a certain splitting condition. We address the challenges of verifying the splitting condition, by introducing simple, applicable conditions that ensure global stability. The simplicity of these conditions is demonstrated through various examples including autoregressive processes and portfolio allocation problems.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11568"
    },
    {
        "doc_id": 213,
        "title": "Taxi dispatching strategies with compensations",
        "authors": [
            "Holger Billhardt",
            "Alberto Fern\u00e1ndez",
            "Sascha Ossowski",
            "Javier Palanca",
            "Javier Bajo"
        ],
        "subjects": [
            "Artificial Intelligence"
        ],
        "abstract": "Urban mobility efficiency is of utmost importance in big cities. Taxi vehicles are key elements in daily traffic activity. The advance of ICT and geo-positioning systems has given rise to new opportunities for improving the efficiency of taxi fleets in terms of waiting times of passengers, cost and time for drivers, traffic density, CO2 emissions, etc., by using more informed, intelligent dispatching. Still, the explicit spatial and temporal components, as well as the scale and, in particular, the dynamicity of the problem of pairing passengers and taxis in big towns, render traditional approaches for solving standard assignment problem useless for this purpose, and call for intelligent approximation strategies based on domain-specific heuristics. Furthermore, taxi drivers are often autonomous actors and may not agree to participate in assignments that, though globally efficient, may not be sufficently beneficial for them individually. This paper presents a new heuristic algorithm for taxi assignment to customers that considers taxi reassignments if this may lead to globally better solutions. In addition, as such new assignments may reduce the expected revenues of individual drivers, we propose an economic compensation scheme to make individually rational drivers agree to proposed modifications in their assigned clients. We carried out a set of experiments, where several commonly used assignment strategies are compared to three different instantiations of our heuristic algorithm. The results indicate that our proposal has the potential to reduce customer waiting times in fleets of autonomous taxis, while being also beneficial from an economic point of view.",
        "comments": "ACM Class:          I.2.1",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11553"
    },
    {
        "doc_id": 214,
        "title": "Accelerating Heterogeneous Tensor Parallelism via Flexible Workload Control",
        "authors": [
            "Zhigang Wang",
            "Xu Zhang",
            "Ning Wang",
            "Chuanfei Xu",
            "Jie Nie",
            "Zhiqiang Wei",
            "Yu Gu",
            "Ge Yu"
        ],
        "subjects": [
            "Distributed, Parallel, and Cluster Computing"
        ],
        "abstract": "Transformer-based models are becoming deeper and larger recently. For better scalability, an underlying training solution in industry is to split billions of parameters (tensors) into many tasks and then run them across homogeneous accelerators (e.g., GPUs). However, such dedicated compute cluster is prohibitively expensive in academia and moderate companies. An economic replacement is to aggregate existing heterogeneous devices and share resources among multi-tenants. Nevertheless, static hardware configurations and dynamic resource contention definitely cause straggling tasks, which heavily slows down the overall training efficiency. Existing works feature contributions mainly tailored for traditional data parallelism. They cannot work well for the new tensor parallelism due to strict communication and correctness constraints.\n  In this paper we first present ZERO-resizing, a novel dynamic workload balancing technique without any data migration. We tune workloads in real-time by temporarily resizing matrices involved in core tensor-related computations. We particularly design data imputation and priority selection policies to respectively satisfy consistency constraint required by normal training and reduce the accuracy loss. We also give a lightweight data migration technique without loss of accuracy, to cope with heavy heterogeneity. Our final SEMI-migration solution is built on top of these two techniques and can adaptively distinguish their respective balancing missions, to achieve an overall success in efficiency and accuracy. Extensive experiments on the representative Colossal-AI platform validate the effectiveness of our proposals.",
        "comments": "13 pages",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11469"
    },
    {
        "doc_id": 215,
        "title": "Local Identification in the Instrumental Variable Multivariate Quantile Regression Model",
        "authors": [
            "Haruki Kono"
        ],
        "subjects": [
            "Econometrics",
            "Statistics Theory"
        ],
        "abstract": "The instrumental variable (IV) quantile regression model introduced by Chernozhukov and Hansen (2005) is a useful tool for analyzing quantile treatment effects in the presence of endogeneity, but when outcome variables are multidimensional, it is silent on the joint distribution of different dimensions of each variable. To overcome this limitation, we propose an IV model built on the optimal-transport-based multivariate quantile that takes into account the correlation between the entries of the outcome variable. We then provide a local identification result for the model. Surprisingly, we find that the support size of the IV required for the identification is independent of the dimension of the outcome vector, as long as the IV is sufficiently informative. Our result follows from a general identification theorem that we establish, which has independent theoretical significance.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11422"
    },
    {
        "doc_id": 216,
        "title": "Agricultural Recommendation System based on Deep Learning: A Multivariate Weather Forecasting Approach",
        "authors": [
            "Md Zubair",
            "Md. Shahidul Salim",
            "Mehrab Mustafy Rahman",
            "Mohammad Jahid Ibna Basher",
            "Shahin Imran",
            "Iqbal H. Sarker"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence"
        ],
        "abstract": "Bangladesh is predominantly an agricultural country, where the agrarian sector plays an essential role in accelerating economic growth and enabling the food security of the people. The performance of this sector has an overwhelming impact on the primary macroeconomic objectives like food security, employment generation, poverty alleviation, human resources development, and other economic and social forces. Although Bangladesh's labor-intensive agriculture has achieved steady increases in food grain production, it often suffered from unfavorable weather conditions such as heavy rainfall, low temperature, and drought. Consequently, these factors hinder the production of food substantially, putting the country's overall food security in danger. In order to have a profitable, sustainable, and farmer-friendly agricultural practice, this paper proposes a context-based crop recommendation system powered by a weather forecast model. With extensive evaluation, the multivariate Stacked Bi-LSTM Network is employed as the weather forecasting model. The proposed weather model can forecast Rainfall, Temperature, Humidity, and Sunshine for any given location in Bangladesh with higher accuracy. These predictions guide our system to assist the farmers in making feasible decisions about planting, irrigation, harvesting, and so on. Additionally, our full-fledged system is capable of alerting the farmers about extreme weather conditions so that preventive measures can be undertaken to protect the crops. Finally, the system is also adept at making knowledge-based crop suggestions for the flood and drought-prone regions of Bangladesh.",
        "comments": "16 pages, 14 figures and 12 tables. Submitted to Engineering Application of Artificial Intelligence (Elsevier)",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11410"
    },
    {
        "doc_id": 217,
        "title": "Fake Google restaurant reviews and the implications for consumers and restaurants",
        "authors": [
            "Shawn Berry"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "The use of online reviews to aid with purchase decisions is popular among consumers as it is a simple heuristic tool based on the reported experiences of other consumers. However, not all online reviews are written by real consumers or reflect actual experiences, and present implications for consumers and businesses. This study examines the effects of fake online reviews written by artificial intelligence (AI) on consumer decision making. Respondents were surveyed about their attitudes and habits concerning online reviews using an online questionnaire (n=351), and participated in a restaurant choice experiment using varying proportions of fake and real reviews. While the findings confirm prior studies, new insights are gained about the confusion for consumers and consequences for businesses when reviews written by AI are believed rather than real reviews. The study presents a fake review detection model using logistic regression modeling to score and flag reviews as a solution.",
        "comments": "pp.1-158, 41 tables, 11 figures. Doctor of Business Administration Dissertation",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11345"
    },
    {
        "doc_id": 218,
        "title": "An income-based approach to modeling commuting distance in the Toronto area",
        "authors": [
            "Shawn Berry"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "The purpose of this article is to propose a novel model of the effects of changes in shelter and driving costs on car commuting distances in the overheated Toronto housing market from 2011 to 2016. The model borrows from theoretical concepts of microeconomics and urban geography to examine the Toronto housing market. Using 2011 and 2016 Census data for census metropolitan areas (CMAs) and census agglomerations (CAs) in Southern Ontario and computed driving costs, the model of car commuting distance is based on variables of allocation of monthly household income to monthly shelter costs and driving costs as a function of the car driving distance to Toronto. Using this model, we can predict the effect on car commuting distance due to changes in any of the variables. The model also offers an explanation for communities of Toronto car commuters beyond a driving radius that we might expect for daily commuting. The model confirms that increases in shelter costs in the Toronto housing market from 2011 to 2016 have forced the boundaries of feasible housing locations outward, and forced households to move farther away, thus increasing car commuting distance.",
        "comments": "pp.1-40, 8 tables, 5 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11343"
    },
    {
        "doc_id": 219,
        "title": "Coevolution of Resource and Strategies in Common-Pool Resource Dilemmas: A Coupled Human-Environmental System Model",
        "authors": [
            "Chengyi Tu",
            "Renfei Chen",
            "Ying Fan",
            "Yongliang Yang"
        ],
        "subjects": [
            "Theoretical Economics"
        ],
        "abstract": "Common-pool resource governance requires users to cooperate and avoid overexploitation, but defection and free-riding often undermine cooperation. We model a human-environmental system that integrates dynamics of resource and users' strategies. The resource follows a logistic function that depends on natural growth rate, carrying capacity, and extraction rates of cooperators and defectors. The users' strategies evolve according to different processes that capture effects of payoff, resource, and noise. We analyze the feedback between resource availability and strategic adaptation, and explores the conditions for the emergence and maintenance of cooperation. We find different processes lead to different regimes of equilibrium solutions and resource levels depending on the parameter configuration and initial conditions. We also show that some processes can enhance the sustainability of the resource by making the users more responsive to the resource scarcity. The paper advances the understanding of human-environmental system and offers insights for resource governance policies and interventions.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11269"
    },
    {
        "doc_id": 220,
        "title": "Estimation with Pairwise Observations",
        "authors": [
            "Felix Chan",
            "Laszlo Matyas"
        ],
        "subjects": [
            "Econometrics",
            "Statistics Theory"
        ],
        "abstract": "The paper introduces a new estimation method for the standard linear regression model. The procedure is not driven by the optimisation of any objective function rather, it is a simple weighted average of slopes from observation pairs. The paper shows that such estimator is consistent for carefully selected weights. Other properties, such as asymptotic distributions, have also been derived to facilitate valid statistical inference. Unlike traditional methods, such as Least Squares and Maximum Likelihood, among others, the estimated residual of this estimator is not by construction orthogonal to the explanatory variables of the model. This property allows a wide range of practical applications, such as the testing of endogeneity, i.e.,the correlation between the explanatory variables and the disturbance terms, and potentially several others.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11229"
    },
    {
        "doc_id": 221,
        "title": "Generalizing Speaker Verification for Spoof Awareness in the Embedding Space",
        "authors": [
            "Xuechen Liu",
            "Md Sahidullah",
            "Kong Aik Lee",
            "Tomi Kinnunen"
        ],
        "subjects": [
            "Cryptography and Security",
            "Artificial Intelligence",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "It is now well-known that automatic speaker verification (ASV) systems can be spoofed using various types of adversaries. The usual approach to counteract ASV systems against such attacks is to develop a separate spoofing countermeasure (CM) module to classify speech input either as a bonafide, or a spoofed utterance. Nevertheless, such a design requires additional computation and utilization efforts at the authentication stage. An alternative strategy involves a single monolithic ASV system designed to handle both zero-effort imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have the potential to provide stronger protections and more economic computations. To this end, we propose to generalize the standalone ASV (G-SASV) against spoofing attacks, where we leverage limited training data from CM to enhance a simple backend in the embedding space, without the involvement of a separate CM module during the test (authentication) phase. We propose a novel yet simple backend classifier based on deep neural networks and conduct the study via domain adaptation and multi-task integration of spoof embeddings at the training stage. Experiments are conducted on the ASVspoof 2019 logical access dataset, where we improve the performance of statistical ASV backends on the joint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and 49.8% in terms of equal error rates, respectively.",
        "comments": "To appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11156"
    },
    {
        "doc_id": 222,
        "title": "Long-term Effects of India's Childhood Immunization Program on Earnings and Consumption Expenditure: Comment",
        "authors": [
            "David Roodman"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Summan, Nandi, and Bloom (2023; SNB) finds that exposure of babies to India's Universal Immunization Programme (UIP) in the late 1980s increased their weekly wages in early adulthood by 0.138 log points and per-capita household consumption 0.028 points. But the results are attained by regressing on age, in years, while controlling for year of birth--two variables that, as constructed, are nearly collinear. The results are therefore attributable to trends during the one-year survey period, such as inflation. A randomization exercise shows that when the true impacts are zero, the SNB estimator averages 0.088 points for wages and 0.039 points for consumption.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11100"
    },
    {
        "doc_id": 223,
        "title": "Information Based Inference in Models with Set-Valued Predictions and Misspecification",
        "authors": [
            "Hiroaki Kaido",
            "Francesca Molinari"
        ],
        "subjects": [
            "Econometrics",
            "Methodology"
        ],
        "abstract": "This paper proposes an information-based inference method for partially identified parameters in incomplete models that is valid both when the model is correctly specified and when it is misspecified. Key features of the method are: (i) it is based on minimizing a suitably defined Kullback-Leibler information criterion that accounts for incompleteness of the model and delivers a non-empty pseudo-true set; (ii) it is computationally tractable; (iii) its implementation is the same for both correctly and incorrectly specified models; (iv) it exploits all information provided by variation in discrete and continuous covariates; (v) it relies on Rao's score statistic, which is shown to be asymptotically pivotal.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11046"
    },
    {
        "doc_id": 224,
        "title": "Bounding Consideration Probabilities in Consider-Then-Choose Ranking Models",
        "authors": [
            "Ben Aoki-Sherwood",
            "Catherine Bregou",
            "David Liben-Nowell",
            "Kiran Tomlinson",
            "Thomas Zeng"
        ],
        "subjects": [
            "Machine Learning",
            "Multiagent Systems",
            "Econometrics"
        ],
        "abstract": "A common theory of choice posits that individuals make choices in a two-step process, first selecting some subset of the alternatives to consider before making a selection from the resulting consideration set. However, inferring unobserved consideration sets (or item consideration probabilities) in this \"consider then choose\" setting poses significant challenges, because even simple models of consideration with strong independence assumptions are not identifiable, even if item utilities are known. We consider a natural extension of consider-then-choose models to a top-$k$ ranking setting, where we assume rankings are constructed according to a Plackett-Luce model after sampling a consideration set. While item consideration probabilities remain non-identified in this setting, we prove that knowledge of item utilities allows us to infer bounds on the relative sizes of consideration probabilities. Additionally, given a condition on the expected consideration set size, we derive absolute upper and lower bounds on item consideration probabilities. We also provide algorithms to tighten those bounds on consideration probabilities by propagating inferred constraints. Thus, we show that we can learn useful information about consideration probabilities despite not being able to identify them precisely. We demonstrate our methods on a ranking dataset from a psychology experiment with two different ranking tasks (one with fixed consideration sets and one with unknown consideration sets). This combination of data allows us to estimate utilities and then learn about unknown consideration probabilities using our bounds.",
        "comments": "11 pages; accepted as an extended abstract to AAMAS '24",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11016"
    },
    {
        "doc_id": 225,
        "title": "Subjective Causality",
        "authors": [
            "Joseph Y. Halpern",
            "Evan Piermont"
        ],
        "subjects": [
            "Theoretical Economics",
            "Artificial Intelligence",
            "Logic in Computer Science"
        ],
        "abstract": "We show that it is possible to understand and identify a decision maker's subjective causal judgements by observing her preferences over interventions. Following Pearl [2000], we represent causality using causal models (also called structural equations models), where the world is described by a collection of variables, related by equations. We show that if a preference relation over interventions satisfies certain axioms (related to standard axioms regarding counterfactuals), then we can define (i) a causal model, (ii) a probability capturing the decision-maker's uncertainty regarding the external factors in the world and (iii) a utility on outcomes such that each intervention is associated with an expected utility and such that intervention $A$ is preferred to $B$ iff the expected utility of $A$ is greater than that of $B$. In addition, we characterize when the causal model is unique. Thus, our results allow a modeler to test the hypothesis that a decision maker's preferences are consistent with some causal model and to identify causal judgements from observed behavior.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10937"
    },
    {
        "doc_id": 226,
        "title": "An Experimental Study of Decentralized Matching",
        "authors": [
            "Federico Echenique",
            "Alejandro Robinson-Cort\u00e9s",
            "Leeat Yariv"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "We present an experimental study of decentralized two-sided matching markets with no transfers. Experimental participants are informed of everyone's preferences and can make arbitrary non-binding match offers that get finalized when a period of market inactivity has elapsed. Several insights emerge. First, stable outcomes are prevalent. Second, while centralized clearinghouses commonly aim at implementing extremal stable matchings, our decentralized markets most frequently culminate in the median stable matching. Third, preferences' cardinal representations impact the stable partners participants match with. Last, the dynamics underlying our results exhibit strategic sophistication, with agents successfully avoiding cycles of blocking pairs.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10872"
    },
    {
        "doc_id": 227,
        "title": "Aberration compensation for the anamorphic triplet",
        "authors": [
            "Dmitry Zhuridov"
        ],
        "subjects": [
            "Optics",
            "Applied Physics",
            "Instrumentation and Detectors"
        ],
        "abstract": "Compensation of the generalized spherical aberrations is discussed for the plane-symmetric and anamorphic optical systems. The compensation rules are derived for an economical three-component double-plane symmetric telescopic system containing two cylindrical mirrors and one toroidal lens. Anamorphic systems, which provide large magnifications in the two orthogonal directions, are presented.",
        "comments": "4 pages, 4 figures",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10762"
    },
    {
        "doc_id": 228,
        "title": "Methodology to assess prosumer participation in European electricity markets",
        "authors": [
            "Rub\u00e9n Rodr\u00edguez-Vilches",
            "Francisco Mart\u00edn-Mart\u00ednez",
            "\u00c1lvaro S\u00e1nchez-Miralles",
            "Javier Rodrigo Guti\u00e9rrez de la C\u00e1mara",
            "Sergio Mu\u00f1oz Delgado"
        ],
        "subjects": [
            "Physics and Society",
            "Systems and Control"
        ],
        "abstract": "The emergence of distributed generation and the electrification of demand have opened the possibility for prosumers to participate in electricity markets, receiving economic benefits on their bills and contributing to the reduction of carbon emissions, aligning with United Nations Sustainable Development Goal 7. Consumers and prosumers can participate through implicit and explicit demand flexibility and (collective) self-consumption. This study analyses the potential markets in which prosumers can participate and indicates whether these are currently open. The markets studied include day-ahead, intraday, ancillary services, adequacy services, constraint management, and local flexibility markets. Additionally, collective self-consumption is analysed as a service through which prosumers can participate in the electricity market. Previous studies are usually focused on a single market or in a single country, making impossible a complete comparison. This analysis has been done in Spain, Italy, Croatia, and the United Kingdom as representative countries to obtain a methodology to assess countries' openness to prosumer participation in electricity markets, comparing regulatory frameworks and assigning scores based on their prosumer inclusion across various markets. This work updates current literature reviews with the changes and a new description of local market designs in Spain. This methodology can be used to compare other countries' grade of openness. The results of this study show that the analysed countries can be categorised into three groups: almost open, partially open, and closed markets. Analysing the differences, recommendations on the following steps to foster user participation are suggested for each group.",
        "comments": "Journal ref:        Renewable and Sustainable Energy Reviews Volume 191, March 2024, 114179",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10696"
    },
    {
        "doc_id": 229,
        "title": "Quantum Computing Enhanced Service Ecosystem for Simulation in Manufacturing",
        "authors": [
            "Wolfgang Maass",
            "Ankit Agrawal",
            "Alessandro Ciani",
            "Sven Danz",
            "Alejandro Delgadillo",
            "Philipp Ganser",
            "Pascal Kienast",
            "Marco Kulig",
            "Valentina K\u00f6nig",
            "Nil Rodellas-Gr\u00e0cia",
            "Rivan Rughubar",
            "Stefan Schr\u00f6der",
            "Marc Stautner",
            "Hannah Stein",
            "Tobias Stollenwerk",
            "Daniel Zeuch",
            "Frank K. Wilhelm"
        ],
        "subjects": [
            "Quantum Physics",
            "Systems and Control"
        ],
        "abstract": "Quantum computing (QC) and machine learning (ML), taken individually or combined into quantum-assisted ML (QML), are ascending computing paradigms whose calculations come with huge potential for speedup, increase in precision, and resource reductions. Likely improvements for numerical simulations in engineering imply the possibility of a strong economic impact on the manufacturing industry. In this project report, we propose a framework for a quantum computing-enhanced service ecosystem for simulation in manufacturing, consisting of various layers ranging from hardware to algorithms to service and organizational layers. In addition, we give insight into the current state of the art of applications research based on QC and QML, both from a scientific and an industrial point of view. We further analyse two high-value use cases with the aim of a quantitative evaluation of these new computing paradigms for industrially-relevant settings.",
        "comments": "10 pages, 3 figures",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10623"
    },
    {
        "doc_id": 230,
        "title": "Dynamic Programming: Finite States",
        "authors": [
            "Thomas J. Sargent",
            "John Stachurski"
        ],
        "subjects": [
            "General Economics",
            "Optimization and Control"
        ],
        "abstract": "This book is about dynamic programming and its applications in economics, finance, and adjacent fields. It brings together recent innovations in the theory of dynamic programming and provides applications and code that can help readers approach the research frontier. The book is aimed at graduate students and researchers, although most chapters are accessible to undergraduate students with solid quantitative backgrounds.",
        "comments": "MSC Class:          90C39",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10473"
    },
    {
        "doc_id": 231,
        "title": "Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning",
        "authors": [
            "Zaina Abu Hweij",
            "Florence Liang",
            "Sophie Zhang"
        ],
        "subjects": [
            "Machine Learning",
            "Signal Processing",
            "Medical Physics"
        ],
        "abstract": "Acute compartment syndrome (ACS) is an orthopedic emergency, caused by elevated pressure within a muscle compartment, that leads to permanent tissue damage and eventually death. Diagnosis of ACS relies heavily on patient-reported symptoms, a method that is clinically unreliable and often supplemented with invasive intracompartmental pressure measurements. This study proposes a continuous, objective, noninvasive diagnostic for ACS. The device detects ACS through a random forest machine learning model that uses pressure readings from force-sensitive resistors (FSRs) placed on the skin. The final diagnosis is exported real-time to a web application via Bluetooth. To validate the diagnostic, a data set containing FSR measurements and the corresponding simulated intracompartmental pressure was created. The diagnostic achieved an accuracy, on par to the invasive gold standard, of 97%. The device excelled in key performance metrics including precision, sensitivity, and F1 score. Manufactured for 73 USD, our device may be an economic alternative to needle-based diagnostics. These results demonstrate the potential of noninvasive ACS diagnostics to meet clinical standards and enhance patient care.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10386"
    },
    {
        "doc_id": 232,
        "title": "Forecasting dengue outbreaks with uncertainty using seasonal weather patterns",
        "authors": [
            "Piumi Chathurangika",
            "Sanjeewa Perera",
            "Kushani De Silva"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "Dengue is a vector-borne disease transmitted to humans by vectors of genus Aedes and is a global threat with health, social, and economic impact in many of the tropical countries including Sri Lanka. The virus transmission is significantly impacted by environmental conditions, with a notable contribution from elevated per-capita vector density. These conditions are dynamic in nature and specially having the tropical climate, Sri Lanka experiences seasonal weather patterns dominated by monsoons. In this work, we investigate the dynamic influence of environmental conditions on dengue emergence in Colombo district where dengue is extremely prevalent in Sri Lanka. A novel approach leveraging the Markov chain Monte Carlo simulations has been employed to identify seasonal patterns of dengue disease emergence, utilizing the dynamics of weather patterns governing in the region. The newly developed algorithm allows us to estimate the timing of dengue outbreaks with uncertainty, enabling accurate forecasts of upcoming disease emergence patterns for better preparedness.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10295"
    },
    {
        "doc_id": 233,
        "title": "Early Prediction of Geomagnetic Storms by Machine Learning Algorithms",
        "authors": [
            "Iris Yan"
        ],
        "subjects": [
            "Machine Learning"
        ],
        "abstract": "Geomagnetic storms (GS) occur when solar winds disrupt Earth's magnetosphere. GS can cause severe damages to satellites, power grids, and communication infrastructures. Estimate of direct economic impacts of a large scale GS exceeds $40 billion a day in the US. Early prediction is critical in preventing and minimizing the hazards. However, current methods either predict several hours ahead but fail to identify all types of GS, or make predictions within short time, e.g., one hour ahead of the occurrence. This work aims to predict all types of geomagnetic storms reliably and as early as possible using big data and machine learning algorithms. By fusing big data collected from multiple ground stations in the world on different aspects of solar measurements and using Random Forests regression with feature selection and downsampling on minor geomagnetic storm instances (which carry majority of the data), we are able to achieve an accuracy of 82.55% on data collected in 2021 when making early predictions three hours in advance. Given that important predictive features such as historic Kp indices are measured every 3 hours and their importance decay quickly with the amount of time in advance, an early prediction of 3 hours ahead of time is believed to be close to the practical limit.",
        "comments": "14 pages, 7 figures",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10290"
    },
    {
        "doc_id": 234,
        "title": "How industrial clusters influence the growth of the regional GDP: A spatial-approach",
        "authors": [
            "Vahidin Jeleskovic",
            "Steffen Loeber"
        ],
        "subjects": [
            "General Economics",
            "Econometrics"
        ],
        "abstract": "In this paper, we employ spatial econometric methods to analyze panel data from German NUTS 3 regions. Our goal is to gain a deeper understanding of the significance and interdependence of industry clusters in shaping the dynamics of GDP. To achieve a more nuanced spatial differentiation, we introduce indicator matrices for each industry sector which allows for extending the spatial Durbin model to a new version of it. This approach is essential due to both the economic importance of these sectors and the potential issue of omitted variables. Failing to account for industry sectors can lead to omitted variable bias and estimation problems. To assess the effects of the major industry sectors, we incorporate eight distinct branches of industry into our analysis. According to prevailing economic theory, these clusters should have a positive impact on the regions they are associated with. Our findings indeed reveal highly significant impacts, which can be either positive or negative, of specific sectors on local GDP growth. Spatially, we observe that direct and indirect effects can exhibit opposite signs, indicative of heightened competitiveness within and between industry sectors. Therefore, we recommend that industry sectors should be taken into consideration when conducting spatial analysis of GDP. Doing so allows for a more comprehensive understanding of the economic dynamics at play.",
        "comments": " ",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10261"
    },
    {
        "doc_id": 235,
        "title": "Nowcasting Madagascar's real GDP using machine learning algorithms",
        "authors": [
            "Franck Ramaharo",
            "Gerzhino Rasolofomanana"
        ],
        "subjects": [
            "General Economics",
            "Machine Learning"
        ],
        "abstract": "We investigate the predictive power of different machine learning algorithms to nowcast Madagascar's gross domestic product (GDP). We trained popular regression models, including linear regularized regression (Ridge, Lasso, Elastic-net), dimensionality reduction model (principal component regression), k-nearest neighbors algorithm (k-NN regression), support vector regression (linear SVR), and tree-based ensemble models (Random forest and XGBoost regressions), on 10 Malagasy quarterly macroeconomic leading indicators over the period 2007Q1--2022Q4, and we used simple econometric models as a benchmark. We measured the nowcast accuracy of each model by calculating the root mean square error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE). Our findings reveal that the Ensemble Model, formed by aggregating individual predictions, consistently outperforms traditional econometric models. We conclude that machine learning models can deliver more accurate and timely nowcasts of Malagasy economic performance and provide policymakers with additional guidance for data-driven decision making.",
        "comments": "13 pages, 6 figures, 5 tables",
        "date": "24 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10255"
    },
    {
        "doc_id": 236,
        "title": "Equilibrium Multiplicity: A Systematic Approach using Homotopies, with an Application to Chicago",
        "authors": [
            "Amine C-L. Ouazad"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Discrete choice models with social interactions or spillovers may exhibit multiple equilibria. This paper provides a systematic approach to enumerating them for a quantitative spatial model with discrete locations, social interactions, and elastic housing supply. The approach relies on two homotopies. A homotopy is a smooth function that transforms the solutions of a simpler city where solutions are known, to a city with heterogeneous locations and finite supply elasticity. The first homotopy is that, in the set of cities with perfectly elastic floor surface supply, an economy with heterogeneous locations is homotopic to an economy with homogeneous locations, whose solutions can be comprehensively enumerated. Such an economy is epsilon close to an economy whose equilibria are the zeros of a system of polynomials. This is a well-studied area of mathematics where the enumeration of equilibria can be guaranteed. The second homotopy is that a city with perfectly elastic housing supply is homotopic to a city with an arbitrary supply elasticity. In a small number of cases, the path may bifurcate and a single path yields two or more equilibria. By running the method on thousands of cities, we obtain a large number of equilibria. Each equilibrium has different population distributions. We provide a method that is computationally feasible for economies with a large number of locations choices, with an empirical application to the City of Chicago. There exist multiple ``counterfactual Chicagos'' consistent with the estimated parameters. Population distribution, prices, and welfare are not uniquely pinned down by amenities. The paper's method can be applied to models in trade and IO. Further applications of algebraic geometry are suggested.",
        "comments": "MSC Class:          91; 90; 65                          ACM Class:          G.3; J.4; I.6",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10181"
    },
    {
        "doc_id": 237,
        "title": "Nowcasting economic activity in European regions using a mixed-frequency dynamic factor model",
        "authors": [
            "Luca Barbaglia",
            "Lorenzo Frattarolo",
            "Niko Hauzenberger",
            "Dominik Hirschbuehl",
            "Florian Huber",
            "Luca Onorante",
            "Michael Pfarrhofer",
            "Luca Tiozzo Pezzoli"
        ],
        "subjects": [
            "Econometrics"
        ],
        "abstract": "Timely information about the state of regional economies can be essential for planning, implementing and evaluating locally targeted economic policies. However, European regional accounts for output are published at an annual frequency and with a two-year delay. To obtain robust and more timely measures in a computationally efficient manner, we propose a mixed-frequency dynamic factor model that accounts for national information to produce high-frequency estimates of the regional gross value added (GVA). We show that our model produces reliable nowcasts of GVA in 162 regions across 12 European countries.",
        "comments": "JEL: C22, C53, R11; keywords: factor models, mixed-frequency, nowcasting, regional data",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10054"
    },
    {
        "doc_id": 238,
        "title": "A Quantile Nelson-Siegel model",
        "authors": [
            "Matteo Iacopini",
            "Aubrey Poon",
            "Luca Rossini",
            "Dan Zhu"
        ],
        "subjects": [
            "Applications",
            "Econometrics"
        ],
        "abstract": "A widespread approach to modelling the interaction between macroeconomic variables and the yield curve relies on three latent factors usually interpreted as the level, slope, and curvature (Diebold et al., 2006). This approach is inherently focused on the conditional mean of the yields and postulates a dynamic linear model where the latent factors smoothly change over time. However, periods of deep crisis, such as the Great Recession and the recent pandemic, have highlighted the importance of statistical models that account for asymmetric shocks and are able to forecast the tails of a variable's distribution. A new version of the dynamic three-factor model is proposed to address this issue based on quantile regressions. The novel approach leverages the potential of quantile regression to model the entire (conditional) distribution of the yields instead of restricting to its mean. An application to US data from the 1970s shows the significant heterogeneity of the interactions between financial and macroeconomic variables across different quantiles. Moreover, an out-of-sample forecasting exercise showcases the proposed method's advantages in predicting the yield distribution tails compared to the standard conditional mean model. Finally, by inspecting the posterior distribution of the three factors during the recent major crises, new evidence is found that supports the greater and longer-lasting negative impact of the great recession on the yields compared to the COVID-19 pandemic.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09874"
    },
    {
        "doc_id": 239,
        "title": "Wealth dynamics in a multi-aggregate closed monetary system",
        "authors": [
            "Andrea Monaco",
            "Matteo Ghio",
            "Adamaria Perrotta"
        ],
        "subjects": [
            "Theoretical Economics"
        ],
        "abstract": "We examine the statistical properties of a closed monetary economy with multi-aggregates interactions. Building upon Yakovenko's single-agent monetary model (Dragulescu and Yakovenko, 2000), we investigate the joint equilibrium distribution of aggregate size and wealth. By comparing theoretical and simulated data, we validate our findings and investigate the influence of both micro dynamics and macro characteristics of the system on the distribution. Additionally, we analyze the system's convergence towards equilibrium under various conditions. Our laboratory model may offer valuable insights into macroeconomic phenomena allowing to reproduce typical wealth distribution features observed in real economy.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09871"
    },
    {
        "doc_id": 240,
        "title": "Game-theoretic Model Predictive Control for Modelling Competitive Supply Chains",
        "authors": [
            "Sophie Hall",
            "Laura Guerrini",
            "Florian D\u00f6rfler",
            "Dominic Liao-McPherson"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "Supply chains transform raw materials into finished goods and distribute them to end consumers. The vast majority of products we use daily are supplied to us through complex global supply chains. This paper proposes a modelling methodology for dynamic competitive supply chains based on game theory and model predictive control. We model each manufacturer in the supply chain as a rational utility maximizing agent that selects their actions by finding an open-loop generalized Nash equilibrium of a multi-stage game. To react to competitors and the state of the market, every agent re-plans their actions in a receding horizon manner based on estimates of market and supplier parameters thereby creating an approximate closed-loop equilibrium policy. We demonstrate through numerical simulations that this modelling approach is computationally tractable and generates economically interpretable behaviors in a variety of settings such as demand spikes, supply shocks, and information asymmetry.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09853"
    },
    {
        "doc_id": 241,
        "title": "Game Representations and Extensions of the Shapley Value",
        "authors": [
            "Pradeep Dubey"
        ],
        "subjects": [
            "Theoretical Economics"
        ],
        "abstract": "We show that any cooperative game can be represented by an assignment of costly facilities to players, in which it is intuitively obvious how to allocate the total cost in an equitable manner. This equitable solution turns out to be the Shapley value of the game, and thus provides as an alternative justification of the value. Game representations also open the door for extending the Shapley value to situations where not all coalitions can form, provided those that can constitute a \"semi-algebra\"; or, more generally, a \"hierarchy\"; or, still more generally, have \"full span\".",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09845"
    },
    {
        "doc_id": 242,
        "title": "A Framework for Digital Currencies for Financial Inclusion in Latin America and the Caribbean",
        "authors": [
            "Gabriel Bizama",
            "Alexander Wu",
            "Bernardo Paniagua",
            "Max Mitre"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "This research aims to provide a framework to assess the contribution of digital currencies to promote financial inclusion, based on a diagnosis of the landscape of financial inclusion and domestic and cross-border payments in Latin America and the Caribbean. It also provides insights from central banks in the region on key aspects regarding a possible implementation of central bank digital currencies. Findings show that although digital currencies development is at an early stage, a well-designed system could reduce the cost of domestic and cross-border payments, improve the settlement of transactions to achieve real-time payments, expand the accessibility of central bank money, incorporate programmable payments and achieve system performance demands.",
        "comments": "32 pages, 7 figures, 3 tables and 3 boxes",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09811"
    },
    {
        "doc_id": 243,
        "title": "AI and the Opportunity for Shared Prosperity: Lessons from the History of Technology and the Economy",
        "authors": [
            "Guy Ben-Ishai",
            "Jeff Dean",
            "James Manyika",
            "Ruth Porat",
            "Hal Varian",
            "Kent Walker"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Recent progress in artificial intelligence (AI) marks a pivotal moment in human history. It presents the opportunity for machines to learn, adapt, and perform tasks that have the potential to assist people, from everyday activities to their most creative and ambitious projects. It could also help businesses and organizations harness knowledge, increase productivity, innovate, transform, and power shared prosperity. This tremendous potential raises two fundamental questions: (1) Will AI actually advance national and global economic transformation to benefit society at large? and (2) What issues must we get right to fully realize AI's economic value, expand prosperity and improve lives everywhere? We explore these questions by considering the recent history of technology and innovation as a guide for the likely impact of AI and what we must do to realize its economic potential to benefit society. While we do not presume the future will be entirely like that past, for reasons we will discuss, we do believe prior experience with technological change offers many useful lessons. We conclude that while progress in AI presents a historic opportunity to advance our economic prosperity and future wellbeing, its economic benefits will not come automatically and that AI risks exacerbating existing economic challenges unless we collectively and purposefully act to enable its potential and address its challenges. We suggest a collective policy agenda - involving developers, deployers and users of AI, infrastructure providers, policymakers, and those involved in workforce training - that may help both realize and harness AI's economic potential and address its risks to our shared prosperity.",
        "comments": "Draft withdrawn to obtain feedback",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09811"
    },
    {
        "doc_id": 244,
        "title": "Empowering Africa: An In-depth Exploration of the Adoption of Artificial Intelligence Across the Continent",
        "authors": [
            "Kinyua Gikunda"
        ],
        "subjects": [
            "Computers and Society"
        ],
        "abstract": "This paper explores the dynamic landscape of Artificial Intelligence (AI) adoption in Africa, analysing its varied applications in addressing socio-economic challenges and fostering development. Examining the African AI ecosystem, the study considers regional nuances, cultural factors, and infrastructural constraints shaping the deployment of AI solutions. Case studies in healthcare, agriculture, finance, and education highlight AI's transformative potential for efficiency, accessibility, and inclusivity. The paper emphasizes indigenous AI innovations and international collaborations contributing to a distinct African AI ecosystem. Ethical considerations, including data privacy and algorithmic bias, are addressed alongside policy frameworks supporting responsible AI implementation. The role of governmental bodies, regulations, and private sector partnerships is explored in creating a conducive AI development environment. Challenges such as digital literacy gaps and job displacement are discussed, with proposed strategies for mitigation. In conclusion, the paper provides a nuanced understanding of AI in Africa, contributing to sustainable development discussions and advocating for an inclusive and ethical AI ecosystem on the continent.",
        "comments": " ",
        "date": "28 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.09457"
    },
    {
        "doc_id": 245,
        "title": "Equity Premium in Efficient Markets",
        "authors": [
            "B. N. Kausik"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Equity premium, the surplus returns of stocks over bonds, has been an enduring puzzle. While numerous prior works approach the problem assuming the utility of money is invariant across contexts, our approach implies that in efficient markets the utility of money is polymorphic, with risk aversion dependent on the information available in each context, i.e. the discount on each future cash flow depends on all information available on that cash flow. Specifically, we prove that in efficient markets, informed investors maximize return on volatility by being risk-neutral with riskless bonds, and risk-averse with equities, thereby resolving the puzzle. We validate our results on historical data with surprising consistency.\n  JEL Classification: C58, G00, G12, G17",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09265"
    },
    {
        "doc_id": 246,
        "title": "Airline delays, congestion internalization and non-price spillover effects of low cost carrier entry",
        "authors": [
            "William E. Bendinelli",
            "Humberto F. A. J. Bettini",
            "Alessandro V. M. Oliveira"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "This paper develops an econometric model of flight delays to investigate the influence of competition and dominance on the incentives of carriers to maintain on-time performance. We consider both the route and the airport levels to inspect the local and global effects of competition, with a unifying framework to test the hypotheses of 1. airport congestion internalization and 2. the market competition-quality relationship in a single econometric model. In particular, we examine the impacts of the entry of low cost carriers (LCC) on the flight delays of incumbent full service carriers in the Brazilian airline industry. The main results indicate a highly significant effect of airport congestion self-internalization in parallel with route-level quality competition. Additionally, the potential competition caused by LCC presence provokes a global effect that suggests the existence of non-price spillovers of the LCC entry to non-entered routes.",
        "comments": "Journal ref:        Transportation Research Part A: Policy and Practice, 85, 39-52 (2016)",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09174"
    },
    {
        "doc_id": 247,
        "title": "AI Thrust: Ranking Emerging Powers for Tech Startup Investment in Latin America",
        "authors": [
            "Abraham Ramos Torres",
            "Laura N Montoya"
        ],
        "subjects": [
            "General Economics",
            "Risk Management"
        ],
        "abstract": "Artificial intelligence (AI) is rapidly transforming the global economy, and Latin America is no exception. In recent years, there has been a growing interest in AI development and implementation in the region. This paper presents a ranking of Latin American (LATAM) countries based on their potential to become emerging powers in AI. The ranking is based on three pillars: infrastructure, education, and finance. Infrastructure is measured by the availability of electricity, high-speed internet, the quality of telecommunications networks, and the availability of supercomputers. Education is measured by the quality of education and the research status. Finance is measured by the cost of investments, history of investments, economic metrics, and current implementation of AI.\n  While Brazil, Chile, and Mexico have established themselves as major players in the AI industry in Latin America, our ranking demonstrates the new emerging powers in the region. According to the results, Argentina, Colombia, Uruguay, Costa Rica, and Ecuador are leading as new emerging powers in AI in Latin America. These countries have strong education systems, well-developed infrastructure, and growing financial resources. The ranking provides a useful tool for policymakers, investors, and businesses interested in AI development in Latin America. It can help to identify emerging LATAM countries with the greatest potential for AI growth and success.",
        "comments": "9 pages, 4 tables, 9 figures",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09056"
    },
    {
        "doc_id": 248,
        "title": "On continuity of state-dependent utilities",
        "authors": [
            "Edoardo Berton",
            "Alessandro Doldi",
            "Marco Maggis"
        ],
        "subjects": [
            "Mathematical Finance",
            "Theoretical Economics"
        ],
        "abstract": "State-dependent preferences for a general Savage's state space were shown in Wakker and Zank (1999) to admit a numerical representation in the form of the integral of a state-dependent utility, as soon as pointwise continuity of the preference ordering is assumed. In this paper we prove that such a state-dependent function inherits pointwise continuity from the preference ordering, providing in this way a positive answer to a conjecture posed in the aforementioned seminal work. We further apply this result to obtain an explicit representation of conditional Chisini means in the form of a conditional certainty equivalent.",
        "comments": "MSC Class:          91B06; 91B08; 60A05",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09054"
    },
    {
        "doc_id": 249,
        "title": "Strategic formation of production networks",
        "authors": [
            "Antoine Mandel",
            "Van-Quy Nguyen",
            "Bach Dong-Xuan"
        ],
        "subjects": [
            "Theoretical Economics"
        ],
        "abstract": "We provide a strategic model of the formation of production networks that subsumes the standard general equilibrium approach. The objective of firms in our setting is to choose their supply relationships so as to maximize their profit at the general equilibrium that unfolds. We show that this objective is equivalent to the maximization by the firms of their eigenvector centrality in the production network. As is common in network formation games based on centrality, there are multiple Nash equilibria in our setting. We have investigated the characteristics and the social efficiency of these equilibria in a stylized version of our model representing international trade networks. We show that the impact of network structure on social welfare is firstly determined by a trade-off between costs of increasing process complexity and positive spillovers on productivity induced by the diversification of the input mix. We further analyze a variant of our model that accounts for the risks of disruption of supply relationships. In this setting, we characterize how social welfare depends on the structure of the production network, the spatial distribution of risks, and the process of shock aggregation in supply chains. We finally show that simple trade policies characterized by sets of links that are either prevented or catalyzed can be a powerful equilibrium selection device.",
        "comments": " ",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08929"
    },
    {
        "doc_id": 250,
        "title": "Spurious Default Probability Projections in Credit Risk Stress Testing Models",
        "authors": [
            "Bernd Engelmann"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "Credit risk stress testing has become an important risk management device which is used both by banks internally and by regulators. Stress testing is complex because it essentially means projecting a bank's full balance sheet conditional on a macroeconomic scenario over multiple years. Part of the complexity stems from using a wide range of model parameters for, e.g., rating transition, write-off rules, prepayment, or origination of new loans. A typical parameterization of a credit risk stress test model specifies parameters linked to an average economic, the through-the-cycle, state. These parameters are transformed to a stressed state by utilizing a macroeconomic model. It will be shown that the model parameterization implies a unique through-the-cycle portfolio which is unrelated to a bank's current portfolio. Independent of the stress imposed to the model, the current portfolio will have a tendency to propagate towards the through-the-cycle portfolio. This could create unwanted spurious effects on projected portfolio default rates especially when a stress test model's parameterization is inconsistent with a bank's current portfolio.",
        "comments": "15 pages, 4 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08892"
    },
    {
        "doc_id": 251,
        "title": "MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data",
        "authors": [
            "Zhengke Sun",
            "Yuliang Ma"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence"
        ],
        "abstract": "The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model, this paper also built a new adaptive adjacency matrix generation method and adjacency matrix attention module. This model mainly used gated temporal convolution and graph convolution to extract temporal and spatial information, respectively. Compared with multiple baselines, our model achieved the best performance on Shanghai taxi GPS trajectory dataset. The code is available at https://github.com/zachysun/Taxi_Traffic_Benchmark.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08727"
    },
    {
        "doc_id": 252,
        "title": "Automated Design Appraisal: Estimating Real Estate Price Growth and Value at Risk due to Local Development",
        "authors": [
            "Adam R. Swietek"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Financial criteria in architectural design evaluation are limited to cost performance. Here, I introduce a method, Automated Design Appraisal (ADA), to predict the market price of a generated building design concept within a local urban context. Integrating ADA with 3D building performance simulations enables financial impact assessment that exceeds the spatial resolution of previous work. Within an integrated impact assessment, ADA measures the direct and localized effect of urban development. To demonstrate its practical utility, I study local devaluation risk due to nearby development associated with changes to visual landscape quality. The results shed light on the relationship between amenities and property value, identifying clusters of properties physically exposed or financially sensitive to local land-use change. Beyond its application as a financial sensitivity tool, ADA serves as a blueprint for architectural design optimization procedures, in which economic performance is evaluated based on learned preferences derived from financial market data.",
        "comments": "18 pages, 7 figures",
        "date": "17 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08645"
    },
    {
        "doc_id": 253,
        "title": "Forking paths in financial economics",
        "authors": [
            "Guillaume Coqueret"
        ],
        "subjects": [
            "General Finance",
            "Methodology"
        ],
        "abstract": "We argue that spanning large numbers of degrees of freedom in empirical analysis allows better characterizations of effects and thus improves the trustworthiness of conclusions. Our ideas are illustrated in three studies: equity premium prediction, asset pricing anomalies and risk premia estimation. In the first, we find that each additional degree of freedom in the protocol expands the average range of $t$-statistics by at least 30%. In the second, we show that resorting to forking paths instead of bootstrapping in multiple testing raises the bar of significance for anomalies: at the 5% confidence level, the threshold for bootstrapped statistics is 4.5, whereas with paths, it is at least 8.2, a bar much higher than those currently used in the literature. In our third application, we reveal the importance of particular steps in the estimation of premia. In addition, we use paths to corroborate prior findings in the three topics. We document heterogeneity in our ability to replicate prior studies: some conclusions seem robust, others do not align with the paths we were able to generate.",
        "comments": " ",
        "date": "25 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08606"
    },
    {
        "doc_id": 254,
        "title": "Non-Banking Sector development effect on Economic Growth. A Nighttime light data approach",
        "authors": [
            "Leonard Mushunje",
            "Maxwell Mashasha"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "This paper uses nighttime light(NTL) data to measure the nexus of the non-banking sector, particularly insurance, and economic growth in South Africa. We hypothesize that insurance sector growth positively propels economic growth due to its economic growth-supportive traits like investment protection and optimal risk mitigation. We also claim that Nighttime light data is a good economic measure than Gross domestic product (GDP). We used weighted regressions to measure the relationships between nighttime light data, GDP, and insurance sector development. We used time series South African GDP data collected from the World Bank for the period running from 2000 to 2018, and the nighttime lights data from the National Geophysical Data Centre (NGDC) in partnership with the National Oceanic and Atmospheric Administration (NOAA). From the models fitted and the reported BIC, AIC, and likelihood ratios, the insurance sector proved to have more predictive power on economic development in South Africa, and radiance light explained economic growth better than GDP and GDP/Capita. We concluded that nighttime data is a good proxy for economic growth than GDP/Capita in emerging economies like South Africa, where secondary data needs to be more robust and sometimes inflated. The findings will guide researchers and policymakers on what drives economic development and what policies to put in place. It would be interesting to extend the current study to other sectors such as micro-finances, mutual and hedge funds.",
        "comments": "28 pages",
        "date": "19 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08596"
    },
    {
        "doc_id": 255,
        "title": "How do we measure trade elasticity for services?",
        "authors": [
            "Satoshi Nakano",
            "Kazuhiko Nishimura"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "This paper is about our attempt of identifying trade elasticities through the variations in the exchange rate, for possible applications to the case of services whose physical transactions are veiled in the trade statistics. The regression analysis to estimate the elasticity entails a situation where the explanatory variable is leaked into the error term through the latent supply equation, causing an endogeneity problem for which an instrumental variable cannot be found. Our identification strategy is to utilize the normalizing condition, which enables the supply parameter to be identified, along with the reduced-form equation of the system of demand and supply equations. We evaluate the performances of the method proposed by applying to several different tangible goods, whose benchmark trade elasticities are estimable by utilizing the information on their physical transactions.",
        "comments": " ",
        "date": "18 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08594"
    },
    {
        "doc_id": 256,
        "title": "Incremento del precio de los combustibles y su incidencia en los productos de la canasta basica del canton el triunfo, provincia del guayas",
        "authors": [
            "Alvear Guzman Katherine",
            "Campozano Buele Jenner",
            "Duran Canarte Paulette",
            "Holguin Cedeno Roger",
            "Mejia Crespin Fernando"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "The objective of this research was to analyze the impact of the increase in the price of fuels and its incidence on the products of the basic basket of the El Triunfo, the province of Guayas. In the present study, the non-experimental quantitative method was used. The study population was limited to the families of the town, seeking to determine how their level of consumption was impacted after the increase in fuels. Just 95 people were randomly taken. The study instrument that was used was surveys, with a focus on the purchasing power of families with respect to the basic basket after the increase in fuel prices. The results were processed through Cronbach's Alpha and reflected in pie charts. The independent and dependent variable that make up our study, were related through a simple linear regression, to determine if they correlate with each other.\n  Keywords: Fuels, Basic basket, Linear regression, Subsidies, Inflation",
        "comments": "in Spanish language",
        "date": "13 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08590"
    },
    {
        "doc_id": 257,
        "title": "Incentivizing Secure Software Development: The Role of Liability (Waiver) and Audit",
        "authors": [
            "Ziyuan Huang",
            "Gergely Bicz\u00f3k",
            "Mingyan Liu"
        ],
        "subjects": [
            "Cryptography and Security",
            "Systems and Control"
        ],
        "abstract": "Misaligned incentives in secure software development have long been the focus of research in the economics of security. Product liability, a powerful legal framework in other industries, has been largely ineffective for software products until recent times. However, the rapid regulatory responses to recent global cyberattacks by both the United States and the European Union, together with the (relative) success of the General Data Protection Regulation in defining both duty and standard of care for software vendors, may just enable regulators to use liability to re-align incentives for the benefit of the digital society. Specifically, the recently proposed United States National Cybersecurity Strategy shifts responsibility for cyber incidents back to software vendors. In doing so, the strategy also puts forward the concept of the liability waiver: if a software company voluntarily undergoes and passes an IT security audit, its liability is waived.\n  In this paper, we analyze this audit scenario from the aspect of the software vendor. We propose a mechanism where a software vendor should first undergo a repeated auditing process in each stage of which the vendor decides whether to quit early or stay with additional security investment. We show that the optimal strategy for an opt-in vendor is to never quit; and exert cumulative investments in either \"one-and-done\" or \"incremental\" manner. We relate the audit mechanism to a liability waiver insurance policy and revealed its effect on reshaping the vendor's risk perception. We also discuss influence of audit quality on the vendor's incentives and pinpoint that a desirable audit rule should be highly accurate and less strict.",
        "comments": "21 pages, 6 figures, submitted to the 23rd Workshop on the Economics of Information Security",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08476"
    },
    {
        "doc_id": 258,
        "title": "Assessing the impact of forced and voluntary behavioral changes on economic-epidemiological co-dynamics: A comparative case study between Belgium and Sweden during the 2020 COVID-19 pandemic",
        "authors": [
            "Tijs W. Alleman",
            "Jan M. Baetens"
        ],
        "subjects": [
            "Econometrics",
            "Computational Engineering, Finance, and Science"
        ],
        "abstract": "During the COVID-19 pandemic, governments faced the challenge of managing population behavior to prevent their healthcare systems from collapsing. Sweden adopted a strategy centered on voluntary sanitary recommendations while Belgium resorted to mandatory measures. Their consequences on pandemic progression and associated economic impacts remain insufficiently understood. This study leverages the divergent policies of Belgium and Sweden during the COVID-19 pandemic to relax the unrealistic -- but persistently used -- assumption that social contacts are not influenced by an epidemic's dynamics. We develop an epidemiological-economic co-simulation model where pandemic-induced behavioral changes are a superposition of voluntary actions driven by fear, prosocial behavior or social pressure, and compulsory compliance with government directives. Our findings emphasize the importance of early responses, which reduce the stringency of measures necessary to safeguard healthcare systems and minimize ensuing economic damage. Voluntary behavioral changes lead to a pattern of recurring epidemics, which should be regarded as the natural long-term course of pandemics. Governments should carefully consider prolonging lockdown longer than necessary because this leads to higher economic damage and a potentially higher second surge when measures are released. Our model can aid policymakers in the selection of an appropriate long-term strategy that minimizes economic damage.",
        "comments": " ",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08442"
    },
    {
        "doc_id": 259,
        "title": "Causal Machine Learning for Moderation Effects",
        "authors": [
            "Nora Bearth",
            "Michael Lechner"
        ],
        "subjects": [
            "Econometrics",
            "Machine Learning"
        ],
        "abstract": "It is valuable for any decision maker to know the impact of decisions (treatments) on average and for subgroups. The causal machine learning literature has recently provided tools for estimating group average treatment effects (GATE) to understand treatment heterogeneity better. This paper addresses the challenge of interpreting such differences in treatment effects between groups while accounting for variations in other covariates. We propose a new parameter, the balanced group average treatment effect (BGATE), which measures a GATE with a specific distribution of a priori-determined covariates. By taking the difference of two BGATEs, we can analyse heterogeneity more meaningfully than by comparing two GATEs. The estimation strategy for this parameter is based on double/debiased machine learning for discrete treatments in an unconfoundedness setting, and the estimator is shown to be $\\sqrt{N}$-consistent and asymptotically normal under standard conditions. Adding additional identifying assumptions allows specific balanced differences in treatment effects between groups to be interpreted causally, leading to the causal balanced group average treatment effect. We explore the finite sample properties in a small-scale simulation study and demonstrate the usefulness of these parameters in an empirical example.",
        "comments": " ",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08290"
    },
    {
        "doc_id": 260,
        "title": "A techno-economic model for avoiding conflicts of interest between owners of offshore wind farms and maintenance suppliers",
        "authors": [
            "Alberto Pliego Marug\u00e1n",
            "Fausto Pedro Garc\u00eda M\u00e1rquez",
            "Jes\u00fas Mar\u00eda Pinar P\u00e9rez"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "General Economics",
            "Systems and Control"
        ],
        "abstract": "Currently, wind energy is one of the most important sources of renewable energy. Offshore locations for wind turbines are increasingly exploited because of their numerous advantages. However, offshore wind farms require high investment in maintenance service. Due to its complexity and special requirements, maintenance service is usually outsourced by wind farm owners. In this paper, we propose a novel approach to determine, quantify, and reduce the possible conflicts of interest between owners and maintenance suppliers. We created a complete techno-economic model to address this problem from an impartial point of view. An iterative process was developed to obtain statistical results that can help stakeholders negotiate the terms of the contract, in which the availability of the wind farm is the reference parameter by which to determine penalisations and incentives. Moreover, a multi-objective programming problem was addressed that maximises the profits of both parties without losing the alignment of their interests. The main scientific contribution of this paper is the maintenance analysis of offshore wind farms from two perspectives: that of the owner and the maintenance supplier. This analysis evaluates the conflicts of interest of both parties. In addition, we demonstrate that proper adjustment of some parameters, such as penalisation, incentives, and resources, and adequate control of availability can help reduce this conflict of interests.",
        "comments": "Published in Renewable and Sustainable Energy Reviews (ELSEVIER) 10 July 2022. DOI: https://doi.org/10.1016/j.rser.2022.112753 Cite as: Marug\u00e1n, A. P., M\u00e1rquez, F. P. G., & P\u00e9rez, J. M. P. (2022). A techno-economic model for avoiding conflicts of interest between owners of offshore wind farms and maintenance suppliers. Renewable and Sustainable Energy Reviews, 168, 112753",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08251"
    },
    {
        "doc_id": 261,
        "title": "A Large-Scale Epidemic Simulation Framework for Realistic Social Contact Networks",
        "authors": [
            "Joy Kitson",
            "Ian Costello",
            "Jiangzhuo Chen",
            "Diego Jim\u00e9nez",
            "Stefan Hoops",
            "Henning Mortveit",
            "Esteban Meneses",
            "Jae-Seung Yeom",
            "Madhav V. Marathe",
            "Abhinav Bhatele"
        ],
        "subjects": [
            "Distributed, Parallel, and Cluster Computing"
        ],
        "abstract": "Global pandemics can wreak havoc and lead to significant social, economic, and personal losses. Preventing the spread of infectious diseases requires implementing interventions at different levels of government, and evaluating the potential impact and efficacy of those preemptive measures. Agent-based modeling can be used for detailed studies of epidemic diffusion and possible interventions. We present Loimos, a highly parallel simulation of epidemic diffusion written on top of Charm++, an asynchronous task-based parallel runtime. Loimos uses a hybrid of time-stepping and discrete-event simulation to model disease spread. We demonstrate that our implementation of Loimos is able to scale to large core counts on an HPC system. In particular, Loimos is able to simulate a US-scale synthetic interaction network in an average of 1.497 seconds per simulation day when executed on 16 nodes on Rivanna at the University of Virginia, processing around 428 billion interactions (person-person edges) in under five minutes for an average of 1.4 billion traversed edges per second (TEPS).",
        "comments": "13 pages (including references), 9 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08124"
    },
    {
        "doc_id": 262,
        "title": "Automated lag-selection for multi-step univariate time series forecast using Bayesian Optimization: Forecast station-wise monthly rainfall of nine divisional cities of Bangladesh",
        "authors": [
            "Rezoanoor Rahman",
            "Fariha Taskin"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "Rainfall is an essential hydrological component, and most of the economic activities of an agrarian country like Bangladesh depend on rainfall. An accurate rainfall forecast can help make necessary decisions and reduce the damages caused by heavy or low to no rainfall. The monthly average rainfall is a time series data, and recently, long short-term memory (LSTM) neural networks are being used heavily for time series forecasting problems. One major challenge of forecasting using LSTMs is to select the appropriate number of lag values. In this research, we considered the number of lag values selected as a hyperparameter of LSTM; it, with the other hyperparameters determining LSTMs structure, has been optimized using Bayesian optimization. We used our proposed method to forecast rainfall for nine different weather stations of Bangladesh. Finally, the performance of the proposed model has been compared with some other LSTM with different lag-selection methods and some several popular machine learning and statistical forecasting models.",
        "comments": "19 pages in total",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08070"
    },
    {
        "doc_id": 263,
        "title": "A new model of trust based on neural information processing",
        "authors": [
            "Scott E. Allen",
            "Ren\u00e9 F. Kizilcec",
            "A. David Redish"
        ],
        "subjects": [
            "General Economics",
            "Human-Computer Interaction",
            "Neurons and Cognition"
        ],
        "abstract": "More than 30 years of research has firmly established the vital role of trust in human organizations and relationships, but the underlying mechanisms by which people build, lose, and rebuild trust remains incompletely understood. We propose a mechanistic model of trust that is grounded in the modern neuroscience of decision making. Since trust requires anticipating the future actions of others, any mechanistic model must be built upon up-to-date theories on how the brain learns, represents, and processes information about the future within its decision-making systems. Contemporary neuroscience has revealed that decision making arises from multiple parallel systems that perform distinct, complementary information processing. Each system represents information in different forms, and therefore learns via different mechanisms. When an act of trust is reciprocated or violated, this provides new information that can be used to anticipate future actions. The taxonomy of neural information representations that is the basis for the system boundaries between neural decision-making systems provides a taxonomy for categorizing different forms of trust and generating mechanistic predictions about how these forms of trust are learned and manifested in human behavior. Three key predictions arising from our model are (1) strategic risk-taking can reveal how to best proceed in a relationship, (2) human organizations and environments can be intentionally designed to encourage trust among their members, and (3) violations of trust need not always degrade trust, but can also provide opportunities to build trust.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08064"
    },
    {
        "doc_id": 264,
        "title": "A Day-to-Day Dynamical Approach to the Most Likely User Equilibrium Problem",
        "authors": [
            "Jiayang Li",
            "Qianni Wang",
            "Liyang Feng",
            "Jun Xie",
            "Yu Marco Nie"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Multiagent Systems",
            "General Economics"
        ],
        "abstract": "The lack of a unique user equilibrium (UE) route flow in traffic assignment has posed a significant challenge to many transportation applications. The maximum-entropy principle, which advocates for the consistent selection of the most likely solution as a representative, is often used to address the challenge. Built on a recently proposed day-to-day (DTD) discrete-time dynamical model called cumulative logit (CULO), this study provides a new behavioral underpinning for the maximum-entropy UE (MEUE) route flow. It has been proven that CULO can reach a UE state without presuming travelers are perfectly rational. Here, we further establish that CULO always converges to the MEUE route flow if (i) travelers have zero prior information about routes and thus are forced to give all routes an equal choice probability, or (ii) all travelers gather information from the same source such that the so-called general proportionality condition is satisfied. Thus, CULO may be used as a practical solution algorithm for the MEUE problem. To put this idea into practice, we propose to eliminate the route enumeration requirement of the original CULO model through an iterative route discovery scheme. We also examine the discrete-time versions of four popular continuous-time dynamical models and compare them to CULO. The analysis shows that the replicator dynamic is the only one that has the potential to reach the MEUE solution with some regularity. The analytical results are confirmed through numerical experiments.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08013"
    },
    {
        "doc_id": 265,
        "title": "Inequality leads to the evolution of intolerance in reputation-based populations",
        "authors": [
            "Luis A. Martinez-Vaquero"
        ],
        "subjects": [
            "Physics and Society"
        ],
        "abstract": "This work studies the impact of economic inequality on the evolution of intolerance through a reputation-based model of indirect reciprocity. Results show that economic inequality is a powerful enhancer of intolerance, inducing the escalation of out-group discrimination even without the presence of new intolerant mutants. It also generates behavior modifications within tolerant disfavored minorities: their members either relax punishments against the uncooperative or prioritize helping the wealthy, even suffering discrimination in return. On the other hand, the redistribution of wealth is proved as a viable solution to avoid the spread of intolerance as long as it increases equality and is implemented before intolerance permeates part of the population.",
        "comments": "Journal ref:        Chaos 33 (3), 033119 (2023)",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07873"
    },
    {
        "doc_id": 266,
        "title": "Evolutionary dynamics of organised crime and terrorist networks",
        "authors": [
            "Luis A. Martinez-Vaquero",
            "Valerio Dolci",
            "Vito Trianni"
        ],
        "subjects": [
            "Physics and Society"
        ],
        "abstract": "Crime is pervasive into modern societies, although with different levels of diffusion across regions. Its dynamics are dependent on various socio-economic factors that make the overall picture particularly complex. While several theories have been proposed to account for the establishment of criminal behaviour, from a modelling perspective organised crime and terrorist networks received much less attention. In particular, the dynamics of recruitment into such organisations deserve specific considerations, as recruitment is the mechanism that makes crime and terror proliferate. We propose a framework able to model such processes in both organised crime and terrorist networks from an evolutionary game theoretical perspective. By means of a stylised model, we are able to study a variety of different circumstances and factors influencing the growth or decline of criminal organisations and terrorist networks, and observe the convoluted interplay between agents that decide to get associated to illicit groups, criminals that prefer to act on their own, and the rest of the civil society.",
        "comments": "Journal ref:        Sci Rep 9, 9727 (2019)",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07869"
    },
    {
        "doc_id": 267,
        "title": "A General Approach for Computing a Consensus in Group Decision Making That Integrates Multiple Ethical Principles",
        "authors": [
            "Francisco Salas-Molina",
            "Filippo Bistaffa",
            "Juan A. Rodriguez-Aguilar"
        ],
        "subjects": [
            "Theoretical Economics"
        ],
        "abstract": "We tackle the problem of computing a consensus according to multiple ethical principles -- which can include, for example, the principle of maximum freedom associated with the Benthamite doctrine and the principle of maximum fairness associated with the Rawlsian principles -- among the preferences of different individuals in the context of Group-Decision-Making. More formally, we put forward a novel formalisation of the above-mentioned problem based on a multinorm approximation problem that aims at minimising multiple p-metric distance functions, where each parameter p represents a given ethical principle. Our contribution incurs obvious benefits from a social-choice perspective. Firstly, our approach significantly generalises state-of-the-art approaches that were limited to only two ethical principles (p set to one, for maximum freedom, and p set to infinity, for maximum fairness). Secondly, our experimental results considering an established test case demonstrate that our approach is capable, thanks to a novel re-weighting scheme, to compute a multi-norm consensus that takes into account each ethical principle in a balanced way, in contrast with state-of-the-art approaches that were heavily biased towards the p=1 ethical principle",
        "comments": "20 pages, 1 table, 1 figure",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07818"
    },
    {
        "doc_id": 268,
        "title": "Improving OCR Quality in 19th Century Historical Documents Using a Combined Machine Learning Based Approach",
        "authors": [
            "David Fleischhacker",
            "Wolfgang Goederle",
            "Roman Kern"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "This paper addresses a major challenge to historical research on the 19th century. Large quantities of sources have become digitally available for the first time, while extraction techniques are lagging behind. Therefore, we researched machine learning (ML) models to recognise and extract complex data structures in a high-value historical primary source, the Schematismus. It records every single person in the Habsburg civil service above a certain hierarchical level between 1702 and 1918 and documents the genesis of the central administration over two centuries. Its complex and intricate structure as well as its enormous size have so far made any more comprehensive analysis of the administrative and social structure of the later Habsburg Empire on the basis of this source impossible. We pursued two central objectives: Primarily, the improvement of the OCR quality, for which we considered an improved structure recognition to be essential; in the further course, it turned out that this also made the extraction of the data structure possible. We chose Faster R-CNN as base for the ML architecture for structure recognition. In order to obtain the required amount of training data quickly and economically, we synthesised Hof- und Staatsschematismus-style data, which we used to train our model. The model was then fine-tuned with a smaller set of manually annotated historical source data. We then used Tesseract-OCR, which was further optimised for the style of our documents, to complete the combined structure extraction and OCR process. Results show a significant decrease in the two standard parameters of OCR-performance, WER and CER (where lower values are better). Combined structure detection and fine-tuned OCR improved CER and WER values by remarkable 71.98 percent (CER) respectively 52.49 percent (WER).",
        "comments": "29 pages, 23 figures, 7 tables",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07787"
    },
    {
        "doc_id": 269,
        "title": "Provisions and Economic Capital for Credit Losses",
        "authors": [
            "Dorinel Bastide",
            "St\u00e9phane Cr\u00e9pey"
        ],
        "subjects": [
            "Risk Management",
            "Probability",
            "General Finance"
        ],
        "abstract": "Based on supermodularity ordering properties, we show that convex risk measures of credit losses are nondecreasing  w.r.t. credit-credit and, in a wrong-way risk setup, credit-market, covariances of elliptically distributed latent factors. These results support the use of such setups for computing credit provisions and economic capital or for conducting stress test exercises and risk management analysis.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07728"
    },
    {
        "doc_id": 270,
        "title": "Impermanent Loss Conditions: An Analysis of Decentralized Exchange Platforms",
        "authors": [
            "Matthias Hafner",
            "Helmut Dietl"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Decentralized exchanges are widely used platforms for trading crypto assets. The most common types work with automated market makers (AMM), allowing traders to exchange assets without needing to find matching counterparties. Thereby, traders exchange against asset reserves managed by smart contracts. These assets are provided by liquidity providers in exchange for a fee. Static analysis shows that small price changes in one of the assets can result in losses for liquidity providers. Despite the success of AMMs, it is claimed that liquidity providers often suffer losses. However, the literature does not adequately consider the dynamic effects of fees over time. Therefore, we investigate the impermanent loss problem in a dynamic setting using Monte Carlo simulations. Our findings indicate that price changes do not necessarily lead to losses. Fees paid by traders and arbitrageurs are equally important. In this respect, we can show that an arbitrage-friendly environment benefits the liquidity provider. Thus, we suggest that AMM developers should promote an arbitrage-friendly environment rather than trying to prevent arbitrage.",
        "comments": "This paper was presented at the CfC 2024 Academic Track Conference",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07689"
    },
    {
        "doc_id": 271,
        "title": "Spatial clusters for demand and supply of childcare services in Italy",
        "authors": [
            "Andreella Angela",
            "Aliverti Emanuele",
            "Caldura Federico",
            "Campostrini Stefano"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "The availability of affordable and high-quality childcare services has become a significant concern in recent years. Such services can facilitate the balance between work and family life, increasing participation in the workforce and promoting gender equality. Furthermore, childcare can also help address the issue of decreasing fertility rates by making it more affordable for parents to have children while maintaining their careers. This is critical, especially for countries that are facing ultralow fertility rates like Italy. The Italian government has included within the recovery and resilience plan financed with Next Generations EU funds an unprecedented investment in order to increase the supply of children's education services and make it more equitably distributed across the country. In this article, we estimate groups of spatial areas with similar structures in terms of coverage (availability of childcare services at the municipality level), public expenditure rates in childcare, as well as other socio-demographic and economic factors, such as female employment, education, and grandparent rates. Our empirical findings confirm how Italy is characterized by a large number of \"sub-regional models\" and how some of these clusters are shared across multiple regions. We provide a preliminary attempt to explain how such patterns are driven by socio-demographic factors and argue that these very different conditions necessitate specific policy decisions. The work highlights the need for regional governance of the children's educational system.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07600"
    },
    {
        "doc_id": 272,
        "title": "Existence of MMS Allocations with Mixed Manna",
        "authors": [
            "Kevin Hsu"
        ],
        "subjects": [
            "Computer Science and Game Theory"
        ],
        "abstract": "Maximin share (MMS) allocations are a popular relaxation of envy-free allocations that have received wide attention in the context of the fair division of indivisible items. Although MMS allocations can fail to exist [1], previous work has found conditions under which they exist. Specifically, MMS allocations exist whenever $m \\leq n+5$ in the context of goods allocation, and this bound is tight in the sense that MMS allocations can fail to exist when $m = n+6$ [2]. Unfortunately, the technique used to establish this result does not generalize readily to the chores and mixed manna settings. This paper generalizes this result to the chores setting and provides a partial solution for the mixed manna setting. Our results depend on the presence of certain types of agents. Specifically, an agent $i$ is a goods agent (resp. chores agent) if every item is a good (resp. chore) to $i$, and a non-negative mixed agent if $i$ is neither a goods nor a chores agent and the MMS guarantee of $i$ is non-negative. In this paper, we prove that an MMS allocation exists if $m \\leq n+5$ and there exists a goods agent, a non-negative mixed agent, or only chores agents.\n  [1] David Kurokawa, Ariel D Procaccia, and Junxing Wang. When can the maximin share guarantee be guaranteed? In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n  [2] Uriel Feige, Ariel Sapir, and Laliv Tauber. A tight negative example for mms fair allocations. In International Conference on Web and Internet Economics, pages 355-372. Springer, 2021.",
        "comments": "11 pages",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07490"
    },
    {
        "doc_id": 273,
        "title": "Unemployment Volatility: When Workers Pay Costs upon Accepting Jobs",
        "authors": [
            "Rich Ryan"
        ],
        "subjects": [
            "Theoretical Economics"
        ],
        "abstract": "When a firm hires a worker, adding the new hire to payroll is costly. These costs reduce the amount of resources that can go to recruiting workers and amplify how unemployment responds to changes in productivity. Workers also incur up-front costs upon accepting jobs. Examples include moving expenses and regulatory fees. I establish that workers' costs lessen the response of unemployment to productivity changes and do not subtract from resources available for recruitment. The influence of workers' costs is bounded by properties of a matching function, which describes how job openings and unemployment produce hires. Using data on job finding that are adjusted for workers' transitions between employment and unemployment and for how the Job Openings and Labor Turnover Survey records hires, I estimate a bound that ascribes limited influence to workers' costs. The results demonstrate that costs paid by workers upon accepting jobs affect outcomes in the labor market (firms threaten workers with paying the up-front costs again if wage negotiations fail), but their influence on volatility is less important than firms' costs.",
        "comments": "31 pages, 3 figures",
        "date": "14 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07423"
    },
    {
        "doc_id": 274,
        "title": "A Comparative Examination of Network and Contract-Based Blockchain Storage Solutions for Decentralized Applications",
        "authors": [
            "Lipeng He"
        ],
        "subjects": [
            "Networking and Internet Architecture"
        ],
        "abstract": "Decentralized applications (DApps), which are innovative blockchain-powered software systems designed to serve as the fundamental building blocks for the next generation of Internet services, have witnessed exponential growth in recent years. This paper thoroughly compares and analyzes two blockchain-based decentralized storage networks (DSNs), which are crucial foundations for DApp and blockchain ecosystems. The study examines their respective mechanisms for data persistence, strategies for enforcing data retention, and token economics. In addition to delving into technical details, the suitability of each storage solution for decentralized application development is assessed, taking into consideration network performance, storage costs, and existing use cases. By evaluating these factors, the paper aims to provide insights into the effectiveness of these technologies in supporting the desirable properties of truly decentralized blockchain applications. In conclusion, the findings of this research are discussed and synthesized, offering valuable perspectives on the capabilities of these technologies. It sheds light on their potential to facilitate the development of DApps and provides an understanding of the ongoing trends in blockchain development.",
        "comments": "13 pages, 1 figure, published in Proceedings of the 3rd International Conference on Digital Economy and Computer Application (DECA 2023)",
        "date": "14 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07417"
    },
    {
        "doc_id": 275,
        "title": "Learning to be Homo Economicus: Can an LLM Learn Preferences from Choice",
        "authors": [
            "Jeongbin Kim",
            "Matthew Kovach",
            "Kyu-Min Lee",
            "Euncheol Shin",
            "Hector Tzavellas"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "This paper explores the use of Large Language Models (LLMs) as decision aids, with a focus on their ability to learn preferences and provide personalized recommendations. To establish a baseline, we replicate standard economic experiments on choice under risk (Choi et al., 2007) with GPT, one of the most prominent LLMs, prompted to respond as (i) a human decision maker or (ii) a recommendation system for customers. With these baselines established, GPT is provided with a sample set of choices and prompted to make recommendations based on the provided data. From the data generated by GPT, we identify its (revealed) preferences and explore its ability to learn from data. Our analysis yields three results. First, GPT's choices are consistent with (expected) utility maximization theory. Second, GPT can align its recommendations with people's risk aversion, by recommending less risky portfolios to more risk-averse decision makers, highlighting GPT's potential as a personalized decision aid. Third, however, GPT demonstrates limited alignment when it comes to disappointment aversion.",
        "comments": " ",
        "date": "14 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07345"
    },
    {
        "doc_id": 276,
        "title": "Individual and Collective Welfare in Risk Sharing with Many States",
        "authors": [
            "Federico Echenique",
            "Farzad Pourbabaee"
        ],
        "subjects": [
            "Theoretical Economics",
            "Computer Science and Game Theory"
        ],
        "abstract": "We provide a quantitative assessment of welfare in the classical model of risk-sharing and exchange under uncertainty. We prove three kinds of results. First, that in an equilibrium allocation, the scope for improving individual welfare by a given margin (an $\\ve$-improvement) vanishes as the number of states increases. Second, that the scope for a change in aggregate resources that may be distributed to enhance individual welfare by a given margin also vanishes. Equivalently: in an inefficient allocation, for a given level of resource sub-optimality (as measured by the coefficient of resource under-utilization), the possibilities for enhancing welfare by perturbing aggregate resources decrease exponentially to zero with the number of states. Finally, we consider efficient risk-sharing in standard models of uncertainty aversion with multiple priors, and show that, in an inefficient allocation, certain sets of priors shrink with the size of the state space.",
        "comments": "MSC Class:          91B50                          ACM Class:          J.4",
        "date": "14 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07337"
    },
    {
        "doc_id": 277,
        "title": "Utilitarian Beliefs in Social Networks: Explaining the Emergence of Hatred",
        "authors": [
            "Houda Nait El Barj",
            "Theophile Sautory"
        ],
        "subjects": [
            "Theoretical Economics"
        ],
        "abstract": "We study the dynamics of opinions in a setting where a leader has a payoff that depends on agents' beliefs and where agents derive psychological utility from their beliefs. Agents sample a signal that maximises their utility and then communicate with each other through a network formed by disjoint social groups. The leader has a choice to target a finite set of social groups with a specific signal to influence their beliefs and maximise his returns. Heterogeneity in agents' preferences allows us to analyse the evolution of opinions as a dynamical system with asymmetric forces. We apply our model to explain the emergence of hatred and the spread of racism in a society. We show that when information is restricted, the equilibrium level of hatred is determined solely by the belief of the most extremist agent in the group regardless of the inherent structure of the network. On the contrary, when information is dense, the space is completely polarised in equilibrium with the presence of multiple \"local truths\" which oscillate in periodic cycles. We find that when preferences are uniformly distributed, the equilibrium level of hatred depends solely on the value of the practical punishment associated with holding a hate belief. Our finding suggests that an optimal policy to reduce hatred should focus on increasing the cost associated with holding a racist belief.",
        "comments": " ",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07178"
    },
    {
        "doc_id": 278,
        "title": "A Note on Uncertainty Quantification for Maximum Likelihood Parameters Estimated with Heuristic Based Optimization Algorithms",
        "authors": [
            "Zachary Porreca"
        ],
        "subjects": [
            "Econometrics"
        ],
        "abstract": "Gradient-based solvers risk convergence to local optima, leading to incorrect researcher inference. Heuristic-based algorithms are able to ``break free\" of these local optima to eventually converge to the true global optimum. However, given that they do not provide the gradient/Hessian needed to approximate the covariance matrix and that the significantly longer computational time they require for convergence likely precludes resampling procedures for inference, researchers often are unable to quantify uncertainty in the estimates they derive with these methods. This note presents a simple and relatively fast two-step procedure to estimate the covariance matrix for parameters estimated with these algorithms. This procedure relies on automatic differentiation, a computational means of calculating derivatives that is popular in machine learning applications. A brief empirical example demonstrates the advantages of this procedure relative to bootstrapping and shows the similarity in standard error estimates between this procedure and that which would normally accompany maximum likelihood estimation with a gradient-based algorithm.",
        "comments": " ",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07176"
    },
    {
        "doc_id": 279,
        "title": "Inference for Synthetic Controls via Refined Placebo Tests",
        "authors": [
            "Lihua Lei",
            "Timothy Sudijono"
        ],
        "subjects": [
            "Methodology",
            "Econometrics",
            "Statistics Theory"
        ],
        "abstract": "The synthetic control method is often applied to problems with one treated unit and a small number of control units. A common inferential task in this setting is to test null hypotheses regarding the average treatment effect on the treated. Inference procedures that are justified asymptotically are often unsatisfactory due to (1) small sample sizes that render large-sample approximation fragile and (2) simplification of the estimation procedure that is implemented in practice. An alternative is permutation inference, which is related to a common diagnostic called the placebo test. It has provable Type-I error guarantees in finite samples without simplification of the method, when the treatment is uniformly assigned. Despite this robustness, the placebo test suffers from low resolution since the null distribution is constructed from only $N$ reference estimates, where $N$ is the sample size. This creates a barrier for statistical inference at a common level like $\u03b1= 0.05$, especially when $N$ is small. We propose a novel leave-two-out procedure that bypasses this issue, while still maintaining the same finite-sample Type-I error guarantee under uniform assignment for a wide range of $N$. Unlike the placebo test whose Type-I error always equals the theoretical upper bound, our procedure often achieves a lower unconditional Type-I error than theory suggests; this enables useful inference in the challenging regime when $\u03b1< 1/N$. Empirically, our procedure achieves a higher power when the effect size is reasonably large and a comparable power otherwise. We generalize our procedure to non-uniform assignments and show how to conduct sensitivity analysis. From a methodological perspective, our procedure can be viewed as a new type of randomization inference different from permutation or rank-based inference, which is particularly effective in small samples.",
        "comments": "36 pages. Comments welcome",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07152"
    },
    {
        "doc_id": 280,
        "title": "Causal machine learning in public policy evaluation -- an application to the conditioning of cash transfers in Morocco",
        "authors": [
            "Patrick Rehill",
            "Nicholas Biddle"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Causal machine learning methods can be used to search for treatment effect heterogeneity in high-dimensional datasets even where we lack a strong enough theoretical framework to select variables or make parametric assumptions about data. This paper uses causal machine learning methods to estimate heterogeneous treatment effects in the case of an experimental study carried out in Morocco which evaluated the effect of conditionalizing a cash transfer program on school attendance compared to a labelled cash transfer. We show that there is little heterogeneity in effects with the average treatment effect across three different conditioning policies all being negative. We then explore if there are any variables in the dataset of 1936 pre-treatment variables that are particularly strong predictors of heterogeneity to try to understand this effect. While there are some variables we expected to be important here based on our theoretical framework, most are atheoretical variables whose effects are difficult to interpret. Household spending variables and child time-use variables are particularly important, however no variables have particularly large effects. The second purpose of this paper is to demonstrate and reflect upon a causal machine learning approach to policy evaluation. In this vein we suggest that findings that are difficult to interpret in this way are not surprising given the atheoretical methodology. We reflect that causal machine learning methods should not replace existing evaluation methodologies, but rather could be a useful tool for working with high-dimensional data and generating hypotheses.",
        "comments": "20 pages, 10 figures",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07075"
    },
    {
        "doc_id": 281,
        "title": "A Dynamic Agent Based Model of the Real Economy with Monopolistic Competition, Perfect Product Differentiation, Heterogeneous Agents, Increasing Returns to Scale and Trade in Disequilibrium",
        "authors": [
            "Subhamon Supantha",
            "Naresh Kumar Sharma"
        ],
        "subjects": [
            "Theoretical Economics",
            "Multiagent Systems"
        ],
        "abstract": "We have used agent-based modeling as our numerical method to artificially simulate a dynamic real economy where agents are rational maximizers of an objective function of Cobb-Douglas type. The economy is characterised by heterogeneous agents, acting out of local or imperfect information, monopolistic competition, perfect product differentiation, allowance for increasing returns to scale technology and trade in disequilibrium. An algorithm for economic activity in each period is devised and a general purpose open source agent-based model is developed which allows for counterfactual inquiries, testing out treatments, analysing causality of various economic processes, outcomes and studying emergent properties. 10,000 simulations, with 10 firms and 80 consumers are run with varying parameters and the results show that from only a few initial conditions the economy reaches equilibrium while in most of the other cases it remains in perpetual disequilibrium. It also shows that from a few initial conditions the economy reaches a disaster where all the consumer wealth falls to zero or only a single producer remains. Furthermore, from some initial conditions, an ideal economy with high wage rate, high consumer utility and no unemployment is also reached. It was also observed that starting from an equal endowment of wealth in consumers and in producers, inequality emerged in the economy. In majority of the cases most of the firms(6-7) shut down because they were not profitable enough and only a few firms remained. Our results highlight that all these varying outcomes are possible for a decentralized market economy with rational optimizing agents.",
        "comments": " ",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07070"
    },
    {
        "doc_id": 282,
        "title": "A simple stochastic nonlinear AR model with application to bubble",
        "authors": [
            "Xuanling Yang",
            "Dong Li",
            "Ting Zhang"
        ],
        "subjects": [
            "Statistics Theory",
            "Econometrics"
        ],
        "abstract": "Economic and financial time series can feature locally explosive behavior when a bubble is formed. The economic or financial bubble, especially its dynamics, is an intriguing topic that has been attracting longstanding attention. To illustrate the dynamics of the local explosion itself, the paper presents a novel, simple, yet useful time series model, called the stochastic nonlinear autoregressive model, which is always strictly stationary and geometrically ergodic and can create long swings or persistence observed in many macroeconomic variables. When a nonlinear autoregressive coefficient is outside of a certain range, the model has periodically explosive behaviors and can then be used to portray the bubble dynamics. Further, the quasi-maximum likelihood estimation (QMLE) of our model is considered, and its strong consistency and asymptotic normality are established under minimal assumptions on innovation. A new model diagnostic checking statistic is developed for model fitting adequacy. In addition two methods for bubble tagging are proposed, one from the residual perspective and the other from the null-state perspective. Monte Carlo simulation studies are conducted to assess the performances of the QMLE and the two bubble tagging methods in finite samples. Finally, the usefulness of the model is illustrated by an empirical application to the monthly Hang Seng Index.",
        "comments": "41 pages, 6 figures",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07038"
    },
    {
        "doc_id": 283,
        "title": "Causative Insights into Open Source Software Security using Large Language Code Embeddings and Semantic Vulnerability Graph",
        "authors": [
            "Nafis Tanveer Islam",
            "Gonzalo De La Torre Parra",
            "Dylan Manual",
            "Murtuza Jadliwala",
            "Peyman Najafirad"
        ],
        "subjects": [
            "Software Engineering"
        ],
        "abstract": "Open Source Software (OSS) security and resilience are worldwide phenomena hampering economic and technological innovation. OSS vulnerabilities can cause unauthorized access, data breaches, network disruptions, and privacy violations, rendering any benefits worthless. While recent deep-learning techniques have shown great promise in identifying and localizing vulnerabilities in source code, it is unclear how effective these research techniques are from a usability perspective due to a lack of proper methodological analysis. Usually, these methods offload a developer's task of classifying and localizing vulnerable code; still, a reasonable study to measure the actual effectiveness of these systems to the end user has yet to be conducted. To address the challenge of proper developer training from the prior methods, we propose a system to link vulnerabilities to their root cause, thereby intuitively educating the developers to code more securely. Furthermore, we provide a comprehensive usability study to test the effectiveness of our system in fixing vulnerabilities and its capability to assist developers in writing more secure code. We demonstrate the effectiveness of our system by showing its efficacy in helping developers fix source code with vulnerabilities. Our study shows a 24% improvement in code repair capabilities compared to previous methods. We also show that, when trained by our system, on average, approximately 9% of the developers naturally tend to write more secure code with fewer vulnerabilities.",
        "comments": " ",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07035"
    },
    {
        "doc_id": 284,
        "title": "FedDriveScore: Federated Scoring Driving Behavior with a Mixture of Metric Distributions",
        "authors": [
            "Lin Lu"
        ],
        "subjects": [
            "Machine Learning",
            "Distributed, Parallel, and Cluster Computing"
        ],
        "abstract": "Scoring the driving performance of various drivers on a unified scale, based on how safe or economical they drive on their daily trips, is essential for the driver profile task. Connected vehicles provide the opportunity to collect real-world driving data, which is advantageous for constructing scoring models. However, the lack of pre-labeled scores impede the use of supervised regression models and the data privacy issues hinder the way of traditionally data-centralized learning on the cloud side for model training. To address them, an unsupervised scoring method is presented without the need for labels while still preserving fairness and objectiveness compared to subjective scoring strategies. Subsequently, a federated learning framework based on vehicle-cloud collaboration is proposed as a privacy-friendly alternative to centralized learning. This framework includes a consistently federated version of the scoring method to reduce the performance degradation of the global scoring model caused by the statistical heterogeneous challenge of local data. Theoretical and experimental analysis demonstrate that our federated scoring model is consistent with the utility of the centrally learned counterpart and is effective in evaluating driving performance.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06953"
    },
    {
        "doc_id": 285,
        "title": "An empirical model of fleet modernization: on the relationship between market concentration and innovation adoption in the Brazilian airline industry",
        "authors": [
            "Alessandro V. M. Oliveira",
            "Thiago Caliari",
            "Rodolfo R. Narcizo"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "The modernization of an airline's fleet can reduce its operating costs, improve the perceived quality of service offered to passengers, and mitigate emissions. The present paper investigates the market incentives that airlines have to adopt technological innovation from manufacturers by acquiring new generation aircraft. We develop an econometric model of fleet modernization in the Brazilian commercial aviation over two decades. We examine the hypothesis of an inverted-U relationship between market concentration and fleet modernization and find evidence that both the extremes of competition and concentration may inhibit innovation adoption by carriers. We find limited evidence associating either hubbing activity or low-cost carriers with the more intense introduction of new types of aircraft models and variants in the industry. Finally, our results suggest that energy cost rises may provoke boosts in fleet modernization in the long term, with carriers possibly targeting more eco-efficient operations up to two years after an upsurge in fuel price.",
        "comments": "Journal ref:        Research in Transportation Business & Management, 43, 100704 (2022)",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06876"
    },
    {
        "doc_id": 286,
        "title": "Deep Learning With DAGs",
        "authors": [
            "Sourabh Balgi",
            "Adel Daoud",
            "Jose M. Pe\u00f1a",
            "Geoffrey T. Wodtke",
            "Jesse Zhou"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Econometrics",
            "Methodology"
        ],
        "abstract": "Social science theories often postulate causal relationships among a set of variables or events. Although directed acyclic graphs (DAGs) are increasingly used to represent these theories, their full potential has not yet been realized in practice. As non-parametric causal models, DAGs require no assumptions about the functional form of the hypothesized relationships. Nevertheless, to simplify the task of empirical evaluation, researchers tend to invoke such assumptions anyway, even though they are typically arbitrary and do not reflect any theoretical content or prior knowledge. Moreover, functional form assumptions can engender bias, whenever they fail to accurately capture the complexity of the causal system under investigation. In this article, we introduce causal-graphical normalizing flows (cGNFs), a novel approach to causal inference that leverages deep neural networks to empirically evaluate theories represented as DAGs. Unlike conventional approaches, cGNFs model the full joint distribution of the data according to a DAG supplied by the analyst, without relying on stringent assumptions about functional form. In this way, the method allows for flexible, semi-parametric estimation of any causal estimand that can be identified from the DAG, including total effects, conditional effects, direct and indirect effects, and path-specific effects. We illustrate the method with a reanalysis of Blau and Duncan's (1967) model of status attainment and Zhou's (2019) model of conditional versus controlled mobility. To facilitate adoption, we provide open-source software together with a series of online tutorials for implementing cGNFs. The article concludes with a discussion of current limitations and directions for future development.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06864"
    },
    {
        "doc_id": 287,
        "title": "Austria's KlimaTicket: Assessing the short-term impact of a cheap nationwide travel pass on demand",
        "authors": [
            "Hannes Wallimann"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Measures to reduce transport-related greenhouse gas emissions are of great importance to policy-makers. A recent example is the nationwide KlimaTicket in Austria, a country with a relatively high share of transport-related emissions. The cheap yearly season ticket introduced in October 2021 allows unlimited access to Austria's public transport network. Using the synthetic control and synthetic difference-in-differences methods, I assess the causal effect of this policy on public transport demand by constructing a data-driven counterfactual out of European railway companies to mimic the number of passengers of the Austrian Federal Railways without the KlimaTicket. The results indicate public transport demand grew slightly faster in Austria, i.e., 3.3 or 6.8 percentage points, depending on the method, than it would have in the absence of the KlimaTicket. However, the growth effect after the COVID-19 pandemic appears only statistically significant when applying the synthetic control method, and the positive effect on public transport demand growth disappears in 2022.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06835"
    },
    {
        "doc_id": 288,
        "title": "QCQP-Net: Reliably Learning Feasible Alternating Current Optimal Power Flow Solutions Under Constraints",
        "authors": [
            "Sihan Zeng",
            "Youngdae Kim",
            "Yuxuan Ren",
            "Kibaek Kim"
        ],
        "subjects": [
            "Optimization and Control",
            "Machine Learning"
        ],
        "abstract": "At the heart of power system operations, alternating current optimal power flow (ACOPF) studies the generation of electric power in the most economical way under network-wide load requirement, and can be formulated as a highly structured non-convex quadratically constrained quadratic program (QCQP). Optimization-based solutions to ACOPF (such as ADMM or interior-point method), as the classic approach, require large amount of computation and cannot meet the need to repeatedly solve the problem as load requirement frequently changes. On the other hand, learning-based methods that directly predict the ACOPF solution given the load input incur little computational cost but often generates infeasible solutions (i.e. violate the constraints of ACOPF). In this work, we combine the best of both worlds -- we propose an innovated framework for learning ACOPF, where the input load is mapped to the ACOPF solution through a neural network in a computationally efficient and reliable manner. Key to our innovation is a specific-purpose \"activation function\" defined implicitly by a QCQP and a novel loss, which enforce constraint satisfaction. We show through numerical simulations that our proposed method achieves superior feasibility rate and generation cost in situations where the existing learning-based approaches fail.",
        "comments": " ",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06820"
    },
    {
        "doc_id": 289,
        "title": "Robust Analysis of Short Panels",
        "authors": [
            "Andrew Chesher",
            "Adam M. Rosen",
            "Yuanqi Zhang"
        ],
        "subjects": [
            "Econometrics"
        ],
        "abstract": "Many structural econometric models include latent variables on whose probability distributions one may wish to place minimal restrictions. Leading examples in panel data models are individual-specific variables sometimes treated as \"fixed effects\" and, in dynamic models, initial conditions. This paper presents a generally applicable method for characterizing sharp identified sets when models place no restrictions on the probability distribution of certain latent variables and no restrictions on their covariation with other variables. In our analysis latent variables on which restrictions are undesirable are removed, leading to econometric analysis robust to misspecification of restrictions on their distributions which are commonplace in the applied panel data literature. Endogenous explanatory variables are easily accommodated. Examples of application to some static and dynamic binary, ordered and multiple discrete choice and censored panel data models are presented.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06611"
    },
    {
        "doc_id": 290,
        "title": "Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information",
        "authors": [
            "Chuanji Shi",
            "Yingying Zhang",
            "Jiaotuan Wang",
            "Qiqi Zhu"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ],
        "abstract": "Urban area-of-interest (AOI) refers to an integrated urban functional zone with defined boundaries. The rapid development of urban commerce has resulted in an increased demand for more precise requirements in defining AOIs. However, existing research primarily concentrates on broad AOI mining for urban planning or regional economic analysis, failing to cater to the precise requirements of mobile Internet online-to-offline businesses. These businesses necessitate accuracy down to a specific community, school, or hospital. In this paper, we propose an end-to-end multimodal deep learning algorithm for detecting AOI fence polygon using remote sensing images and multi-semantics reference information. We then evaluate its timeliness through a cascaded module that incorporates dynamic human mobility and logistics address information. Specifically, we begin by selecting a point-of-interest (POI) of specific category, and use it to recall corresponding remote sensing images, nearby POIs, road nodes, human mobility, and logistics addresses to build a multimodal detection model based on transformer encoder-decoder architecture, titled AOITR. In the model, in addition to the remote sensing images, multi-semantic information including core POI and road nodes is embedded and reorganized as the query content part for the transformer decoder to generate the AOI polygon. Meanwhile, relatively dynamic distribution features of human mobility, nearby POIs, and logistics addresses are used for AOI reliability evaluation through a cascaded feedforward network. The experimental results demonstrate that our algorithm significantly outperforms two existing methods.",
        "comments": "22 pages, 11 figures",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06550"
    },
    {
        "doc_id": 291,
        "title": "Analysis of the Impact of Central bank Digital Currency on the Demand for Transactional Currency",
        "authors": [
            "Ruimin Song",
            "Tiantian Zhao",
            "Chunhui Zhou"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "This paper takes the development of Central bank digital currencies as a perspective, introduces it into the Baumol-Tobin money demand theoretical framework, establishes the transactional money demand model under Central bank Digital Currency, and qualitatively analyzes the influence mechanism of Central bank digital currencies on transactional money demand; meanwhile, quarterly data from 2010-2022 are selected to test the relationship between Central bank digital currencies and transactional money demand through the ARDL model. The long-run equilibrium and short-run dynamics between the demand for Central bank digital currencies and transactional currency are examined by ARDL model. The empirical results show that the issuance and circulation of Central bank digital currencies will reduce the demand for transactional money. Based on the theoretical analysis and empirical test, this paper proposes that China should explore a more effective Currency policy in the context of Central bank digital currencies while promoting the development of Central bank digital currencies in a prudent manner in the future.",
        "comments": "Central bank digital currencies; transactional money demand; ARDL model. arXiv admin note: text overlap with arXiv:2310.07326",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06457"
    },
    {
        "doc_id": 292,
        "title": "Causally Aware Generative Adversarial Networks for Light Pollution Control",
        "authors": [
            "Yuyao Zhang",
            "Ke Guo",
            "Xiao Zhou"
        ],
        "subjects": [
            "Computers and Society"
        ],
        "abstract": "Artificial light plays an integral role in modern cities, significantly enhancing human productivity and the efficiency of civilization. However, excessive illumination can lead to light pollution, posing non-negligible threats to economic burdens, ecosystems, and human health. Despite its critical importance, the exploration of its causes remains relatively limited within the field of artificial intelligence, leaving an incomplete understanding of the factors contributing to light pollution and sustainable illumination planning distant. To address this gap, we introduce a novel framework named Causally Aware Generative Adversarial Networks (CAGAN). This innovative approach aims to uncover the fundamental drivers of light pollution within cities and offer intelligent solutions for optimal illumination resource allocation in the context of sustainable urban development. We commence by examining light pollution across 33,593 residential areas in seven global metropolises. Our findings reveal substantial influences on light pollution levels from various building types, notably grasslands, commercial centers and residential buildings as significant contributors. These discovered causal relationships are seamlessly integrated into the generative modeling framework, guiding the process of generating light pollution maps for diverse residential areas. Extensive experiments showcase CAGAN's potential to inform and guide the implementation of effective strategies to mitigate light pollution. Our code and data are publicly available at https://github.com/zhangyuuao/Light_Pollution_CAGAN.",
        "comments": "9pages, 9figures, accepted by AAAI2024, AI for Social Impact (Special Track)",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06453"
    },
    {
        "doc_id": 293,
        "title": "Secure Targeted Message Dissemination in IoT Using Blockchain Enabled Edge Computing",
        "authors": [
            "Muhammad Baqer Mollah",
            "Md Abul Kalam Azad",
            "Yinghui Zhang"
        ],
        "subjects": [
            "Cryptography and Security",
            "Networking and Internet Architecture"
        ],
        "abstract": "Smart devices are considered as an integral part of Internet of Things (IoT), have an aim to make a dynamic network to exchange information, collect data, analysis, and make optimal decisions in an autonomous way to achieve more efficient, automatic, and economical services. Message dissemination among these smart devices allows adding new features, sending updated instructions, alerts or safety messages, informing the pricing information or billing amount, incentives, and installing security patches. On one hand, such message disseminations are directly beneficial to the all parties involved in the IoT system. On the other hand, due to remote procedure, smart devices, vendors, and other involved authorities might have to meet a number of security, privacy, and performance related concerns while disseminating messages among targeted devices. To this end, in this paper, we design STarEdgeChain, a security and privacy aware targeted message dissemination in IoT to show how blockchain along with advanced cryptographic techniques are devoted to address such concerns. In fact, the STarEdgeChain employs a permissioned blockchain assisted edge computing in order to expedite a single signcrypted message dissemination among targeted groups of devices, at the same time avoiding the dependency of utilizing multiple unicasting approaches. Finally, we develop a software prototype of STarEdgeChain and show it's practicability for smart devices. The codes are publicly available at https://github.com/mbaqer/Blockchain-IoT",
        "comments": "12 pages",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06384"
    },
    {
        "doc_id": 294,
        "title": "Exposure effects are not automatically useful for policymaking",
        "authors": [
            "Eric Auerbach",
            "Jonathan Auerbach",
            "Max Tabord-Meehan"
        ],
        "subjects": [
            "Econometrics",
            "Methodology"
        ],
        "abstract": "We thank Savje (2023) for a thought-provoking article and appreciate the opportunity to share our perspective as social scientists. In his article, Savje recommends misspecified exposure effects as a way to avoid strong assumptions about interference when analyzing the results of an experiment. In this invited discussion, we highlight a limiation of Savje's recommendation: exposure effects are not generally useful for evaluating social policies without the strong assumptions that Savje seeks to avoid.",
        "comments": "Invited Discussion Paper",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06264"
    },
    {
        "doc_id": 295,
        "title": "Temporary exclusion in repeated contests",
        "authors": [
            "Yaron Azrieli"
        ],
        "subjects": [
            "Theoretical Economics"
        ],
        "abstract": "Consider a large population of agents who repeatedly compete for awards, as in the case of researchers who can annually apply for grants from a science foundation. A key objective for the principal is to efficiently allocate resources to the highest quality applications, but the review process is often inherently noisy. Imperfect selection of winners may encourage low quality applications, which in turn forces the designer to commit more resources to the reviewing process and can further increase the misallocation. We study \\emph{temporary exclusion} as a potential solution to these problems. With exclusion, an agent is ineligible to apply in the current period if they were rejected (or if they applied) in the previous period. Such policy introduces intertemporal incentives to the participation decision and encourages self-selection. We characterize the steady-state equilibria of this dynamic game and compare the outcomes to the benchmark case without exclusion. In particular, we show that whenever the benefit from winning is large, exclusion leads to fewer low quality applications and higher welfare for agents.",
        "comments": " ",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06257"
    },
    {
        "doc_id": 296,
        "title": "The Shapley Value in Database Management",
        "authors": [
            "Leopoldo Bertossi",
            "Benny Kimelfeld",
            "Ester Livshits",
            "Mika\u00ebl Monet"
        ],
        "subjects": [
            "Databases"
        ],
        "abstract": "Attribution scores can be applied in data management to quantify the contribution of individual items to conclusions from the data, as part of the explanation of what led to these conclusions. In Artificial Intelligence, Machine Learning, and Data Management, some of the common scores are deployments of the Shapley value, a formula for profit sharing in cooperative game theory. Since its invention in the 1950s, the Shapley value has been used for contribution measurement in many fields, from economics to law, with its latest researched applications in modern machine learning. Recent studies investigated the application of the Shapley value to database management. This article gives an overview of recent results on the computational complexity of the Shapley value for measuring the contribution of tuples to query answers and to the extent of inconsistency with respect to integrity constraints. More specifically, the article highlights lower and upper bounds on the complexity of calculating the Shapley value, either exactly or approximately, as well as solutions for realizing the calculation in practice.",
        "comments": "12 pages, including references. This is the authors version of the corresponding SIGMOD Record article",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06234"
    },
    {
        "doc_id": 297,
        "title": "CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine Learning Methods",
        "authors": [
            "Yue Chen",
            "Xingyi Andrew",
            "Salintip Supasanya"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "Historically, the economic recession often came abruptly and disastrously. For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from October 2007 to March 2009. If we could detect the signals of the crisis earlier, we could have taken preventive measures. Therefore, driven by such motivation, we use advanced machine learning techniques, including Random Forest and Extreme Gradient Boosting, to predict any potential market crashes mainly in the US market. Also, we would like to compare the performance of these methods and examine which model is better for forecasting US stock market crashes. We apply our models on the daily financial market data, which tend to be more responsive with higher reporting frequencies. We consider 75 explanatory variables, including general US stock market indexes, SP 500 sector indexes, as well as market indicators that can be used for the purpose of crisis prediction. Finally, we conclude, with selected classification metrics, that the Extreme Gradient Boosting method performs the best in predicting US stock market crisis events.",
        "comments": "14 pages, 9 figures",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06172"
    },
    {
        "doc_id": 298,
        "title": "Grassroots Innovation Actors: Their Role and Positioning in Economic Ecosystems -- A Comparative Study Through Complex Network Analysis",
        "authors": [
            "Marcelo S. Tedesco",
            "Francisco Javier Ramos Soria"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "This study offers an examination of grassroots innovation actors and their integration within larger economic ecosystems. Through a comparative analysis in Oaxaca, Mexico; La Plata, Argentina; and Araucania, Chile, this research sheds light on the vital role that grassroots innovation plays in broader economic ecosystems. Using Complex Network Analysis and the TE-SER model, the study unveils how these actors interact, collaborate, and influence major economic ecosystems in the context of complex social challenges. The findings highlight that actors from the grassroots innovation ecosystem make up a significant portion of the larger innovation-driven entrepreneurial economic ecosystem, accounting for between 20% and 30% in all three cases and are strategically positioned within the ecosystem's structural network. Additionally, this study emphasizes the potential for greater integration of grassroots innovation actors to leverage resources and foster socio-economic development. The research concludes by advocating for further studies in similar socio-economic contexts to enhance our understanding of integration dynamics and mutual benefits between grassroots innovation ecosystems and other larger economic systems.",
        "comments": "21 pages",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06163"
    },
    {
        "doc_id": 299,
        "title": "A Statistical Field Perspective on Capital Allocation and Accumulation: Individual dynamics",
        "authors": [
            "Pierre Gosselin",
            "A\u00efleen Lotz"
        ],
        "subjects": [
            "General Finance",
            "High Energy Physics - Theory"
        ],
        "abstract": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups. The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends to their investors. Financial capital is thus a function of dividends and stock valuations, whereas physical capital is a function of the total capital allocated by the financial sector. Whereas our previous work focused on the background fields that describe potential long-term equilibria, here we compute the transition functions of individual agents and study their probabilistic dynamics in the background field, as a function of their initial state. We show that capital accumulation depends on various factors. The probability associated with each firm's trajectories is the result of several contradictory effects: the firm tends to shift towards sectors with the greatest long-term return, but must take into account the impact of its shift on its attractiveness for investors throughout its trajectory. Since this trajectory depends largely on the average capital of transition sectors, a firm's attractiveness during its relocation depends on the relative level of capital in those sectors. Thus, an under-capitalized firm reaching a high-capital sector will experience a loss of attractiveness, and subsequently, in investors. Moreover, the firm must also consider the effects of competition in the intermediate sectors. An under-capitalized firm will tend to be ousted out towards sectors with lower average capital, while an over-capitalized firm will tend to shift towards higher averagecapital sectors. For investors, capital allocation depends on their short and long-term returns. These returns are not independent: in the short-term, returns are composed of both the firm's dividends and the increase in its stock prices. In the long-term, returns are based on the firm's growth expectations, but also, indirectly, on expectations of higher stock prices. Investors' capital allocation directly depends on the volatility of stock prices and {\\ldots}rms'dividends. Investors will tend to reallocate their capital to maximize their short and long-term returns. The higher their level of capital, the stronger the reallocation will be.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2312.16173, arXiv:2205.03087",
        "date": "30 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06142"
    },
    {
        "doc_id": 300,
        "title": "Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning",
        "authors": [
            "Philip Amortila",
            "Tongyi Cao",
            "Akshay Krishnamurthy"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Optimization and Control"
        ],
        "abstract": "A pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ. As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. In this paper, we study the effect of distribution shift in the presence of model misspecification, specifically focusing on $L_{\\infty}$-misspecified regression and adversarial covariate shift, where the regression target remains fixed while the covariate distribution changes arbitrarily. We show that empirical risk minimization, or standard least squares regression, can result in undesirable misspecification amplification where the error due to misspecification is amplified by the density ratio between the training and testing distributions. As our main result, we develop a new algorithm -- inspired by robust optimization techniques -- that avoids this undesirable behavior, resulting in no misspecification amplification while still obtaining optimal statistical rates. As applications, we use this regression procedure to obtain new guarantees in offline and online reinforcement learning with misspecification and establish new separations between previously studied structural conditions and notions of coverage.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12216"
    },
    {
        "doc_id": 301,
        "title": "Empirical martingale projections via the adapted Wasserstein distance",
        "authors": [
            "Jose Blanchet",
            "Johannes Wiesel",
            "Erica Zhang",
            "Zhenyuan Zhang"
        ],
        "subjects": [
            "Probability"
        ],
        "abstract": "Given a collection of multidimensional pairs $\\{(X_i,Y_i):1 \\leq i\\leq n\\}$, we study the problem of projecting the associated suitably smoothed empirical measure onto the space of martingale couplings (i.e. distributions satisfying $\\mathbb{E}[Y|X]=X$) using the adapted Wasserstein distance. We call the resulting distance the smoothed empirical martingale projection distance (SE-MPD), for which we obtain an explicit characterization. We also show that the space of martingale couplings remains invariant under the smoothing operation. We study the asymptotic limit of the SE-MPD, which converges at a parametric rate as the sample size increases if the pairs are either i.i.d. or satisfy appropriate mixing assumptions. Additional finite-sample results are also investigated. Using these results, we introduce a novel consistent martingale coupling hypothesis test, which we apply to test the existence of arbitrage opportunities in recently introduced neural network-based generative models for asset pricing calibration.",
        "comments": "55 pages, 7 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12197"
    },
    {
        "doc_id": 302,
        "title": "Poincar\u00e9 inequality and quantitative De Giorgi method for hypoelliptic operators",
        "authors": [
            "Francesca Anceschi",
            "Helge Dietert",
            "Jessica Guerand",
            "Am\u00e9lie Loher",
            "Cl\u00e9ment Mouhot",
            "Annalaura Rebucci"
        ],
        "subjects": [
            "Analysis of PDEs"
        ],
        "abstract": "We propose a systematic elementary approach based on trajectories to prove Poincar\u00e9 inequalities for hypoelliptic equations with an arbitrary number of H\u00f6rmander commutators, both in the local and in the non-local case. Our method generalises and simplifies the method introduced in Guerand-Mouhot (2022) in the local case with one commutator, and extended in Loher (2024) to the non-local case with one commutator. It draws inspiration from the paper Niebel-Zacher (2022), although we use different trajectories. We deduce Harnack inequalities and H\u00f6lder regularity along the line of the De Giorgi method. Our results recover those in Anceschi-Rebucci (2022) in the local case, and are new in the non-local case.",
        "comments": "21 pages, 4 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12194"
    },
    {
        "doc_id": 303,
        "title": "Concentration inequalities for the sample correlation coefficient",
        "authors": [
            "Daniel Salnikov"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "The sample correlation coefficient $R$ plays an important role in many statistical analyses. We study the moments of $R$ under the bivariate Gaussian model assumption, provide a novel approximation for its finite sample mean and connect it with known results for the variance. We exploit these approximations to present non-asymptotic concentration inequalities for $R$. Finally, we illustrate our results in a simulation experiment that further validates the approximations presented in this work.",
        "comments": "10 pages, preprint",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12190"
    },
    {
        "doc_id": 304,
        "title": "Tracking before detection using partial orders and optimization",
        "authors": [
            "Michael Robinson",
            "Michael Stein",
            "Henry S. Owen"
        ],
        "subjects": [
            "Dynamical Systems",
            "Computational Engineering, Finance, and Science"
        ],
        "abstract": "This article addresses the problem of multi-object tracking by using a non-deterministic model of target behaviors with hard constraints. To capture the evolution of target features as well as their locations, we permit objects to lie in a general topological target configuration space, rather than a Euclidean space. We obtain tracker performance bounds based on sample rates, and derive a flexible, agnostic tracking algorithm. We demonstrate our algorithm on two scenarios involving laboratory and field data.",
        "comments": "MSC Class:          37N99",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12182"
    },
    {
        "doc_id": 305,
        "title": "Robust stability analysis of an energy-efficient control in a Networked Control System with application to Unmanned Ground Vehicles",
        "authors": [
            "Antonio Gonzalez",
            "Angel Cuenca",
            "Julian Salt",
            "Jelle Jacobs"
        ],
        "subjects": [
            "Systems and Control",
            "Optimization and Control"
        ],
        "abstract": "In this paper, the robust stability and disturbance rejection performance analysis of an energy-efficient control is addressed in the framework of Networked Control System (NCS). The control scheme under study integrates periodic event-triggered control, packet-based control, time-varying Kalman filter, dual-rate control and prediction techniques, whose design is aimed at reducing energy consumption and bandwidth usage. The robust stability against time-varying model uncertainties is analyzed by means of a suficient condition based on Linear Matrix Inequalities (LMI). Finally, the effectiveness of the proposed approach is experimentally validated in a tracking control for an Unmanned Ground Vehicle (UGV), which is a battery-constrained mobile device with limited computation capacities.",
        "comments": "38 pages, 12 figures, Information Sciences, 2021",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12172"
    },
    {
        "doc_id": 306,
        "title": "Paralinearization of free boundary problems in fluid dynamics",
        "authors": [
            "Thomas Alazard"
        ],
        "subjects": [
            "Analysis of PDEs"
        ],
        "abstract": "A classical topic in the mathematical theory of hydrodynamics is to study the evolution of the free surface separating air from an incompressible perfect fluid. The goal of this survey is to examine this problem for two important sets of equations: the water wave equations and the Hele-Shaw equations, including the Muskat problem. These equations are different in nature, dispersive or parabolic, but we will see that they can be studied using related tools. In particular, we will discuss a paradifferential approach to these problems.",
        "comments": "survey article",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12155"
    },
    {
        "doc_id": 307,
        "title": "Extension property for partial automorphisms of the $n$-partite and semigeneric tournaments",
        "authors": [
            "Jan Hubi\u010dka",
            "Colin Jahel",
            "Mat\u011bj Kone\u010dn\u00fd",
            "Marcin Sabok"
        ],
        "subjects": [
            "Combinatorics",
            "Discrete Mathematics",
            "Logic"
        ],
        "abstract": "We present a proof of the extension property for partial automorphisms (EPPA) for classes of finite $n$-partite tournaments for $n \\in \\{2,3,\\ldots,\u03c9\\}$, and for the class of finite semigeneric tournaments. We also prove that the generic $\u03c9$-partite tournament and the generic semigeneric tournament have ample generics.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12153"
    },
    {
        "doc_id": 308,
        "title": "Maximum principles for weakly $1$-coercive operators with applications to capillary and prescribed mean curvature graphs",
        "authors": [
            "Luis J. Al\u00edas",
            "Giulio Colombo",
            "Marco Rigoli"
        ],
        "subjects": [
            "Analysis of PDEs",
            "Differential Geometry"
        ],
        "abstract": "In this paper we establish maximum principles for weakly 1-coercive operators $L$ on complete, non-compact Riemannian manifolds $M$. In particular, we search for conditions under which one can guarantee that solutions $u$ of differential equations of the form $L(u)\\geq f(u)$ satisfy $f(u)\\leq 0$ on $M$. The case of weakly $p$-coercive operators with $p>1$, including the $p$-Laplacian and in particular the Laplace-Beltrami operator for $p=2$, has been considered in a recent paper of ours. As a consequence of the main results we infer comparison principles for that kind of operators. Furthermore we apply them to geometric situations dealing with the mean curvature operator, which is a typical weakly 1-coercive operator. We first consider the case of $\\mathcal C^1$ operators $L$ acting on functions $u$ of class $\\mathcal C^2$ and, in the last section of the paper, we show how our results can be extended to the case of less regular operators $L$ acting on functions $u$ which are just continuous and locally $W^{1,1}$ regular.",
        "comments": "28 pages. Invited contribution to a special issue in honour of Professor Marcos Dajczer on the occasion of his 75th birthday. Comments are welcome!",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12152"
    },
    {
        "doc_id": 309,
        "title": "Uncoded Storage Coded Transmission Elastic Computing with Straggler Tolerance in Heterogeneous Systems",
        "authors": [
            "Xi Zhong",
            "Joerg Kliewer",
            "Mingyue Ji"
        ],
        "subjects": [
            "Information Theory",
            "Distributed, Parallel, and Cluster Computing",
            "Optimization and Control"
        ],
        "abstract": "In 2018, Yang et al. introduced a novel and effective approach, using maximum distance separable (MDS) codes, to mitigate the impact of elasticity in cloud computing systems. This approach is referred to as coded elastic computing. Some limitations of this approach include that it assumes all virtual machines have the same computing speeds and storage capacities, and it cannot tolerate stragglers for matrix-matrix multiplications. In order to resolve these limitations, in this paper, we introduce a new combinatorial optimization framework, named uncoded storage coded transmission elastic computing (USCTEC), for heterogeneous speeds and storage constraints, aiming to minimize the expected computation time for matrix-matrix multiplications, under the consideration of straggler tolerance. Within this framework, we propose optimal solutions with straggler tolerance under relaxed storage constraints. Moreover, we propose a heuristic algorithm that considers the heterogeneous storage constraints. Our results demonstrate that the proposed algorithm outperforms baseline solutions utilizing cyclic storage placements, in terms of both expected computation time and storage size.",
        "comments": "6 pages, 1 figure, accepted in ICC 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12151"
    },
    {
        "doc_id": 310,
        "title": "An Efficient Finite Difference-based Implicit Solver for Phase-Field Equations with Spatially and Temporally Varying Parameters",
        "authors": [
            "Zirui Mao",
            "G. R. Liu",
            "Michael J. Demkowicz"
        ],
        "subjects": [
            "Numerical Analysis",
            "Mathematical Physics"
        ],
        "abstract": "The phase field method is an effective tool for modeling microstructure evolution in materials. Many efficient implicit numerical solvers have been proposed for phase field simulations under uniform and time-invariant model parameters. We use Eyre's theorem to develop an unconditionally stable implicit solver for spatially non-uniform and time-varying model parameters. The accuracy, unconditional stability, and efficiency of the solver is validated against benchmarking examples. In its current form, the solver requires a uniform mesh and may only be applied to problems with periodic, Neumann, or mixed periodic and Neumann boundary conditions.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12147"
    },
    {
        "doc_id": 311,
        "title": "A short note on similarity of operator-valued multishifts",
        "authors": [
            "Soumitra Ghara",
            "Surjit Kumar",
            "Shailesh Trivedi"
        ],
        "subjects": [
            "Functional Analysis"
        ],
        "abstract": "A complete characterization of the similarity between two operator-valued multishifts with invertible operator weights is obtained purely in terms of operator weights. This generalizes several existing results of the unitary equivalence of two (multi)shifts. Further, we utilize the aforementioned similarity criteria to determine the similarity between two tuples of operators of multiplication by the coordinate functions on certain reproducing kernel Hilbert spaces determined by diagonal kernels.",
        "comments": "10 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12144"
    },
    {
        "doc_id": 312,
        "title": "Chebyshev Varieties",
        "authors": [
            "Za\u00efneb Bel-Afia",
            "Chiara Meroni",
            "Simon Telen"
        ],
        "subjects": [
            "Algebraic Geometry",
            "Numerical Analysis"
        ],
        "abstract": "Chebyshev varieties are algebraic varieties parametrized by Chebyshev polynomials or their multivariate generalizations. We determine the dimension, degree, singular locus and defining equations of these varieties. We explain how they play the role of toric varieties in sparse polynomial root finding, when monomials are replaced by Chebyshev polynomials. We present numerical root finding algorithms that exploit our results.",
        "comments": "27 pages, 11 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12140"
    },
    {
        "doc_id": 313,
        "title": "Gradient Preserving Operator Inference: Data-Driven Reduced-Order Models for Equations with Gradient Structure",
        "authors": [
            "Yuwei Geng",
            "Jasdeep Singh",
            "Lili Ju",
            "Boris Kramer",
            "Zhu Wang"
        ],
        "subjects": [
            "Numerical Analysis"
        ],
        "abstract": "Hamiltonian Operator Inference has been introduced in [Sharma, H., Wang, Z., Kramer, B., Physica D: Nonlinear Phenomena, 431, p.133122, 2022] to learn structure-preserving reduced-order models (ROMs) for Hamiltonian systems. This approach constructs a low-dimensional model using only data and knowledge of the Hamiltonian function. Such ROMs can keep the intrinsic structure of the system, allowing them to capture the physics described by the governing equations. In this work, we extend this approach to more general systems that are either conservative or dissipative in energy, and which possess a gradient structure. We derive the optimization problems for inferring structure-preserving ROMs that preserve the gradient structure. We further derive an {\\em a priori} error estimate for the reduced-order approximation. To test the algorithms, we consider semi-discretized partial differential equations with gradient structure, such as the parameterized wave and Korteweg-de-Vries equations in the conservative case and the one- and two-dimensional Allen-Cahn equations in the dissipative case. The numerical results illustrate the accuracy, structure-preservation properties, and predictive capabilities of the gradient-preserving Operator Inference ROMs.",
        "comments": "30 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12138"
    },
    {
        "doc_id": 314,
        "title": "Generalized Minkowski formulas and rigidity results for anisotropic capillary hypersurfaces",
        "authors": [
            "Jinyu Gao",
            "Guanghan Li"
        ],
        "subjects": [
            "Differential Geometry"
        ],
        "abstract": "We show the generalization of Hsiung-Minkowski integral formula for anisotropic capillary hypersurfaces in the half-space, which includes the weighted Hsiung-Minkowski formula and classical anisotropic Minkowski identity for closed hypersurfaces as special cases. As applications, we prove some anisotropic Alexandrov-type theorems and rigidity results for anisotropic capillary hypersurfaces. Specially, the uniqueness of the solution to the anisotropic Orlicz-Christoffel-Minkowski problem is obtained.",
        "comments": "16 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12137"
    },
    {
        "doc_id": 315,
        "title": "Accelerating Continuous Variable Coherent Ising Machines via Momentum",
        "authors": [
            "Robin Brown",
            "Davide Venturelli",
            "Marco Pavone",
            "David E. Bernal Neira"
        ],
        "subjects": [
            "Optimization and Control",
            "Emerging Technologies",
            "Quantum Physics"
        ],
        "abstract": "The Coherent Ising Machine (CIM) is a non-conventional architecture that takes inspiration from physical annealing processes to solve Ising problems heuristically. Its dynamics are naturally continuous and described by a set of ordinary differential equations that have been proven to be useful for the optimization of continuous variables non-convex quadratic optimization problems. The dynamics of such Continuous Variable CIMs (CV-CIM) encourage optimization via optical pulses whose amplitudes are determined by the negative gradient of the objective; however, standard gradient descent is known to be trapped by local minima and hampered by poor problem conditioning. In this work, we propose to modify the CV-CIM dynamics using more sophisticated pulse injections based on tried-and-true optimization techniques such as momentum and Adam. Through numerical experiments, we show that the momentum and Adam updates can significantly speed up the CV-CIM's convergence and improve sample diversity over the original CV-CIM dynamics. We also find that the Adam-CV-CIM's performance is more stable as a function of feedback strength, especially on poorly conditioned instances, resulting in an algorithm that is more robust, reliable, and easily tunable. More broadly, we identify the CIM dynamical framework as a fertile opportunity for exploring the intersection of classical optimization and modern analog computing.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12135"
    },
    {
        "doc_id": 316,
        "title": "Growth of products of subsets in finite simple groups",
        "authors": [
            "Daniele Dona",
            "Attila Mar\u00f3ti",
            "L\u00e1szl\u00f3 Pyber"
        ],
        "subjects": [
            "Group Theory"
        ],
        "abstract": "We prove that the product of a subset and a normal subset inside any finite simple non-abelian group $G$ grows rapidly. More precisely, if $A$ and $B$ are two subsets with $B$ normal and neither of them is too large inside $G$, then $|AB| \\geq |A||B|^{1-\u03b5}$ where $\u03b5>0$ can be taken arbitrarily small. This is a somewhat surprising strengthening of a theorem of Liebeck, Schul, Shalev.",
        "comments": "7 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12128"
    },
    {
        "doc_id": 317,
        "title": "A game theoretic model of preventive demographic measures",
        "authors": [
            "O. A. Malafeyev",
            "M. A. Suvorov"
        ],
        "subjects": [
            "Optimization and Control"
        ],
        "abstract": "The article constructs a game-theoretic model of preventive measures for the antagonistic game with nature, that is such a model in which two participants take part, one of which is a regulating body (government) that makes legislative decisions depending on the emerging external and internal conditions affecting the course of the demographic process, the second participant - internal and external conditions of the environment (nature). An example of such a game is given and the optimal strategy for the government is calculated.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12124"
    },
    {
        "doc_id": 318,
        "title": "Centralization in Block Building and Proposer-Builder Separation",
        "authors": [
            "Maryam Bahrani",
            "Pranav Garimidi",
            "Tim Roughgarden"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Cryptography and Security",
            "Distributed, Parallel, and Cluster Computing",
            "Theoretical Economics"
        ],
        "abstract": "The goal of this paper is to rigorously interrogate conventional wisdom about centralization in block-building (due to, e.g., MEV and private order flow) and the outsourcing of block-building by validators to specialists (i.e., proposer-builder separation):\n  1. Does heterogeneity in skills and knowledge across block producers inevitably lead to centralization?\n  2. Does proposer-builder separation eliminate heterogeneity and preserve decentralization among proposers?\n  This paper develops mathematical models and results that offer answers to these questions:\n  1. In a game-theoretic model with endogenous staking, heterogeneous block producer rewards, and staking costs, we quantify the extent to which heterogeneous rewards lead to concentration in the equilibrium staking distribution.\n  2. In a stochastic model in which heterogeneous block producers repeatedly reinvest rewards into staking, we quantify, as a function of the block producer heterogeneity, the rate at which stake concentrates on the most sophisticated block producers.\n  3. In a model with heterogeneous proposers and specialized builders, we quantify, as a function of the competitiveness of the builder ecosystem, the extent to which proposer-builder separation reduces the heterogeneity in rewards across different proposers.\n  Our models and results take advantage of connections to contest design, P\u00f3lya urn processes, and auction theory.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12120"
    },
    {
        "doc_id": 319,
        "title": "Envelopes of Horospheres and Weingarten Surfaces in Hyperbolic 3-Space",
        "authors": [
            "Charles L. Epstein"
        ],
        "subjects": [
            "Differential Geometry",
            "Analysis of PDEs",
            "Complex Variables"
        ],
        "abstract": "We derive basic differential geometric formulae for surfaces in hyperbolic space represented as envelopes of horospheres. The dual notion of parallel hypersurfaces is also studied. The representation is applied to prove existence and regularity theorems for Weingarten surfaces in H^3, which satisfy (1-a)K = a(2-H), for an a < 0, and have a specified boundary curve at infinity. These surfaces are shown to be closely connected to conformal mappings of domains in S^2 into the unit disk and provide Riemannian interpretations for some conformal invariants associated to such mappings.\n  This paper was originally written in 1984, before I learned to use TeX, and was typed by one of the secretaries in the Princeton Math Department. It was more or less, my first original work after my dissertation. For some reason, I was not able to get this paper published in a timely manner. The results and perspective in this paper have proved to be useful to a variety of people, some of whom asked me to render the article into TeX and post it to the arXiv. I had been seriously thinking about doing this, when Martin Bridgemen sent me a transcription of my original article into TeX. I am extremely grateful to him for the effort he has put into this project.\n  The paper is now formatted in a more or less modern AMS-article style, but for lots of additional punctuation, a few corrections and some minor stylistic changes, the content has been largely reproduced as it originally was. Remarks about the 'state-of-the-art' in hyperbolic geometry are obviously way out of date, as there has been enormous progress in many aspects of this still rich subject.",
        "comments": "I am enormously grateful to Martin and the community of mathematicians who have let me know, over the years, that this work was of some use to them",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12115"
    },
    {
        "doc_id": 320,
        "title": "A quantitative version of the Steinhaus theorem",
        "authors": [
            "Alex Iosevich",
            "Jonathan Pakianathan"
        ],
        "subjects": [
            "Classical Analysis and ODEs"
        ],
        "abstract": "The classical Steinhaus theorem (\\cite{Steinhaus1920}) says that if $A \\subset {\\Bbb R}^d$ has positive Lebesgue measure than $A-A=\\{x-y: x,y \\in A\\}$ contains an open ball. We obtain some quantitative lower bounds on the size of this ball and in some cases, relate it to natural geometric properties of $\\partial A$. We also study the process $K_n =\\frac{1}{2}(K_{n-1} - K_{n-1})$ when $K_0$ is a compact subset of $\\mathbb{R}^d$ and determine various aspects of its convergence to $Conv(K_1)$, the convex hull of $K_1$. We discuss some connections with convex geometry, Weyl tube formula and the Kakeya needle problem.\n  \\noindent\n  {\\it Keywords: Measure theory, Steinhaus theorem, Convex geometry, Weyl tube formula.}\n  \\noindent\n  2020 {\\it Mathematics Subject Classification:} Primary: 28A75, 52A27. Secondary: 52A30, 53A07.",
        "comments": "MSC Class:          28A75; 52A27; 52A30; 53A07",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12112"
    },
    {
        "doc_id": 321,
        "title": "Differentiation of integral Mittag-Leffler and integral Wright functions with respect to parameters",
        "authors": [
            "Alexander Apelblat",
            "Juan Luis Gonz\u00e1lez-Santander"
        ],
        "subjects": [
            "Classical Analysis and ODEs"
        ],
        "abstract": "Derivatives with respect to the parameters of the integral Mittag-Leffler function and the integral Wright function, recently introduced by us, are calculated. These derivatives can be expressed in the form of infinite sums of quotients of the digamma and gamma functions. In some particular cases, these infinite sums are calculated in closed-form with the help of MATHEMATICA. However, parameter differentiation reduction formulas are explicitly derived in order to check some of the results given by MATHEMATICA, as well as to provide many other new results. In addition, we present these infinite sums graphically for particular values of the parameters. Finally, new results for parameter derivatives of the Mittag-Leffler and Wright functions are reported in the Appendices.",
        "comments": "MSC Class:          33E12; 33C10; 33C20",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12110"
    },
    {
        "doc_id": 322,
        "title": "Engineered complete intersections: slightly degenerate Bernstein--Kouchnirenko--Khovanskii",
        "authors": [
            "Alexander Esterov"
        ],
        "subjects": [
            "Algebraic Geometry"
        ],
        "abstract": "Geometry of sparse systems of polynomial equations (i.e. the ones with prescribed monomials and generic coefficients) is well studied in terms of their Newton polytopes. The results of this study are colloquially known as the Bernstein--Kouchnirenko--Khovanskii toolkit, and unfortunately are not applicable to many important systems, whose coefficients slightly fail to be generic.\n  This for instance happens if some of the equations are obtained from another one by taking partial derivatives or permuting the variables, or the equations are linear, realizing a non-trivial matroid, or in more advanced settings such as generalized Calabi--Yau complete intersections.\n  Such interesting examples (as well as many others) turn out to belong to a natural class of ``systems of equations that are nondegenerate upon cancellations''. We extend to this class several classical and folklore results of the Bernstein--Kouchnirenko--Khovanskii toolkit, such as the ones regarding the number and regularity of solutions, their irreducibility, tropicalization and Calabi--Yau-ness.",
        "comments": "29 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12099"
    },
    {
        "doc_id": 323,
        "title": "The inverse problem for a class of implicit differential equations and the coisotropic embedding theorem",
        "authors": [
            "Luca Schiavone"
        ],
        "subjects": [
            "Analysis of PDEs",
            "Mathematical Physics",
            "Differential Geometry"
        ],
        "abstract": "We carry on the approach used in [Sch] to provide a solution for the inverse problem of the calculus of variations for Maxwell equations in vacuum and we provide an abstract theory including all implicit differential equations that can be formulated in terms of vector fields over pre-symplectic manifolds.",
        "comments": "MSC Class:          53Dxx; 53Zxx; 49Sxx",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12096"
    },
    {
        "doc_id": 324,
        "title": "On the growth of resolvent of Toeplitz operators",
        "authors": [
            "Leonid Golinskii",
            "Stanislas Kupin",
            "Anna Vishnyakova"
        ],
        "subjects": [
            "Spectral Theory",
            "Complex Variables"
        ],
        "abstract": "We study the growth of the resolvent of a Toeplitz operator $T_b$, defined on the Hardy space, in terms of the distance to its spectrum $\u03c3(T_b)$. We are primarily interested in the case when the symbol $b$ is a Laurent polynomial (\\emph{i.e., } the matrix $T_b$ is banded). We show that for an arbitrary such symbol the growth of the resolvent is quadratic, and under certain additional assumption it is linear. We also prove the quadratic growth of the resolvent for a certain class of non-rational symbols.",
        "comments": "15 pages, 0 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12095"
    },
    {
        "doc_id": 325,
        "title": "Quantum Eigensolver for General Matrices",
        "authors": [
            "Xiao-Ming Zhang",
            "Yukun Zhang",
            "Wenhao He",
            "Xiao Yuan"
        ],
        "subjects": [
            "Quantum Physics",
            "Mesoscale and Nanoscale Physics",
            "Data Structures and Algorithms",
            "Numerical Analysis",
            "Computational Physics"
        ],
        "abstract": "The eigenvalue problem, a cornerstone in linear algebra, provides profound insights into studying matrix properties. Quantum algorithms addressing this problem have hitherto been constrained to special normal matrices assuming spectral decomposition, leaving the extension to general matrices an open challenge. In this work, we present a novel family of quantum algorithms tailored for solving the eigenvalue problem for general matrices, encompassing scenarios with complex eigenvalues or even defective matrices. Our approach begins by tackling the task of searching for an eigenvalue without additional constraints. For diagonalizable matrices, our algorithm has $\\tilde O(\\varepsilon^{-1})$ complexity with an error $\\varepsilon$, achieving the nearly Heisenberg scaling. Subsequently, we study the identification of eigenvalues closest to a specified point or line, extending the results for ground energy and energy gap problems in Hermitian matrices. We achieve an accuracy scaling of $\\tilde O(\\varepsilon^{-2})$ for general diagonalizable matrices, further refining to $\\tilde O(\\varepsilon^{-1})$ under the condition of real eigenvalues or constant distance from the reference point. The algorithm's foundation lies in the synergy of three techniques: the relationship between eigenvalues of matrix $A$ and the minimum singular value of $A-\u03bcI$, quantum singular value threshold subroutine extended from quantum singular-value estimation, and problem-specific searching algorithms. Our algorithms find applications in diverse domains, including estimating the relaxation time of Markov chains, solving Liouvillian gaps in open quantum systems, and verifying PT-symmetry broken/unbroken phases. These applications underscore the significance of our quantum eigensolvers for problems across various disciplines.",
        "comments": "6+10 pages, 1+1 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12091"
    },
    {
        "doc_id": 326,
        "title": "Sch\u00f6n complete intersections",
        "authors": [
            "Alexander Esterov"
        ],
        "subjects": [
            "Algebraic Geometry"
        ],
        "abstract": "A complete intersection $f_1=\\cdots=f_k=0$ is sch\u00f6n, if $f_1=\\cdots=f_j=0$ defines a sch\u00f6n subvariety of an algebraic torus for every $j\\leqslant k$. This class includes nondegenerate complete intersections, critical loci of their coordinate projections, other simplest Thom--Boardman and multiple point strata of such projections, generalized Calabi--Yau complete intersections, equaltions of polynomial optimization, hyperplane arrangement complements, and many other interesting special varieties.\n  We study their Euler characteristics, connectednes, Calabi--Yau-ness, tropicalizations, etc., extending (in part conjecturally) the respective classical results about nondegenerate complete intersections.",
        "comments": "22 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12090"
    },
    {
        "doc_id": 327,
        "title": "Evaluations of $ \\sum_{k=1}^\\infty \\frac{x^k}{k^2\\binom{3k}{k}}$ and related series",
        "authors": [
            "Zhi-Wei Sun",
            "Yajun Zhou"
        ],
        "subjects": [
            "Combinatorics",
            "Number Theory"
        ],
        "abstract": "We perform polylogarithmic reductions for several classes of infinite sums motivated by Z.-W. Sun's related works in 2022--2023. For certain choices of parameters, these series can be expressed by cyclotomic multiple zeta values of levels $4$, $5$, $6$, $7$, $8$, $9$, $10$, and $12$. In particular, we obtain closed forms of the series $$\\sum_{k=0}^\\infty\\frac{x_0^k}{(k+1)\\binom{3k}k} \\ \\ \\text{and}\\ \\ \\sum_{k=1}^\\infty\\frac{x_0^k}{k^2\\binom{3k}k}$$ for any $x_0\\in(-27/4,27/4)$.",
        "comments": "23 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12083"
    },
    {
        "doc_id": 328,
        "title": "On the stability of hybrid polycycles",
        "authors": [
            "Paulo Santana",
            "Leonardo Pereira Serantola"
        ],
        "subjects": [
            "Dynamical Systems"
        ],
        "abstract": "In this paper we provide the stability of generic polycycles of hybrid planar vector fields, extending previous known results in the literature.",
        "comments": "10 pages and 6 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12081"
    },
    {
        "doc_id": 329,
        "title": "Beyond TreeSHAP: Efficient Computation of Any-Order Shapley Interactions for Tree Ensembles",
        "authors": [
            "Maximilian Muschalik",
            "Fabian Fumagalli",
            "Barbara Hammer",
            "Eyke H\u00fcllermeier"
        ],
        "subjects": [
            "Machine Learning"
        ],
        "abstract": "While shallow decision trees may be interpretable, larger ensemble models like gradient-boosted trees, which often set the state of the art in machine learning problems involving tabular data, still remain black box models. As a remedy, the Shapley value (SV) is a well-known concept in explainable artificial intelligence (XAI) research for quantifying additive feature attributions of predictions. The model-specific TreeSHAP methodology solves the exponential complexity for retrieving exact SVs from tree-based models. Expanding beyond individual feature attribution, Shapley interactions reveal the impact of intricate feature interactions of any order. In this work, we present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley interactions for predictions of tree-based models. TreeSHAP-IQ is supported by a mathematical framework that exploits polynomial arithmetic to compute the interaction scores in a single recursive traversal of the tree, akin to Linear TreeSHAP. We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore interactions on well-established benchmark datasets.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12069"
    },
    {
        "doc_id": 330,
        "title": "Annular Links from Thompson's Group $T$",
        "authors": [
            "Louisa Liles"
        ],
        "subjects": [
            "Geometric Topology"
        ],
        "abstract": "In 2014 Jones showed how to associate links in the $3$-sphere to elements of Thompson's group $F$. We provide an analogue of this program for annular links and Thompson's group $T$. The main result is that any edge-signed graph embedded in the annulus is the Tait graph of an annular link built from an element of $T$. In analogy to the work of Aiello and Conti, we also show that the coefficients of certain unitary representations of $T$ recover the Jones polynomial of annular links.",
        "comments": "MSC Class:          57K14; 57K10",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12065"
    },
    {
        "doc_id": 331,
        "title": "Entropy numbers and box dimension of polynomials and holomorphic functions",
        "authors": [
            "Daniel Carando",
            "Carlos D'Andrea",
            "Leodan A. Torres",
            "Pablo Turco"
        ],
        "subjects": [
            "Functional Analysis"
        ],
        "abstract": "We study entropy numbers and box dimension of (the image of) homogeneous polynomials and holomorphic functions between Banach spaces. First, we see that entropy numbers and box dimensions of subsets of Banach spaces are related. We show that the box dimension of the image of a ball under a homogeneous polynomial is finite if and only if it spans a finite-dimensional subspace, but this is not true for holomorphic functions. Furthermore, we relate the entropy numbers of a holomorphic function to those of the polynomials of its Taylor series expansion. As a consequence, if the box dimension of the image of a ball by a holomorphic function $f$ is finite, then the entropy numbers of the polynomials in the Taylor series expansion of $f$ at any point of the ball belong to $\\ell_p$ for every $p>1$.",
        "comments": "17 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12059"
    },
    {
        "doc_id": 332,
        "title": "Asymptotic Analysis and Uniqueness of blowup solutions of non-quantized singular mean field equations",
        "authors": [
            "Daniele Bartoclucci",
            "Wen Yang",
            "Lei Zhang"
        ],
        "subjects": [
            "Analysis of PDEs"
        ],
        "abstract": "For singular mean field equations defined on a compact Riemann surface, we prove the uniqueness of bubbling solutions as far as blowup points are either regular points or non-quantized singular sources. In particular the uniqueness result covers the most general case improving all previous works of Bartolucci-Jevnikar-Lee-Yang \\cite{bart-4,bart-4-2} and Wu-Zhang \\cite{wu-zhang-ccm}. For example, unlike previous results, we drop the assumption of singular sources being critical points of a suitably defined Kirchoff-Routh type functional. Based on refined estimates which allow a major simplification of previous proofs, our new argument is robust and flexible enough to be applied to a wide range of problems requiring a delicate blowup analysis.",
        "comments": "76 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12057"
    },
    {
        "doc_id": 333,
        "title": "Navier-Stokes-Cahn-Hilliard equations on evolving surfaces",
        "authors": [
            "Charles M. Elliott",
            "Thomas Sales"
        ],
        "subjects": [
            "Analysis of PDEs"
        ],
        "abstract": "We derive a system of equations which can be seen as an evolving surface version of the diffuse interface \"Model H\" of Hohenberg and Halperin. We then consider the well-posedness for the corresponding (tangential) system when one prescribes the evolution of the surface. This well-posedness theory is considered for smooth potentials with polynomial growth, and a thermodynamically relevant singular potential - with some restrictions on initial data in the latter case.",
        "comments": "56 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12044"
    },
    {
        "doc_id": 334,
        "title": "Energy-Conserving Hermite Methods for Maxwell's Equations",
        "authors": [
            "Daniel Appelo",
            "Thomas Hagstrom",
            "Yann-Meing Law-Kam-Cio"
        ],
        "subjects": [
            "Numerical Analysis"
        ],
        "abstract": "Energy-conserving Hermite methods for solving Maxwell's equations in dielectric and dispersive media are described and analyzed. In three space dimensions methods of order $2m$ to $2m+2$ require $(m+1)^3$ degrees-of-freedom per node for each field variable and can be explicitly marched in time with steps independent of $m$. We prove stability for time steps limited only by domain-of-dependence requirements along with error estimates in a special seminorm associated with the interpolation process. Numerical experiments are presented which demonstrate that Hermite methods of very high order enable the efficient simulation of electromagnetic wave propagation over thousands of wavelengths.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12043"
    },
    {
        "doc_id": 335,
        "title": "A Skew-Symmetric Energy Stable Almost Dissipation Free Formulation of the Compressible Navier-Stokes Equations",
        "authors": [
            "Jan Nordstr\u00f6m"
        ],
        "subjects": [
            "Analysis of PDEs"
        ],
        "abstract": "We show that a specific skew-symmetric formulation of the nonlinear terms in the compressible Navier-Stokes equations leads to an energy rate in terms of surface integrals only. No dissipative volume integrals contribute to the energy rate. We also discuss boundary conditions that bounds the surface integrals.",
        "comments": "MSC Class:          35M33                          ACM Class:          G.1.8",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12038"
    },
    {
        "doc_id": 336,
        "title": "A Survey of Advances in Optimization Methods for Wireless Communication System Design",
        "authors": [
            "Ya-Feng Liu",
            "Tsung-Hui Chang",
            "Mingyi Hong",
            "Zheyu Wu",
            "Anthony Man-Cho So",
            "Eduard A. Jorswieck",
            "Wei Yu"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing",
            "Optimization and Control"
        ],
        "abstract": "Mathematical optimization is now widely regarded as an indispensable modeling and solution tool for the design of wireless communications systems. While optimization has played a significant role in the revolutionary progress in wireless communication and networking technologies from 1G to 5G and onto the future 6G, the innovations in wireless technologies have also substantially transformed the nature of the underlying mathematical optimization problems upon which the system designs are based and have sparked significant innovations in the development of methodologies to understand, to analyze, and to solve those problems. In this paper, we provide a comprehensive survey of recent advances in mathematical optimization theory and algorithms for wireless communication system design. We begin by illustrating common features of mathematical optimization problems arising in wireless communication system design. We discuss various scenarios and use cases and their associated mathematical structures from an optimization perspective. We then provide an overview of recent advances in mathematical optimization theory and algorithms, from nonconvex optimization, global optimization, and integer programming, to distributed optimization and learning-based optimization. The key to successful solution of mathematical optimization problems is in carefully choosing and/or developing suitable optimization algorithms (or neural network architectures) that can exploit the underlying problem structure. We conclude the paper by identifying several open research challenges and outlining future research directions.",
        "comments": "47 pages, 10 figures, submitted for possible publication",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12025"
    },
    {
        "doc_id": 337,
        "title": "A Simulation of Optimal Dryness When Moving in the Rain or Snow Using MATLAB",
        "authors": [
            "Neil Zhao",
            "Emilee Brockner",
            "Asia Winslow",
            "Megan Seraydarian"
        ],
        "subjects": [
            "Discrete Mathematics",
            "Mathematical Software"
        ],
        "abstract": "The classic question of whether one should walk or run in the rain to remain the least wet has inspired a myriad of solutions ranging from physically performing test runs in raining conditions to mathematically modeling human movement through rain. This manuscript approaches the classical problem by simulating movement through rainfall using MATLAB. Our simulation was generalizable to include snowfall as well. An increase in walking speed resulted in a corresponding decrease in raindrop and snowflake collisions. When raindrops or snowflakes were given a horizontal movement vector due to wind, a local minimum in collisions was achieved when moving in parallel with the same horizontal speed as the raindrop; no local minimum was detected with antiparallel movement. In general, our simulation revealed that the faster one moves, the drier one remains.",
        "comments": "15 pages, 9 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12023"
    },
    {
        "doc_id": 338,
        "title": "Capacity in Besov and Triebel-Lizorkin spaces with generalized smoothness",
        "authors": [
            "Nijjwal Karak",
            "Debarati Mondal"
        ],
        "subjects": [
            "Functional Analysis"
        ],
        "abstract": "We prove a lower bound estimate for capacities in Hajlasz-Besov, Hajlasz-Triebel-Lizorkin and Hajlasz-Sobolev spaces with generalized smoothness defined on metric spaces in terms of Netrusov-Hausdorff content or Hausdorff content.",
        "comments": "To appear in Georgian Mathematical Journal",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12020"
    },
    {
        "doc_id": 339,
        "title": "Annotated square root computation in Liber Abaci and De Practica Geometrie by Fibonacci",
        "authors": [
            "Trond Steihaug"
        ],
        "subjects": [
            "History and Overview"
        ],
        "abstract": "We study the square root computation by Leonardo Fibonacci (or Leonardo of Pisa) in his MSS Liber Abaci from c1202 and c1228 and De Practica Geometrie from c1220. We annotate a translation of Liber Abaci based on transcripts from 1857 and 2020 and a translation from 2002 and a transcription of De Practica Geometrie from 1862 and a translation from 2008. We show that Fibonacci is demonstrating the same method for all examples in the MSS and that this method deviates from the traditional description of the digit--by--digit method. The description of the method used by Fibonacci is all verbal and summarized in tables for each square root example. The manuscripts and transcription of the Latin texts are incomplete for some of the examples and the transcription and translation contains minor discrepancies and some of the tables are incomplete and the missing digits are inserted.",
        "comments": "35 pages and 16 included figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12016"
    },
    {
        "doc_id": 340,
        "title": "On a class of interdiction problems with partition matroids: complexity and polynomial-time algorithms",
        "authors": [
            "Sergey S. Ketkov",
            "Oleg A. Prokopyev"
        ],
        "subjects": [
            "Computational Complexity",
            "Optimization and Control"
        ],
        "abstract": "In this study, we consider a class of linear matroid interdiction problems, where the feasible sets for the upper-level decision-maker (referred to as the leader) and the lower-level decision-maker (referred to as the follower) are given by partition matroids with a common ground set. In contrast to classical network interdiction models where the leader is subject to a single budget constraint, in our setting, both the leader and the follower are subject to several independent cardinality constraints and engage in a zero-sum game. While a single-level linear integer programming problem over a partition matroid is known to be polynomially solvable, we prove that the considered bilevel problem is NP-hard, even when the objective function coefficients are all binary. On a positive note, it turns out that, if the number of cardinality constraints is fixed for either the leader or the follower, then the considered class of bilevel problems admits several polynomial-time solution schemes. Specifically, these schemes are based on a single-level dual reformulation, a dynamic programming-based approach, and a 2-flip local search algorithm for the leader.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12010"
    },
    {
        "doc_id": 341,
        "title": "Arithmetic degrees of dynamical systems over fields of characteristic zero",
        "authors": [
            "Wenbin Luo",
            "Jiarui Song"
        ],
        "subjects": [
            "Number Theory",
            "Algebraic Geometry"
        ],
        "abstract": "In this article, we generalize the arithmetic degree and its related theory to dynamical systems defined over an arbitrary field $\\mathbf{k}$ of characteristic $0$. We first consider a dynamical system $(X,f)$ over a finitely generated field $K$ over $\\mb{Q}$, we introduce the arithmetic degrees for $\\ov{K}$-points by using Moriwaki heights. We study the arithmetic dynamical degree of $(X,f)$ and establish the relative degree formula. The relative degree formula gives a proof of the fundamental inequality $\\ov\u03b1(f,x)\\leq \u03bb_1(f)$ in this setting. By taking a spread-out, we extend the definition of arithmetic degrees to dynamical systems over the field $\\mathbf k$ of characteristic $0$. We demonstrate that our definition is independent of the choice of the spread-out. Moreover, in this setting, we prove certain special cases of the Kawaguchi-Silverman conjecture. A main novelty of this paper is that, we give a characterization of arithmetic degrees of \"transcendental points\" in the case $\\mathbf k=\\C$, from which we deduce that $\u03b1(f,x)=\u03bb_1(f)$ for very general $x\\in X(\\mb{C})$ when $f$ is an endomorphism.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11982"
    },
    {
        "doc_id": 342,
        "title": "On the uniqueness of compiling graphs under the parity transformation",
        "authors": [
            "Florian Dreier",
            "Wolfgang Lechner"
        ],
        "subjects": [
            "Quantum Physics",
            "Mathematical Physics",
            "Combinatorics"
        ],
        "abstract": "In this article, we establish a mathematical framework that utilizes concepts from graph theory to define the parity transformation as a mapping that encompasses all possible compiled hypergraphs, and investigate uniqueness properties of this mapping in more detail. By introducing so-called loop labelings, we derive an alternative expression of the preimage of any set of compiled hypergraphs under this encoding procedure when all equivalences classes of graphs are being considered. We then deduce equivalent conditions for the injectivity of the parity transformation on any subset of all equivalences classes of graphs. Moreover, we show concrete examples of optimization problems demonstrating that the parity transformation is not an injective mapping, and also introduce an important class of plaquette layouts and their corresponding set of constraints whose preimage is uniquely determined. In addition, we provide an algorithm which is based on classical algorithms from theoretical computer science and computes a compiled physical layout in this class in polynomial time.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11980"
    },
    {
        "doc_id": 343,
        "title": "Dualizations of approximations, $\\aleph_1$-projectivity, and Vop\u011bnka's Principles",
        "authors": [
            "Asmae Ben Yassine",
            "Jan Trlifaj"
        ],
        "subjects": [
            "Representation Theory",
            "Category Theory",
            "Logic",
            "Rings and Algebras"
        ],
        "abstract": "The approximation classes of modules that arise as components of cotorsion pairs are tied up by Salce's duality. Here we consider general approximation classes of modules and investigate possibilities of dualization in dependence on closure properties of these classes. While some proofs are easily dualized, other dualizations require large cardinal principles, and some fail in ZFC, with counterexamples provided by classes of $\\aleph_1$-projective modules over non-perfect rings. For example, we show that Vop\u011bnka's Principle implies that each covering class of modules closed under homomorphic images is of the form Gen($M$) for a module $M$, and that the latter property restricted to classes generated by $\\aleph_1$-free abelian groups implies Weak Vop\u011bnka's Principle.",
        "comments": "MSC Class:          16D90 (Primary); 03E55; 16D40; 18G25; 20K20 (Secondary)",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11979"
    },
    {
        "doc_id": 344,
        "title": "A finiteness principle for distance functions on Riemannian surfaces with H\u00f6lder continuous curvature",
        "authors": [
            "Rotem Assouline"
        ],
        "subjects": [
            "Differential Geometry",
            "Metric Geometry"
        ],
        "abstract": "We study distance functions from geodesics to points on Riemannian surfaces with H\u00f6lder continuous Gauss curvature, and prove a finiteness principle in the spirit of Whitney extension theory for such functions. Our result can be viewed as a finiteness principle for isometric embedding of a certain type of metric spaces into Riemannian surfaces, with control over the H\u00f6lder seminorm of the Gauss curvature.",
        "comments": "38 pages, 2 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11962"
    },
    {
        "doc_id": 345,
        "title": "General duality and dual attainment for adapted transport",
        "authors": [
            "Daniel Kr\u0161ek",
            "Gudmund Pammer"
        ],
        "subjects": [
            "Probability",
            "Optimization and Control",
            "Mathematical Finance"
        ],
        "abstract": "We investigate duality and existence of dual optimizers for several adapted optimal transport problems under minimal assumptions. This includes the causal and bicausal transport, the barycenter problem, and a general multimarginal problem incorporating causality constraints. Moreover, we discuss applications of our results in robust finance. We consider a non-dominated model of several financial markets where stocks are traded dynamically, but the joint stock dynamics are unknown. We show that a no-arbitrage assumption in a quasi-sure sense naturally leads to sets of multicausal couplings. Consequently, computing the robust superhedging price is equivalent to solving an adapted transport problem, and finding a superhedging strategy means solving the corresponding dual.",
        "comments": "32 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11958"
    },
    {
        "doc_id": 346,
        "title": "Biquandle Power Brackets",
        "authors": [
            "Neslihan G\u00fcg\u00fcmc\u00fc",
            "Sam Nelson"
        ],
        "subjects": [
            "Geometric Topology",
            "Quantum Algebra"
        ],
        "abstract": "In this paper, we introduce biquandle power brackets, an infinite family of invariants of oriented links containing the classical skein invariants and the quandle and biquandle 2-cocycle invariants as special cases. Biquandle power brackets are generalizations of biquandle brackets in which the values of Kauffman states also depend on the biquandle colors they admit. We provide example computations and discuss the relationship between these new invariants and the previous cases.",
        "comments": "14 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11956"
    },
    {
        "doc_id": 347,
        "title": "Weighted holomorphic polynomial approximation",
        "authors": [
            "S. Charpentier",
            "N. Levenberg",
            "F. Wielonsky"
        ],
        "subjects": [
            "Complex Variables",
            "Classical Analysis and ODEs"
        ],
        "abstract": "For $G$ an open set in $\\mathbb{C}$ and $W$ a non-vanishing holomorphic function in $G$, in the late 1990's, Pritsker and Varga characterized pairs $(G,W)$ having the property that any $f$ holomorphic in $G$ can be locally uniformly approximated in $G$ by weighted holomorphic polynomials $\\{W(z)^np_n(z)\\}, \\ deg(p_n)\\leq n$. We further develop their theory in first proving a quantitative Bernstein-Walsh type theorem for certain pairs $(G,W)$. Then we consider the special case where $W(z)=1/(1+z)$ and $G$ is a loop of the lemniscate $\\{z\\in \\mathbb{C}: |z(z+1)|=1/4\\}$. We show the normalized measures associated to the zeros of the $n-th$ order Taylor polynomial about $0$ of the function $(1+z)^{-n}$ converge to the weighted equilibrium measure of $\\overline G$ with weight $|W|$ as $n\\to \\infty$. This mimics the motivational case of Pritsker and Varga where $G$ is the inside of the Szego curve and $W(z)=e^{-z}$. Lastly, we initiate a study of weighted holomorphic polynomial approximation in $\\mathbb{C}^n, \\ n>1$.",
        "comments": "21 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11955"
    },
    {
        "doc_id": 348,
        "title": "On the steadiness of symmetric solutions to two dimensional dispersive models",
        "authors": [
            "Long Pei",
            "Fengyang Xiao",
            "Pan Zhang"
        ],
        "subjects": [
            "Analysis of PDEs",
            "Mathematical Physics"
        ],
        "abstract": "In this paper, we consider the steadiness of symmetric solutions to two dispersive models in shallow water and hyperelastic mechanics, respectively. These models are derived previously in the two-dimensional setting and can be viewed as the generalization of the Camassa-Holm and Kadomtsev-Petviashvili equations. For these two models, we prove that symmetry of classical solutions implies steadiness in the horizontal direction. We also confirm the such connection between symmetry and steadiness in weak formulation, which includes in particular the peaked solutions.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11953"
    },
    {
        "doc_id": 349,
        "title": "A Stabilised Semi-Implicit Double-Point Material Point Method for Soil-Water Coupled Problems",
        "authors": [
            "Mian Xie",
            "Pedro Navas",
            "Susana Lopez-Querol"
        ],
        "subjects": [
            "Numerical Analysis"
        ],
        "abstract": "A semi-implicit two-phase double-point Material Point Method (MPM) formulation, based on the incremental fractional-step method to model large deformation geotechnical problems has been derived. The semi-implicit formulation has two advantages compared with the explicit approach: the time step is independent of the water phase, and the pore pressure field is more stable. The semi-implicit MPM models based on the incremental fractional-step method available in the literature consist of modelling the soil and water mixture using a single set of material points only, in order to save computational time. In this study, we further derive this formulation with two sets of material points to represent the soil and water phases separately. The stress oscillations that are frequently found in the water and soil phases are stabilised with this approach. A new stabilisation method is developed based on the modified F-bar method. The proposed method is validated with two numerical examples under small and large deformations, respectively. After that, Nor-Sand constitutive soil model is used to simulate landslides. Numerical examples show an excellent performance of the proposed coupled MPM and the stabilisation method. The formulation with two sets of material points yields significantly different but more reliable results in the landslides analysis, compared with the single-point approach. Additionally, this research shows that the additional computational cost caused by the additional water material points is acceptable. Therefore, it is recommended to use two sets of material points for certain large deformation geotechnical problems.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11951"
    },
    {
        "doc_id": 350,
        "title": "The Ensemble Kalman Filter for Dynamic Inverse Problems",
        "authors": [
            "Simon Weissmann",
            "Neil K. Chada",
            "Xin T. Tong"
        ],
        "subjects": [
            "Numerical Analysis",
            "Methodology"
        ],
        "abstract": "In inverse problems, the goal is to estimate unknown model parameters from noisy observational data. Traditionally, inverse problems are solved under the assumption of a fixed forward operator describing the observation model. In this article, we consider the extension of this approach to situations where we have a dynamic forward model, motivated by applications in scientific computation and engineering. We specifically consider this extension for a derivative-free optimizer, the ensemble Kalman inversion (EKI). We introduce and justify a new methodology called dynamic-EKI, which is a particle-based method with a changing forward operator. We analyze our new method, presenting results related to the control of our particle system through its covariance structure. This analysis includes moment bounds and an ensemble collapse, which are essential for demonstrating a convergence result. We establish convergence in expectation and validate our theoretical findings through experiments with dynamic-EKI applied to a 2D Darcy flow partial differential equation.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11948"
    },
    {
        "doc_id": 351,
        "title": "Friedrichs systems on an interval",
        "authors": [
            "Marko Erceg",
            "Sandeep Kumar Soni"
        ],
        "subjects": [
            "Analysis of PDEs"
        ],
        "abstract": "There has been significant developments in the classification of boundary conditions of positive symmetric systems, also known as Friedrichs systems, after the introduction of operator theoretic framework. We take a step forward towards applying the abstract theory to the classical framework by studying Friedrichs systems on an interval. Dealing with some difficulties related to the smoothness of eigenvectors, here we present an explicit expression for the dimensions of the kernels of Friedrichs operators only in terms of the values of the coefficients at the end-points of the interval. In particular, this allows for a characterisation of all admissible boundary conditions, i.e.~those leading to bijective realisations.",
        "comments": "MSC Class:          34B05; 35F45; 46C05; 47B28",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11941"
    },
    {
        "doc_id": 352,
        "title": "Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent",
        "authors": [
            "Zhiyu Liu",
            "Zhi Han",
            "Yandong Tang",
            "Xi-Le Zhao",
            "Yao Wang"
        ],
        "subjects": [
            "Machine Learning",
            "Optimization and Control",
            "Machine Learning"
        ],
        "abstract": "This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it is worth noting that our method does not require the precise estimation of the tensor tubal-rank. Even in cases where the tubal-rank is slightly overestimated, our approach continues to demonstrate robust performance. A series of experiments have been carried out to demonstrate that, as compared to other popular ones, our approach exhibits superior performance in multiple scenarios, in terms of the faster computational speed and the smaller convergence error.",
        "comments": "13 pages, 4 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11940"
    },
    {
        "doc_id": 353,
        "title": "A new proof of the Willmore inequality via a divergence inequality",
        "authors": [
            "Carla Cederbaum",
            "Anabel Miehe"
        ],
        "subjects": [
            "Analysis of PDEs",
            "Differential Geometry"
        ],
        "abstract": "We present a new proof of the Willmore inequality for an arbitrary bounded domain $\u03a9\\subset\\mathbb{R}^{n}$ with smooth boundary. Our proof is based on a parametric geometric inequality involving the electrostatic potential for the domain $\u03a9$; this geometric inequality is derived from a geometric differential inequality in divergence form. Our parametric geometric inequality also allows us to give new proofs of the quantitative Willmore-type and the weighted Minkowski inequalities by Agostiniani and Mazzieri.",
        "comments": "25 pages, 1 figure. Comments very welcome",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11939"
    },
    {
        "doc_id": 354,
        "title": "On the existential theory of the completions of a global field",
        "authors": [
            "Philip Dittmann",
            "Arno Fehm"
        ],
        "subjects": [
            "Logic",
            "Number Theory"
        ],
        "abstract": "We discuss the common existential theory of all or almost all completions of a global function field.",
        "comments": "MSC Class:          03C60; 12L05; 12L12; 12J20; 14G05; 11G25",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11930"
    },
    {
        "doc_id": 355,
        "title": "Large deviations of the empirical spectral measure of supercritical sparse Wigner matrices",
        "authors": [
            "Fanny Augeri"
        ],
        "subjects": [
            "Probability",
            "Mathematical Physics",
            "Combinatorics"
        ],
        "abstract": "Let $\u039e$ be the adjacency matrix of an Erd\u0151s-R\u00e9nyi graph on $n$ vertices and with parameter $p$ and consider $A$ a $n\\times n$ centered random symmetric matrix with bounded i.i.d. entries above the diagonal. When the mean degree $np$ diverges, the empirical spectral measure of the normalized Hadamard product $(A \\circ \u039e)/\\sqrt{np}$ converges weakly in probability to the semicircle law. In the regime where $p\\ll 1$ and $ np \\gg \\log n$, we prove a large deviations principle for the empirical spectral measure with speed $n^2p$ and with a good rate function solution of a certain variational problem. The rate function reveals in particular that the only possible deviations at the exponential scale $n^2p$ are around measures coming from Quadratic Vector Equations. As a byproduct, we obtain a large deviations principle for the empirical spectral measure of supercritical Erd\u0151s-R\u00e9nyi graphs.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11925"
    },
    {
        "doc_id": 356,
        "title": "Ultraknots and limit knots",
        "authors": [
            "Benjamin Bode"
        ],
        "subjects": [
            "Geometric Topology"
        ],
        "abstract": "We prove that every knot type in $\\mathbb{R}^3$ can be parametrised by a smooth function $f:S^1\\to\\mathbb{R}^3$, $f(t)=(x(t),y(t),z(t))$ such that all derivatives $f^{(n)}(t)=(x^{(n)}(t),y^{(n)}(t),z^{(n)}(t))$, $n\\in\\mathbb{N}$, parametrise knots and every knot type appears in the corresponding sequence of knots. We also study knot types that arise as limits of such sequences.",
        "comments": "17 pages, 1 figure",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11918"
    },
    {
        "doc_id": 357,
        "title": "Raviolo vertex algebras, cochains and conformal blocks",
        "authors": [
            "Luigi Alfonsi",
            "Hyungrok Kim",
            "Charles A. S. Young"
        ],
        "subjects": [
            "Quantum Algebra"
        ],
        "abstract": "Raviolo vertex algebras were introduced recently by Garner and Williams in arXiv:2308.04414. Working at the level of cochain complexes, in the present paper we construct spaces of conformal blocks, or more precisely their duals, coinvariants, in the raviolo setting. We prove that the raviolo state-field map correctly captures the limiting behaviour of coinvariants as marked points collide.",
        "comments": "47 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11917"
    },
    {
        "doc_id": 358,
        "title": "Local Diversity of Condorcet Domains",
        "authors": [
            "Alexander Karpov",
            "Klas Markstr\u00f6m",
            "S\u00f8ren Riis",
            "Bei Zhou"
        ],
        "subjects": [
            "Theoretical Economics",
            "Discrete Mathematics"
        ],
        "abstract": "Several of the classical results in social choice theory demonstrate that in order for many voting systems to be well-behaved the set domain of individual preferences must satisfy some kind of restriction, such as being single-peaked on a political axis. As a consequence it becomes interesting to measure how diverse the preferences in a well-behaved domain can be.\n  In this paper we introduce an egalitarian approach to measuring preference diversity, focusing on the abundance of distinct suborders one subsets of the alternative. We provide a common generalisation of the frequently used concepts of ampleness and copiousness.\n  We give a detailed investigation of the abundance for Condorcet domains. Our theorems imply a ceiling for the local diversity in domains on large sets of alternatives, which show that in this measure Black's single-peaked domain is in fact optimal. We also demonstrate that for some numbers of alternatives, there are Condorcet domains which have largest local diversity without having maximum order.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11912"
    },
    {
        "doc_id": 359,
        "title": "3D Space Trajectories and beyond: Abstract Art Creation with 3D Printing",
        "authors": [
            "Thierry Dana-Picard",
            "Matias Tejera",
            "Eva Ulbrich"
        ],
        "subjects": [
            "Computational Geometry",
            "Mathematical Software",
            "Symbolic Computation"
        ],
        "abstract": "We present simple models of trajectories in space, both in 2D and in 3D. The first examples, which model bicircular moves in the same direction, are classical curves (epicycloids, etc.). Then, we explore bicircular moves in reverse direction and tricircular moves in 2D and 3D, to explore complex visualisations of extraplanetary movements. These moves are studied in a plane setting. Then, adding increasing complexity, we explore them in a non planar setting (which is a closer model of the real situation). The exploration is followed by using these approaches for creating mathematical art in 2D and 3D printed objects, providing new ways of mathematical representations. Students' activities are organized around this exploration.",
        "comments": "In Proceedings ADG 2023, arXiv:2401.10725",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11909"
    },
    {
        "doc_id": 360,
        "title": "The Locus Story of a Rocking Camel in a Medical Center in the City of Freistadt",
        "authors": [
            "Anna K\u00e4ferb\u00f6ck",
            "Zolt\u00e1n Kov\u00e1cs"
        ],
        "subjects": [
            "Robotics",
            "Computational Geometry",
            "Mathematical Software",
            "Symbolic Computation"
        ],
        "abstract": "We give an example of automated geometry reasoning for an imaginary classroom project by using the free software package GeoGebra Discovery. The project is motivated by a publicly available toy, a rocking camel, installed at a medical center in Upper Austria. We explain how the process of a false conjecture, experimenting, modeling, a precise mathematical setup, and then a proof by automated reasoning could help extend mathematical knowledge at secondary school level and above.",
        "comments": "In Proceedings ADG 2023, arXiv:2401.10725",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11908"
    },
    {
        "doc_id": 361,
        "title": "Solving with GeoGebra Discovery an Austrian Mathematics Olympiad problem: Lessons Learned",
        "authors": [
            "Bel\u00e9n Ari\u00f1o-Morera",
            "Zolt\u00e1n Kov\u00e1cs",
            "Tom\u00e1s Recio",
            "Piedad Tolmos"
        ],
        "subjects": [
            "Symbolic Computation",
            "Artificial Intelligence",
            "Computational Geometry",
            "Mathematical Software"
        ],
        "abstract": "We address, through the automated reasoning tools in GeoGebra Discovery, a problem from a regional phase of the Austrian Mathematics Olympiad 2023. Trying to solve this problem gives rise to four different kind of feedback: the almost instantaneous, automated solution of the proposed problem; the measure of its complexity, according to some recent proposals; the automated discovery of a generalization of the given assertion, showing that the same statement is true over more general polygons than those mentioned in the problem; and the difficulties associated to the analysis of the surprising and involved high number of degenerate cases that appear when using the LocusEquation command in this problem. In our communication we will describe and reflect on these diverse issues, enhancing its exemplar role for showing some of the advantages, problems, and current fields of development of GeoGebra Discovery.",
        "comments": "In Proceedings ADG 2023, arXiv:2401.10725",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11906"
    },
    {
        "doc_id": 362,
        "title": "Automated Completion of Statements and Proofs in Synthetic Geometry: an Approach based on Constraint Solving",
        "authors": [
            "Salwa Tabet Gonzalez",
            "Predrag Jani\u010di\u0107",
            "Julien Narboux"
        ],
        "subjects": [
            "Artificial Intelligence",
            "Logic in Computer Science",
            "Mathematical Software"
        ],
        "abstract": "Conjecturing and theorem proving are activities at the center of mathematical practice and are difficult to separate. In this paper, we propose a framework for completing incomplete conjectures and incomplete proofs. The framework can turn a conjecture with missing assumptions and with an under-specified goal into a proper theorem. Also, the proposed framework can help in completing a proof sketch into a human-readable and machine-checkable proof. Our approach is focused on synthetic geometry, and uses coherent logic and constraint solving. The proposed approach is uniform for all three kinds of tasks, flexible and, to our knowledge, unique such approach.",
        "comments": "In Proceedings ADG 2023, arXiv:2401.10725",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11898"
    },
    {
        "doc_id": 363,
        "title": "Shape uncertainty quantification of Maxwell eigenvalues and -modes with application to TESLA cavities",
        "authors": [
            "J\u00fcrgen D\u00f6lz",
            "David Ebert",
            "Sebastian Sch\u00f6ps",
            "Anna Ziegler"
        ],
        "subjects": [
            "Numerical Analysis",
            "Computational Engineering, Finance, and Science"
        ],
        "abstract": "We consider Maxwell eigenvalue problems on uncertain shapes with perfectly conducting TESLA cavities being the driving example. Due to the shape uncertainty, the resulting eigenvalues and eigenmodes are also uncertain and it is well known that the eigenvalues may exhibit crossings or bifurcations under perturbation. We discuss how the shape uncertainties can be modelled using the domain mapping approach and how the deformation mapping can be expressed as coefficients in Maxwell's equations. Using derivatives of these coefficients and derivatives of the eigenpairs, we follow a perturbation approach to compute approximations of mean and covariance of the eigenpairs. For small perturbations, these approximations are faster and more accurate than Monte Carlo or similar sampling-based strategies. Numerical experiments for a three-dimensional 9-cell TESLA cavity are presented.",
        "comments": "MSC Class:          47A75; 65J10; 65C05; 65C20; 35Q61",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11890"
    },
    {
        "doc_id": 364,
        "title": "$q$-Rational functions and interpolation with complete Nevanlinna Pick Kernels",
        "authors": [
            "Daniel Alpay",
            "Paula Cerejeiras",
            "Uwe Kaehler",
            "Baruch Schneider"
        ],
        "subjects": [
            "Complex Variables"
        ],
        "abstract": "In this paper we introduce the concept of matrix-valued $q$-rational functions. In comparison to the classic case we give different characterizations with principal emphasise on realizations and discuss algebraic manipulations. We also study the concept of Schur multipliers and complete Nevanlinna Pick kernels in this context and provide first applications in terms of an interpolation problem using Schur multipliers and complete Nevanlinna Pick kernels.",
        "comments": "MSC Class:          47A56; 05A30; 30C10; 32A70",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11887"
    },
    {
        "doc_id": 365,
        "title": "Some Properties of Proper Power Graphs in Finite Abelian Groups",
        "authors": [
            "Dhawlath. G",
            "Raja. V"
        ],
        "subjects": [
            "Group Theory",
            "Combinatorics"
        ],
        "abstract": "The power graph of a group $G$, denoted as $P(G)$, constitutes a simple undirected graph characterized by its vertex set $G$. Specifically, vertices $a,b$ exhibit adjacency exclusively if $a$ belongs to the cyclic subgroup generated by $b$ or vice versa. The corresponding proper power graph of $G$ is obtained by taking $P(G)$ and removing a vertex corresponding to the identity element, which is denoted as $P^*(G)$. In the context of finite abelian groups, this article establishes the sufficient and necessary conditions for the proper power graph's connectedness. Moreover, a precise upper bound for the diameter of $P^*(G)$ in finite abelian groups is provided with sharpness. This article also explores the study of vertex connectivity, center, and planarity.",
        "comments": "12 pages and 2 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11873"
    },
    {
        "doc_id": 366,
        "title": "On the strong Feller property of the heat equation on quantum graphs with Kirchoff noise",
        "authors": [
            "Mohamed Fkirine",
            "Mih\u00e1ly Kov\u00e1cs",
            "Eszter Sikolya"
        ],
        "subjects": [
            "Dynamical Systems",
            "Probability"
        ],
        "abstract": "We consider a so-called quantum graph with standard continuity and Kirchhoff vertex conditions where the Kirchhoff vertex condition is perturbed by Gaussian noise. The strong Feller property of the stochastic parabolic problem is investigated by considering the associated Kirchhoff null-controllability problem. We give an abstract characterization of both problems and apply the abstract result to show that the quantum graph setting is very different from the classical one dimensional boundary noise setting, where the transition semigroup is known to be strong Feller, by giving examples and counterexamples to the strong Feller property. We also comment on the existence and uniqueness of the invariant measure and the regularity of the solution.",
        "comments": "MSC Class:          Primary: 81Q35; 60H15; 35R60; Secondary: 35R02; 47D06; 93E03",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11866"
    },
    {
        "doc_id": 367,
        "title": "Improving Small Language Models' Mathematical Reasoning via Mix Thoughts Distillation",
        "authors": [
            "Xunyu Zhu",
            "Jian Li",
            "Yong Liu",
            "Can Ma",
            "Weiping Wang"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence"
        ],
        "abstract": "This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Mix Thoughts Distillation (MTD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while MTD enables these models to achieve state-of-the-art reasoning performance.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11864"
    },
    {
        "doc_id": 368,
        "title": "Superdiffusive limits for Bessel-driven stochastic kinetics",
        "authors": [
            "Miha Bre\u0161ar",
            "Conrado da Costa",
            "Aleksandar Mijatovi\u0107",
            "Andrew Wade"
        ],
        "subjects": [
            "Probability"
        ],
        "abstract": "We prove anomalous-diffusion scaling for a one-dimensional stochastic kinetic dynamics, in which the stochastic drift is driven by an exogenous Bessel noise, and also includes endogenous volatility which is permitted to have arbitrary dependence with the exogenous noise. We identify the superdiffusive scaling exponent for the model, and prove a weak convergence result on the corresponding scale. We show how our result extends to admit, as exogenous noise processes, not only Bessel processes but more general processes satisfying certain asymptotic conditions.",
        "comments": "15 pages, 1 figure, for a short YouTube video describing the results, see https://youtu.be/O20plic5Ko8?si=-cg5XGdZlkO9WvYr",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11863"
    },
    {
        "doc_id": 369,
        "title": "Optimization in Sanger Sequencing",
        "authors": [
            "Luisa Carpente",
            "Ana Cerdeira-Pena",
            "Silvia Lorenzo-Freire",
            "\u00c1ngeles S. Places"
        ],
        "subjects": [
            "Data Structures and Algorithms",
            "Optimization and Control"
        ],
        "abstract": "The main objective of this paper is to solve the optimization problem that is associated with the classification of DNA samples in PCR plates for Sanger sequencing. To achieve this goal, we design an integer linear programming model. Given that the real instances involve the classification of thousands of samples and the linear model can only be solved for small instances, the paper includes a heuristic to cope with bigger problems. The heuristic algorithm is based on the simulated annealing technique. This algorithm obtains satisfactory solutions to the problem in a short amount of time. It has been tested with real data and yields improved results compared to some commercial software typically used in (clinical) laboratories. Moreover, the algorithm has already been implemented in the laboratory and is being successfully used.",
        "comments": "\u00a92019. This manuscript version is made available under the CC-BY-NC-ND 4.0 license (https://creativecommons.org/licenses/by-nc-nd/4.0/). This version of the article has been accepted for publication in Computers & Operations Research. The Version of Record is available online at https://doi.org/10.1016/j.cor.2019.05.011",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11854"
    },
    {
        "doc_id": 370,
        "title": "Self-Labeling the Job Shop Scheduling Problem",
        "authors": [
            "Andrea Corsini",
            "Angelo Porrello",
            "Simone Calderara",
            "Mauro Dell'Amico"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Combinatorics"
        ],
        "abstract": "In this work, we propose a Self-Supervised training strategy specifically designed for combinatorial problems. One of the main obstacles in applying supervised paradigms to such problems is the requirement of expensive target solutions as ground-truth, often produced with costly exact solvers. Inspired by Semi- and Self-Supervised learning, we show that it is possible to easily train generative models by sampling multiple solutions and using the best one according to the problem objective as a pseudo-label. In this way, we iteratively improve the model generation capability by relying only on its self-supervision, completely removing the need for optimality information. We prove the effectiveness of this Self-Labeling strategy on the Job Shop Scheduling (JSP), a complex combinatorial problem that is receiving much attention from the Reinforcement Learning community. We propose a generative model based on the well-known Pointer Network and train it with our strategy. Experiments on two popular benchmarks demonstrate the potential of this approach as the resulting models outperform constructive heuristics and current state-of-the-art Reinforcement Learning proposals.",
        "comments": "ACM Class:          I.2; G.2",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11849"
    },
    {
        "doc_id": 371,
        "title": "Sparse discovery of differential equations based on multi-fidelity Gaussian process",
        "authors": [
            "Yuhuang Meng",
            "Yue Qiu"
        ],
        "subjects": [
            "Numerical Analysis",
            "Machine Learning"
        ],
        "abstract": "Sparse identification of differential equations aims to compute the analytic expressions from the observed data explicitly. However, there exist two primary challenges. Firstly, it exhibits sensitivity to the noise in the observed data, particularly for the derivatives computations. Secondly, existing literature predominantly concentrates on single-fidelity (SF) data, which imposes limitations on its applicability due to the computational cost. In this paper, we present two novel approaches to address these problems from the view of uncertainty quantification. We construct a surrogate model employing the Gaussian process regression (GPR) to mitigate the effect of noise in the observed data, quantify its uncertainty, and ultimately recover the equations accurately. Subsequently, we exploit the multi-fidelity Gaussian processes (MFGP) to address scenarios involving multi-fidelity (MF), sparse, and noisy observed data. We demonstrate the robustness and effectiveness of our methodologies through several numerical experiments.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11825"
    },
    {
        "doc_id": 372,
        "title": "A note on evolution equations with modified Hartree Nonlinearity",
        "authors": [
            "Khaldi Said"
        ],
        "subjects": [
            "Analysis of PDEs"
        ],
        "abstract": "We introduce a mathematical model in $\\mathbb{R}^{n}$ for evolution equations with modified generalized Hartree nonlinearity given by $S_{\u03b1,p,q}(u)=I_\u03b1(|u|^{p+q}).$ One can see that this nonlinearity is not integrable due to the boundedness property of Riesz potential. In other words, we cannot deal with the Cauchy problem of semi-linear evolution equations with $S_{\u03b1,p,q}(u)$ and $L^{1}$-initial velocity.\n  We will show that $S_{\u03b1,p,q}(u)$ produces the same semi-critical exponent that guarantees the global existence of small data solutions as in the well known generalized Hartree nonlinearity $H_{\u03b1,p,q}(u)=|u|^{p}I_\u03b1(|u|^{q})$ provided that the initial velocity belongs to $L^{m}(\\mathbb{R}^{n})$, with $m>1$.\n  We can expect a relation between some physical systems that are modeled and solved using Hartree nonlinearity and those in their modified form due to this coincidence property in the semi-critical exponent.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11821"
    },
    {
        "doc_id": 373,
        "title": "SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese",
        "authors": [
            "Liang Xu",
            "Hang Xue",
            "Lei Zhu",
            "Kangkang Zhao"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence"
        ],
        "abstract": "We introduce SuperCLUE-Math6(SC-Math6), a new benchmark dataset to evaluate the mathematical reasoning abilities of Chinese language models. SC-Math6 is designed as an upgraded Chinese version of the GSM8K dataset with enhanced difficulty, diversity, and application scope. It consists of over 2000 mathematical word problems requiring multi-step reasoning and providing natural language solutions. We propose an innovative scheme to quantify the reasoning capability of large models based on performance over problems with different reasoning steps. Experiments on 12 representative Chinese models demonstrate a clear stratification of reasoning levels, with top models like GPT-4 showing superior performance. SC-Math6 fills the gap in Chinese mathematical reasoning benchmarks and provides a comprehensive testbed to advance the intelligence of Chinese language models.",
        "comments": "8 pages, 7 figures, 4 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11819"
    },
    {
        "doc_id": 374,
        "title": "Direct constructions of some group divisible designs with block size $4$ and up to $50$ points",
        "authors": [
            "R. Julian. R. Abel",
            "Thomas Britz",
            "Yudhistira A. Bunjamin",
            "Diana Combe"
        ],
        "subjects": [
            "Combinatorics"
        ],
        "abstract": "In this note, we give direct constructions of some group divisible designs (GDDs) with block size $4$ that have up to $50$ points.",
        "comments": "arXiv admin note: text overlap with arXiv:2309.12823",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11809"
    },
    {
        "doc_id": 375,
        "title": "The weakness of finding descending sequences in ill-founded linear orders",
        "authors": [
            "Jun Le Goh",
            "Arno Pauly",
            "Manlio Valenti"
        ],
        "subjects": [
            "Logic",
            "Logic in Computer Science",
            "Combinatorics"
        ],
        "abstract": "We prove that the Weihrauch degree of the problem of finding a bad sequence in a non-well quasi order ($\\mathsf{BS}$) is strictly above that of finding a descending sequence in an ill-founded linear order ($\\mathsf{DS}$). This corrects our mistaken claim in arXiv:2010.03840, which stated that they are Weihrauch equivalent. We prove that K\u00f6nig's lemma $\\mathsf{KL}$ is not Weihrauch reducible to $\\mathsf{DS}$ either, resolving the main open question raised in arXiv:2010.03840.",
        "comments": "4 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11807"
    },
    {
        "doc_id": 376,
        "title": "Type problem and the first eigenvalue",
        "authors": [
            "Bo-Yong Chen",
            "Yuanpu Xiong"
        ],
        "subjects": [
            "Differential Geometry",
            "Complex Variables"
        ],
        "abstract": "In this paper, we study the relationship between the type problem and the asymptotic behavior of the first eigenvalues $\u03bb_1(B_r)$ of ``balls'' $B_r:=\\{\u03c1<r\\}$ on a complete Riemannian manfold $M$ as $r\\rightarrow +\\infty$, where $\u03c1$ is a Lipschitz continuous exhaustion function with $|\\nabla\u03c1|\\leq1$ a.e. on $M$. We show that $M$ is hyperbolic whenever \\[ \u039b_*:= \\liminf_{r\\rightarrow +\\infty} \\{ r^2 \u03bb_1(B_r)\\} >18.624\\cdots. \\] Moreover, an upper bound of $\u039b_*$ in terms of volume growth $\u03bd_*:=\\liminf_{r\\rightarrow +\\infty} \\frac{\\log |B_r|}{\\log r}$ is given as follows \\[ {\u039b_*} \\lesssim \\begin{cases} \u03bd_*^2,\\ \\ \\ &\u03bd_*\\gg1,\\\\ \u03bd_*\\log\\frac{1}{\u03bd_*},&1<\u03bd_*\\ll1. \\end{cases} \\] The exponent $2$ for $\u03bd_*\\gg1$ turns out to be the best possible.",
        "comments": "21 pages. Comments welcome!",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11803"
    },
    {
        "doc_id": 377,
        "title": "Spherical Density-Equalizing Map for Genus-0 Closed Surfaces",
        "authors": [
            "Zhiyuan Lyu",
            "Lok Ming Lui",
            "Gary P. T. Choi"
        ],
        "subjects": [
            "Graphics",
            "Computational Geometry",
            "Differential Geometry",
            "Numerical Analysis"
        ],
        "abstract": "Density-equalizing maps are a class of mapping methods in which the shape deformation is driven by prescribed density information. In recent years, they have been widely used for data visualization on planar domains and planar parameterization of open surfaces. However, the theory and computation of density-equalizing maps for closed surfaces are much less explored. In this work, we develop a novel method for computing spherical density-equalizing maps for genus-0 closed surfaces. Specifically, we first compute a conformal parameterization of the given genus-0 closed surface onto the unit sphere. Then, we perform density equalization on the spherical domain based on the given density information to achieve a spherical density-equalizing map. The bijectivity of the mapping is guaranteed using quasi-conformal theory. We further propose a method for incorporating the harmonic energy and landmark constraints into our formulation to achieve landmark-aligned spherical density-equalizing maps balancing different distortion measures. Using the proposed methods, a large variety of spherical parameterizations can be achieved. Applications to surface registration, remeshing, and data visualization are presented to demonstrate the effectiveness of our methods.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11795"
    },
    {
        "doc_id": 378,
        "title": "Obtaining the pseudoinverse solution of singular range-symmetric linear systems with GMRES-type methods",
        "authors": [
            "Kai Du",
            "Jia-Jun Fan",
            "Fang Wang"
        ],
        "subjects": [
            "Numerical Analysis"
        ],
        "abstract": "It is well known that for singular inconsistent range-symmetric linear systems, the generalized minimal residual (GMRES) method determines a least squares solution without breakdown. The reached least squares solution may be or not be the pseudoinverse solution. We show that a lift strategy can be used to obtain the pseudoinverse solution. In addition, we propose a new iterative method named RSMAR (minimum $\\mathbf A$-residual) for range-symmetric linear systems $\\mathbf A\\mathbf x=\\mathbf b$. At step $k$ RSMAR minimizes $\\|\\mathbf A\\mathbf r_k\\|$ in the $k$th Krylov subspace generated with $\\{\\mathbf A, \\mathbf r_0\\}$ rather than $\\|\\mathbf r_k\\|$, where $\\mathbf r_k$ is the $k$th residual vector and $\\|\\cdot\\|$ denotes the Euclidean vector norm. We show that RSMAR and GMRES terminate with the same least squares solution when applied to range-symmetric linear systems. We provide two implementations for RSMAR. Our numerical experiments show that RSMAR is the most suitable method among GMRES-type methods for singular inconsistent range-symmetric linear systems.",
        "comments": "22 pages, 4 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11788"
    },
    {
        "doc_id": 379,
        "title": "A Comparative Study of Numerical Methods for Approximating the Solutions of a Macroscopic Automated-Vehicle Traffic Flow Model",
        "authors": [
            "George Titakis",
            "Iasson Karafyllis",
            "Dionysis Theodosis",
            "Ioannis Papamichail",
            "Markos Papageorgiou"
        ],
        "subjects": [
            "Numerical Analysis"
        ],
        "abstract": "In this paper, a particle method is used to approximate the solutions of a \"fluid-like\" macroscopic traffic flow model for automated vehicles. It is shown that this method preserves certain differential inequalities that hold for the macroscopic traffic model: mass is preserved, the mechanical energy is decaying and an energy functional is also decaying. To demonstrate the advantages of the particle method under consideration, a comparison with other numerical methods for viscous compressible fluid models is provided. Since the solutions of the macroscopic traffic model can be approximated by the solutions of a reduced model consisting of a single nonlinear heat-type partial differential equation, the numerical solutions produced by the particle method are also compared with the numerical solutions of the reduced model. Finally, a traffic simulation scenario and a comparison with the Aw-Rascle-Zhang (ARZ) model are provided, illustrating the advantages of the use of automated vehicles.",
        "comments": "34 pages, 19 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11787"
    },
    {
        "doc_id": 380,
        "title": "EPIC: a provable accelerated Eigensolver based on Preconditioning and Implicit Convexity",
        "authors": [
            "Nian Shao",
            "Wenbin Chen",
            "Zhaojun Bai"
        ],
        "subjects": [
            "Numerical Analysis"
        ],
        "abstract": "This paper is concerned with the extraction of the smallest eigenvalue and the corresponding eigenvector of a symmetric positive definite matrix pencil. We reveal implicit convexity of the eigenvalue problem in Euclidean space. A provable accelerated eigensolver based on preconditioning and implicit convexity (EPIC) is proposed. Theoretical analysis shows the acceleration of EPIC with the rate of convergence resembling the expected rate of convergence of the well-known locally optimal preconditioned conjugate gradient (LOPCG). A complete proof of the expected rate of convergence of LOPCG is elusive so far. Numerical results confirm our theoretical findings of EPIC.",
        "comments": "MSC Class:          15A08; 65F08; 65F15; 90C25",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11786"
    },
    {
        "doc_id": 381,
        "title": "Kleisli categories, T-categories and internal categories",
        "authors": [
            "Dominique Bourn"
        ],
        "subjects": [
            "Category Theory"
        ],
        "abstract": "We investigate the properties of the Kleisli category KlT of a monad (T,\u03bb,\u03bc) on a category E and in particular the existence of (some kind of) pullbacks. This culminates when the monad is cartesian. In this case, we show that any T-category in E in the sense of A. Burroni coincides with a special kind of internal category in KlT . So, it is the case in particular for T -operads and T -multicategories. More unexpectedly, this, in turn, sheds new lights on internal categories and n-categories.",
        "comments": "46 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11781"
    },
    {
        "doc_id": 382,
        "title": "Numerical Solutions for Stochastic Continuous-time Algebraic Riccati Equations",
        "authors": [
            "Tsung-Ming Huang",
            "Yueh-Cheng Kuo",
            "Ren-Cang Li",
            "Wen-Wei Lin"
        ],
        "subjects": [
            "Optimization and Control"
        ],
        "abstract": "We are concerned with efficient numerical methods for stochastic continuous-time algebraic Riccati equations (SCARE). Such equations frequently arise from the state-dependent Riccati equation approach which is perhaps the only systematic way today to study nonlinear control problems. Often involved Riccati-type equations are of small scale, but have to be solved repeatedly in real time. Important applications include the 3D missile/target engagement, the F16 aircraft flight control, and the quadrotor optimal control, to name a few. A new inner-outer iterative method that combines the fixed-point strategy and the structure-preserving doubling algorithm (SDA) is proposed. It is proved that the method is monotonically convergent, and in particular, taking the zero matrix as initial, the method converges to the desired stabilizing solution. Previously, Newton's method has been called to solve SCARE, but it was mostly investigated from its theoretic aspect than numerical aspect in terms of robust and efficient numerical implementation. For that reason, we revisit Newton's method for SCARE, focusing on how to calculate each Newton iterative step efficiently so that Newton's method for SCARE can become practical. It is proposed to use our new inner-outer iterative method, which is provably convergent, to provide critical initial starting points for Newton's method to ensure its convergence. Finally several numerical experiments are conducted to validate the new method and robust implementation of Newton's method.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11774"
    },
    {
        "doc_id": 383,
        "title": "Bordism of flow modules and exact Lagrangians",
        "authors": [
            "Noah Porcelli",
            "Ivan Smith"
        ],
        "subjects": [
            "Symplectic Geometry",
            "Algebraic Topology"
        ],
        "abstract": "For a stably framed Liouville manifold X , we construct a \"Donaldson-Fukaya category over the sphere spectrum\" F(X; S). The objects are closed exact Lagrangians whose Gauss maps are nullhomotopic compatibly with the ambient stable framing, and the morphisms are bordism classes of framed flow modules over Lagrangian Floer flow categories; this is enriched in modules over the framed bordism ring. We develop an obstruction theory for lifting quasi-isomorphisms in the usual Fukaya category to quasi-isomorphisms in appropriate truncations or quotients of F(X; S). Applications include constraints on the smooth structure of exact Lagrangians in certain plumbings, and the construction of non-trivial symplectic mapping classes which act trivially on the integral Fukaya category for a wide class of affine varieties.",
        "comments": "Comments welcome!",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11766"
    },
    {
        "doc_id": 384,
        "title": "G. S. Tseytin's seven-relation semigroup with undecidable word problem",
        "authors": [
            "Carl-Fredrik Nyberg-Brodda"
        ],
        "subjects": [
            "History and Overview",
            "Group Theory"
        ],
        "abstract": "We give an introduction to the ideas behind G. S. Tseytin's 1958 construction of a seven-relation semigroup with undecidable word problem. We give a history of the ideas leading up to its construction, some intuition for the proof, and provide an overview of some subsequent results and developments which stem from this remarkable semigroup. An English translation of Tseytin's article by the author of the present article is supplemented at the end of the article (the Russian original was published in Trudy Mat. Inst. Steklov 52 (1958), pp. 172--189).",
        "comments": "22 page original article + 16 page translation. Comments welcome!",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11757"
    },
    {
        "doc_id": 385,
        "title": "Univalent Enriched Categories and the Enriched Rezk Completion",
        "authors": [
            "Niels van der Weide"
        ],
        "subjects": [
            "Logic in Computer Science",
            "Category Theory"
        ],
        "abstract": "Enriched categories are categories whose sets of morphisms are enriched with extra structure. Such categories play a prominent role in the study of higher categories, homotopy theory, and the semantics of programming languages. In this paper, we study univalent enriched categories. We prove that all essentially surjective and fully faithful functors between univalent enriched categories are equivalences, and we show that every enriched category admits a Rezk completion. Finally, we use the Rezk completion for enriched categories to construct univalent enriched Kleisli categories.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11752"
    },
    {
        "doc_id": 386,
        "title": "On Rosser theories",
        "authors": [
            "Yong Cheng"
        ],
        "subjects": [
            "Logic"
        ],
        "abstract": "Rosser theories play an important role in the study of the incompleteness phenomenon and mete-mathematics of arithmetic. In this paper, we first define notions of $n$-Rosser theories, exact $n$-Rosser theories, effectively $n$-Rosser theories and effectively exact $n$-Rosser theories (see Definition 1.8). Our definitions are not restricted to arithmetic languages. Then we systematically examine properties of $n$-Rosser theories and relationships among them. Especially, we generalize some important theorems about Rosser theories for RE sets in the literature to $n$-Rosser theories in a general setting.",
        "comments": "26 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11749"
    },
    {
        "doc_id": 387,
        "title": "Transition probability of discrete geodesic flow on the standard non-uniform quotient of $PGL_3$",
        "authors": [
            "Sanghoon Kwon"
        ],
        "subjects": [
            "Dynamical Systems"
        ],
        "abstract": "We describe the local transition probability of a singular diagonal action on the standard non-uniform quotient of $PGL_3$ associated to the type 1 geodesic flow. As a consequence, we deduce the strongly positive recurrence property of the geodesic flow.",
        "comments": "11 pages, 6 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11747"
    },
    {
        "doc_id": 388,
        "title": "Finite horizon optimal control of reaction-diffusion SIV epidemic system with stochastic environment",
        "authors": [
            "Zong Wang"
        ],
        "subjects": [
            "Optimization and Control",
            "Dynamical Systems"
        ],
        "abstract": "This contribution mainly focuses on the finite horizon optimal control problems of a susceptible-infected-vaccinated(SIV) epidemic system governed by reaction-diffusion equations and Markov switching. Stochastic dynamic programming is employed to find the optimal vaccination effort and economic return for a stochastic reaction diffusion SIV epidemic model. To achieve this, a key step is to show the existence and uniqueness of invariant measure for the model. Then, we obtained the necessary and sufficient conditions for the near-optimal control. Furthermore, we give an algorithm to approximate the Hamilton-Jacobi Bellman (HJB) equation. Finally, some numerical simulations are presented to confirm our analytic results.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11744"
    },
    {
        "doc_id": 389,
        "title": "On partial endomorphisms of a star graph",
        "authors": [
            "Ilinka Dimitrova",
            "V\u00edtor H. Fernandes",
            "J\u00f6rg Koppitz"
        ],
        "subjects": [
            "Rings and Algebras"
        ],
        "abstract": "In this paper we consider the monoids of all partial endomorphisms, of all partial weak endomorphisms, of all injective partial endomorphisms, of all partial strong endomorphisms and of all partial strong weak endomorphisms of a star graph with a finite number of vertices. Our main objective is to determine their ranks. We also describe their Green's relations, calculate their cardinalities and study their regularity.",
        "comments": "MSC Class:          20M20; 20M10; 05C12; 05C25",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11741"
    },
    {
        "doc_id": 390,
        "title": "Sphractal: Estimating the Fractal Dimension of Surfaces Computed from Precise Atomic Coordinates via Box-Counting Algorithm",
        "authors": [
            "Jonathan Yik Chang Ting",
            "Andrew Thomas Agars Wood",
            "Amanda Susan Barnard"
        ],
        "subjects": [
            "Mathematical Software",
            "Atomic Physics",
            "Computational Physics"
        ],
        "abstract": "The fractal dimension of a surface allows its degree of roughness to be characterised quantitatively. However, limited effort has been attempted to compute the fractal dimension of surfaces computed from precisely known atomic coordinates from computational biomolecular and nanomaterial studies. This work proposes methods to estimate the fractal dimension of the surface of any three-dimensional object composed of spheres, by representing it as either a voxelised point cloud or a mathematically exact surface, and computing its box-counting dimension. Sphractal is published as a Python package that provides these functionalities, and its utility is demonstrated on a set of simulated palladium nanoparticle data.",
        "comments": "46 pages, 26 figures, submitted to Advanced Theory and Simulations",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11737"
    },
    {
        "doc_id": 391,
        "title": "Approximate solutions to a nonlinear functional differential equation",
        "authors": [
            "Nicholas Hale",
            "Enrique Thomann",
            "JAC Weideman"
        ],
        "subjects": [
            "Classical Analysis and ODEs",
            "Numerical Analysis"
        ],
        "abstract": "A functional differential equation related to the logistic equation is studied by a combination of numerical and perturbation methods. Parameter regions are identified where the solution to the nonlinear problem is approximated well by known series solutions of the linear version of the equation. The solution space for a certain class of functions is then mapped out using a continuation approach.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11733"
    },
    {
        "doc_id": 392,
        "title": "M\u00f6bius Inversion and Duality for Summations of Stable Graphs",
        "authors": [
            "Zhiyuan Wang",
            "Jian Zhou"
        ],
        "subjects": [
            "Combinatorics",
            "Mathematical Physics",
            "Algebraic Geometry"
        ],
        "abstract": "Using the stratifications of Deligne-Mumford moduli spaces $\\overline{\\mathcal M}_{g,n}$ indexed by stable graphs, we introduce a partially ordered set of stable graphs by defining a partial ordering on the set of connected stable graphs of genus $g$ with $n$ external edges. By modifying the usual definition of zeta function and M\u00f6bius function of a poset, we introduce generalized ($\\mathbb Q$-valued) zeta function and generalized ($\\mathbb Q$-valued) M\u00f6bius function of the poset of stable graphs. We use them to proved a generalized M\u00f6bius inversion formula for functions on the poset of stable graphs. Two applications related to duality in earlier work are also presented.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11717"
    },
    {
        "doc_id": 393,
        "title": "Integrality of Hecke eigenvalues and the growth of Hecke fields",
        "authors": [
            "Kenji Sakugawa",
            "Shingo Sugiyama"
        ],
        "subjects": [
            "Number Theory"
        ],
        "abstract": "We prove that Hecke eigenvalues for any Hilbert and Siegel modular forms are algebraic integers. Our method does not rely on cohomologicality nor Galois representations. We apply the integrality of Hecke eigenvalues for Hilbert modular forms of non-parallel weight to the estimation of the growth of Hecke fields of Hilbert cusp forms with non-vanishing central $L$-values. As a further application, we give the growth of the fields of rationality of cuspidal automorphic representations of ${\\rm GL}_{2d}(\\mathbb{A}_\\mathbb{Q})$ for a prime number $d$ with non-vanishing central $L$-values. We also apply the integrality of Hecke eigenvalues for holomorphic Siegel cusp forms of general degree in order to give the growth of the Hecke fields of those forms.",
        "comments": "MSC Class:          Primary 11F60; Secondary 11F41; 11F46; 11F75; 11R04",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11716"
    },
    {
        "doc_id": 394,
        "title": "Conjugate Direction Methods Under Inconsistent Systems",
        "authors": [
            "Alexander Lim",
            "Yang Liu",
            "Fred Roosta"
        ],
        "subjects": [
            "Numerical Analysis"
        ],
        "abstract": "Since the development of the conjugate gradient (CG) method in 1952 by Hestenes and Stiefel, CG, has become an indispensable tool in computational mathematics for solving positive definite linear systems. On the other hand, the conjugate residual (CR) method, closely related CG and introduced by Stiefel in 1955 for the same settings, remains relatively less known outside the numerical linear algebra community. Since their inception, these methods -- henceforth collectively referred to as conjugate direction methods -- have been extended beyond positive definite to indefinite, albeit consistent, settings. Going one step further, in this paper, we investigate theoretical and empirical properties of these methods under inconsistent systems. Among other things, we show that small modifications to the original algorithms allow for the pseudo-inverse solution. Furthermore, we show that CR is essentially equivalent to the minimum residual method, proposed by Paige and Saunders in 1975, in such contexts. Lastly, we conduct a series of numerical experiments to shed lights on their numerical stability (or lack thereof) and their performance for inconsistent systems. Surprisingly, we will demonstrate that, unlike CR and contrary to popular belief, CG can exhibit significant numerical instability, bordering on catastrophe in some instances.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11714"
    },
    {
        "doc_id": 395,
        "title": "A First Step Towards Runtime Analysis of Evolutionary Neural Architecture Search",
        "authors": [
            "Zeqiong Lv",
            "Chao Qian",
            "Yanan Sun"
        ],
        "subjects": [
            "Neural and Evolutionary Computing"
        ],
        "abstract": "Evolutionary neural architecture search (ENAS) employs evolutionary algorithms to find high-performing neural architectures automatically, and has achieved great success. However, compared to the empirical success, its rigorous theoretical analysis has yet to be touched. This work goes preliminary steps toward the mathematical runtime analysis of ENAS. In particular, we define a binary classification problem UNIFORM, and formulate an explicit fitness function to represent the relationship between neural architecture and classification accuracy. Furthermore, we consider (1+1)-ENAS algorithm with mutation to optimize the neural architecture, and obtain the following runtime bounds: 1) the one-bit mutation finds the optimum in an expected runtime of $O(n)$ and $\u03a9(\\log n)$; 2) the multi-bit mutation finds the optimum in an expected runtime of $\u0398(n)$. These theoretical results show that one-bit and multi-bit mutations achieve nearly the same performance on UNIFORM. We provide insight into the choices of mutation in the ENAS community: although multi-bit mutation can change the step size to prevent a local trap, this may not always improve runtime. Empirical results also verify the equivalence of these two mutation operators. This work begins the runtime analysis of ENAS, laying the foundation for further theoretical studies to guide the design of ENAS.",
        "comments": "8 pages, 3 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11712"
    },
    {
        "doc_id": 396,
        "title": "A translation of \"On the positive representation of polynomials\" by Ernst Meissner",
        "authors": [
            "Hannah K. Wayment-Steele"
        ],
        "subjects": [
            "History and Overview"
        ],
        "abstract": "We provide an English translation of \"\u00dcber positive Darstellungen von Polynomen\" by Ernst Meissner, originally published 1911 in Mathematische Annalen (70) 223-235.",
        "comments": "We welcome feedback on this translation to wayment (at) brandeis (dot) edu",
        "date": "10 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.11693"
    },
    {
        "doc_id": 397,
        "title": "Dynamic learning of synchronization in nonlinear coupled systems",
        "authors": [
            "Yong Wu",
            "Qianming Ding",
            "Weifang Huang",
            "Tianyu Li",
            "Dong Yu",
            "Ya Jia"
        ],
        "subjects": [
            "Biological Physics",
            "Chaotic Dynamics"
        ],
        "abstract": "Synchronous phenomenon exists in various natural and engineering systems. To obtain synchronization states within nonlinear coupled systems, we develop mathematical optimization techniques for the dynamic learning of synchronization (DLS) to capture the state differences between nodes within the system and dynamically adjust weights. By utilizing our novel algorithm formulae, nonlinear coupled systems can maintain a stable state of synchronization after appropriate weight adjustments. We present several variants of the DLS techniques including the self-adaptive, the supervised and the hybrid methods, all of which reliably demonstrate their capability to facilitate synchronization in heterogeneous networks, such as small-world, scale-free, and random networks. As application of DLS technique, we first validate the efficacy of our DLS technique by using a simple FitzHugh-Nagumo neural network, and the effects of the technique on both global and local synchronization are studied. Then the DLS technique is applied to a complex nonlinear system, i.e., the network of Hodgkin-Huxley neurons with chemical synaptic coupling model. This study offers a novel perspective on comprehending synchronization phenomena within nonlinear systems.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11691"
    },
    {
        "doc_id": 398,
        "title": "Two Necessary and Sufficient Conditions to the Solvability of the Exterior Dirichlet Problem for the Monge-Amp\u00e8re Equation",
        "authors": [
            "Cong Wang",
            "Jiguang Bao"
        ],
        "subjects": [
            "Analysis of PDEs"
        ],
        "abstract": "The present paper provides two necessary and sufficient conditions for the existence of solutions to the exterior Dirichlet problem of the Monge-Amp\u00e8re equation with prescribed asymptotic behavior at infinity. By an adapted smooth approximation argument, we prove that the problem is solvable if and only if the boundary value is semi-convex with respect to the inner boundary, which is our first proposed new concept. Along the lines of Perron's method for Laplace equation, we obtain the threshold for solvability in the asymptotic behavior at infinity of the solution, and remove the $C^2$ regularity assumptions on the boundary value and on the inner boundary which are required in the proofs of the corresponding existence theorems in the recent literatures.",
        "comments": "30 pages, 2 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11690"
    },
    {
        "doc_id": 399,
        "title": "A system of NLS arising in optical material without Galilean symmetry",
        "authors": [
            "Yuan Li",
            "Kai Wang",
            "Qingxuan Wang"
        ],
        "subjects": [
            "Analysis of PDEs"
        ],
        "abstract": "We consider a system of NLS with cubic interactions arising in nonlinear optics without Galilean symmetry. The absence of Galilean symmetry can lead to many difficulties, such as global existence and blowup problems; see [Comm. Partial Differential Equations 46, 11 (2021), 2134-2170]. In this paper, we mainly focus on the influence of the absence of this symmetry on the traveling waves of the NLS system. Firstly, we obtain the existence of traveling solitary wave solutions that are non-radial and complex-valued. Secondly, using the asymptotic analysis method, when the frequency is sufficiently large, we establish the high frequency limit of the traveling solitary wave solution. Finally, for the mass critical case, we provide a novel condition for the existence of global solutions which is significantly different from the classical. In particular, this new condition breaks the traditional optimal assumption about initial data.",
        "comments": "28 pages",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11683"
    },
    {
        "doc_id": 400,
        "title": "Quality-Aware Hydraulic Control in Drinking Water Networks via Controllability Proxies",
        "authors": [
            "Salma M. Elsherif",
            "Mohamad H. Kazma",
            "Ahmad F. Taha"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "The operation of water distribution networks is a complex procedure aimed at efficiently delivering consumers with adequate water quantity while ensuring its safe quality. An added challenge is the dependency of the water quality dynamics on the system's hydraulics, which influences the performance of the water quality controller. Prior research has addressed either solving the optimum operational hydraulic setting problem or regulating the water quality dynamics as separate problems. Additionally, there have been efforts to couple these two problems and solve one compact problem resulting in trade-offs between the contradictory objectives. In contrast, this paper examines the dependency and influence from a control-theoretic standpoint. More specifically, we explore the influence of accountability for water quality controllability improvement when addressing the pump scheduling problem. We examine its effects on the cumulative cost of the interconnected systems as well as the subsequent performance of the water quality controller. To achieve this, we develop a framework that incorporates different controllability metrics within the operational hydraulic optimization problem; its aim is attaining an adequate level of water quality control across the system. We assess the aforementioned aspects' performance on various scaled networks with a wide range of numerical scenarios.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12214"
    },
    {
        "doc_id": 401,
        "title": "Programmable EM Sensor Array for Golden-Model Free Run-time Trojan Detection and Localization",
        "authors": [
            "Hanqiu Wang",
            "Max Panoff",
            "Zihao Zhan",
            "Shuo Wang",
            "Christophe Bobda",
            "Domenic Forte"
        ],
        "subjects": [
            "Cryptography and Security",
            "Signal Processing"
        ],
        "abstract": "Side-channel analysis has been proven effective at detecting hardware Trojans in integrated circuits (ICs). However, most detection techniques rely on large external probes and antennas for data collection and require a long measurement time to detect Trojans. Such limitations make these techniques impractical for run-time deployment and ineffective in detecting small Trojans with subtle side-channel signatures. To overcome these challenges, we propose a Programmable Sensor Array (PSA) for run-time hardware Trojan detection, localization, and identification. PSA is a tampering-resilient integrated on-chip magnetic field sensor array that can be re-programmed to change the sensors' shape, size, and location. Using PSA, EM side-channel measurement results collected from sensors at different locations on an IC can be analyzed to localize and identify the Trojan. The PSA has better performance than conventional external magnetic probes and state-of-the-art on-chip single-coil magnetic field sensors. We fabricated an AES-128 test chip with four AES Hardware Trojans. They were successfully detected, located, and identified with the proposed on-chip PSA within 10 milliseconds using our proposed cross-domain analysis.",
        "comments": "6 pages, 5 figures, Accepted at DATE2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12193"
    },
    {
        "doc_id": 402,
        "title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation",
        "authors": [
            "Zachary Novack",
            "Julian McAuley",
            "Taylor Berg-Kirkpatrick",
            "Nicholas J. Bryan"
        ],
        "subjects": [
            "Sound",
            "Artificial Intelligence",
            "Machine Learning",
            "Audio and Speech Processing"
        ],
        "abstract": "We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose frame-work for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents. Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency. We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control - all without ever fine-tuning the underlying model. When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models. Sound examples can be found at https://DITTO-Music.github.io/web/.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12179"
    },
    {
        "doc_id": 403,
        "title": "Waveform-Domain Complementary Signal Sets for Interrupted Sampling Repeater Jamming Suppression",
        "authors": [
            "Hanning Su",
            "Qinglong Bao",
            "Jiameng Pan",
            "Fucheng Guo",
            "Weidong Hu"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "The interrupted-sampling repeater jamming (ISRJ) is coherent and has the characteristic of suppression and deception to degrade the radar detection capabilities. The study focuses on anti-ISRJ techniques in the waveform domain, primarily capitalizing on waveform design and and anti-jamming signal processing methods in the waveform domain. By exploring the relationship between waveform-domain adaptive matched filtering (WD-AMF) output and waveform-domain signals, we demonstrate that ISRJ can be effectively suppressed when the transmitted waveform exhibits waveform-domain complementarity. We introduce a phase-coded (PC) waveform set with waveform-domain complementarity and propose a method for generating such waveform sets of arbitrary code lengths. The performance of WD-AMF are further developed due to the designed waveforms, and simulations affirm the superior adaptive anti-jamming capabilities of the designed waveforms compared to traditional ones. Remarkably, this improved performance is achieved without the need for prior knowledge of ISRJ interference parameters at either the transmitter or receiver stages.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12173"
    },
    {
        "doc_id": 404,
        "title": "Robust stability analysis of an energy-efficient control in a Networked Control System with application to Unmanned Ground Vehicles",
        "authors": [
            "Antonio Gonzalez",
            "Angel Cuenca",
            "Julian Salt",
            "Jelle Jacobs"
        ],
        "subjects": [
            "Systems and Control",
            "Optimization and Control"
        ],
        "abstract": "In this paper, the robust stability and disturbance rejection performance analysis of an energy-efficient control is addressed in the framework of Networked Control System (NCS). The control scheme under study integrates periodic event-triggered control, packet-based control, time-varying Kalman filter, dual-rate control and prediction techniques, whose design is aimed at reducing energy consumption and bandwidth usage. The robust stability against time-varying model uncertainties is analyzed by means of a suficient condition based on Linear Matrix Inequalities (LMI). Finally, the effectiveness of the proposed approach is experimentally validated in a tracking control for an Unmanned Ground Vehicle (UGV), which is a battery-constrained mobile device with limited computation capacities.",
        "comments": "38 pages, 12 figures, Information Sciences, 2021",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12172"
    },
    {
        "doc_id": 405,
        "title": "Dynamic Semantic Compression for CNN Inference in Multi-access Edge Computing: A Graph Reinforcement Learning-based Autoencoder",
        "authors": [
            "Nan Li",
            "Alexandros Iosifidis",
            "Qi Zhang"
        ],
        "subjects": [
            "Image and Video Processing",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "This paper studies the computational offloading of CNN inference in dynamic multi-access edge computing (MEC) networks. To address the uncertainties in communication time and computation resource availability, we propose a novel semantic compression method, autoencoder-based CNN architecture (AECNN), for effective semantic extraction and compression in partial offloading. In the semantic encoder, we introduce a feature compression module based on the channel attention mechanism in CNNs, to compress intermediate data by selecting the most informative features. In the semantic decoder, we design a lightweight decoder to reconstruct the intermediate data through learning from the received compressed data to improve accuracy. To effectively trade-off communication, computation, and inference accuracy, we design a reward function and formulate the offloading problem of CNN inference as a maximization problem with the goal of maximizing the average inference accuracy and throughput over the long term. To address this maximization problem, we propose a graph reinforcement learning-based AECNN (GRL-AECNN) method, which outperforms existing works DROO-AECNN, GRL-BottleNet++ and GRL-DeepJSCC under different dynamic scenarios. This highlights the advantages of GRL-AECNN in offloading decision-making in dynamic MEC.",
        "comments": "arXiv admin note: text overlap with arXiv:2211.13745",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12167"
    },
    {
        "doc_id": 406,
        "title": "ScoreDec: A Phase-preserving High-Fidelity Audio Codec with A Generalized Score-based Diffusion Post-filter",
        "authors": [
            "Yi-Chiao Wu",
            "Dejan Markovi\u0107",
            "Steven Krenn",
            "Israel D. Gebru",
            "Alexander Richard"
        ],
        "subjects": [
            "Audio and Speech Processing"
        ],
        "abstract": "Although recent mainstream waveform-domain end-to-end (E2E) neural audio codecs achieve impressive coded audio quality with a very low bitrate, the quality gap between the coded and natural audio is still significant. A generative adversarial network (GAN) training is usually required for these E2E neural codecs because of the difficulty of direct phase modeling. However, such adversarial learning hinders these codecs from preserving the original phase information. To achieve human-level naturalness with a reasonable bitrate, preserve the original phase, and get rid of the tricky and opaque GAN training, we develop a score-based diffusion post-filter (SPF) in the complex spectral domain and combine our previous AudioDec with the SPF to propose ScoreDec, which can be trained using only spectral and score-matching losses. Both the objective and subjective experimental results show that ScoreDec with a 24~kbps bitrate encodes and decodes full-band 48~kHz speech with human-level naturalness and well-preserved phase information.",
        "comments": "5 pages, 3 figures, 2 tables. Proc. ICASSP, 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12160"
    },
    {
        "doc_id": 407,
        "title": "Efficient Resource Allocation and User Association in NOMA-Enabled Vehicular-Aided HetNets with High Altitude Platforms",
        "authors": [
            "Ali Nauman",
            "Mashael Maashi",
            "Hend K. Alkahtani",
            "Fahd N. Al-Wesabi",
            "Nojood O Aljehane",
            "Mohammed Assiri",
            "Sara Saadeldeen Ibrahim",
            "Wali Ullah Khan"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "The increasing demand for massive connectivity and high data rates has made the efficient use of existing spectrum resources an increasingly challenging problem. Non-orthogonal multiple access (NOMA) is a potential solution for future heterogeneous networks (HetNets) due to its high capacity and spectrum efficiency. In this study, we analyze an uplink NOMA-enabled vehicular-aided HetNet, where multiple vehicular user equipment (VUEs) share the access link spectrum, and a high-altitude platform (HAP) communicates with roadside units (RSUs) through a backhaul communication link. We propose an improved algorithm for user association that selects VUEs for HAPs based on channel coefficient ratios and terrestrial VUEs based on a caching-state backhaul communication link. The joint optimization problems aim to maximize a utility function that considers VUE transmission rates and cross-tier interference while meeting the constraints of backhaul transmission rates and QoS requirements of each VUE. The joint resource allocation optimization problem consists of three sub-problems: bandwidth allocation, user association, and transmission power allocation. We derive a closed-form solution for bandwidth allocation and solve the transmission power allocation sub-problem iteratively using Taylor expansion to transform a non-convex term into a convex one. Our proposed three-stage iterative algorithm for resource allocation integrates all three sub-problems and is shown to be effective through simulation results. Specifically, the results demonstrate that our solution achieves performance improvements over existing approaches.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12141"
    },
    {
        "doc_id": 408,
        "title": "Evaluation of QCNN-LSTM for Disability Forecasting in Multiple Sclerosis Using Sequential Multisequence MRI",
        "authors": [
            "John D. Mayfield",
            "Issam El Naqa"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Emerging Technologies",
            "Image and Video Processing"
        ],
        "abstract": "Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term Memory (LSTM) models were studied to provide sequential relationships for each timepoint in MRIs of patients with Multiple Sclerosis (MS). In this pilot study, we compared three QCNN-LSTM models for binary classification of MS disability benchmarked against classical neural network architectures. Our hypothesis is that quantum models will provide competitive performance. Methods Matrix Product State (MPS), reverse Multistate Entanglement Renormalization Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM layer to process near-annual MRI data of patients diagnosed with MS. These were benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision Transformer (ViViT). Predicted logits were measured against ground truth labels of each patient's Extended Disability Severity Score (EDSS) using binary cross-entropy loss. Training/validation/holdout testing was partitioned using 5-fold cross validation with a total split of 60:20:20. Levene's test of variance was used to measure statistical difference and Student's t-test for paired model differences in mean. Results The MPS-LSTM, reverse MERA-LSTM, and TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively (p-value 0.915). VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73 and 0.77, respectively (p-value 0.631). Overall variance and mean were not statistically significant (p-value 0.713), however, time to train was significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218, respectively, p-value <0.001). Conclusion QCNN-LSTM models perform competitively to their classical counterparts with greater efficiency in train time. Clinically, these can add value in terms of efficiency to time-dependent deep learning prediction of disease progression based upon medical imaging.",
        "comments": "ACM Class:          I.2.0; I.2.6",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12132"
    },
    {
        "doc_id": 409,
        "title": "Energy-aware Trajectory Optimization for UAV-mounted RIS and Full-duplex Relay",
        "authors": [
            "Dimitrios Tyrovolas",
            "Nikos A. Mitsiou",
            "Thomas G. Boufikos",
            "Prodromos-Vasileios Mekikis",
            "Sotiris A. Tegos",
            "Panagiotis D. Diamantoulakis",
            "Sotiris Ioannidis",
            "Christos K. Liaskos",
            "George K. Karagiannidis"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "In the evolving landscape of sixth-generation (6G) wireless networks, unmanned aerial vehicles (UAVs) have emerged as transformative tools for dynamic and adaptive connectivity. However, dynamically adjusting their position to offer favorable communication channels introduces operational challenges in terms of energy consumption, especially when integrating advanced communication technologies like reconfigurable intelligent surfaces (RISs) and full-duplex relays (FDRs). To this end, by recognizing the pivotal role of UAV mobility, the paper introduces an energy-aware trajectory design for UAV-mounted RISs and UAV-mounted FDRs using the decode and forward (DF) protocol, aiming to maximize the network minimum rate and enhance user fairness, while taking into consideration the available on-board energy. Specifically, this work highlights their distinct energy consumption characteristics and their associated integration challenges by developing appropriate energy consumption models for both UAV-mounted RISs and FDRs that capture the intricate relationship between key factors such as weight, and their operational characteristics. Furthermore, a joint time-division multiple access (TDMA) user scheduling-UAV trajectory optimization problem is formulated, considering the power dynamics of both systems, while assuring that the UAV energy is not depleted mid-air. Finally, simulation results underscore the importance of energy considerations in determining the optimal trajectory and scheduling and provide insights into the performance comparison of UAV-mounted RISs and FDRs in UAV-assisted wireless networks.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12107"
    },
    {
        "doc_id": 410,
        "title": "Consistency Based Unsupervised Self-training For ASR Personalisation",
        "authors": [
            "Jisi Zhang",
            "Vandana Rajan",
            "Haaris Mehmood",
            "David Tuckey",
            "Pablo Peso Parada",
            "Md Asif Jalal",
            "Karthikeyan Saravanan",
            "Gil Ho Lee",
            "Jungin Lee",
            "Seokyeong Jung"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "On-device Automatic Speech Recognition (ASR) models trained on speech data of a large population might underperform for individuals unseen during training. This is due to a domain shift between user data and the original training data, differed by user's speaking characteristics and environmental acoustic conditions. ASR personalisation is a solution that aims to exploit user data to improve model robustness. The majority of ASR personalisation methods assume labelled user data for supervision. Personalisation without any labelled data is challenging due to limited data size and poor quality of recorded audio samples. This work addresses unsupervised personalisation by developing a novel consistency based training method via pseudo-labelling. Our method achieves a relative Word Error Rate Reduction (WERR) of 17.3% on unlabelled training data and 8.1% on held-out data compared to a pre-trained model, and outperforms the current state-of-the art methods.",
        "comments": "Accepted for IEEE ASRU 2023",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12085"
    },
    {
        "doc_id": 411,
        "title": "DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI",
        "authors": [
            "Sergio Morell-Ortega",
            "Marina Ruiz-Perez",
            "Marien Gadea",
            "Roberto Vivo-Hernando",
            "Gregorio Rubio",
            "Fernando Aparici",
            "Mariam de la Iglesia-Vaya",
            "Gwenaelle Catheline",
            "Pierrick Coup\u00e9",
            "Jos\u00e9 V. Manj\u00f3n"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Neurons and Cognition"
        ],
        "abstract": "This paper introduces a novel multimodal and high-resolution human brain cerebellum lobule segmentation method. Unlike current tools that operate at standard resolution ($1 \\text{ mm}^{3}$) or using mono-modal data, the proposed method improves cerebellum lobule segmentation through the use of a multimodal and ultra-high resolution ($0.125 \\text{ mm}^{3}$) training dataset. To develop the method, first, a database of semi-automatically labelled cerebellum lobules was created to train the proposed method with ultra-high resolution T1 and T2 MR images. Then, an ensemble of deep networks has been designed and developed, allowing the proposed method to excel in the complex cerebellum lobule segmentation task, improving precision while being memory efficient. Notably, our approach deviates from the traditional U-Net model by exploring alternative architectures. We have also integrated deep learning with classical machine learning methods incorporating a priori knowledge from multi-atlas segmentation, which improved precision and robustness. Finally, a new online pipeline, named DeepCERES, has been developed to make available the proposed method to the scientific community requiring as input only a single T1 MR image at standard resolution.",
        "comments": "20 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12074"
    },
    {
        "doc_id": 412,
        "title": "Resource-constrained stereo singing voice cancellation",
        "authors": [
            "Clara Borrelli",
            "James Rae",
            "Dogac Basaran",
            "Matt McVicar",
            "Mehrez Souden",
            "Matthias Mauch"
        ],
        "subjects": [
            "Sound",
            "Machine Learning",
            "Audio and Speech Processing"
        ],
        "abstract": "We study the problem of stereo singing voice cancellation, a subtask of music source separation, whose goal is to estimate an instrumental background from a stereo mix. We explore how to achieve performance similar to large state-of-the-art source separation networks starting from a small, efficient model for real-time speech separation. Such a model is useful when memory and compute are limited and singing voice processing has to run with limited look-ahead. In practice, this is realised by adapting an existing mono model to handle stereo input. Improvements in quality are obtained by tuning model parameters and expanding the training set. Moreover, we highlight the benefits a stereo model brings by introducing a new metric which detects attenuation inconsistencies between channels. Our approach is evaluated using objective offline metrics and a large-scale MUSHRA trial, confirming the effectiveness of our techniques in stringent listening tests.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12068"
    },
    {
        "doc_id": 413,
        "title": "NEUROSEC: FPGA-Based Neuromorphic Audio Security",
        "authors": [
            "Murat Isik",
            "Hiruna Vishwamith",
            "Yusuf Sur",
            "Kayode Inadagbo",
            "I. Can Dikmen"
        ],
        "subjects": [
            "Cryptography and Security",
            "Emerging Technologies",
            "Machine Learning",
            "Neural and Evolutionary Computing",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "Neuromorphic systems, inspired by the complexity and functionality of the human brain, have gained interest in academic and industrial attention due to their unparalleled potential across a wide range of applications. While their capabilities herald innovation, it is imperative to underscore that these computational paradigms, analogous to their traditional counterparts, are not impervious to security threats. Although the exploration of neuromorphic methodologies for image and video processing has been rigorously pursued, the realm of neuromorphic audio processing remains in its early stages. Our results highlight the robustness and precision of our FPGA-based neuromorphic system. Specifically, our system showcases a commendable balance between desired signal and background noise, efficient spike rate encoding, and unparalleled resilience against adversarial attacks such as FGSM and PGD. A standout feature of our framework is its detection rate of 94%, which, when compared to other methodologies, underscores its greater capability in identifying and mitigating threats within 5.39 dB, a commendable SNR ratio. Furthermore, neuromorphic computing and hardware security serve many sensor domains in mission-critical and privacy-preserving applications.",
        "comments": "Audio processing, FPGA, Hardware Security, Neuromorphic Computing",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12055"
    },
    {
        "doc_id": 414,
        "title": "Look, Listen and Recognise: Character-Aware Audio-Visual Subtitling",
        "authors": [
            "Bruno Korbar",
            "Jaesung Huh",
            "Andrew Zisserman"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "The goal of this paper is automatic character-aware subtitle generation. Given a video and a minimal amount of metadata, we propose an audio-visual method that generates a full transcript of the dialogue, with precise speech timestamps, and the character speaking identified. The key idea is to first use audio-visual cues to select a set of high-precision audio exemplars for each character, and then use these exemplars to classify all speech segments by speaker identity. Notably, the method does not require face detection or tracking. We evaluate the method over a variety of TV sitcoms, including Seinfeld, Fraiser and Scrubs. We envision this system being useful for the automatic generation of subtitles to improve the accessibility of the vast amount of videos available on modern streaming services. Project page : \\url{https://www.robots.ox.ac.uk/~vgg/research/look-listen-recognise/}",
        "comments": "Accepted for publication in ICASSP 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12039"
    },
    {
        "doc_id": 415,
        "title": "A Survey of Advances in Optimization Methods for Wireless Communication System Design",
        "authors": [
            "Ya-Feng Liu",
            "Tsung-Hui Chang",
            "Mingyi Hong",
            "Zheyu Wu",
            "Anthony Man-Cho So",
            "Eduard A. Jorswieck",
            "Wei Yu"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing",
            "Optimization and Control"
        ],
        "abstract": "Mathematical optimization is now widely regarded as an indispensable modeling and solution tool for the design of wireless communications systems. While optimization has played a significant role in the revolutionary progress in wireless communication and networking technologies from 1G to 5G and onto the future 6G, the innovations in wireless technologies have also substantially transformed the nature of the underlying mathematical optimization problems upon which the system designs are based and have sparked significant innovations in the development of methodologies to understand, to analyze, and to solve those problems. In this paper, we provide a comprehensive survey of recent advances in mathematical optimization theory and algorithms for wireless communication system design. We begin by illustrating common features of mathematical optimization problems arising in wireless communication system design. We discuss various scenarios and use cases and their associated mathematical structures from an optimization perspective. We then provide an overview of recent advances in mathematical optimization theory and algorithms, from nonconvex optimization, global optimization, and integer programming, to distributed optimization and learning-based optimization. The key to successful solution of mathematical optimization problems is in carefully choosing and/or developing suitable optimization algorithms (or neural network architectures) that can exploit the underlying problem structure. We conclude the paper by identifying several open research challenges and outlining future research directions.",
        "comments": "47 pages, 10 figures, submitted for possible publication",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12025"
    },
    {
        "doc_id": 416,
        "title": "NLCG-Net: A Model-Based Zero-Shot Learning Framework for Undersampled Quantitative MRI Reconstruction",
        "authors": [
            "Xinrui Jiang",
            "Yohan Jun",
            "Jaejin Cho",
            "Mengze Gao",
            "Xingwang Yong",
            "Berkin Bilgic"
        ],
        "subjects": [
            "Image and Video Processing",
            "Machine Learning",
            "Signal Processing"
        ],
        "abstract": "Typical quantitative MRI (qMRI) methods estimate parameter maps after image reconstructing, which is prone to biases and error propagation. We propose a Nonlinear Conjugate Gradient (NLCG) optimizer for model-based T2/T1 estimation, which incorporates U-Net regularization trained in a scan-specific manner. This end-to-end method directly estimates qMRI maps from undersampled k-space data using mono-exponential signal modeling with zero-shot scan-specific neural network regularization to enable high fidelity T1 and T2 mapping. T2 and T1 mapping results demonstrate the ability of the proposed NLCG-Net to improve estimation quality compared to subspace reconstruction at high accelerations.",
        "comments": "8 pages, 5 figures, submitted to International Society for Magnetic Resonance in Medicine 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12004"
    },
    {
        "doc_id": 417,
        "title": "Lightweight Protection for Privacy in Offloaded Speech Understanding",
        "authors": [
            "Dongqi Cai",
            "Shangguang Wang",
            "Zeling Zhang",
            "Felix Xiaozhu Lin",
            "Mengwei Xu"
        ],
        "subjects": [
            "Sound",
            "Cryptography and Security",
            "Audio and Speech Processing"
        ],
        "abstract": "Speech is a common input method for mobile embedded devices, but cloud-based speech recognition systems pose privacy risks. Disentanglement-based encoders, designed to safeguard user privacy by filtering sensitive information from speech signals, unfortunately require substantial memory and computational resources, which limits their use in less powerful devices. To overcome this, we introduce a novel system, XXX, optimized for such devices. XXX is built on the insight that speech understanding primarily relies on understanding the entire utterance's long-term dependencies, while privacy concerns are often linked to short-term details. Therefore, XXX focuses on selectively masking these short-term elements, preserving the quality of long-term speech understanding. The core of XXX is an innovative differential mask generator, grounded in interpretable learning, which fine-tunes the masking process. We tested XXX on the STM32H7 microcontroller, assessing its performance in various potential attack scenarios. The results show that XXX maintains speech understanding accuracy and privacy at levels comparable to existing encoders, but with a significant improvement in efficiency, achieving up to 53.3$\\times$ faster processing and a 134.1$\\times$ smaller memory footprint.",
        "comments": "under review",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11983"
    },
    {
        "doc_id": 418,
        "title": "Enhancing Safety in Nonlinear Systems: Design and Stability Analysis of Adaptive Cruise Control",
        "authors": [
            "Fan Yang",
            "Haoqi Li",
            "Maolong Lv",
            "Jiangping Hu",
            "Qingrui Zhou",
            "Bijoy K. Ghosh"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "The safety of autonomous driving systems, particularly self-driving vehicles, remains of paramount concern. These systems exhibit affine nonlinear dynamics and face the challenge of executing predefined control tasks while adhering to state and input constraints to mitigate risks. However, achieving safety control within the framework of control input constraints, such as collision avoidance and maintaining system states within secure boundaries, presents challenges due to limited options. In this study, we introduce a novel approach to address safety concerns by transforming safety conditions into control constraints with a relative degree of 1. This transformation is facilitated through the design of control barrier functions, enabling the creation of a safety control system for affine nonlinear networks. Subsequently, we formulate a robust control strategy that incorporates safety protocols and conduct a comprehensive analysis of its stability and reliability. To illustrate the effectiveness of our approach, we apply it to a specific problem involving adaptive cruise control. Through simulations, we validate the efficiency of our model in ensuring safety without compromising control performance. Our approach signifies significant progress in the field, providing a practical solution to enhance safety for autonomous driving systems operating within the context of affine nonlinear dynamics.",
        "comments": "11pages,9figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11961"
    },
    {
        "doc_id": 419,
        "title": "Observation-Guided Meteorological Field Downscaling at Station Scale: A Benchmark and a New Method",
        "authors": [
            "Zili Liu",
            "Hao Chen",
            "Lei Bai",
            "Wenyuan Li",
            "Keyan Chen",
            "Zhengyi Wang",
            "Wanli Ouyang",
            "Zhengxia Zou",
            "Zhenwei Shi"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Image and Video Processing"
        ],
        "abstract": "Downscaling (DS) of meteorological variables involves obtaining high-resolution states from low-resolution meteorological fields and is an important task in weather forecasting. Previous methods based on deep learning treat downscaling as a super-resolution task in computer vision and utilize high-resolution gridded meteorological fields as supervision to improve resolution at specific grid scales. However, this approach has struggled to align with the continuous distribution characteristics of meteorological fields, leading to an inherent systematic bias between the downscaled results and the actual observations at meteorological stations. In this paper, we extend meteorological downscaling to arbitrary scattered station scales, establish a brand new benchmark and dataset, and retrieve meteorological states at any given station location from a coarse-resolution meteorological field. Inspired by data assimilation techniques, we integrate observational data into the downscaling process, providing multi-scale observational priors. Building on this foundation, we propose a new downscaling model based on hypernetwork architecture, namely HyperDS, which efficiently integrates different observational information into the model training, achieving continuous scale modeling of the meteorological field. Through extensive experiments, our proposed method outperforms other specially designed baseline models on multiple surface variables. Notably, the mean squared error (MSE) for wind speed and surface pressure improved by 67% and 19.5% compared to other methods. We will release the dataset and code subsequently.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11960"
    },
    {
        "doc_id": 420,
        "title": "Maximizing Spectral and Energy Efficiency in Multi-user MIMO OFDM Systems with RIS and Hardware Impairment",
        "authors": [
            "Mohammad Soleymani",
            "Ignacio Santamaria",
            "Aydin Sezgin",
            "Eduard Jorswieck"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "An emerging technology to enhance the spectral efficiency (SE) and energy efficiency (EE) of wireless communication systems is reconfigurable intelligent surface (RIS), which is shown to be very powerful in single-carrier systems. However, in multi-user orthogonal frequency division multiplexing (OFDM) systems, RIS may not be as promising as in single-carrier systems since an independent optimization of RIS elements at each sub-carrier is impossible in multi-carrier systems. Thus, this paper investigates the performance of various RIS technologies like regular (reflective and passive), simultaneously transmit and reflect (STAR), and multi-sector beyond diagonal (BD) RIS in multi-user multiple-input multiple-output (MIMO) OFDM broadcast channels (BC). This requires to formulate and solve a joint MIMO precoding and RIS optimization problem. The obtained solution reveals that RIS can significantly improve the system performance even when the number of RIS elements is relatively low. Moreover, we develop resource allocation schemes for STAR-RIS and multi-sector BD-RIS in MIMO OFDM BCs, and show that these RIS technologies can outperform a regular RIS, especially when the regular RIS cannot assist the communications for all the users.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11921"
    },
    {
        "doc_id": 421,
        "title": "A Training-Free Defense Framework for Robust Learned Image Compression",
        "authors": [
            "Myungseo Song",
            "Jinyoung Choi",
            "Bohyung Han"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "We study the robustness of learned image compression models against adversarial attacks and present a training-free defense technique based on simple image transform functions. Recent learned image compression models are vulnerable to adversarial attacks that result in poor compression rate, low reconstruction quality, or weird artifacts. To address the limitations, we propose a simple but effective two-way compression algorithm with random input transforms, which is conveniently applicable to existing image compression models. Unlike the na\u00efve approaches, our approach preserves the original rate-distortion performance of the models on clean images. Moreover, the proposed algorithm requires no additional training or modification of existing models, making it more practical. We demonstrate the effectiveness of the proposed techniques through extensive experiments under multiple compression models, evaluation metrics, and attack scenarios.",
        "comments": "10 pages and 14 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11902"
    },
    {
        "doc_id": 422,
        "title": "Fully Differentiable Ray Tracing via Discontinuity Smoothing for Radio Network Optimization",
        "authors": [
            "Jerome Eertmans",
            "Laurent Jacques",
            "Claude Oestges"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "Recently, Differentiable Ray Tracing has been successfully applied in the field of wireless communications for learning radio materials or optimizing the transmitter orientation. However, in the frame of gradient based optimization, obstruction of the rays by objects can cause sudden variations in the related objective functions or create entire regions where the gradient is zero. As these issues can dramatically impact convergence, this paper presents a novel Ray Tracing framework that is fully differentiable with respect to any scene parameter, but also provides a loss function continuous everywhere, thanks to specific local smoothing techniques. Previously non-continuous functions are replaced by a smoothing function, that can be exchanged with any function having similar properties. This function is also configurable via a parameter that determines how smooth the approximation should be. The present method is applied on a basic one-transmitter-multi-receiver scenario, and shows that it can successfully find the optimal solution. As a complementary resource, a 2D Python library, DiffeRT2d, is provided in Open Access, with examples and a comprehensive documentation.",
        "comments": "5 pages, 5 figures, accepted at EuCAP 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11882"
    },
    {
        "doc_id": 423,
        "title": "A Review of Physics-Informed Machine Learning Methods with Applications to Condition Monitoring and Anomaly Detection",
        "authors": [
            "Yuandi Wu",
            "Brett Sicard",
            "Stephen Andrew Gadsden"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Systems and Control"
        ],
        "abstract": "This study presents a comprehensive overview of PIML techniques in the context of condition monitoring. The central concept driving PIML is the incorporation of known physical laws and constraints into machine learning algorithms, enabling them to learn from available data while remaining consistent with physical principles. Through fusing domain knowledge with data-driven learning, PIML methods offer enhanced accuracy and interpretability in comparison to purely data-driven approaches. In this comprehensive survey, detailed examinations are performed with regard to the methodology by which known physical principles are integrated within machine learning frameworks, as well as their suitability for specific tasks within condition monitoring. Incorporation of physical knowledge into the ML model may be realized in a variety of methods, with each having its unique advantages and drawbacks. The distinct advantages and limitations of each methodology for the integration of physics within data-driven models are detailed, considering factors such as computational efficiency, model interpretability, and generalizability to different systems in condition monitoring and fault detection. Several case studies and works of literature utilizing this emerging concept are presented to demonstrate the efficacy of PIML in condition monitoring applications. From the literature reviewed, the versatility and potential of PIML in condition monitoring may be demonstrated. Novel PIML methods offer an innovative solution for addressing the complexities of condition monitoring and associated challenges. This comprehensive survey helps form the foundation for future work in the field. As the technology continues to advance, PIML is expected to play a crucial role in enhancing maintenance strategies, system reliability, and overall operational efficiency in engineering systems.",
        "comments": "Paper has been submitted for review to the journal Expert Systems with Applications (December 31, 2023). 90 pages, 22 figures, 9 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11860"
    },
    {
        "doc_id": 424,
        "title": "LKFormer: Large Kernel Transformer for Infrared Image Super-Resolution",
        "authors": [
            "Feiwei Qin",
            "Kang Yan",
            "Changmiao Wang",
            "Ruiquan Ge",
            "Yong Peng",
            "Kai Zhang"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Given the broad application of infrared technology across diverse fields, there is an increasing emphasis on investigating super-resolution techniques for infrared images within the realm of deep learning. Despite the impressive results of current Transformer-based methods in image super-resolution tasks, their reliance on the self-attentive mechanism intrinsic to the Transformer architecture results in images being treated as one-dimensional sequences, thereby neglecting their inherent two-dimensional structure. Moreover, infrared images exhibit a uniform pixel distribution and a limited gradient range, posing challenges for the model to capture effective feature information. Consequently, we suggest a potent Transformer model, termed Large Kernel Transformer (LKFormer), to address this issue. Specifically, we have designed a Large Kernel Residual Depth-wise Convolutional Attention (LKRDA) module with linear complexity. This mainly employs depth-wise convolution with large kernels to execute non-local feature modeling, thereby substituting the standard self-attentive layer. Additionally, we have devised a novel feed-forward network structure called Gated-Pixel Feed-Forward Network (GPFN) to augment the LKFormer's capacity to manage the information flow within the network. Comprehensive experimental results reveal that our method surpasses the most advanced techniques available, using fewer parameters and yielding considerably superior performance.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11859"
    },
    {
        "doc_id": 425,
        "title": "Adversarial speech for voice privacy protection from Personalized Speech generation",
        "authors": [
            "Shihao Chen",
            "Liping Chen",
            "Jie Zhang",
            "KongAik Lee",
            "Zhenhua Ling",
            "Lirong Dai"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers' voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker's speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.",
        "comments": "Accepted by icassp 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11857"
    },
    {
        "doc_id": 426,
        "title": "MOSformer: Momentum encoder-based inter-slice fusion transformer for medical image segmentation",
        "authors": [
            "De-Xing Huang",
            "Xiao-Hu Zhou",
            "Xiao-Liang Xie",
            "Shi-Qi Liu",
            "Zhen-Qiu Feng",
            "Mei-Jiang Gui",
            "Hao Li",
            "Tian-Yu Xiang",
            "Xiu-Ling Liu",
            "Zeng-Guang Hou"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Medical image segmentation takes an important position in various clinical applications. Deep learning has emerged as the predominant solution for automated segmentation of volumetric medical images. 2.5D-based segmentation models bridge computational efficiency of 2D-based models and spatial perception capabilities of 3D-based models. However, prevailing 2.5D-based models often treat each slice equally, failing to effectively learn and exploit inter-slice information, resulting in suboptimal segmentation performances. In this paper, a novel Momentum encoder-based inter-slice fusion transformer (MOSformer) is proposed to overcome this issue by leveraging inter-slice information at multi-scale feature maps extracted by different encoders. Specifically, dual encoders are employed to enhance feature distinguishability among different slices. One of the encoders is moving-averaged to maintain the consistency of slice representations. Moreover, an IF-Swin transformer module is developed to fuse inter-slice multi-scale features. The MOSformer is evaluated on three benchmark datasets (Synapse, ACDC, and AMOS), establishing a new state-of-the-art with 85.63%, 92.19%, and 85.43% of DSC, respectively. These promising results indicate its competitiveness in medical image segmentation. Codes and models of MOSformer will be made publicly available upon acceptance.",
        "comments": "Under Review",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11856"
    },
    {
        "doc_id": 427,
        "title": "Privacy-Preserving Data Fusion for Traffic State Estimation: A Vertical Federated Learning Approach",
        "authors": [
            "Qiqing Wang",
            "Kaidi Yang"
        ],
        "subjects": [
            "Machine Learning",
            "Cryptography and Security",
            "Systems and Control"
        ],
        "abstract": "This paper proposes a privacy-preserving data fusion method for traffic state estimation (TSE). Unlike existing works that assume all data sources to be accessible by a single trusted party, we explicitly address data privacy concerns that arise in the collaboration and data sharing between multiple data owners, such as municipal authorities (MAs) and mobility providers (MPs). To this end, we propose a novel vertical federated learning (FL) approach, FedTSE, that enables multiple data owners to collaboratively train and apply a TSE model without having to exchange their private data. To enhance the applicability of the proposed FedTSE in common TSE scenarios with limited availability of ground-truth data, we further propose a privacy-preserving physics-informed FL approach, i.e., FedTSE-PI, that integrates traffic models into FL. Real-world data validation shows that the proposed methods can protect privacy while yielding similar accuracy to the oracle method without privacy considerations.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11836"
    },
    {
        "doc_id": 428,
        "title": "Intelligibility Enhancement of Acoustic Noisy Speech for Autism Spectrum Disorder Condition",
        "authors": [
            "M. Pillonetto",
            "A. Queiroz",
            "R. Coelho"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "This work introduces a time domain personalized method (pGTFF0) to achieve intelligibility improvement of noisy speech for Autism Spectrum Disorder (ASD) situation. For this proposal, harmonic features estimated from speech frames are considered as center frequencies of Gammatone auditory filterbanks. A gain factor is further applied to the output of the filtered samples. The key goal is the emulation of an external noise filtering tailored for individuals with ASD. A perceptual listening test demonstrates that ASD volunteers attained lower intelligibility rates than Neurotypical (NT). The proposed solution is compared to three competing approaches considering four acoustic noises at different signal-to-noise ratios. Two objective measures (ESTOI and PESQ) are also adopted for evaluation. The experimental results show that the personalized solution outperformed the competing approaches in terms of intelligibility and quality improvement.",
        "comments": "5 pages, 3 figues, 2 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11832"
    },
    {
        "doc_id": 429,
        "title": "Harmonic Detection from Noisy Speech with Auditory Frame Gain for Intelligibility Enhancement",
        "authors": [
            "A. Queiroz",
            "R. Coelho"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "This paper introduces a novel (HDAG - Harmonic Detection for Auditory Gain) method for speech intelligibility enhancement in noisy scenarios. In the proposed scheme, a series of selective Gammachirp filters are adopted to emphasize the harmonic components of speech reducing the masking effects of acoustic noises. The fundamental frequency are estimated by the HHT-Amp technique. Harmonic patterns estimated with low accuracy are detected and adjusted according the FSFFE low/high pitch separation. The central frequencies of the filterbank are defined considering the third octave subbands which are best suited to cover the regions most relevant to intelligibility. Before signal reconstruction, the gammachirp filtered components are amplified by gain factors regulated by FSFFE classification. The proposed HDAG solution and three baseline techniques are examined considering six background noises with four signal-to-noise ratios. Three objective measures are adopted for the evaluation of speech intelligibility and quality. Several experiments are conducted to demonstrate that the proposed scheme achieves better speech intelligibility improvement when compared to the competing approaches. A perceptual listening test is further considered and corroborates with the objective results.",
        "comments": "9 pages, 6 figures, 4 tables",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11829"
    },
    {
        "doc_id": 430,
        "title": "Performance Analysis of Fluid Antenna-aided Backscatter Communications Systems",
        "authors": [
            "Farshad Rostami Ghadi",
            "Masoud Kaveh",
            "Kai-Kit Wong"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "This paper studies the performance of backscatter communications (BC) over emerging fluid antenna (FA) technology. In particular, a single-antenna source sends information to a FA reader through the wireless forward (i.e., source-to-tag) and backscatter (tag-to-reader) channels. For the considered BC, we first derive the cumulative distribution function (CDF) of the equivalent channel at the FA receiver, and then we obtain closed-form expressions of the outage probability (OP) and delay outage rate (DOR) under a correlated Rayleigh distribution. Moreover, in order to gain more insights into the system performance, we present analytical expressions of the OP and DOR at the high SNR regime. Numerical results indicate that considering the FA at the reader can significantly improve the performance of BC in terms of the OP and DOR compared with a single-antenna reader.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11820"
    },
    {
        "doc_id": 431,
        "title": "Analyzing the coupling process of distributed mixed real-virtual prototypes",
        "authors": [
            "Peter Baumann",
            "Lars Mikelsons",
            "Oliver Kotte",
            "Dieter Schramm"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "The ongoing connection and automation of vehicles leads to a closer interaction of the individual vehicle components, which demands for consideration throughout the entire development process. In the design phase, this is achieved through co-simulation of component models. However, complex co-simulation environments are rarely (re-)used in the verification and validation phases, in which mixed real-virtual prototypes (e.g. Hardware-in-the-Loop) are already available. One reason for this are coupling errors such as time-delays, which inevitably occur in co-simulation of virtual and real-time systems, and which influence system behavior in an unknown and generally detrimental way. This contribution introduces a novel, adaptive method to compensate for constant time-delays in potentially highly nonlinear, spatially distributed mixed real-virtual prototypes, using small feedforward neural networks. Their optimal initialization with respect to defined frequency domain features results from a-priori frequency domain analysis of the entire coupled system, including coupling faults and compensation methods. A linear and a nonlinear example demonstrate the method and emphasize its suitability for nonlinear systems due to online training and adaptation. As the compensation method requires knowledge only of the bandwidths, the proposed method is applicable to distributed mixed real-virtual prototypes in general.",
        "comments": "8 pages, 12 figures, published at 33rd Annual European Simulation and Modelling Conference, ESM 2019",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11779"
    },
    {
        "doc_id": 432,
        "title": "Advancing Accessibility: Voice Cloning and Speech Synthesis for Individuals with Speech Disorders",
        "authors": [
            "Vinotha R",
            "Hepsiba D",
            "L. D. Vijay Anand",
            "Deepak John Reji"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Sound"
        ],
        "abstract": "Neural Text-to-speech (TTS) synthesis is a powerful technology that can generate speech using neural networks. One of the most remarkable features of TTS synthesis is its capability to produce speech in the voice of different speakers. This paper introduces voice cloning and speech synthesis https://pypi.org/project/voice-cloning/ an open-source python package for helping speech disorders to communicate more effectively as well as for professionals seeking to integrate voice cloning or speech synthesis capabilities into their projects. This package aims to generate synthetic speech that sounds like the natural voice of an individual, but it does not replace the natural human voice. The architecture of the system comprises a speaker verification system, a synthesizer, a vocoder, and noise reduction. Speaker verification system trained on a varied set of speakers to achieve optimal generalization performance without relying on transcriptions. Synthesizer is trained using both audio and transcriptions that generate Mel spectrogram from a text and vocoder which converts the generated Mel Spectrogram into corresponding audio signal. Then the audio signal is processed by a noise reduction algorithm to eliminate unwanted noise and enhance speech clarity. The performance of synthesized speech from seen and unseen speakers are then evaluated using subjective and objective evaluation such as Mean Opinion Score (MOS), Gross Pitch Error (GPE), and Spectral distortion (SD). The model can create speech in distinct voices by including speaker characteristics that are chosen randomly.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11771"
    },
    {
        "doc_id": 433,
        "title": "Massive Synchrony in Distributed Antenna Systems",
        "authors": [
            "Erik G. Larsson"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "Distributed antennas must be phase-calibrated (phase-synchronized) for certain operations, such as reciprocity-based joint coherent downlink beamforming, to work. We use rigorous signal processing tools to analyze the accuracy of calibration protocols that are based on over-the-air measurements between antennas, with a focus on scalability aspects for large systems. We show that (i) for some who-measures-on-whom topologies, the errors in the calibration process are unbounded when the network grows; and (ii) despite that conclusion, it is optimal -- irrespective of the topology -- to solve a single calibration problem for the entire system and use the result everywhere to support the beamforming. The analyses are exemplified by investigating specific topologies, including lines, rings, and two-dimensional surfaces.",
        "comments": "Journal ref:        IEEE Transactions on Signal Processing, 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11730"
    },
    {
        "doc_id": 434,
        "title": "Beyond the Manual Touch: Situational-aware Force Control for Increased Safety in Robot-assisted Skullbase Surgery",
        "authors": [
            "Hisashi Ishida",
            "Deepa Galaiya",
            "Nimesh Nagururu",
            "Francis Creighton",
            "Peter Kazanzides",
            "Russell Taylor",
            "Manish Sahu"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "Purpose - Skullbase surgery demands exceptional precision when removing bone in the lateral skull base. Robotic assistance can alleviate the effect of human sensory-motor limitations. However, the stiffness and inertia of the robot can significantly impact the surgeon's perception and control of the tool-to-tissue interaction forces. Methods - We present a situational-aware, force control technique aimed at regulating interaction forces during robot-assisted skullbase drilling. The contextual interaction information derived from the digital twin environment is used to enhance sensory perception and suppress undesired high forces. Results - To validate our approach, we conducted initial feasibility experiments involving a medical and two engineering students. The experiment focused on further drilling around critical structures following cortical mastoidectomy. The experiment results demonstrate that robotic assistance coupled with our proposed control scheme effectively limited undesired interaction forces when compared to robotic assistance without the proposed force control. Conclusions - The proposed force control techniques show promise in significantly reducing undesired interaction forces during robot-assisted skullbase surgery. These findings contribute to the ongoing efforts to enhance surgical precision and safety in complex procedures involving the lateral skull base.",
        "comments": "*These authors contributed equally to this work",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11721"
    },
    {
        "doc_id": 435,
        "title": "Integrating 3D Slicer with a Dynamic Simulator for Situational Aware Robotic Interventions",
        "authors": [
            "Manish Sahu",
            "Hisashi Ishida",
            "Laura Connolly",
            "Hongyi Fan",
            "Anton Deguet",
            "Peter Kazanzides",
            "Francis X. Creighton",
            "Russell H. Taylor",
            "Adnan Munawar"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "Image-guided robotic interventions represent a transformative frontier in surgery, blending advanced imaging and robotics for improved precision and outcomes. This paper addresses the critical need for integrating open-source platforms to enhance situational awareness in image-guided robotic research. We present an open-source toolset that seamlessly combines a physics-based constraint formulation framework, AMBF, with a state-of-the-art imaging platform application, 3D Slicer. Our toolset facilitates the creation of highly customizable interactive digital twins, that incorporates processing and visualization of medical imaging, robot kinematics, and scene dynamics for real-time robot control. Through a feasibility study, we showcase real-time synchronization of a physical robotic interventional environment in both 3D Slicer and AMBF, highlighting low-latency updates and improved visualization.",
        "comments": "*These authors contributed equally",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11715"
    },
    {
        "doc_id": 436,
        "title": "Haptic-Assisted Collaborative Robot Framework for Improved Situational Awareness in Skull Base Surgery",
        "authors": [
            "Hisashi Ishida",
            "Manish Sahu",
            "Adnan Munawar",
            "Nimesh Nagururu",
            "Deepa Galaiya",
            "Peter Kazanzides",
            "Francis X. Creighton",
            "Russell H. Taylor"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "Skull base surgery is a demanding field in which surgeons operate in and around the skull while avoiding critical anatomical structures including nerves and vasculature. While image-guided surgical navigation is the prevailing standard, limitation still exists requiring personalized planning and recognizing the irreplaceable role of a skilled surgeon. This paper presents a collaboratively controlled robotic system tailored for assisted drilling in skull base surgery. Our central hypothesis posits that this collaborative system, enriched with haptic assistive modes to enforce virtual fixtures, holds the potential to significantly enhance surgical safety, streamline efficiency, and alleviate the physical demands on the surgeon. The paper describes the intricate system development work required to enable these virtual fixtures through haptic assistive modes. To validate our system's performance and effectiveness, we conducted initial feasibility experiments involving a medical student and two experienced surgeons. The experiment focused on drilling around critical structures following cortical mastoidectomy, utilizing dental stone phantom and cadaveric models. Our experimental results demonstrate that our proposed haptic feedback mechanism enhances the safety of drilling around critical structures compared to systems lacking haptic assistance. With the aid of our system, surgeons were able to safely skeletonize the critical structures without breaching any critical structure even under obstructed view of the surgical site.",
        "comments": "*These authors contributed equally",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11709"
    },
    {
        "doc_id": 437,
        "title": "Keep Decoding Parallel with Effective Knowledge Distillation from Language Models to End-to-end Speech Recognisers",
        "authors": [
            "Michael Hentschel",
            "Yuta Nishikawa",
            "Tatsuya Komatsu",
            "Yusuke Fujita"
        ],
        "subjects": [
            "Computation and Language",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "This study presents a novel approach for knowledge distillation (KD) from a BERT teacher model to an automatic speech recognition (ASR) model using intermediate layers. To distil the teacher's knowledge, we use an attention decoder that learns from BERT's token probabilities. Our method shows that language model (LM) information can be more effectively distilled into an ASR model using both the intermediate layers and the final layer. By using the intermediate layers as distillation target, we can more effectively distil LM knowledge into the lower network layers. Using our method, we achieve better recognition accuracy than with shallow fusion of an external LM, allowing us to maintain fast parallel decoding. Experiments on the LibriSpeech dataset demonstrate the effectiveness of our approach in enhancing greedy decoding with connectionist temporal classification (CTC).",
        "comments": "Accepted at ICASSP 2024",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11700"
    },
    {
        "doc_id": 438,
        "title": "Emulation-based Stabilization for Networked Control Systems with Stochastic Channels",
        "authors": [
            "Wei Ren",
            "Wei Wang",
            "Zhuo-Rui Pan",
            "Xi-Ming Sun",
            "Andrew R. Teel",
            "Dragan Nesic"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "This paper studies the stabilization problem of networked control systems (NCSs) with random packet dropouts caused by stochastic channels. To describe the effects of stochastic channels on the information transmission, the transmission times are assumed to be deterministic, whereas the packet transmission is assumed to be random. We first propose a stochastic scheduling protocol to model random packet dropouts, and address the properties of the proposed stochastic scheduling protocol. The proposed scheduling protocol provides a unified modelling framework for a general class of random packet dropouts due to different stochastic channels. Next, the proposed scheduling protocol is embedded into the closed-loop system, which leads to a stochastic hybrid model for NCSs with random packet dropouts. Based on this stochastic hybrid model, we follow the emulation approach to establish sufficient conditions to guarantee uniform global asymptotical stability in probability. In particular, an upper bound on the maximally allowable transmission interval is derived explicitly for all stochastic protocols satisfying Lyapunov conditions that guarantee uniform global asymptotic stability in probability. Finally, two numerical examples are presented to demonstrate the derived results.",
        "comments": "12 pages, 4 figures, accepted",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11677"
    },
    {
        "doc_id": 439,
        "title": "Rethinking Cross-Attention for Infrared and Visible Image Fusion",
        "authors": [
            "Lihua Jian",
            "Songlei Xiong",
            "Han Yan",
            "Xiaoguang Niu",
            "Shaowu Wu",
            "Di Zhang"
        ],
        "subjects": [
            "Image and Video Processing"
        ],
        "abstract": "The salient information of an infrared image and the abundant texture of a visible image can be fused to obtain a comprehensive image. As can be known, the current fusion methods based on Transformer techniques for infrared and visible (IV) images have exhibited promising performance. However, the attention mechanism of the previous Transformer-based methods was prone to extract common information from source images without considering the discrepancy information, which limited fusion performance. In this paper, by reevaluating the cross-attention mechanism, we propose an alternate Transformer fusion network (ATFuse) to fuse IV images. Our ATFuse consists of one discrepancy information injection module (DIIM) and two alternate common information injection modules (ACIIM). The DIIM is designed by modifying the vanilla cross-attention mechanism, which can promote the extraction of the discrepancy information of the source images. Meanwhile, the ACIIM is devised by alternately using the vanilla cross-attention mechanism, which can fully mine common information and integrate long dependencies. Moreover, the successful training of ATFuse is facilitated by a proposed segmented pixel loss function, which provides a good trade-off for texture detail and salient structure preservation. The qualitative and quantitative results on public datasets indicate our ATFFuse is effective and superior compared to other state-of-the-art methods.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11675"
    },
    {
        "doc_id": 440,
        "title": "RTA-Former: Reverse Transformer Attention for Polyp Segmentation",
        "authors": [
            "Zhikai Li",
            "Murong Yi",
            "Ali Uneri",
            "Sihan Niu",
            "Craig Jones"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "Polyp segmentation is a key aspect of colorectal cancer prevention, enabling early detection and guiding subsequent treatments. Intelligent diagnostic tools, including deep learning solutions, are widely explored to streamline and potentially automate this process. However, even with many powerful network architectures, there still comes the problem of producing accurate edge segmentation. In this paper, we introduce a novel network, namely RTA-Former, that employs a transformer model as the encoder backbone and innovatively adapts Reverse Attention (RA) with a transformer stage in the decoder for enhanced edge segmentation. The results of the experiments illustrate that RTA-Former achieves state-of-the-art (SOTA) performance in five polyp segmentation datasets. The strong capability of RTA-Former holds promise in improving the accuracy of Transformer-based polyp segmentation, potentially leading to better clinical decisions and patient outcomes. Our code will be publicly available on GitHub.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11671"
    },
    {
        "doc_id": 441,
        "title": "Streaming Bilingual End-to-End ASR model using Attention over Multiple Softmax",
        "authors": [
            "Aditya Patil",
            "Vikas Joshi",
            "Purvi Agrawal",
            "Rupesh Mehta"
        ],
        "subjects": [
            "Audio and Speech Processing",
            "Computation and Language",
            "Sound"
        ],
        "abstract": "Even with several advancements in multilingual modeling, it is challenging to recognize multiple languages using a single neural model, without knowing the input language and most multilingual models assume the availability of the input language. In this work, we propose a novel bilingual end-to-end (E2E) modeling approach, where a single neural model can recognize both languages and also support switching between the languages, without any language input from the user. The proposed model has shared encoder and prediction networks, with language-specific joint networks that are combined via a self-attention mechanism. As the language-specific posteriors are combined, it produces a single posterior probability over all the output symbols, enabling a single beam search decoding and also allowing dynamic switching between the languages. The proposed approach outperforms the conventional bilingual baseline with 13.3%, 8.23% and 1.3% word error rate relative reduction on Hindi, English and code-mixed test sets, respectively.",
        "comments": "Published in IEEE's Spoken Language Technology (SLT) 2022, 8 pages (6 + 2 for references), 5 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11645"
    },
    {
        "doc_id": 442,
        "title": "Optimizing performance in elastic optical networks using advanced reconfigurable optical add-drop multiplexers: A novel design approach and comprehensive analysis",
        "authors": [
            "Faranak Khosravi",
            "Mehdi Shadaram"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "Network operators diversify service offerings and enhance network efficiency by leveraging bandwidth-variable transceivers and colorless flexible-grid reconfigurable optical add-drop multiplexers (ROADMs). Nonetheless, the paradigm shift from rigid to elastic optical networks (EONs) has affected several key parameters, including bit rate, center frequency spacing, modulation format, and optical reach. This study investigated the transformative impact of emerging technologies on the design and structure of optical network architectures, including spectrally efficient multicarrier systems and bandwidth-variable wavelength-selective switches. A cost-effective ROADM architecture applying an order-based connecting approach was introduced, which presented a high connectivity level and a blockage probability of less than 10-4. When this architecture was implemented in the EON, the data transportation rate was 1 Tb/s. This outcome successfully accommodated a 20 % surge in traffic demand, while the optimized network architecture significantly improved fiber utilization by 3.4 %. Consequently, this study contributed a practical and efficient solution for implementing flexible optical networks, effectively addressing current concerns and propelling the optical communication system sector forward.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11640"
    },
    {
        "doc_id": 443,
        "title": "Reframing Offline Reinforcement Learning as a Regression Problem",
        "authors": [
            "Prajwal Koirala",
            "Cody Fleming"
        ],
        "subjects": [
            "Machine Learning",
            "Systems and Control"
        ],
        "abstract": "The study proposes the reformulation of offline reinforcement learning as a regression problem that can be solved with decision trees. Aiming to predict actions based on input states, return-to-go (RTG), and timestep information, we observe that with gradient-boosted trees, the agent training and inference are very fast, the former taking less than a minute. Despite the simplification inherent in this reformulated problem, our agent demonstrates performance that is at least on par with established methods. This assertion is validated by testing it across standard datasets associated with D4RL Gym-MuJoCo tasks. We further discuss the agent's ability to generalize by testing it on two extreme cases, how it learns to model the return distributions effectively even with highly skewed expert datasets, and how it exhibits robust performance in scenarios with sparse/delayed rewards.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11630"
    },
    {
        "doc_id": 444,
        "title": "Real-Time Systems Optimization with Black-box Constraints and Hybrid Variables",
        "authors": [
            "Sen Wang",
            "Dong Li",
            "Shao-Yu Huang",
            "Xuanliang Deng",
            "Ashrarul H. Sifat",
            "Changhee Jung",
            "Ryan Williams",
            "Haibo Zeng"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "When optimizing real-time systems, designers often face a challenging problem where the schedulability constraints are non-convex, non-continuous, or lack an analytical form to understand their properties. Although the optimization framework NORTH proposed in previous work is general (it works with arbitrary schedulability analysis) and scalable, it can only handle problems with continuous variables, which limits its application. In this paper, we extend the applications of the framework NORTH to problems with a hybrid of continuous and discrete variables. This is achieved in a coordinate-descent method, where the continuous and discrete variables are optimized separately during iterations. The new framework, NORTH+, improves around 20% solution quality than NORTH in experiments.",
        "comments": "Workshop on OPtimization for Embedded and ReAl-time systems (OPERA 2023) co-located with the 44th IEEE Real-Time Systems Symposium (RTSS)",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11620"
    },
    {
        "doc_id": 445,
        "title": "Another Way to the Top: Exploit Contextual Clustering in Learned Image Coding",
        "authors": [
            "Yichi Zhang",
            "Zhihao Duan",
            "Ming Lu",
            "Dandan Ding",
            "Fengqing Zhu",
            "Zhan Ma"
        ],
        "subjects": [
            "Image and Video Processing"
        ],
        "abstract": "While convolution and self-attention are extensively used in learned image compression (LIC) for transform coding, this paper proposes an alternative called Contextual Clustering based LIC (CLIC) which primarily relies on clustering operations and local attention for correlation characterization and compact representation of an image. As seen, CLIC expands the receptive field into the entire image for intra-cluster feature aggregation. Afterward, features are reordered to their original spatial positions to pass through the local attention units for inter-cluster embedding. Additionally, we introduce the Guided Post-Quantization Filtering (GuidedPQF) into CLIC, effectively mitigating the propagation and accumulation of quantization errors at the initial decoding stage. Extensive experiments demonstrate the superior performance of CLIC over state-of-the-art works: when optimized using MSE, it outperforms VVC by about 10% BD-Rate in three widely-used benchmark datasets; when optimized using MS-SSIM, it saves more than 50% BD-Rate over VVC. Our CLIC offers a new way to generate compact representations for image compression, which also provides a novel direction along the line of LIC development.",
        "comments": "The 38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024)",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11615"
    },
    {
        "doc_id": 446,
        "title": "$\\texttt{immrax}$: A Parallelizable and Differentiable Toolbox for Interval Analysis and Mixed Monotone Reachability in JAX",
        "authors": [
            "Akash Harapanahalli",
            "Saber Jafarpour",
            "Samuel Coogan"
        ],
        "subjects": [
            "Systems and Control",
            "Machine Learning",
            "Optimization and Control"
        ],
        "abstract": "We present an implementation of interval analysis and mixed monotone interval reachability analysis as function transforms in Python, fully composable with the computational framework JAX. The resulting toolbox inherits several key features from JAX, including computational efficiency through Just-In-Time Compilation, GPU acceleration for quick parallelized computations, and Automatic Differentiability. We demonstrate the toolbox's performance on several case studies, including a reachability problem on a vehicle model controlled by a neural network, and a robust closed-loop optimal control problem for a swinging pendulum.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11608"
    },
    {
        "doc_id": 447,
        "title": "Thermal Image Calibration and Correction using Unpaired Cycle-Consistent Adversarial Networks",
        "authors": [
            "Hossein Rajoli",
            "Pouya Afshin",
            "Fatemeh Afghah"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning",
            "Image and Video Processing"
        ],
        "abstract": "Unmanned aerial vehicles (UAVs) offer a flexible and cost-effective solution for wildfire monitoring. However, their widespread deployment during wildfires has been hindered by a lack of operational guidelines and concerns about potential interference with aircraft systems. Consequently, the progress in developing deep-learning models for wildfire detection and characterization using aerial images is constrained by the limited availability, size, and quality of existing datasets. This paper introduces a solution aimed at enhancing the quality of current aerial wildfire datasets to align with advancements in camera technology. The proposed approach offers a solution to create a comprehensive, standardized large-scale image dataset. This paper presents a pipeline based on CycleGAN to enhance wildfire datasets and a novel fusion method that integrates paired RGB images as attribute conditioning in the generators of both directions, improving the accuracy of the generated images.",
        "comments": "This paper has been accepted at the Asilomar 2023 Conference and will be published",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11582"
    },
    {
        "doc_id": 448,
        "title": "Deterministic Multi-stage Constellation Reconfiguration Using Integer Linear Programing and Sequential Decision-Making Methods",
        "authors": [
            "Hang Woon Lee",
            "David O. Williams Rogers",
            "Brycen D. Pearl",
            "Hao Chen",
            "Koki Ho"
        ],
        "subjects": [
            "Optimization and Control",
            "Systems and Control"
        ],
        "abstract": "In this paper, we address the problem of reconfiguring Earth observation satellite constellation systems through multiple stages. The Multi-stage Constellation Reconfiguration Problem (MCRP) aims to maximize the total observation rewards obtained by covering a set of targets of interest through the active manipulation of the orbits and relative phasing of constituent satellites. In this paper, we consider deterministic problem settings in which the targets of interest are known a priori. We propose a novel integer linear programming formulation for MCRP, capable of obtaining provably optimal solutions. To overcome computational intractability due to the combinatorial explosion in solving large-scale instances, we introduce two computationally efficient sequential decision-making methods based on the principles of a myopic policy and a rolling horizon procedure. The computational experiments demonstrate that the devised sequential decision-making approaches yield high-quality solutions with improved computational efficiency over the baseline MCRP. Finally, a case study using Hurricane Harvey data showcases the advantages of multi-stage constellation reconfiguration over single-stage and no-reconfiguration scenarios.",
        "comments": "37 pages, 13 figures, submitted to the Journal of Spacecraft and Rockets",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11567"
    },
    {
        "doc_id": 449,
        "title": "Nigel -- Mechatronic Design and Robust Sim2Real Control of an Over-Actuated Autonomous Vehicle",
        "authors": [
            "Chinmay Vilas Samak",
            "Tanmay Vilas Samak",
            "Javad Mohammadpour Velni",
            "Venkat Narayan Krovi"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "Simulation to reality (sim2real) transfer from a dynamics and controls perspective usually involves re-tuning or adapting the designed algorithms to suit real-world operating conditions, which often violates the performance guarantees established originally. This work presents a generalizable framework for achieving reliable sim2real transfer of autonomy-oriented control systems using multi-model multi-objective robust optimal control synthesis, which lends well to uncertainty handling and disturbance rejection with theoretical guarantees. Particularly, this work is centered around an actuation-redundant scaled autonomous vehicle called Nigel, with independent all-wheel drive and independent all-wheel steering architecture, whose enhanced configuration space bodes well for robust control applications. To this end, we present a systematic study on the complete mechatronic design, dynamics modeling, parameter identification, and robust stabilizing as well as steady-state tracking control of Nigel using the proposed framework, with experimental validation.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11542"
    },
    {
        "doc_id": 450,
        "title": "Model Predictive Approach for Detumbling an Underactuated Satellite",
        "authors": [
            "Kota Kondo",
            "Yasuhiro Yoshimura",
            "Mai Bando",
            "Shuji Nagasaki",
            "Toshiya Hanada"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "This research proposes an innovative approach to detumble satellites' triple-axis angular velocities with only one single-axis magnetic torquer. Since magnetic torque is generated perpendicularly to magnetorquers, no intended control torque along the magnetorquer can be produced, which makes systems underactuated. Our paper introduces a control method using Model Predictive Control (MPC) and compares it with B-dot control algorithm. By applying these control laws to Kyushu University Light Curve Inversion (Q-Li) Demonstration Satellite in numerical simulations, we describe the applicability of these control laws to underactuated systems.",
        "comments": "15 pages, 4 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11539"
    },
    {
        "doc_id": 451,
        "title": "Maintenance cost assessment for heterogeneous multi-component systems incorporating perfect inspections and waiting time to maintenance",
        "authors": [
            "Luc\u00eda Bautista",
            "Inma T. Castro",
            "Luis Landesa"
        ],
        "subjects": [
            "Systems and Control",
            "Probability"
        ],
        "abstract": "Most existing research about complex systems maintenance assumes they consist of the same type of components. However, systems can be assembled with heterogeneous components (for example degrading and non-degrading components) that require different maintenance actions. Since industrial systems become more and more complex, more research about the maintenance of systems with heterogeneous components is needed. For this reason, in this paper, a system consisting of two groups of components: degrading and non-degrading components is analyzed. The main novelty of this paper is the evaluation of a maintenance policy at system-level coordinating condition-based maintenance for the degrading components, delay time to the maintenance and an inspection strategy for this heterogeneous system. To that end, an analytic cost model is built using the semi-regenerative processes theory. Furthermore, a safety constraint related to the reliability of the degrading components is imposed. To find the optimal maintenance strategy, meta-heuristic algorithms are used.",
        "comments": "27 pages, 4 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11538"
    },
    {
        "doc_id": 452,
        "title": "Nonlinear Model Predictive Detumbling of Small Satellites with a Single-axis Magnetorquer",
        "authors": [
            "Kota Kondo",
            "Ilya Kolmanovsky",
            "Yasuhiro Yoshimura",
            "Mai Bando",
            "Shuji Nagasaki",
            "Toshiya Hanada"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "Various actuators are used in spacecraft to achieve attitude stabilization, including thrusters, momentum wheels, and control moment gyros. Small satellites, however, have stringent size, weight, and cost constraints, which makes many actuator choices prohibitive. Consequently, magnetic torquers have commonly been applied to spacecraft to attenuate angular rates. Approaches for dealing with under-actuation due to magnetic control torque's dependency on the magnetic field and required high magnetic flux densities have been previously considered. Generally speaking, control of a satellite that becomes under-actuated as a result of on-board failures has been a recurrent theme in the literature. Methods for controlling spacecraft with fewer actuators than degrees of freedom are increasingly in demand due to the increased number of small satellite launches. Magnetic torquers have been extensively investigated for momentum management of spacecraft with momentum wheels and for nutation damping of spin satellites, momentum-biased, and dual-spin satellites. Nonetheless, severely under-actuated small spacecraft that carry only a single-axis magnetic torquer have not been previously treated. This note considers the detumbling of a small spacecraft using only a single-axis magnetic torquer. Even with a three-axis magnetic torquer, the spacecraft is under-actuated, while, in the case of only a single axis magnetic torquer, the problem is considerably more demanding. Our note examines the feasibility of spacecraft attitude control with a single-axis magnetic torquer and possible control methods that can be used.",
        "comments": "20 pages, 6 figures. Journal of Guidance, Control, and Dynamics (2021)",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11536"
    },
    {
        "doc_id": 453,
        "title": "Pulse Width Modulation Method Applied to Nonlinear Model Predictive Control on an Under-actuated Small Satellite",
        "authors": [
            "Kota Kondo",
            "Yasuhiro Yoshimura",
            "Shiji Nagasaki",
            "Toshiya Hanada"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "Among various satellite actuators, magnetic torquers have been widely equipped for stabilization and attitude control of small satellites. Although magnetorquers are generally used with other actuators, such as momentum wheels, this paper explores a control method where only a magnetic actuation is available. We applied a nonlinear optimal control method, Nonlinear Model Predictive Control (NMPC), to small satellites, employing the generalized minimal residual (GMRES) method, which generates continuous control inputs. Onboard magnetic actuation systems often find it challenging to produce smooth magnetic moments as a control input; hence, we employ the Pulse Width Modulation (PWM) method, which discretizes a control input and reduces the burden on actuators. In our case, the PWM approach discretizes control torques generated by the NMPC scheme. This study's main contributions are investigating the NMPC and the GMRES method applied to small spacecraft and presenting the PWM control system's feasibility.",
        "comments": "19 pages, 10 figures. In AIAA Scitech 2021 Forum",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11533"
    },
    {
        "doc_id": 454,
        "title": "CaBuAr: California Burned Areas dataset for delineation",
        "authors": [
            "Daniele Rege Cambrin",
            "Luca Colomba",
            "Paolo Garza"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning",
            "Image and Video Processing"
        ],
        "abstract": "Forest wildfires represent one of the catastrophic events that, over the last decades, caused huge environmental and humanitarian damages. In addition to a significant amount of carbon dioxide emission, they are a source of risk to society in both short-term (e.g., temporary city evacuation due to fire) and long-term (e.g., higher risks of landslides) cases. Consequently, the availability of tools to support local authorities in automatically identifying burned areas plays an important role in the continuous monitoring requirement to alleviate the aftereffects of such catastrophic events. The great availability of satellite acquisitions coupled with computer vision techniques represents an important step in developing such tools. This paper introduces a novel open dataset that tackles the burned area delineation problem, a binary segmentation problem applied to satellite imagery. The presented resource consists of pre- and post-fire Sentinel-2 L2A acquisitions of California forest fires that took place starting in 2015. Raster annotations were generated from the data released by California's Department of Forestry and Fire Protection. Moreover, in conjunction with the dataset, we release three different baselines based on spectral indexes analyses, SegFormer, and U-Net models.",
        "comments": "Accepted at the IEEE Geoscience and Remote Sensing Magazine",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11519"
    },
    {
        "doc_id": 455,
        "title": "Integration of Large Language Models in Control of EHD Pumps for Precise Color Synthesis",
        "authors": [
            "Yanhong Peng",
            "Ceng Zhang",
            "Chenlong Hu",
            "Zebing Mao"
        ],
        "subjects": [
            "Robotics",
            "Artificial Intelligence",
            "Systems and Control"
        ],
        "abstract": "This paper presents an innovative approach to integrating Large Language Models (LLMs) with Arduino-controlled Electrohydrodynamic (EHD) pumps for precise color synthesis in automation systems. We propose a novel framework that employs fine-tuned LLMs to interpret natural language commands and convert them into specific operational instructions for EHD pump control. This approach aims to enhance user interaction with complex hardware systems, making it more intuitive and efficient. The methodology involves four key steps: fine-tuning the language model with a dataset of color specifications and corresponding Arduino code, developing a natural language processing interface, translating user inputs into executable Arduino code, and controlling EHD pumps for accurate color mixing. Conceptual experiment results, based on theoretical assumptions, indicate a high potential for accurate color synthesis, efficient language model interpretation, and reliable EHD pump operation. This research extends the application of LLMs beyond text-based tasks, demonstrating their potential in industrial automation and control systems. While highlighting the limitations and the need for real-world testing, this study opens new avenues for AI applications in physical system control and sets a foundation for future advancements in AI-driven automation technologies.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11500"
    },
    {
        "doc_id": 456,
        "title": "HARDCORE: H-field and power loss estimation for arbitrary waveforms with residual, dilated convolutional neural networks in ferrite cores",
        "authors": [
            "Nikolas F\u00f6rster",
            "Wilhelm Kirchg\u00e4ssner",
            "Till Piepenbrock",
            "Oliver Schweins",
            "Oliver Wallscheid"
        ],
        "subjects": [
            "Systems and Control",
            "Machine Learning",
            "Applied Physics"
        ],
        "abstract": "The MagNet Challenge 2023 calls upon competitors to develop data-driven models for the material-specific, waveform-agnostic estimation of steady-state power losses in toroidal ferrite cores. The following HARDCORE (H-field and power loss estimation for Arbitrary waveforms with Residual, Dilated convolutional neural networks in ferrite COREs) approach shows that a residual convolutional neural network with physics-informed extensions can serve this task efficiently when trained on observational data beforehand. One key solution element is an intermediate model layer which first reconstructs the bh curve and then estimates the power losses based on the curve's area rendering the proposed topology physically interpretable. In addition, emphasis was placed on expert-based feature engineering and information-rich inputs in order to enable a lean model architecture. A model is trained from scratch for each material, while the topology remains the same. A Pareto-style trade-off between model size and estimation accuracy is demonstrated, which yields an optimum at as low as 1755 parameters and down to below 8\\,\\% for the 95-th percentile of the relative error for the worst-case material with sufficient samples.",
        "comments": "Competition submission version",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11488"
    },
    {
        "doc_id": 457,
        "title": "ColorVideoVDP: A visual difference predictor for image, video and display distortions",
        "authors": [
            "Rafal K. Mantiuk",
            "Param Hanji",
            "Maliha Ashraf",
            "Yuta Asano",
            "Alexandre Chapiro"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Graphics",
            "Image and Video Processing"
        ],
        "abstract": "ColorVideoVDP is a video and image quality metric that models spatial and temporal aspects of vision, for both luminance and color. The metric is built on novel psychophysical models of chromatic spatiotemporal contrast sensitivity and cross-channel contrast masking. It accounts for the viewing conditions, geometric, and photometric characteristics of the display. It was trained to predict common video streaming distortions (e.g. video compression, rescaling, and transmission errors), and also 8 new distortion types related to AR/VR displays (e.g. light source and waveguide non-uniformities). To address the latter application, we collected our novel XR-Display-Artifact-Video quality dataset (XR-DAVID), comprised of 336 distorted videos. Extensive testing on XR-DAVID, as well as several datasets from the literature, indicate a significant gain in prediction performance compared to existing metrics. ColorVideoVDP opens the doors to many novel applications which require the joint automated spatiotemporal assessment of luminance and color distortions, including video streaming, display specification and design, visual comparison of results, and perceptually-guided quality optimization.",
        "comments": "28 pages",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11485"
    },
    {
        "doc_id": 458,
        "title": "Distributed Traffic Signal Control of Interconnected Intersections: A Two-Lane Traffic Network Model",
        "authors": [
            "Xinfeng Ru",
            "Weiguo Xia",
            "Ting Bai"
        ],
        "subjects": [
            "Systems and Control",
            "Adaptation and Self-Organizing Systems"
        ],
        "abstract": "Practical and accurate traffic models play an important role in capturing real traffic dynamics and then in achieving effective control performance. This paper studies traffic signal control in a traffic network with multiple interconnected intersections, where the target is to balance the vehicle density on each lane by controlling the green times of each phase at every intersection. Different from traditional road-based modeling schemes, a two-lane intersection model is first proposed to model the flow propagation in a more accurate way. A distributed model predictive control (MPC) method is then presented to assign the green times. To enable the real-time feasibility of the proposed approach, the alternating direction method of multipliers (ADMM) is incorporated with the distributed MPC scheme for solving the problem. Finally, the simulation studies performed in VISSIM for a six-intersection traffic network in Dalian, China, show the effectiveness and characteristics of the proposed method.",
        "comments": "journal paper",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11483"
    },
    {
        "doc_id": 459,
        "title": "Battery-Free Sensor Array for Wireless Multi-Depth In-Situ Sensing",
        "authors": [
            "Hongzhi Guo",
            "Adam Kamrath"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "Underground in-situ sensing plays a vital role in precision agriculture and infrastructure monitoring. While existing sensing systems utilize wires to connect an array of sensors at various depths for spatial-temporal data collection, wireless underground sensor networks offer a cable-free alternative. However, these wireless sensors are typically battery-powered, necessitating periodic recharging or replacement. This paper proposes a battery-free sensor array which can be used for wireless multi-depth in-situ sensing. Utilizing Near Field Communication (NFC)-which can penetrate soil with negligible signal power loss-this sensor array can form a virtual magnetic waveguide, achieving long communication ranges. An analytical model has been developed to offer insights and determine optimal design parameters. Moreover, a prototype, constructed using off-the-shelf NFC sensors, was tested to validate the proposed concept. While this system is primarily designed for underground applications, it holds potential for other multi-depth in-situ sensing scenarios, including underwater environments.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11479"
    },
    {
        "doc_id": 460,
        "title": "Task-specific regularization loss towards model calibration for reliable lung cancer detection",
        "authors": [
            "Mehar Prateek Kalra",
            "Mansi Singhal",
            "Rohan Raju Dhanakashirur"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "Lung cancer is one of the significant causes of cancer-related deaths globally. Early detection and treatment improve the chances of survival. Traditionally CT scans have been used to extract the most significant lung infection information and diagnose cancer. This process is carried out manually by an expert radiologist. The imbalance in the radiologists-to-population ratio in a country like India implies significant work pressure on them and thus raises the need to automate a few of their responsibilities. The tendency of modern-day Deep Neural networks to make overconfident mistakes limit their usage to detect cancer. In this paper, we propose a new task-specific loss function to calibrate the neural network to reduce the risk of overconfident mistakes. We use the state-of-the-art Multi-class Difference in Confidence and Accuracy (MDCA) loss in conjunction with the proposed task-specific loss function to achieve the same. We also integrate post-hoc calibration by performing temperature scaling on top of the train-time calibrated model. We demonstrate 5.98% improvement in the Expected Calibration Error (ECE) and a 17.9% improvement in Maximum Calibration Error (MCE) as compared to the best-performing SOTA algorithm.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11464"
    },
    {
        "doc_id": 461,
        "title": "Energy Consumption Analysis for Continuous Phase Modulation in Smart-Grid Internet of Things of beyond 5G",
        "authors": [
            "Hongjian Gao",
            "Yang Lu",
            "Shaoshi Yang",
            "Jingsheng Tan",
            "Longlong Nie",
            "Xinyi Qu"
        ],
        "subjects": [
            "Signal Processing",
            "Networking and Internet Architecture"
        ],
        "abstract": "Wireless sensor network (WSN) underpinning the smart-grid Internet of Things (SG-IoT) has been a popular research topic in recent years due to its great potential for enabling a wide range of important applications. However, the energy consumption (EC) characteristic of sensor nodes is a key factor that affects the operational performance (e.g., lifetime of sensors) and the total cost of ownership of WSNs. In this paper, to find the modulation techniques suitable for WSNs, we investigate the EC characteristic of continuous phase modulation (CPM), which is an attractive modulation scheme candidate for WSNs because of its constant envelope property. We first develop an EC model for the sensor nodes of WSNs by considering the circuits and a typical communication protocol that relies on automatic repeat request (ARQ)-based retransmissions to ensure successful data delivery. Then, we use this model to analyze the EC characteristic of CPM under various configurations of modulation parameters. Furthermore, we compare the EC characteristic of CPM with that of other representative modulation schemes, such as offset quadrature phase-shift keying (OQPSK) and quadrature amplitude modulation (QAM), which are commonly used in communication protocols of WSNs. Our analysis and simulation results provide insights into the EC characteristics of multiple modulation schemes in the context of WSNs; thus, they are beneficial for designing energy-efficient SG-IoT in the beyond-5G (B5G) and the 6G era.",
        "comments": "7 figures, 2 tables",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11449"
    },
    {
        "doc_id": 462,
        "title": "Towards Non-Robocentric Dynamic Landing of Quadrotor UAVs",
        "authors": [
            "Li-Yu Lo",
            "Boyang Li",
            "Chih-Yung Wen",
            "Ching-Wei Chang"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "In this work, we propose a dynamic landing solution without the need for onboard exteroceptive sensors and an expensive computation unit, where all localization and control modules are carried out on the ground in a non-inertial frame. Our system starts with a relative state estimator of the aerial robot from the perspective of the landing platform, where the state tracking of the UAV is done through a set of onboard LED markers and an on-ground camera; the state is expressed geometrically on manifold, and is returned by Iterated Extended Kalman filter (IEKF) algorithm. Subsequently, a motion planning module is developed to guide the landing process, formulating it as a minimum jerk trajectory by applying the differential flatness property. Considering visibility and dynamic constraints, the problem is solved using quadratic programming, and the final motion primitive is expressed through piecewise polynomials. Through a series of experiments, the applicability of this approach is validated by successfully landing 18 cm x 18 cm quadrotor on a 43 cm x 43 cm platform, exhibiting performance comparable to conventional methods. Finally, we provide comprehensive hardware and software details to the research community for future reference.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11445"
    },
    {
        "doc_id": 463,
        "title": "IoT Cloud RAN Testbed for Ultra-Precise TDoA-based Localization in LPWANs",
        "authors": [
            "Thomas Maul",
            "Joerg Robert",
            "Sebastian Klob"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "There have been many research efforts in the area of localization in recent years. Especially within the Internet of Things (IoT), the knowledge of position information for individual components is of great interest, for example, in asset tracking, to name just one. However, many of these use cases require a high energy efficiency, making a GNSS-based approach infeasible. One promising candidate can be found in Low Power Wide Area Networks (LPWAN), which enable battery lifetimes of up to 20 years. However, no gold standard for localization exists for these types of networks. Our work proposes a testbed architecture that allows the investigation and development of localization algorithms within LPWA Networks. The concept is built on a Cloud Radio Access Network (CRAN) architecture that allows the streaming of IQ from remote base stations to a central processing unit. Furthermore, the architecture is expanded by a synchronization concept based on Signals of Opportunity (SoO) to enable the testbed for runtime-based positioning. Therefore, we propose a hardware concept consisting of antennas and a low-cost off-the-shelf software-defined radio (SDR)-based frontend architecture and a software framework using a hypertext transfer protocol (HTTP)-based server and client architecture. The proposed system is installed in an urban environment. Initial measurements are conducted, where it can be shown that the proposed architecture can be used for highly precise Time Difference of Arrival (TDoA) measurements, offering the possibility of time synchronization down to approximately 200 ps and frequency synchronization of 3 mHz.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11435"
    },
    {
        "doc_id": 464,
        "title": "Joint Downlink and Uplink Optimization for RIS-Aided FDD MIMO Communication Systems",
        "authors": [
            "Gyoseung Lee",
            "Hyeongtaek Lee",
            "Donghwan Kim",
            "Jaehoon Chung",
            "A. Lee. Swindlehurst",
            "Junil Choi"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "This paper investigates reconfigurable intelligent surface (RIS)-aided frequency division duplexing (FDD) communication systems. Since the downlink and uplink signals are simultaneously transmitted in FDD, the phase shifts at the RIS should be designed to support both transmissions. Considering a single-user multiple-input multiple-output system, we formulate a weighted sum-rate maximization problem to jointly maximize the downlink and uplink system performance. To tackle the non-convex optimization problem, we adopt an alternating optimization (AO) algorithm, in which two phase shift optimization techniques are developed to handle the unit-modulus constraints induced by the reflection coefficients at the RIS. The first technique exploits the manifold optimization-based algorithm, while the second uses a lower-complexity AO approach. Numerical results verify that the proposed techniques rapidly converge to local optima and significantly improve the overall system performance compared to existing benchmark schemes.",
        "comments": "Accepted to IEEE Transactions on Wireless Communications",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11429"
    },
    {
        "doc_id": 465,
        "title": "Joint UAV Deployment and Resource Allocation in THz-Assisted MEC-Enabled Integrated Space-Air-Ground Networks",
        "authors": [
            "Yan Kyaw Tun",
            "Gy\u00f6rgy D\u00e1n",
            "Yu Min Park",
            "Choong Seon Hong"
        ],
        "subjects": [
            "Networking and Internet Architecture",
            "Signal Processing"
        ],
        "abstract": "Multi-access edge computing (MEC)-enabled integrated space-air-ground (SAG) networks have drawn much attention recently, as they can provide communication and computing services to wireless devices in areas that lack terrestrial base stations (TBSs). Leveraging the ample bandwidth in the terahertz (THz) spectrum, in this paper, we propose MEC-enabled integrated SAG networks with collaboration among unmanned aerial vehicles (UAVs). We then formulate the problem of minimizing the energy consumption of devices and UAVs in the proposed MEC-enabled integrated SAG networks by optimizing tasks offloading decisions, THz sub-bands assignment, transmit power control, and UAVs deployment. The formulated problem is a mixed-integer nonlinear programming (MILP) problem with a non-convex structure, which is challenging to solve. We thus propose a block coordinate descent (BCD) approach to decompose the problem into four sub-problems: 1) device task offloading decision problem, 2) THz sub-band assignment and power control problem, 3) UAV deployment problem, and 4) UAV task offloading decision problem. We then propose to use a matching game, concave-convex procedure (CCP) method, successive convex approximation (SCA), and block successive upper-bound minimization (BSUM) approaches for solving the individual subproblems. Finally, extensive simulations are performed to demonstrate the effectiveness of our proposed algorithm.",
        "comments": "36 pages, 8 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11419"
    },
    {
        "doc_id": 466,
        "title": "Optimal detection of non-overlapping images via combinatorial auction",
        "authors": [
            "Simon Anuk",
            "Tamir Bendory",
            "Amichai Painsky"
        ],
        "subjects": [
            "Image and Video Processing",
            "Signal Processing",
            "Optimization and Control"
        ],
        "abstract": "This paper studies the classical problem of detecting the location of multiple image occurrences in a two-dimensional, noisy measurement. Assuming the image occurrences do not overlap, we formulate this task as a constrained maximum likelihood optimization problem. We show that the maximum likelihood estimator is equivalent to an instance of the winner determination problem from the field of combinatorial auction, and that the solution can be obtained by searching over a binary tree. We then design a pruning mechanism that significantly accelerates the runtime of the search. We demonstrate on simulations and electron microscopy data sets that the proposed algorithm provides accurate detection in challenging regimes of high noise levels and densely packed image occurrences.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11413"
    },
    {
        "doc_id": 467,
        "title": "Robust Beamforming for Downlink Multi-Cell Systems: A Bilevel Optimization Perspective",
        "authors": [
            "Xingdi Chen",
            "Yu Xiong",
            "Kai Yang"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "Utilization of inter-base station cooperation for information processing has shown great potential in enhancing the overall quality of communication services (QoS) in wireless communication networks. Nevertheless, such cooperations require the knowledge of channel state information (CSI) at base stations (BSs), which is assumed to be perfectly known. However, CSI errors are inevitable in practice which necessitates beamforming techniques that can achieve robust performance in the presence of channel estimation errors. Existing approaches relax the robust beamforming design problems into semidefinite programming (SDP), which can only achieve a solution that is far from being optimal. To this end, this paper views robust beamforming design problems from a bilevel optimization perspective. In particular, we focus on maximizing the worst-case weighted sum-rate (WSR) in the downlink multi-cell multi-user multiple-input single-output (MISO) system considering bounded CSI errors. We first reformulate this problem into a bilevel optimization problem and then develop an efficient algorithm based on the cutting plane method. A distributed optimization algorithm has also been developed to facilitate the parallel processing in practical settings. Numerical results are provided to confirm the effectiveness of the proposed algorithm in terms of performance and complexity, particularly in the presence of CSI uncertainties.",
        "comments": "accepted at AAAI2024",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11409"
    },
    {
        "doc_id": 468,
        "title": "Application of a Novel Model Reduction Technique to the Assessment of Boundedness/Stability of Some Delay Time-Varying Vector Nonlinear Systems",
        "authors": [
            "Mark A. Pinsky"
        ],
        "subjects": [
            "Systems and Control",
            "Differential Geometry"
        ],
        "abstract": "This paper develops a new approach to the assessment of the boundedness/stability of some vector nonlinear systems with delays and variable coefficients. The approach rests on the development of scalar counterparts to the original vector systems. We show that the solutions to these scalar auxiliary nonlinear equations with delay and variable coefficients bound from the above the norms of solutions to the original equations with the matched history functions. This prompts the assessment of the boundedness/stability traits of the vector systems through the abridged evaluation of the dynamics of their scalar counterparts. The latter task is achieved in effortless simulations or through the application of simplified analytical inferences. Consequently, we convey some novel boundedness/ stability criteria and estimate the radiuses of the balls imbedded in the boundedness/stability regions. Lastly, we authenticate our inferences in representative simulations that also measure their accuracy.",
        "comments": "17 pages",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11398"
    },
    {
        "doc_id": 469,
        "title": "Joint User Scheduling and Computing Resource Allocation Optimization in Asynchronous Mobile Edge Computing Networks",
        "authors": [
            "Yihan Cang",
            "Ming Chen",
            "Yijin Pan",
            "Zhaohui Yang",
            "Ye Hu",
            "Haijian Sun",
            "Mingzhe Chen"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "In this paper, the problem of joint user scheduling and computing resource allocation in asynchronous mobile edge computing (MEC) networks is studied. In such networks, edge devices will offload their computational tasks to an MEC server, using the energy they harvest from this server. To get their tasks processed on time using the harvested energy, edge devices will strategically schedule their task offloading, and compete for the computational resource at the MEC server. Then, the MEC server will execute these tasks asynchronously based on the arrival of the tasks. This joint user scheduling, time and computation resource allocation problem is posed as an optimization framework whose goal is to find the optimal scheduling and allocation strategy that minimizes the energy consumption of these mobile computing tasks. To solve this mixed-integer non-linear programming problem, the general benders decomposition method is adopted which decomposes the original problem into a primal problem and a master problem. Specifically, the primal problem is related to computation resource and time slot allocation, of which the optimal closed-form solution is obtained. The master problem regarding discrete user scheduling variables is constructed by adding optimality cuts or feasibility cuts according to whether the primal problem is feasible, which is a standard mixed-integer linear programming problem and can be efficiently solved. By iteratively solving the primal problem and master problem, the optimal scheduling and resource allocation scheme is obtained. Simulation results demonstrate that the proposed asynchronous computing framework reduces 87.17% energy consumption compared with conventional synchronous computing counterpart.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11377"
    },
    {
        "doc_id": 470,
        "title": "Self-supervised Contrastive Learning for 6G UM-MIMO THz Communications: Improving Robustness Under Imperfect CSI",
        "authors": [
            "Rafid Umayer Murshed",
            "Md Saheed Ullah",
            "Mohammad Saquib",
            "Moe Z. Win"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "This paper investigates the potential of contrastive learning in 6G ultra-massive multiple-input multiple-output (UM-MIMO) communication systems, specifically focusing on hybrid beamforming under imperfect channel state information (CSI) conditions at THz. UM-MIMO systems are promising for future 6G wireless communication networks due to their high spectral efficiency and capacity. The accuracy of CSI significantly influences the performance of UM-MIMO systems. However, acquiring perfect CSI is challenging due to various practical constraints such as channel estimation errors, feedback delays, and hardware imperfections. To address this issue, we propose a novel self-supervised contrastive learning-based approach for hybrid beamforming, which is robust against imperfect CSI. We demonstrate the power of contrastive learning to tackle the challenges posed by imperfect CSI and show that our proposed method results in improved system performance in terms of achievable rate compared to traditional methods.",
        "comments": "6 pages, 7 figures, Submitted to IEEE International Conference on Communications, 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11376"
    },
    {
        "doc_id": 471,
        "title": "Modeling Considerations for Developing Deep Space Autonomous Spacecraft and Simulators",
        "authors": [
            "Christopher Agia",
            "Guillem Casadesus Vila",
            "Saptarshi Bandyopadhyay",
            "David S. Bayard",
            "Kar-Ming Cheung",
            "Charles H. Lee",
            "Eric Wood",
            "Ian Aenishanslin",
            "Steven Ardito",
            "Lorraine Fesq",
            "Marco Pavone",
            "Issa A. D. Nesnas"
        ],
        "subjects": [
            "Robotics",
            "Systems and Control"
        ],
        "abstract": "To extend the limited scope of autonomy used in prior missions for operation in distant and complex environments, there is a need to further develop and mature autonomy that jointly reasons over multiple subsystems, which we term system-level autonomy. System-level autonomy establishes situational awareness that resolves conflicting information across subsystems, which may necessitate the refinement and interconnection of the underlying spacecraft and environment onboard models. However, with a limited understanding of the assumptions and tradeoffs of modeling to arbitrary extents, designing onboard models to support system-level capabilities presents a significant challenge.\n  In this paper, we provide a detailed analysis of the increasing levels of model fidelity for several key spacecraft subsystems, with the goal of informing future spacecraft functional- and system-level autonomy algorithms and the physics-based simulators on which they are validated. We do not argue for the adoption of a particular fidelity class of models but, instead, highlight the potential tradeoffs and opportunities associated with the use of models for onboard autonomy and in physics-based simulators at various fidelity levels. We ground our analysis in the context of deep space exploration of small bodies, an emerging frontier for autonomous spacecraft operation in space, where the choice of models employed onboard the spacecraft may determine mission success. We conduct our experiments in the Multi-Spacecraft Concept and Autonomy Tool (MuSCAT), a software suite for developing spacecraft autonomy algorithms.",
        "comments": "Project page: https://sites.google.com/stanford.edu/spacecraft-models. 20 pages, 8 figures. Accepted to the IEEE Conference on Aerospace (AeroConf) 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11371"
    },
    {
        "doc_id": 472,
        "title": "Self-sustaining Software Systems (S4): Towards Improved Interpretability and Adaptation",
        "authors": [
            "Christian Cabrera",
            "Andrei Paleyes",
            "Neil D. Lawrence"
        ],
        "subjects": [
            "Software Engineering",
            "Artificial Intelligence",
            "Systems and Control"
        ],
        "abstract": "Software systems impact society at different levels as they pervasively solve real-world problems. Modern software systems are often so sophisticated that their complexity exceeds the limits of human comprehension. These systems must respond to changing goals, dynamic data, unexpected failures, and security threats, among other variable factors in real-world environments. Systems' complexity challenges their interpretability and requires autonomous responses to dynamic changes. Two main research areas explore autonomous systems' responses: evolutionary computing and autonomic computing. Evolutionary computing focuses on software improvement based on iterative modifications to the source code. Autonomic computing focuses on optimising systems' performance by changing their structure, behaviour, or environment variables. Approaches from both areas rely on feedback loops that accumulate knowledge from the system interactions to inform autonomous decision-making. However, this knowledge is often limited, constraining the systems' interpretability and adaptability. This paper proposes a new concept for interpretable and adaptable software systems: self-sustaining software systems (S4). S4 builds knowledge loops between all available knowledge sources that define modern software systems to improve their interpretability and adaptability. This paper introduces and discusses the S4 concept.",
        "comments": "Accepted at The 1st International Workshop New Trends in Software Architecture (SATrends) 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11370"
    },
    {
        "doc_id": 473,
        "title": "A Fast Effective Greedy Approach for MU-MIMO Beam Selection in mm-Wave and THz Communications",
        "authors": [
            "Rafid Umayer Murshed",
            "Md Saheed Ullah",
            "Mohammad Saquib"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "This paper addresses the beam-selection challenges in Multi-User Multiple Input Multiple Output (MU-MIMO) beamforming for mm-wave and THz channels, focusing on the pivotal aspect of spectral efficiency (SE) and computational efficiency. We introduce a novel approach, the Greedy Interference-Optimized Singular Vector Beam-selection (G-IOSVB) algorithm, which offers a strategic balance between high SE and low computational complexity. Our study embarks on a comparative analysis of G-IOSVB against the traditional IOSVB and the exhaustive Singular-Vector Beamspace Search (SVBS) algorithms. The findings reveal that while SVBS achieves the highest SE, it incurs significant computational costs, approximately 162 seconds per channel realization. In contrast, G-IOSVB aligns closely with IOSVB in SE performance yet is markedly more computationally efficient. Heatmaps vividly demonstrate this efficiency, highlighting G-IOSVB's reduced computation time without sacrificing SE. We also delve into the mathematical intricacies of G-IOSVB, demonstrating its theoretical and practical superiority through rigorous expressions and detailed algorithmic analysis. The numerical results illustrate that G-IOSVB stands out as an efficient, practical solution for MU-MIMO systems, making it a promising candidate for high-speed, high-efficiency wireless communication networks.",
        "comments": "Accepted for Lecture presentation at the 58th Annual Conference on Information Sciences and Systems, to be held at Princeton University from March 13-15, 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11369"
    },
    {
        "doc_id": 474,
        "title": "Asynchronous Parallel Reinforcement Learning for Optimizing Propulsive Performance in Fin Ray Control",
        "authors": [
            "Xin-Yang Liu",
            "Dariush Bodaghi",
            "Qian Xue",
            "Xudong Zheng",
            "Jian-Xun Wang"
        ],
        "subjects": [
            "Fluid Dynamics",
            "Machine Learning",
            "Systems and Control"
        ],
        "abstract": "Fish fin rays constitute a sophisticated control system for ray-finned fish, facilitating versatile locomotion within complex fluid environments. Despite extensive research on the kinematics and hydrodynamics of fish locomotion, the intricate control strategies in fin-ray actuation remain largely unexplored. While deep reinforcement learning (DRL) has demonstrated potential in managing complex nonlinear dynamics; its trial-and-error nature limits its application to problems involving computationally demanding environmental interactions. This study introduces a cutting-edge off-policy DRL algorithm, interacting with a fluid-structure interaction (FSI) environment to acquire intricate fin-ray control strategies tailored for various propulsive performance objectives. To enhance training efficiency and enable scalable parallelism, an innovative asynchronous parallel training (APT) strategy is proposed, which fully decouples FSI environment interactions and policy/value network optimization. The results demonstrated the success of the proposed method in discovering optimal complex policies for fin-ray actuation control, resulting in a superior propulsive performance compared to the optimal sinusoidal actuation function identified through a parametric grid search. The merit and effectiveness of the APT approach are also showcased through comprehensive comparison with conventional DRL training strategies in numerical experiments of controlling nonlinear dynamics.",
        "comments": "37 pages, 12 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11349"
    },
    {
        "doc_id": 475,
        "title": "Decentralized Optimization in Networks with Arbitrary Delays",
        "authors": [
            "Tomas Ortega",
            "Hamid Jafarkhani"
        ],
        "subjects": [
            "Optimization and Control",
            "Multiagent Systems",
            "Signal Processing",
            "Systems and Control"
        ],
        "abstract": "We consider the problem of decentralized optimization in networks with communication delays. To accommodate delays, we need decentralized optimization algorithms that work on directed graphs. Existing approaches require nodes to know their out-degree to achieve convergence. We propose a novel gossip-based algorithm that circumvents this requirement, allowing decentralized optimization in networks with communication delays. We prove that our algorithm converges on non-convex objectives, with the same main complexity order term as centralized Stochastic Gradient Descent (SGD), and show that the graph topology and the delays only affect the higher order terms. We provide numerical simulations that illustrate our theoretical results.",
        "comments": "Accepted to IEEE ICC 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11344"
    },
    {
        "doc_id": 476,
        "title": "Error bounds of constant gain least-mean-squares algorithms",
        "authors": [
            "Chang Liu",
            "Antwan D. Clark"
        ],
        "subjects": [
            "Signal Processing",
            "Systems and Control"
        ],
        "abstract": "Constant gain least-mean-squares (LMS) algorithms have a wide range of applications in trajectory tracking problems, but the formal convergence of LMS in mean square is not yet fully established. This work provides an upper bound on the constant gain that guarantees a bounded mean-squared error of LMS for a general design vector. These results highlight the role of the fourth-order moment of the design vector. Numerical examples demonstrate the applicability of this upper bound in setting a constant gain in LMS, while existing criteria may fail. We also provide the associated error bound, which can be applied to design vectors with linearly dependent elements.",
        "comments": "6 pages",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11333"
    },
    {
        "doc_id": 477,
        "title": "A Hierarchical Decision-Based Maintenance for a Complex Modular System Driven by the { MoMA} Algorithm",
        "authors": [
            "M. L. Gamiz",
            "D. Montoro-Cazorla",
            "M. C. Segovia-Garcia"
        ],
        "subjects": [
            "Systems and Control",
            "Methodology"
        ],
        "abstract": "This paper presents a maintenance policy for a modular system formed by K independent modules (n-subsystems) subjected to environmental conditions (shocks). For the modeling of this complex system, the use of the Matrix-Analytical Method (MAM) is proposed under a layered approach according to its hierarchical structure. Thus, the operational state of the system (top layer) depends on the states of the modules (middle layer), which in turn depend on the states of their components (bottom layer). This allows a detailed description of the system operation to plan maintenance actions appropriately and optimally. We propose a hierarchical decision-based maintenance strategy with periodic inspections as follows: at the time of the inspection, the condition of the system is first evaluated. If intervention is necessary, the modules are then checked to make individual decisions based on their states, and so on. Replacement or repair will be carried out as appropriate. An optimization problem is formulated as a function of the length of the inspection period and the intervention cost incurred over the useful life of the system. Our method shows the advantages, providing compact and implementable expressions. The model is illustrated on a submarine Electrical Control Unit (ECU).",
        "comments": "43 pages, 6 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11328"
    },
    {
        "doc_id": 478,
        "title": "Weakly-Supervised Semantic Segmentation of Circular-Scan, Synthetic-Aperture-Sonar Imagery",
        "authors": [
            "Isaac J. Sledge",
            "Dominic M. Byrne",
            "Jonathan L. King",
            "Steven H. Ostertag",
            "Denton L. Woods",
            "James L. Prater",
            "Jermaine L. Kennedy",
            "Timothy M. Marston",
            "Jose C. Principe"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning",
            "Image and Video Processing"
        ],
        "abstract": "We propose a weakly-supervised framework for the semantic segmentation of circular-scan synthetic-aperture-sonar (CSAS) imagery. The first part of our framework is trained in a supervised manner, on image-level labels, to uncover a set of semi-sparse, spatially-discriminative regions in each image. The classification uncertainty of each region is then evaluated. Those areas with the lowest uncertainties are then chosen to be weakly labeled segmentation seeds, at the pixel level, for the second part of the framework. Each of the seed extents are progressively resized according to an unsupervised, information-theoretic loss with structured-prediction regularizers. This reshaping process uses multi-scale, adaptively-weighted features to delineate class-specific transitions in local image content. Content-addressable memories are inserted at various parts of our framework so that it can leverage features from previously seen images to improve segmentation performance for related images.\n  We evaluate our weakly-supervised framework using real-world CSAS imagery that contains over ten seafloor classes and ten target classes. We show that our framework performs comparably to nine fully-supervised deep networks. Our framework also outperforms eleven of the best weakly-supervised deep networks. We achieve state-of-the-art performance when pre-training on natural imagery. The average absolute performance gap to the next-best weakly-supervised network is well over ten percent for both natural imagery and sonar imagery. This gap is found to be statistically significant.",
        "comments": "Submitted to the IEEE Journal of Oceanic Engineering",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11313"
    },
    {
        "doc_id": 479,
        "title": "RoTIR: Rotation-Equivariant Network and Transformers for Fish Scale Image Registration",
        "authors": [
            "Ruixiong Wang",
            "Alin Achim",
            "Renata Raele-Rolfe",
            "Qiao Tong",
            "Dylan Bergen",
            "Chrissy Hammond",
            "Stephen Cross"
        ],
        "subjects": [
            "Image and Video Processing"
        ],
        "abstract": "Image registration is an essential process for aligning features of interest from multiple images. With the recent development of deep learning techniques, image registration approaches have advanced to a new level. In this work, we present 'Rotation-Equivariant network and Transformers for Image Registration' (RoTIR), a deep-learning-based method for the alignment of fish scale images captured by light microscopy. This approach overcomes the challenge of arbitrary rotation and translation detection, as well as the absence of ground truth data. We employ feature-matching approaches based on Transformers and general E(2)-equivariant steerable CNNs for model creation. Besides, an artificial training dataset is employed for semi-supervised learning. Results show RoTIR successfully achieves the goal of fish scale image registration.",
        "comments": "7 pages, 4 figures, 2 tables",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11270"
    },
    {
        "doc_id": 480,
        "title": "Word-Level ASR Quality Estimation for Efficient Corpus Sampling and Post-Editing through Analyzing Attentions of a Reference-Free Metric",
        "authors": [
            "Golara Javadi",
            "Kamer Ali Yuksel",
            "Yunsu Kim",
            "Thiago Castro Ferreira",
            "Mohamed Al-Badrashiny"
        ],
        "subjects": [
            "Computation and Language",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "In the realm of automatic speech recognition (ASR), the quest for models that not only perform with high accuracy but also offer transparency in their decision-making processes is crucial. The potential of quality estimation (QE) metrics is introduced and evaluated as a novel tool to enhance explainable artificial intelligence (XAI) in ASR systems. Through experiments and analyses, the capabilities of the NoRefER (No Reference Error Rate) metric are explored in identifying word-level errors to aid post-editors in refining ASR hypotheses. The investigation also extends to the utility of NoRefER in the corpus-building process, demonstrating its effectiveness in augmenting datasets with insightful annotations. The diagnostic aspects of NoRefER are examined, revealing its ability to provide valuable insights into model behaviors and decision patterns. This has proven beneficial for prioritizing hypotheses in post-editing workflows and fine-tuning ASR models. The findings suggest that NoRefER is not merely a tool for error detection but also a comprehensive framework for enhancing ASR systems' transparency, efficiency, and effectiveness. To ensure the reproducibility of the results, all source codes of this study are made publicly available.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11268"
    },
    {
        "doc_id": 481,
        "title": "AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking",
        "authors": [
            "Mehmet Y. Turali",
            "Mehmet E. Lorasdagi",
            "Ali T. Koc",
            "Suleyman S. Kozat"
        ],
        "subjects": [
            "Machine Learning",
            "Signal Processing",
            "Machine Learning"
        ],
        "abstract": "We study the problem of feature selection in general machine learning (ML) context, which is one of the most critical subjects in the field. Although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. To this end, we introduce the ``Adaptive Feature Selection with Binary Masking\" (AFS-BM) which remedies these problems. AFS-BM achieves this by joint optimization for simultaneous feature selection and model training. In particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. This approach leads to significant improvements in model accuracy and a reduction in computational requirements. We provide an extensive set of experiments where we compare AFS-BM with the established feature selection methods using well-known datasets from real-life competitions. Our results show that AFS-BM makes significant improvement in terms of accuracy and requires significantly less computational complexity. This is due to AFS-BM's ability to dynamically adjust to the changing importance of features during the training process, which an important contribution to the field. We openly share our code for the replicability of our results and to facilitate further research.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11250"
    },
    {
        "doc_id": 482,
        "title": "Hierarchical Cell-Free Massive MIMO for High Capacity with Simple Implementation",
        "authors": [
            "Wei Jiang",
            "Hans D. Schotten"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "Cell-free massive multi-input multi-output (MIMO) has recently gained much attention for its potential in shaping the landscape of sixth-generation (6G) wireless systems. This paper proposes a hierarchical network architecture tailored for cell-free massive MIMO, seamlessly integrating co-located and distributed antennas. A central base station (CBS), equipped with an antenna array, positions itself near the center of the coverage area, complemented by distributed access points spanning the periphery. The proposed architecture remarkably outperforms conventional cell-free networks, demonstrating superior sum throughput while maintaining a comparable worst-case per-user spectral efficiency. Meanwhile, the implementation cost associated with the fronthaul network is substantially diminished.",
        "comments": "2024 IEEE International Conference on Communications (ICC-2024)",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11236"
    },
    {
        "doc_id": 483,
        "title": "Susceptibility of Adversarial Attack on Medical Image Segmentation Models",
        "authors": [
            "Zhongxuan Wang",
            "Leo Xu"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "The nature of deep neural networks has given rise to a variety of attacks, but little work has been done to address the effect of adversarial attacks on segmentation models trained on MRI datasets. In light of the grave consequences that such attacks could cause, we explore four models from the U-Net family and examine their responses to the Fast Gradient Sign Method (FGSM) attack. We conduct FGSM attacks on each of them and experiment with various schemes to conduct the attacks. In this paper, we find that medical imaging segmentation models are indeed vulnerable to adversarial attacks and that there is a negligible correlation between parameter size and adversarial attack success. Furthermore, we show that using a different loss function than the one used for training yields higher adversarial attack success, contrary to what the FGSM authors suggested. In future efforts, we will conduct the experiments detailed in this paper with more segmentation models and different attacks. We will also attempt to find ways to counteract the attacks by using model ensembles or special data augmentations. Our code is available at https://github.com/ZhongxuanWang/adv_attk",
        "comments": "6 pages, 8 figures, presented at 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI) conference",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11224"
    },
    {
        "doc_id": 484,
        "title": "3D Receiver for Molecular Communications in Internet of Organoids",
        "authors": [
            "Shaojie Zhang",
            "Ozgur B. Akan"
        ],
        "subjects": [
            "Emerging Technologies",
            "Systems and Control"
        ],
        "abstract": "Organoids have garnered attention due to their effectiveness in modeling the 3D structure of organ interactions. However, the communication engineering perspective has received relatively little attention. One way to achieve organoids communication is molecular communication (MC). Molecular communication is a bio-inspired communication paradigm that uses molecules as information carriers. It is considered one of the most promising methods for enabling the Internet of Nano-Things (IoNT) and nanonetworks. BioFETs are commonly used to implement practical MC receivers. However, most previous analyses have focused on a planar device, neglecting considerations like the threshold voltage and its potential 3D structure. This paper introduces the first FinFET-based MC receiver that covers both the top and side gates with receptors. Both binding noise and flicker noise are considered in the analysis. The performance, in terms of signal-to-noise ratio (SNR) and symbol error probability (SEP), is compared with that of the 2D receiver.",
        "comments": "10 pages, 11 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11214"
    },
    {
        "doc_id": 485,
        "title": "Joint Beamforming Optimization and Mode Selection for RDARS-aided MIMO Systems",
        "authors": [
            "Jintao Wang",
            "Chengzhi Ma",
            "Shiqi Gong",
            "Xi Yang",
            "Shaodan Ma"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "Considering the appealing distribution gains of distributed antenna systems (DAS) and passive gains of reconfigurable intelligent surface (RIS), a flexible reconfigurable architecture called reconfigurable distributed antenna and reflecting surface (RDARS) is proposed. RDARS encompasses DAS and RIS as two special cases and maintains the advantages of distributed antennas while reducing the hardware cost by replacing some active antennas with low-cost passive reflecting surfaces. In this paper, we present a RDARS-aided uplink multi-user communication system and investigate the system transmission reliability with the newly proposed architecture. Specifically, in addition to the distribution gain and the reflection gain provided by the connection and reflection modes, respectively, we also consider the dynamic mode switching of each element which introduces an additional degree of freedom (DoF) and thus results in a selection gain. As such, we aim to minimize the total sum mean-square-error (MSE) of all data streams by jointly optimizing the receive beamforming matrix, the reflection phase shifts and the channel-aware placement of elements in the connection mode. To tackle this nonconvex problem with intractable binary and cardinality constraints, we propose an inexact block coordinate descent (BCD) based penalty dual decomposition (PDD) algorithm with the guaranteed convergence. Since the PDD algorithm usually suffers from high computational complexity, a low-complexity greedy-search-based alternating optimization (AO) algorithm is developed to yield a semi-closed-form solution with acceptable performance. Numerical results demonstrate the superiority of the proposed architecture compared to the conventional fully passive RIS or DAS. Furthermore, some insights about the practical implementation of RDARS are provided.",
        "comments": "13 pages, 9 figures. This paper has been submitted to IEEE journal for possible publication",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11205"
    },
    {
        "doc_id": 486,
        "title": "Transversally exponentially stable Euclidean space extension technique for discrete time systems",
        "authors": [
            "Soham Shanbhag",
            "Dong Eui Chang"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "We propose a modification technique for discrete time systems for exponentially fast convergence to compact sets. The extension technique allows us to use tools defined on Euclidean spaces to systems evolving on manifolds by modifying the dynamics of the system such that the manifold is an attractor set. We show the stability properties of this technique using the simulation of the rigid body rotation system on the unit sphere $S^3$. We also show the improvement afforded due to this technique on a Luenberger like observer designed for the rigid body rotation system on $S^3$.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11200"
    },
    {
        "doc_id": 487,
        "title": "Projected Belief Networks With Discriminative Alignment for Acoustic Event Classification: Rivaling State of the Art CNNs",
        "authors": [
            "Paul M. Baggenstoss",
            "Kevin Wilkinghoff",
            "Felix Govaers",
            "Frank Kurth"
        ],
        "subjects": [
            "Machine Learning",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "The projected belief network (PBN) is a generative stochastic network with tractable likelihood function based on a feed-forward neural network (FFNN). The generative function operates by \"backing up\" through the FFNN. The PBN is two networks in one, a FFNN that operates in the forward direction, and a generative network that operates in the backward direction. Both networks co-exist based on the same parameter set, have their own cost functions, and can be separately or jointly trained. The PBN therefore has the potential to possess the best qualities of both discriminative and generative classifiers. To realize this potential, a separate PBN is trained on each class, maximizing the generative likelihood function for the given class, while minimizing the discriminative cost for the FFNN against \"all other classes\". This technique, called discriminative alignment (PBN-DA), aligns the contours of the likelihood function to the decision boundaries and attains vastly improved classification performance, rivaling that of state of the art discriminative networks. The method may be further improved using a hidden Markov model (HMM) as a component of the PBN, called PBN-DA-HMM. This paper provides a comprehensive treatment of PBN, PBN-DA, and PBN-DA-HMM. In addition, the results of two new classification experiments are provided. The first experiment uses air-acoustic events, and the second uses underwater acoustic data consisting of marine mammal calls. In both experiments, PBN-DA-HMM attains comparable or better performance as a state of the art CNN, and attain a factor of two error reduction when combined with the CNN.",
        "comments": "15 Pages. Submitted to IEEE-TNNLS",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11199"
    },
    {
        "doc_id": 488,
        "title": "Machine learning based state observer for discrete time systems evolving on Lie groups",
        "authors": [
            "Soham Shanbhag",
            "Dong Eui Chang"
        ],
        "subjects": [
            "Systems and Control",
            "Machine Learning"
        ],
        "abstract": "In this paper, a machine learning based observer for systems evolving on manifolds is designed such that the state of the observer is restricted to the Lie group on which the system evolves. Conventional techniques involving machine learning based observers on systems evolving on Lie groups involve designing charts for the Lie group, training a machine learning based observer for each chart, and switching between the trained models based on the state of the system. We propose a novel deep learning based technique whose predictions are restricted to a measure 0 subset of Euclidean space without using charts. Using this network, we design an observer ensuring that the state of the observer is restricted to the Lie group, and predicting the state using only one trained algorithm. The deep learning network predicts an ``error term'' on the Lie algebra of the Lie group, uses the map from the Lie algebra to the group, and uses the group action and the present state to estimate the state at the next epoch. This model being purely data driven does not require the model of the system. The proposed algorithm provides a novel framework for constraining the output of machine learning networks to a measure 0 subset of a Euclidean space without chart specific training and without requiring switching. We show the validity of this method using Monte Carlo simulations performed of the rigid body rotation and translation system.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11196"
    },
    {
        "doc_id": 489,
        "title": "Triple-Refined Hybrid-Field Beam Training for mmWave Extremely Large-Scale MIMO",
        "authors": [
            "Kangjian Chen",
            "Chenhao Qi",
            "Octavia A. Dobre",
            "Geoffrey Ye Li"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "This paper investigates beam training for extremely large-scale multiple-input multiple-output systems. By considering both the near field and far field, a triple-refined hybrid-field beam training scheme is proposed, where high-accuracy estimates of channel parameters are obtained through three steps of progressive beam refinement. First, the hybrid-field beam gain (HFBG)-based first refinement method is developed. Based on the analysis of the HFBG, the first-refinement codebook is designed and the beam training is performed accordingly to narrow down the potential region of the channel path. Then, the maximum likelihood (ML)-based and principle of stationary phase (PSP)-based second refinement methods are developed. By exploiting the measurements of the beam training, the ML is used to estimate the channel parameters. To avoid the high computational complexity of ML, closed-form estimates of the channel parameters are derived according to the PSP. Moreover, the Gaussian approximation (GA)-based third refinement method is developed. The hybrid-field neighboring search is first performed to identify the potential region of the main lobe of the channel steering vector. Afterwards, by applying the GA, a least-squares estimator is developed to obtain the high-accuracy channel parameter estimation. Simulation results verify the effectiveness of the proposed scheme.",
        "comments": "Journal ref:        IEEE Transactions on Wireless Communications, 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11195"
    },
    {
        "doc_id": 490,
        "title": "Angular velocity and linear acceleration measurement bias estimators for the rigid body system with global exponential convergence",
        "authors": [
            "Soham Shanbhag",
            "Dong Eui Chang"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "Rigid body systems usually consider measurements of the pose of the body using onboard cameras/LiDAR systems, that of linear acceleration using an accelerometer and of angular velocity using an IMU. However, the measurements of the linear acceleration and angular velocity are usually biased with an unknown constant or slowly varying bias. We propose a measurement bias estimator for such systems under assumption of boundedness of angular velocity. We also provide continuous estimates to the state of the system, i.e. the pose, linear velocity, and position of the body. These estimates are globally exponentially convergent to the state of the rigid body system. We propose two bias estimators designed with the estimate of the pose in the ambient Euclidean space of the Special Euclidean group and show global exponential convergence of the proposed observers to the state of the system. The first observer assumes knowledge of bounds of the angular velocity, while the second observer uses a Riccati observer to overcome this limitation. We show the convergence with an example of a rigid body rotation and translation system on the special Euclidean group. We show that the observer is able to estimate the bias using data collected from an Intel Realsense camera.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11191"
    },
    {
        "doc_id": 491,
        "title": "Globally exponentially convergent observer for systems evolving on matrix Lie groups",
        "authors": [
            "Soham Shanbhag",
            "Dong Eui Chang"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "We propose a globally exponentially convergent observer for the dynamical system evolving on matrix Lie groups with bounded velocity with unknown bound. We design the observer in the ambient Euclidean space and show exponential convergence of the observer to the state of the system. We show the convergence with an example of a rigid body rotation and translation system on the special Euclidean group. We compare the proposed observer with an observer present in the literature.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11189"
    },
    {
        "doc_id": 492,
        "title": "Predictive stability filters for nonlinear dynamical systems affected by disturbances",
        "authors": [
            "Alexandre Didier",
            "Andrea Zanelli",
            "Kim P. Wabersich",
            "Melanie N. Zeilinger"
        ],
        "subjects": [
            "Systems and Control",
            "Optimization and Control"
        ],
        "abstract": "Predictive safety filters provide a way of projecting potentially unsafe inputs onto the set of inputs that guarantee recursive state and input constraint satisfaction. Unsafe inputs, proposed, e.g. by a human or learning-based controller, can thereby be filtered by leveraging model predictive control techniques. In this paper, we extend this framework such that in addition, robust asymptotic stability of the closed-loop system can be guaranteed by enforcing a decrease of an implicit Lyapunov function which is constructed using a predicted system trajectory. Differently from previous results, we show robust asymptotic stability on an extended state consisting of the system state and a warmstart input sequence and establish robust asymptotic stability for the augmented dynamics with respect to a predefined disturbance. The proposed strategy is applied to an automotive lane keeping example in simulation.",
        "comments": "Submitted to NMPC'24",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11183"
    },
    {
        "doc_id": 493,
        "title": "Data-Driven Target Localization: Benchmarking Gradient Descent Using the Cram\u00e9r-Rao Bound",
        "authors": [
            "Shyam Venkatasubramanian",
            "Sandeep Gogineni",
            "Bosung Kang",
            "Ali Pezeshki",
            "Muralidhar Rangaswamy",
            "Vahid Tarokh"
        ],
        "subjects": [
            "Signal Processing",
            "Machine Learning"
        ],
        "abstract": "In modern radar systems, precise target localization using azimuth and velocity estimation is paramount. Traditional unbiased estimation methods have leveraged gradient descent algorithms to reach the theoretical limits of the Cram\u00e9r Rao Bound (CRB) for the error of the parameter estimates. In this study, we present a data-driven neural network approach that outperforms these traditional techniques, demonstrating improved accuracies in target azimuth and velocity estimation. Using a representative simulated scenario, we show that our proposed neural network model consistently achieves improved parameter estimates due to its inherently biased nature, yielding a diminished mean squared error (MSE). Our findings underscore the potential of employing deep learning methods in radar systems, paving the way for more accurate localization in cluttered and dynamic environments.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11176"
    },
    {
        "doc_id": 494,
        "title": "Generalizing Speaker Verification for Spoof Awareness in the Embedding Space",
        "authors": [
            "Xuechen Liu",
            "Md Sahidullah",
            "Kong Aik Lee",
            "Tomi Kinnunen"
        ],
        "subjects": [
            "Cryptography and Security",
            "Artificial Intelligence",
            "Sound",
            "Audio and Speech Processing"
        ],
        "abstract": "It is now well-known that automatic speaker verification (ASV) systems can be spoofed using various types of adversaries. The usual approach to counteract ASV systems against such attacks is to develop a separate spoofing countermeasure (CM) module to classify speech input either as a bonafide, or a spoofed utterance. Nevertheless, such a design requires additional computation and utilization efforts at the authentication stage. An alternative strategy involves a single monolithic ASV system designed to handle both zero-effort imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have the potential to provide stronger protections and more economic computations. To this end, we propose to generalize the standalone ASV (G-SASV) against spoofing attacks, where we leverage limited training data from CM to enhance a simple backend in the embedding space, without the involvement of a separate CM module during the test (authentication) phase. We propose a novel yet simple backend classifier based on deep neural networks and conduct the study via domain adaptation and multi-task integration of spoof embeddings at the training stage. Experiments are conducted on the ASVspoof 2019 logical access dataset, where we improve the performance of statistical ASV backends on the joint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and 49.8% in terms of equal error rates, respectively.",
        "comments": "To appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11156"
    },
    {
        "doc_id": 495,
        "title": "Enhancing System-Level Safety in Mixed-Autonomy Platoon via Safe Reinforcement Learning",
        "authors": [
            "Jingyuan Zhou",
            "Longhao Yan",
            "Kaidi Yang"
        ],
        "subjects": [
            "Systems and Control"
        ],
        "abstract": "Connected and automated vehicles (CAVs) have recently gained prominence in traffic research, thanks to the advancements in communication technology and autonomous driving. A variety of longitudinal control strategies for CAVs have been developed to enhance traffic efficiency, stability, and safety in mixed-autonomy scenarios. Deep reinforcement learning (DRL) is one promising strategy for mixed-autonomy platoon control since it can tackle complex scenarios in real-time. However, there are three research gaps for DRL-based mixed-autonomy platoon control. First, incorporating safety considerations into DRL typically relies on designing collision avoidance-based reward functions, which lack collision-free guarantees. Second, current DRL-based-control approaches for mixed traffic only consider the safety of CAVs, with little attention paid to the surrounding HDVs. To address the research gaps, we introduce a differentiable safety layer that converts DRL actions into safe actions with collision-free guarantees. This process relies on solving a differentiable quadratic programming problem that incorporates control barrier function-based (CBF) safety constraints for both CAV and its following HDVs to achieve system-level safety. Moreover, constructing CBF constraints needs system dynamics for the following HDVs, and thus we employ an online system identification module to estimate the car-following dynamics of the surrounding HDVs. The proposed safe reinforcement learning approach explicitly integrates system-level safety constraints into the training process and enables our method to adapt to varying safety-critical scenarios. Simulation results demonstrate that our proposed method effectively ensures CAV safety and improves HDV safety in mixed platoon environments while simultaneously enhancing traffic capacity and string stability.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11148"
    },
    {
        "doc_id": 496,
        "title": "Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities",
        "authors": [
            "Georgios Ioannides",
            "Aman Chadha",
            "Aaron Elkins"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Sound",
            "Audio and Speech Processing",
            "Signal Processing"
        ],
        "abstract": "We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy across a diverse range of tasks, including emotion recognition in speech, image classification, and text classification, thereby establishing its robustness and versatility in handling multi-modal data. Furthermore, we introduce the Importance Factor (IF), a new learning-based metric that enhances the explainability of models trained with GAAM-based methods. Overall, GAAM represents an advancement towards development of better performing and more explainable attention models across multiple modalities.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11143"
    },
    {
        "doc_id": 497,
        "title": "Wideband Beamforming for RIS Assisted Near-Field Communications",
        "authors": [
            "Ji Wang",
            "Jian Xiao",
            "Yixuan Zou",
            "Wenwu Xie",
            "Yuanwei Liu"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "A near-field wideband beamforming scheme is investigated for reconfigurable intelligent surface (RIS) assisted multiple-input multiple-output (MIMO) systems, in which a deep learning-based end-to-end (E2E) optimization framework is proposed to maximize the system spectral efficiency. To deal with the near-field double beam split effect, the base station is equipped with frequency-dependent hybrid precoding architecture by introducing sub-connected true time delay (TTD) units, while two specific RIS architectures, namely true time delay-based RIS (TTD-RIS) and virtual subarray-based RIS (SA-RIS), are exploited to realize the frequency-dependent passive beamforming at the RIS. Furthermore, the efficient E2E beamforming models without explicit channel state information are proposed, which jointly exploits the uplink channel training module and the downlink wideband beamforming module. In the proposed network architecture of the E2E models, the classical communication signal processing methods, i.e., polarized filtering and sparsity transform, are leveraged to develop a signal-guided beamforming network. Numerical results show that the proposed E2E models have superior beamforming performance and robustness to conventional beamforming benchmarks. Furthermore, the tradeoff between the beamforming gain and the hardware complexity is investigated for different frequency-dependent RIS architectures, in which the TTD-RIS can achieve better spectral efficiency than the SA-RIS while requiring additional energy consumption and hardware cost.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11141"
    },
    {
        "doc_id": 498,
        "title": "Reconfigurable Intelligent Surface-Enabled Array Radar for Interference Mitigation",
        "authors": [
            "Shengyao Chen",
            "Qi Feng",
            "Longyao Ran",
            "Feng Xi",
            "Zhong Liu"
        ],
        "subjects": [
            "Signal Processing"
        ],
        "abstract": "Conventional active array radars often jointly design the transmit and receive beamforming for effectively suppressing interferences. To further promote the interference suppression performance, this paper introduces a reconfigurable intelligent surface (RIS) to assist the radar receiver because the RIS has the ability to bring plentiful additional degrees-of-freedom. To maximize the output signal-to-interference-plus-noise ratio (SINR) of receive array, we formulate the codesign of transmit beamforming and RIS-aided receive beamforming into a nonconvex constrained fractional programming problem, and then propose an alternating minimization-based algorithm to jointly optimize the transmit beamformer, receive beamformer and RIS reflection coefficients. Specifically, we offer the closed-form optimal solutions of transmit and receive beamformers according to the minimum variance distortionless response principle, and translate the RIS reflection coefficients design into a series of unimodular quadratic programming (UQP) subproblems by employing the Dinkelbach transform. To tackle the UQP subproblems efficiently, we propose a second-order Riemannian Newton method (RNM) with improved Riemannian Newton direction, which avoids the line search and has better convergence speed than typical first-order Riemannian manifold optimization methods. Moreover, we derive the convergence of the proposed codesign algorithm by deducing the explicit convergence condition of RNM. We also analyze the computational complexity. Numerical results demonstrate that the proposed RIS-aided array radar has superior performance of interference suppression to the RIS-free one, and the SINR improvement is proportional to the number of RIS elements.",
        "comments": "28 pages, 9 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11137"
    },
    {
        "doc_id": 499,
        "title": "A Finger on the Pulse of Cardiovascular Health: Smartphone Photoplethysmography-Based Pulse Waveform Analysis for Blood Pressure Measurement",
        "authors": [
            "Ivan Liu",
            "Fangyuan Liu",
            "Qi Zhong",
            "Shiguang Ni"
        ],
        "subjects": [
            "Signal Processing",
            "Computers and Society"
        ],
        "abstract": "Routine blood pressure (BP) monitoring, crucial for health assessment, faces challenges such as limited access to medical-grade equipment and expertise. Portable cuff BP devices, on the other hand, are cumbersome to carry all day and often cost-prohibitive in less developed countries. Besides, these sphygmomanometer-based devices can cause discomfort and disrupt blood flow during measurement. This study explores the use of smartphones for continuous BP monitoring, focusing on overcoming the trust barriers associated with the opacity of machine learning models in predicting BP from low-quality PPG signals. Our approach included developing models based on cardiovascular literature, using simple statistical methods to estimate BP from smartphone PPG signals with comprehensive data pre-processing, applying SHAP for enhanced interpretability and feature identification, and comparing our methods against standard references using Bland-Altman analysis. Validated with data from 125 participants, the study demonstrated significant correlations in waveform features between smartphone and reference BP monitoring devices. The cross-validation of linear regression [MAE=9.86 and 8.01 mmHg for systolic blood pressure (SBP) and diastolic blood pressure (DBP), respectively] and random forest model (MAE=8.91 and 6.68 mmHg for SBP and DBP) using waveform-only variables demonstrated the feasibility of using a smartphone to estimate BP. Although SHAP analysis identified key feature sets, Bland-Altman results did not fully meet established thresholds (84.64% and 94.69% of MAE<15 mmHg for SBP and DBP, respectively). The study suggests the potential of smartphone cameras to enhance the accuracy and interpretability of machine learning models for daily BP estimation, but also indicates that smartphone PPG-based BP prediction is not yet a replacement for traditional medical devices.",
        "comments": "33 pages, 9 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11117"
    }
]