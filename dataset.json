[
    {
        "doc_id": 1,
        "title": "Clustering-based spatial interpolation of parametric post-processing models",
        "authors": [
            "S\u00e1ndor Baran",
            "M\u00e1ria Lakatos"
        ],
        "subjects": [
            "Applications",
            "Methodology"
        ],
        "abstract": "Since the start of the operational use of ensemble prediction systems, ensemble-based probabilistic forecasting has become the most advanced approach in weather prediction. However, despite the persistent development of the last three decades, ensemble forecasts still often suffer from the lack of calibration and might exhibit systematic bias, which calls for some form of statistical post-processing. Nowadays, one can choose from a large variety of post-processing approaches, where parametric methods provide full predictive distributions of the investigated weather quantity. Parameter estimation in these models is based on training data consisting of past forecast-observation pairs, thus post-processed forecasts are usually available only at those locations where training data are accessible. We propose a general clustering-based interpolation technique of extending calibrated predictive distributions from observation stations to any location in the ensemble domain where there are ensemble forecasts at hand. Focusing on the ensemble model output statistics (EMOS) post-processing technique, in a case study based on wind speed ensemble forecasts of the European Centre for Medium-Range Weather Forecasts, we demonstrate the predictive performance of various versions of the suggested method and show its superiority over the regionally estimated and interpolated EMOS models and the raw ensemble forecasts as well.",
        "comments": "19 pages, 6 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14393"
    },
    {
        "doc_id": 2,
        "title": "Single-file dynamics with general charge measures",
        "authors": [
            "\u017diga Krajnik"
        ],
        "subjects": [
            "Statistical Mechanics"
        ],
        "abstract": "We study charge fluctuations in single-file dynamics with general charge measures. The exact finite-time distribution of charge fluctuations is obtained in terms of a dressing transformation acting on the finite-time distribution of particle fluctuations. The transformation is mapped to a simple substitution rule for corresponding full-counting statistics. By taking the asymptotics of the dressing transformation we analyze typical and large scale charge fluctuations. Typical charge fluctuations in equilibrium states with vanishing mean charge are anomalous while large charge fluctuations undergo first and second order dynamical phase transitions out of equilibrium.",
        "comments": "18+9 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14378"
    },
    {
        "doc_id": 3,
        "title": "Modeling Global Surface Dust Deposition Using Physics-Informed Neural Networks",
        "authors": [
            "Constanza A. Molina Catricheo",
            "Fabrice Lambert",
            "Julien Salomon",
            "Elwin van 't Wout"
        ],
        "subjects": [
            "Geophysics"
        ],
        "abstract": "Paleoclimatic measurements serve to understand geophysical processes and evaluate climate model performances. However, their spatial coverage is generally sparse and unevenly distributed across the globe. Statistical interpolation methods are the prevalent techniques to grid such data, but these purely data-driven approaches sometimes produce results that are incoherent with our knowledge of the physical world. Physics-Informed Neural Networks (PINNs) follow an innovative approach to data analysis and physical modeling through machine learning, as they incorporate physical principles into the data-driven learning process. Here, we develop PINNs to reconstruct global maps of atmospheric dust surface deposition fluxes from measurement data in paleoclimatic archives, for the Holocene and Last Glacial Maximum periods. We design an advection-diffusion equation to consider dominant wind directions at various latitudes, which prevents dust particles from flowing upwind. Our PINN improves on standard kriging interpolation by allowing variable asymmetry around data points. The reconstructions display realistic dust plumes from continental sources towards ocean basins following prevailing winds.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14372"
    },
    {
        "doc_id": 4,
        "title": "Minimum Covariance Determinant: Spectral Embedding and Subset Size Determination",
        "authors": [
            "Qiang Heng",
            "Kenneth Lange"
        ],
        "subjects": [
            "Methodology",
            "Computation"
        ],
        "abstract": "This paper introduces several ideas to the minimum covariance determinant problem for outlier detection and robust estimation of means and covariances. We leverage the principal component transform to achieve dimension reduction, paving the way for improved analyses. Our best subset selection algorithm strategically combines statistical depth and concentration steps. To ascertain the appropriate subset size and number of principal components, we introduce a novel bootstrap procedure that estimates the instability of the best subset algorithm. The parameter combination exhibiting minimal instability proves ideal for the purposes of outlier detection and robust estimation. Rigorous benchmarking against prominent MCD variants showcases our approach's superior capability in outlier detection and computational speed in high dimensions. Application to a fruit spectra data set and a cancer genomics data set illustrates our claims.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14359"
    },
    {
        "doc_id": 5,
        "title": "Multiply Robust Estimation of Causal Effect Curves for Difference-in-Differences Designs",
        "authors": [
            "Gary Hettinger",
            "Youjin Lee",
            "Nandita Mitra"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Researchers commonly use difference-in-differences (DiD) designs to evaluate public policy interventions. While established methodologies exist for estimating effects in the context of binary interventions, policies often result in varied exposures across regions implementing the policy. Yet, existing approaches for incorporating continuous exposures face substantial limitations in addressing confounding variables associated with intervention status, exposure levels, and outcome trends. These limitations significantly constrain policymakers' ability to fully comprehend policy impacts and design future interventions. In this study, we propose innovative estimators for causal effect curves within the DiD framework, accounting for multiple sources of confounding. Our approach accommodates misspecification of a subset of treatment, exposure, and outcome models while avoiding any parametric assumptions on the effect curve. We present the statistical properties of the proposed methods and illustrate their application through simulations and a study investigating the diverse effects of a nutritional excise tax.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14355"
    },
    {
        "doc_id": 6,
        "title": "Evolving higher-order synergies reveals a trade-off between stability and information integration capacity in complex systems",
        "authors": [
            "Thomas F. Varley",
            "Joshua Bongard"
        ],
        "subjects": [
            "Information Theory",
            "Dynamical Systems",
            "Chaotic Dynamics",
            "Cellular Automata and Lattice Gases"
        ],
        "abstract": "There has recently been an explosion of interest in how \"higher-order\" structures emerge in complex systems. This \"emergent\" organization has been found in a variety of natural and artificial systems, although at present the field lacks a unified understanding of what the consequences of higher-order synergies and redundancies are for systems. Typical research treat the presence (or absence) of synergistic information as a dependent variable and report changes in the level of synergy in response to some change in the system. Here, we attempt to flip the script: rather than treating higher-order information as a dependent variable, we use evolutionary optimization to evolve boolean networks with significant higher-order redundancies, synergies, or statistical complexity. We then analyse these evolved populations of networks using established tools for characterizing discrete dynamics: the number of attractors, average transient length, and Derrida coefficient. We also assess the capacity of the systems to integrate information. We find that high-synergy systems are unstable and chaotic, but with a high capacity to integrate information. In contrast, evolved redundant systems are extremely stable, but have negligible capacity to integrate information. Finally, the complex systems that balance integration and segregation (known as Tononi-Sporns-Edelman complexity) show features of both chaosticity and stability, with a greater capacity to integrate information than the redundant systems while being more stable than the random and synergistic systems. We conclude that there may be a fundamental trade-off between the robustness of a systems dynamics and its capacity to integrate information (which inherently requires flexibility and sensitivity), and that certain kinds of complexity naturally balance this trade-off.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14347"
    },
    {
        "doc_id": 7,
        "title": "Uncovering Heterogeneity of Solar Flare Mechanism With Mixture Models",
        "authors": [
            "Bach Viet Do",
            "Yang Chen",
            "XuanLong Nguyen",
            "Ward Manchester"
        ],
        "subjects": [
            "Solar and Stellar Astrophysics",
            "Applications",
            "Methodology"
        ],
        "abstract": "The physics of solar flares occurring on the Sun is highly complex and far from fully understood. However, observations show that solar eruptions are associated with the intense kilogauss fields of active regions, where free energies are stored with field-aligned electric currents. With the advent of high-quality data sources such as the Geostationary Operational Environmental Satellites (GOES) and Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager (HMI), recent works on solar flare forecasting have been focusing on data-driven methods. In particular, black box machine learning and deep learning models are increasingly adopted in which underlying data structures are not modeled explicitly. If the active regions indeed follow the same laws of physics, there should be similar patterns shared among them, reflected by the observations. Yet, these black box models currently used in the literature do not explicitly characterize the heterogeneous nature of the solar flare data, within and between active regions. In this paper, we propose two finite mixture models designed to capture the heterogeneous patterns of active regions and their associated solar flare events. With extensive numerical studies, we demonstrate the usefulness of our proposed method for both resolving the sample imbalance issue and modeling the heterogeneity for rare energetic solar flare events.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14345"
    },
    {
        "doc_id": 8,
        "title": "Class-attribute Priors: Adapting Optimization to Heterogeneity and Fairness Objective",
        "authors": [
            "Xuechen Zhang",
            "Mingchen Li",
            "Jiasi Chen",
            "Christos Thrampoulidis",
            "Samet Oymak"
        ],
        "subjects": [
            "Machine Learning",
            "Computers and Society",
            "Machine Learning"
        ],
        "abstract": "Modern classification problems exhibit heterogeneities across individual classes: Each class may have unique attributes, such as sample size, label quality, or predictability (easy vs difficult), and variable importance at test-time. Without care, these heterogeneities impede the learning process, most notably, when optimizing fairness objectives. Confirming this, under a gaussian mixture setting, we show that the optimal SVM classifier for balanced accuracy needs to be adaptive to the class attributes. This motivates us to propose CAP: An effective and general method that generates a class-specific learning strategy (e.g. hyperparameter) based on the attributes of that class. This way, optimization process better adapts to heterogeneities. CAP leads to substantial improvements over the naive approach of assigning separate hyperparameters to each class. We instantiate CAP for loss function design and post-hoc logit adjustment, with emphasis on label-imbalanced problems. We show that CAP is competitive with prior art and its flexibility unlocks clear benefits for fairness objectives beyond balanced accuracy. Finally, we evaluate CAP on problems with label noise as well as weighted test objectives to showcase how CAP can jointly adapt to different heterogeneities.",
        "comments": "15 pages, 8 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14343"
    },
    {
        "doc_id": 9,
        "title": "Estimation of partially known Gaussian graphical models with score-based structural priors",
        "authors": [
            "Mart\u00edn Sevilla",
            "Antonio Garc\u00eda Marques",
            "Santiago Segarra"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach.",
        "comments": "15 pages, 5 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14340"
    },
    {
        "doc_id": 10,
        "title": "Case-crossover designs and overdispersion with application in air pollution epidemiology",
        "authors": [
            "Samuel Perreault",
            "Gracia Y. Dong",
            "Alex Stringer",
            "Hwashin Shin",
            "Patrick Brown"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Over the last three decades, case-crossover designs have found many applications in health sciences, especially in air pollution epidemiology. They are typically used, in combination with partial likelihood techniques, to define a conditional logistic model for the responses, usually health outcomes, conditional on the exposures. Despite the fact that conditional logistic models have been shown equivalent, in typical air pollution epidemiology setups, to specific instances of the well-known Poisson time series model, it is often claimed that they cannot allow for overdispersion. This paper clarifies the relationship between case-crossover designs, the models that ensue from their use, and overdispersion. In particular, we propose to relax the assumption of independence between individuals traditionally made in case-crossover analyses, in order to explicitly introduce overdispersion in the conditional logistic model. As we show, the resulting overdispersed conditional logistic model coincides with the overdispersed, conditional Poisson model, in the sense that their likelihoods are simple re-expressions of one another. We further provide the technical details of a Bayesian implementation of the proposed case-crossover model, which we use to demonstrate, by means of a large simulation study, that standard case-crossover models can lead to dramatically underestimated coverage probabilities, while the proposed models do not. We also perform an illustrative analysis of the association between air pollution and morbidity in Toronto, Canada, which shows that the proposed models are more robust than standard ones to outliers such as those associated with public holidays.",
        "comments": "MSC Class:          62J12; 62F15; 62P10                          ACM Class:          G.3",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14338"
    },
    {
        "doc_id": 11,
        "title": "Common Randomness Generation from Finite Compound Sources",
        "authors": [
            "Rami Ezzine",
            "Moritz Wiese",
            "Christian Deppe",
            "Holger Boche"
        ],
        "subjects": [
            "Information Theory"
        ],
        "abstract": "We investigate the problem of generating common randomness (CR) from finite compound sources aided by unidirectional communication over rate-limited perfect channels. The two communicating parties, often referred to as terminals, observe independent and identically distributed (i.i.d.) samples of a finite compound source and aim to agree on a common random variable with a high probability for every possible realization of the source state. Both parties know the set of source states as well as their statistics. However, they are unaware of the actual realization of the source state. We establish a single-letter lower and upper bound on the compound CR capacity for the specified model. Furthermore, we present two special scenarios where the established bounds coincide.",
        "comments": "arXiv admin note: text overlap with arXiv:2305.05524",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14323"
    },
    {
        "doc_id": 12,
        "title": "Using Geographically Weighted Models to Explore Temporal and Spatial Varying Impacts on Commute Trip Change Due to Covid-19",
        "authors": [
            "Saeed Saleh Namadi",
            "Behnam Tahmasbi",
            "Asal Mehditabrizi",
            "Aref Darzi",
            "Deb Niemeier"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "COVID-19 has deeply affected daily life and travel behaviors. Understanding these changes is crucial, prompting an investigation into socio-demographic and socio-economic factors. This study used large-scale mobile device location data in Washington, D.C., Maryland, and Virginia (DMV area) to unveil the impacts of these variables on commute trip changes. It reflected short and long-term impacts through linear regression and geographically weighted regression models. Findings indicated that counties with a higher percentage of people using walking and biking during the initial phase of COVID-19 experienced greater reductions in commute trips. For the long-term effect in November, the impact of active modes became insignificant, and individuals using public modes showed more significant trip reductions. Positive correlations were observed between median income levels and reduced commute trips. Sectors requiring ongoing outdoor operations during the pandemic showed substantial negative correlations. In the DMV area, counties with a higher proportion of Democratic voters experienced less trip reduction. Applying Geographically Weighted Regression models captured local spatial relationships, showing the emergence of local correlations as the pandemic evolved, suggesting a geographical impact pattern. Initially global, the pandemic's impact on commuting behaviors became more influenced by spatial factors over time, showing localized effects.",
        "comments": "28 pages, 8 figures, accepted at TRR 2024",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14306"
    },
    {
        "doc_id": 13,
        "title": "The overlooked significance of the unbiased exponential phasefactor in the determination of the finite-density lattice QCD equation of state",
        "authors": [
            "Sabarnya Mitra"
        ],
        "subjects": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology",
            "Nuclear Experiment",
            "Nuclear Theory"
        ],
        "abstract": "Within the framework of (2+1)-flavor QCD at finite temperature and chemical potential, we present results using high statistics data and demonstrate how the phasefactor of low order unbiased exponential resummation offers excellent prediction, proving to be an alternative reliable estimator of the radius of convergence of the eighth order QCD Taylor series at finite baryon density measured using the ratio and the Merci-Roberts estimators. We construct a new non-trivial unbiased phasefactor for complex isospin chemical potentials $\\muI$ and highlight its novelty. We find that this new unbiased phasefactor is very much capable of indicating the onset of non-monotonicity in finite $\\muI$ thermodynamics, which we illustrate by comparing the phasefactor results with that of low order cumulants of $\\muI$ fluctuations for non-vanishing $\\muI$. We also furnish results establishing that this unbiased phasefactor is reliable in manifesting the beginning of the overlap problem for finite, real $\\muI$. The errorbars increase drastically across the indications provided by the phasefactor which becomes very apparent from the coincidence between the phasefactor and the maximum of the errorbar slopes.",
        "comments": "9 pages, 6 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14299"
    },
    {
        "doc_id": 14,
        "title": "\"All of Me\": Mining Users' Attributes from their Public Spotify Playlists",
        "authors": [
            "Pier Paolo Tricomi",
            "Luca Pajola",
            "Luca Pasa",
            "Mauro Conti"
        ],
        "subjects": [
            "Cryptography and Security",
            "Machine Learning",
            "Social and Information Networks"
        ],
        "abstract": "In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. These publicly accessible playlists transcend the boundaries of mere musical preferences: they serve as sources of rich insights into users' attributes and identities. For example, the musical preferences of elderly individuals may lean more towards Frank Sinatra, while Billie Eilish remains a favored choice among teenagers. These playlists thus become windows into the diverse and evolving facets of one's musical identity.\n  In this work, we investigate the relationship between Spotify users' attributes and their public playlists. In particular, we focus on identifying recurring musical characteristics associated with users' individual attributes, such as demographics, habits, or personality traits. To this end, we conducted an online survey involving 739 Spotify users, yielding a dataset of 10,286 publicly shared playlists encompassing over 200,000 unique songs and 55,000 artists. Through extensive statistical analyses, we first assess a deep connection between a user's Spotify playlists and their real-life attributes. For instance, we found individuals high in openness often create playlists featuring a diverse array of artists, while female users prefer Pop and K-pop music genres. Building upon these observed associations, we create accurate predictive models for users' attributes, presenting a novel DeepSet application that outperforms baselines in most of these users' attributes.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14296"
    },
    {
        "doc_id": 15,
        "title": "Heteroscedasticity-aware stratified sampling to improve uplift modeling",
        "authors": [
            "Bj\u00f6rn Bokelmann",
            "Stefan Lessmann"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "In many business applications, including online marketing and customer churn prevention, randomized controlled trials (RCT's) are conducted to investigate on the effect of specific treatment (coupon offers, advertisement mailings,...). Such RCT's allow for the estimation of average treatment effects as well as the training of (uplift) models for the heterogeneity of treatment effects between individuals. The problem with these RCT's is that they are costly and this cost increases with the number of individuals included into the RCT. For this reason, there is research how to conduct experiments involving a small number of individuals while still obtaining precise treatment effect estimates. We contribute to this literature a heteroskedasticity-aware stratified sampling (HS) scheme, which leverages the fact that different individuals have different noise levels in their outcome and precise treatment effect estimation requires more observations from the \"high-noise\" individuals than from the \"low-noise\" individuals. By theory as well as by empirical experiments, we demonstrate that our HS-sampling yields significantly more precise estimates of the ATE, improves uplift models and makes their evaluation more reliable compared to RCT data sampled completely randomly. Due to the relative ease of application and the significant benefits, we expect HS-sampling to be valuable in many real-world applications.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14294"
    },
    {
        "doc_id": 16,
        "title": "Speech foundation models on intelligibility prediction for hearing-impaired listeners",
        "authors": [
            "Santiago Cuervo",
            "Ricard Marxer"
        ],
        "subjects": [
            "Sound",
            "Machine Learning",
            "Audio and Speech Processing"
        ],
        "abstract": "Speech foundation models (SFMs) have been benchmarked on many speech processing tasks, often achieving state-of-the-art performance with minimal adaptation. However, the SFM paradigm has been significantly less explored for applications of interest to the speech perception community. In this paper we present a systematic evaluation of 10 SFMs on one such application: Speech intelligibility prediction. We focus on the non-intrusive setup of the Clarity Prediction Challenge 2 (CPC2), where the task is to predict the percentage of words correctly perceived by hearing-impaired listeners from speech-in-noise recordings. We propose a simple method that learns a lightweight specialized prediction head on top of frozen SFMs to approach the problem. Our results reveal statistically significant differences in performance across SFMs. Our method resulted in the winning submission in the CPC2, demonstrating its promise for speech perception applications.",
        "comments": "To be presented in ICASSP 2024",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14289"
    },
    {
        "doc_id": 17,
        "title": "Information Leakage Detection through Approximate Bayes-optimal Prediction",
        "authors": [
            "Pritha Gupta",
            "Marcel Wever",
            "Eyke H\u00fcllermeier"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "In today's data-driven world, the proliferation of publicly available information intensifies the challenge of information leakage (IL), raising security concerns. IL involves unintentionally exposing secret (sensitive) information to unauthorized parties via systems' observable information. Conventional statistical approaches, which estimate mutual information (MI) between observable and secret information for detecting IL, face challenges such as the curse of dimensionality, convergence, computational complexity, and MI misestimation. Furthermore, emerging supervised machine learning (ML) methods, though effective, are limited to binary system-sensitive information and lack a comprehensive theoretical framework. To address these limitations, we establish a theoretical framework using statistical learning theory and information theory to accurately quantify and detect IL. We demonstrate that MI can be accurately estimated by approximating the log-loss and accuracy of the Bayes predictor. As the Bayes predictor is typically unknown in practice, we propose to approximate it with the help of automated machine learning (AutoML). First, we compare our MI estimation approaches against current baselines, using synthetic data sets generated using the multivariate normal (MVN) distribution with known MI. Second, we introduce a cut-off technique using one-sided statistical tests to detect IL, employing the Holm-Bonferroni correction to increase confidence in detection decisions. Our study evaluates IL detection performance on real-world data sets, highlighting the effectiveness of the Bayes predictor's log-loss estimation, and finds our proposed method to effectively estimate MI on synthetic data sets and thus detect ILs accurately.",
        "comments": "Under submission in JMLR",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14283"
    },
    {
        "doc_id": 18,
        "title": "How far can we see back in time in high-energy collisions using charm quarks?",
        "authors": [
            "Laszlo Gyulai",
            "Gabor Biro",
            "Robert Vertesi",
            "Gergely Gabor Barnafoldi"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "Nuclear Theory"
        ],
        "abstract": "We use open charm production to estimate how far we can see back in time in high-energy hadron-hadron collisions. We analyze the transverse momentum distributions of the identified D mesons from pp, p-Pb and A-A collisions at the ALICE and STAR experiments covering the energy range from $\\sqrt{s_{\\rm NN}} = 200$ GeV up to 7 TeV. Within a non-extensive statistical framework, the common Tsallis parameters for D mesons represent higher temperature and more degrees of freedom than that of light-flavour hadrons. The production of D mesons corresponds to a significantly earlier proper time, $\u03c4_{\\rm D} = (0.18 \\pm 0.06) \u03c4_{\\rm LF}$.",
        "comments": "18 pages, 6 figures, 1 table",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14282"
    },
    {
        "doc_id": 19,
        "title": "An Instance-Based Approach to the Trace Reconstruction Problem",
        "authors": [
            "Kayvon Mazooji",
            "Ilan Shomorony"
        ],
        "subjects": [
            "Information Theory",
            "Data Structures and Algorithms",
            "Probability",
            "Statistics Theory"
        ],
        "abstract": "In the trace reconstruction problem, one observes the output of passing a binary string $s \\in \\{0,1\\}^n$ through a deletion channel $T$ times and wishes to recover $s$ from the resulting $T$ \"traces.\" Most of the literature has focused on characterizing the hardness of this problem in terms of the number of traces $T$ needed for perfect reconstruction either in the worst case or in the average case (over input sequences $s$). In this paper, we propose an alternative, instance-based approach to the problem. We define the \"Levenshtein difficulty\" of a problem instance $(s,T)$ as the probability that the resulting traces do not provide enough information for correct recovery with full certainty. One can then try to characterize, for a specific $s$, how $T$ needs to scale in order for the Levenshtein difficulty to go to zero, and seek reconstruction algorithms that match this scaling for each $s$. For a class of binary strings with alternating long runs, we precisely characterize the scaling of $T$ for which the Levenshtein difficulty goes to zero. For this class, we also prove that a simple \"Las Vegas algorithm\" has an error probability that decays to zero with the same rate as that with which the Levenshtein difficulty tends to zero.",
        "comments": "7 pages, accepted for publication in the proceedings of the 58th Annual Conference on Information Sciences and Systems (CISS 2024)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14277"
    },
    {
        "doc_id": 20,
        "title": "Conservation laws and the foundations of quantum mechanics",
        "authors": [
            "Yakir Aharonov",
            "Sandu Popescu",
            "Daniel Rohrlich"
        ],
        "subjects": [
            "Quantum Physics"
        ],
        "abstract": "In a recent paper, PNAS, 118, e1921529118 (2021), it was argued that while the standard definition of conservation laws in quantum mechanics, which is of a statistical character, is perfectly valid, it misses essential features of nature and it can and must be revisited to address the issue of conservation/non-conservation in individual cases. Specifically, in the above paper an experiment was presented in which it can be proven that in some individual cases energy is not conserved, despite being conserved statistically. It was felt however that this is worrisome, and that something must be wrong if there are individual instances in which conservation doesn't hold, even though this is not required by the standard conservation law. Here we revisit that experiment and show that although its results are correct, there is a way to circumvent them and ensure individual case conservation in that situation. The solution is however quite unusual, challenging one of the basic assumptions of quantum mechanics, namely that any quantum state can be prepared, and it involves a time-holistic, double non-conservation effect. Our results bring new light on the role of the preparation stage of the initial state of a particle and on the interplay of conservation laws and frames of reference. We also conjecture that when such a full analysis of any conservation experiment is performed, conservation is obeyed in every individual case.",
        "comments": "Journal ref:        PNAS, 120 (41) e2220810120 (2023)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14261"
    },
    {
        "doc_id": 21,
        "title": "spINAR: An R Package for Semiparametric and Parametric Estimation and Bootstrapping of Integer-Valued Autoregressive (INAR) Models",
        "authors": [
            "Maxime Faymonville",
            "Javiera Riffo",
            "Jonas Rieger",
            "Carsten Jentsch"
        ],
        "subjects": [
            "Computation"
        ],
        "abstract": "Although the statistical literature extensively covers continuous-valued time series processes and their parametric, non-parametric and semiparametric estimation, the literature on count data time series is considerably less advanced. Among the count data time series models, the integer-valued autoregressive (INAR) model is arguably the most popular one finding applications in a wide variety of fields such as medical sciences, environmentology and economics. While many contributions have been made during the last decades, the majority of the literature focuses on parametric INAR models and estimation techniques. Our emphasis is on the complex but efficient and non-restrictive semiparametric estimation of INAR models. The appeal of this approach lies in the absence of a commitment to a parametric family of innovation distributions. In this paper, we describe the need and the features of our R package spINAR which combines semiparametric simulation, estimation and bootstrapping of INAR models also covering its parametric versions.",
        "comments": "3 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14239"
    },
    {
        "doc_id": 22,
        "title": "At the junction between deep learning and statistics of extremes: formalizing the landslide hazard definition",
        "authors": [
            "Ashok Dahal",
            "Rapha\u00ebl Huser",
            "Luigi Lombardo"
        ],
        "subjects": [
            "Machine Learning",
            "Geophysics",
            "Applications",
            "Machine Learning"
        ],
        "abstract": "The most adopted definition of landslide hazard combines spatial information about landslide location (susceptibility), threat (intensity), and frequency (return period). Only the first two elements are usually considered and estimated when working over vast areas. Even then, separate models constitute the standard, with frequency being rarely investigated. Frequency and intensity are intertwined and depend on each other because larger events occur less frequently and vice versa. However, due to the lack of multi-temporal inventories and joint statistical models, modelling such properties via a unified hazard model has always been challenging and has yet to be attempted. Here, we develop a unified model to estimate landslide hazard at the slope unit level to address such gaps. We employed deep learning, combined with a model motivated by extreme-value theory to analyse an inventory of 30 years of observed rainfall-triggered landslides in Nepal and assess landslide hazard for multiple return periods. We also use our model to further explore landslide hazard for the same return periods under different climate change scenarios up to the end of the century. Our results show that the proposed model performs excellently and can be used to model landslide hazard in a unified manner. Geomorphologically, we find that under both climate change scenarios (SSP245 and SSP885), landslide hazard is likely to increase up to two times on average in the lower Himalayan regions while remaining the same in the middle Himalayan region whilst decreasing slightly in the upper Himalayan region areas.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14210"
    },
    {
        "doc_id": 23,
        "title": "Statistical Characterization of RIS-assisted UAV Communications in Terrestrial and Non-Terrestrial Networks Under Channel Aging",
        "authors": [
            "Thanh Luan Nguyen",
            "Georges Kaddoum",
            "Tri Nhu Do",
            "Zygmunt J. Haas"
        ],
        "subjects": [
            "Signal Processing",
            "Information Theory"
        ],
        "abstract": "This paper studies the statistical characterization of ground-to-UAV (G2A) and reconfigurable intelligent surface (RIS)-assisted UAV-to-ground (A2G) communications in terrestrial and non-terrestrial networks under the impact of channel aging. We first model the G2A and A2G signal-to-noise ratios as non-central complex Gaussian quadratic random variables (RVs) and derive their exact probability density functions, offering a unique characterization for the A2G SNR as the product of two scaled non-central chi-square RVs. Moreover, we also find that, for a large number of RIS elements, the RIS-assisted A2G channel can be characterized as a single Rician fading channel. Our results reveal the presence of channel hardening in A2G communication under low UAV speeds, where we derive the maximum target spectral efficiency (SE) for a system to maintain a consistent required outage level. Meanwhile, high UAV speeds, exceeding 50 m/s, lead to a significant performance degradation, which cannot be mitigated by increasing the number of RIS elements.",
        "comments": "6 pages, 3 figures and 7 subfigures, IEEE ICC'24 (Revision),",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14203"
    },
    {
        "doc_id": 24,
        "title": "Clinical Melanoma Diagnosis with Artificial Intelligence: Insights from a Prospective Multicenter Study",
        "authors": [
            "Lukas Heinlein",
            "Roman C. Maron",
            "Achim Hekler",
            "Sarah Haggenm\u00fcller",
            "Christoph Wies",
            "Jochen S. Utikal",
            "Friedegund Meier",
            "Sarah Hobelsberger",
            "Frank F. Gellrich",
            "Mildred Sergon",
            "Axel Hauschild",
            "Lars E. French",
            "Lucie Heinzerling",
            "Justin G. Schlager",
            "Kamran Ghoreschi",
            "Max Schlaak",
            "Franz J. Hilke",
            "Gabriela Poch",
            "S\u00f6ren Korsing",
            "Carola Berking",
            "Markus V. Heppt",
            "Michael Erdmann",
            "Sebastian Haferkamp",
            "Konstantin Drexler",
            "Dirk Schadendorf",
            "et al. (5 additional authors not shown)"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Applications"
        ],
        "abstract": "Early detection of melanoma, a potentially lethal type of skin cancer with high prevalence worldwide, improves patient prognosis. In retrospective studies, artificial intelligence (AI) has proven to be helpful for enhancing melanoma detection. However, there are few prospective studies confirming these promising results. Existing studies are limited by low sample sizes, too homogenous datasets, or lack of inclusion of rare melanoma subtypes, preventing a fair and thorough evaluation of AI and its generalizability, a crucial aspect for its application in the clinical setting. Therefore, we assessed 'All Data are Ext' (ADAE), an established open-source ensemble algorithm for detecting melanomas, by comparing its diagnostic accuracy to that of dermatologists on a prospectively collected, external, heterogeneous test set comprising eight distinct hospitals, four different camera setups, rare melanoma subtypes, and special anatomical sites. We advanced the algorithm with real test-time augmentation (R-TTA, i.e. providing real photographs of lesions taken from multiple angles and averaging the predictions), and evaluated its generalization capabilities. Overall, the AI showed higher balanced accuracy than dermatologists (0.798, 95% confidence interval (CI) 0.779-0.814 vs. 0.781, 95% CI 0.760-0.802; p<0.001), obtaining a higher sensitivity (0.921, 95% CI 0.900- 0.942 vs. 0.734, 95% CI 0.701-0.770; p<0.001) at the cost of a lower specificity (0.673, 95% CI 0.641-0.702 vs. 0.828, 95% CI 0.804-0.852; p<0.001). As the algorithm exhibited a significant performance advantage on our heterogeneous dataset exclusively comprising melanoma-suspicious lesions, AI may offer the potential to support dermatologists particularly in diagnosing challenging cases.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14193"
    },
    {
        "doc_id": 25,
        "title": "Graph-accelerated Markov Chain Monte Carlo using Approximate Samples",
        "authors": [
            "Leo L. Duan",
            "Anirban Bhattacharya"
        ],
        "subjects": [
            "Computation"
        ],
        "abstract": "In recent years, it has become increasingly easy to obtain approximate posterior samples via efficient computation algorithms, such as those in variational Bayes. On the other hand, concerns exist on the accuracy of uncertainty estimates, which make it tempting to consider exploiting the approximate samples in canonical Markov chain Monte Carlo algorithms. A major technical barrier is that the approximate sample, when used as a proposal in Metropolis-Hastings steps, tends to have a low acceptance rate as the dimension increases. In this article, we propose a simple yet general solution named ''graph-accelerated Markov Chain Monte Carlo''. We first build a graph with each node location assigned to an approximate sample, then we run Markov chain Monte Carlo with random walks over the graph. In the first stage, we optimize the choice of graph edges to enforce small differences in posterior density/probability between neighboring nodes, while encouraging edges to correspond to large distances in the parameter space. This optimized graph allows us to accelerate a canonical Markov transition kernel through mixing with a large-jump Metropolis-Hastings step, when collecting Markov chain samples at the second stage. Due to its simplicity, this acceleration can be applied to most of the existing Markov chain Monte Carlo algorithms. We theoretically quantify the rate of acceptance as dimension increases, and show the effects on improved mixing time. We demonstrate our approach through improved mixing performances for challenging sampling problems, such as those involving multiple modes, non-convex density contour, or large-dimension latent variables.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14186"
    },
    {
        "doc_id": 26,
        "title": "Adapting tree-based multiple imputation methods for multi-level data? A simulation study",
        "authors": [
            "Ketevan Gurtskaia",
            "Jakob Schwerter",
            "Philipp Doebler"
        ],
        "subjects": [
            "Applications",
            "Machine Learning"
        ],
        "abstract": "This simulation study evaluates the effectiveness of multiple imputation (MI) techniques for multilevel data. It compares the performance of traditional Multiple Imputation by Chained Equations (MICE) with tree-based methods such as Chained Random Forests with Predictive Mean Matching and Extreme Gradient Boosting. Adapted versions that include dummy variables for cluster membership are also included for the tree-based methods. Methods are evaluated for coefficient estimation bias, statistical power, and type I error rates on simulated hierarchical data with different cluster sizes (25 and 50) and levels of missingness (10\\% and 50\\%). Coefficients are estimated using random intercept and random slope models. The results show that while MICE is preferred for accurate rejection rates, Extreme Gradient Boosting is advantageous for reducing bias. Furthermore, the study finds that bias levels are similar across different cluster sizes, but rejection rates tend to be less favorable with fewer clusters (lower power, higher type I error). In addition, the inclusion of cluster dummies in tree-based methods improves estimation for Level 1 variables, but is less effective for Level 2 variables. When data become too complex and MICE is too slow, extreme gradient boosting is a good alternative for hierarchical data.\n  Keywords: Multiple imputation; multi-level data; MICE; missRanger; mixgb",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14161"
    },
    {
        "doc_id": 27,
        "title": "Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations",
        "authors": [
            "Xinyue Xu",
            "Yi Qin",
            "Lu Mi",
            "Hao Wang",
            "Xiaomeng Li"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not help correct highly correlated concepts (e.g., \"yellow belly\"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label \"Kentucky Warbler\" and a concept \"black bill\", what is the probability that the model correctly predicts another concept \"black crown\"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural networks to define the joint energy of candidate (input, concept, class) tuples. With such a unified interface, prediction, concept correction, and conditional dependency quantification are then represented as conditional probabilities, which are generated by composing different energy functions. Our ECBMs address both limitations of existing CBMs, providing higher accuracy and richer concept interpretations. Empirical results show that our approach outperforms the state-of-the-art on real-world datasets.",
        "comments": "Accepted by ICLR 2024",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14142"
    },
    {
        "doc_id": 28,
        "title": "Performance Analysis for Near-Field ISAC: A Holographic MIMO Design",
        "authors": [
            "Boqun Zhao",
            "Chongjun Ouyang",
            "Xingqi Zhang",
            "Yuanwei Liu"
        ],
        "subjects": [
            "Information Theory",
            "Signal Processing"
        ],
        "abstract": "A near-field holographic multiple-input multiple-output (MIMO) based integrated sensing and communications (ISAC) framework is proposed for both downlink and uplink scenarios, where spherical wave-based model is considered to capture the characteristics of the near field. The coupling effect introduced by the densely spaced antennas of the holographic MIMO are characterized by spatially correlated Rayleigh fading. Based on the proposed framework, by considering both instantaneous channel state information (CSI) and statistical CSI, closed-form expressions are derived for sensing rates (SRs), communication rates (CRs), and outage probabilities under different ISAC designs. Further insights are gained by examining high signal-to-noise ratio slopes and diversity orders. Specifically, 1) for the downlink case, a sensing-centric (S-C) design and a communications-centric (C-C) design are investigated based on different beamforming strategies, and a Pareto optimal design is proposed to characterize the attainable SR-CR region; and 2) for the uplink case, the S-C design and the C-C design are distinguished by the interference cancellation order of the communication signal and the sensing signal, and the rate region is obtained through a time-sharing strategy. Numerical results reveal that the proposed ISAC system achieves more extensive rate regions than the conventional frequency-division sensing and communications system, highlighting its superior performance.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14129"
    },
    {
        "doc_id": 29,
        "title": "On a Novel Skewed Generalized t Distribution: Properties, Estimations and its Applications",
        "authors": [
            "Chengdi Lian",
            "Yaohua Rong",
            "Weihu Cheng"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "With the progress of information technology, large amounts of asymmetric, leptokurtic and heavy-tailed data are arising in various fields, such as finance, engineering, genetics and medicine. It is very challenging to model those kinds of data, especially for extremely skewed data, accompanied by very high kurtosis or heavy tails. In this paper, we propose a class of novel skewed generalized t distribution (SkeGTD) as a scale mixture of skewed generalized normal. The proposed SkeGTD has excellent adaptiveness to various data, because of its capability of allowing for a large range of skewness and kurtosis and its compatibility of the separated location, scale, skewness and shape parameters. We investigate some important properties of this family of distributions. The maximum likelihood estimation, L-moments estimation and two-step estimation for the SkeGTD are explored. To illustrate the usefulness of the proposed methodology, we present simulation studies and analyze two real datasets.",
        "comments": "34 pages, 4 figures; Communications in Statistics - Theory and Methods, accepted, January 2024",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14122"
    },
    {
        "doc_id": 30,
        "title": "ODC and ROC curves, comparison curves, and stochastic dominance",
        "authors": [
            "Teresa Ledwina",
            "Adam Zagda\u0144ski"
        ],
        "subjects": [
            "Methodology",
            "Statistics Theory"
        ],
        "abstract": "We discuss two novel approaches to the classical two-sample problem. Our starting point are properly standardized and combined, very popular in several areas of statistics and data analysis, ordinal dominance and receiver characteristic curves, denoted by ODC and ROC, respectively. The proposed new curves are termed the comparison curves. Their estimates, being weighted rank processes on (0,1), form the basis of inference. These weighted processes are intuitive, well-suited for visual inspection of data at hand, and are also useful for constructing some formal inferential procedures. They can be applied to several variants of two-sample problem. Their use can help to improve some existing procedures both in terms of power and the ability to identify the sources of departures from the postulated model. To simplify interpretation of finite sample results we restrict attention to values of the processes on a finite grid of points. This results in the so-called bar plots (B-plots) which readably summarize the information contained in the data. What is more, we show that B-plots along with adjusted simultaneous acceptance regions provide principled information about where the model departs from the data. This leads to a framework which facilitates identification of regions with locally significant differences.\n  We show an implementation of the considered techniques to a standard stochastic dominance testing problem. Some min-type statistics are introduced and investigated. A simulation study compares two tests pertinent to the comparison curves to well-established tests in the literature and demonstrates the strong and competitive performance of the former in many typical situations. Some real data applications illustrate simplicity and practical usefulness of the proposed approaches. A range of other applications of considered weighted processes is briefly discussed too.",
        "comments": "45 pages, 5 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14094"
    },
    {
        "doc_id": 31,
        "title": "LongMemory.jl: Generating, Estimating, and Forecasting Long Memory Models in Julia",
        "authors": [
            "J. Eduardo Vera-Vald\u00e9s"
        ],
        "subjects": [
            "Mathematical Software",
            "Computation"
        ],
        "abstract": "LongMemory.jl is a package for time series long memory modelling in Julia. The package provides functions to generate long memory, estimate model parameters, and forecast. Generating methods include fractional differencing, stochastic error duration, and cross-sectional aggregation. Estimators include the classic ones used to estimate the Hurst effect, those inspired by log-periodogram regression, and parametric ones. Forecasting is provided for all parametric estimators. Moreover, the package adds plotting capabilities to illustrate long memory dynamics and forecasting. This article presents the theoretical developments for long memory modelling, show examples using the data included with the package, and compares the properties of LongMemory.jl with current alternatives, including benchmarks. For some of the theoretical developments, LongMemory.jl provides the first publicly available implementation in any programming language. A notable feature of this package is that all functions are implemented in the same programming language, taking advantage of the ease of use and speed provided by Julia. Therefore, all code is accessible to the user. Multiple dispatch, a novel feature of the language, is used to speed computations and provide consistent calls to related methods. The package is related to the R packages LongMemoryTS and fracdiff.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14077"
    },
    {
        "doc_id": 32,
        "title": "Influence of climate variability on the potential forage production of a mown permanent grassland in the French Massif Central",
        "authors": [
            "I\u00f1igo G\u00f3mara",
            "Gianni Bellocchi",
            "Rapha\u00ebl Martin",
            "Bel\u00e9n Rodr\u00edguez-Fonseca",
            "Margarita Ruiz-Ramos"
        ],
        "subjects": [
            "Atmospheric and Oceanic Physics"
        ],
        "abstract": "Climate Services (CS) provide support to decision makers across socio-economic sectors. In the agricultural sector, one of the most important CS applications is to provide timely and accurate yield forecasts based on climate prediction. In this study, the Pasture Simulation model (PaSim) was used to simulate, for the period 1959-2015, the forage production of a mown grassland system (Laqueuille, Massif Central of France) under different management conditions, with meteorological inputs extracted from the SAFRAN atmospheric database. The aim was to generate purely climate-dependent timeseries of optimal forage production, a variable that was maximized by brighter and warmer weather conditions at the grassland. A long-term increase was observed in simulated forage yield, with the 1995-2015 average being 29% higher than the 1959-1979 average. Such increase seems consistent with observed rising trends in temperature and CO2, and multi-decadal changes in incident solar radiation. At interannual timescales, sea surface temperature anomalies of the Mediterranean (MED), Tropical North Atlantic (TNA), equatorial Pacific (El Ni\u00f1o Southern Oscillation) and the North Atlantic Oscillation (NAO) index were found robustly correlated with annual forage yield values. Relying only on climatic predictors, we developed a stepwise statistical multi-regression model with leave-one-out cross-validation. Under specific management conditions (e.g., three annual cuts) and from one to five months in advance, the generated model successfully provided a p-value<0.01 in correlation (t-test), a root mean square error percentage (%RMSE) of 14.6% and a 71.43% hit rate predicting above/below average years in terms of forage yield collection.",
        "comments": "Journal ref:        Gomara I, Bellocchi G, Martin R, Rodriguez-Fonseca B, Ruiz-Ramos M (2020) Agricultural and Forest Meteorology, 280, 107768",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14053"
    },
    {
        "doc_id": 33,
        "title": "Testing Alpha in High Dimensional Linear Factor Pricing Models with Dependent Observations",
        "authors": [
            "Huifang Ma",
            "Long Feng",
            "Zhaojun Wang",
            "Jigang Bao"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "In this study, we introduce three distinct testing methods for testing alpha in high dimensional linear factor pricing model that deals with dependent data. The first method is a sum-type test procedure, which exhibits high performance when dealing with dense alternatives. The second method is a max-type test procedure, which is particularly effective for sparse alternatives. For a broader range of alternatives, we suggest a Cauchy combination test procedure. This is predicated on the asymptotic independence of the sum-type and max-type test statistics. Both simulation studies and practical data application demonstrate the effectiveness of our proposed methods when handling dependent observations.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14052"
    },
    {
        "doc_id": 34,
        "title": "Diverse and Lifespan Facial Age Transformation Synthesis with Identity Variation Rationality Metric",
        "authors": [
            "Jiu-Cheng Xie",
            "Jun Yang",
            "Wenqing Wang",
            "Feng Xu",
            "Hao Gao"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Face aging has received continuous research attention over the past two decades. Although previous works on this topic have achieved impressive success, two longstanding problems remain unsettled: 1) generating diverse and plausible facial aging patterns at the target age stage; 2) measuring the rationality of identity variation between the original portrait and its syntheses with age progression or regression. In this paper, we introduce DLAT + , the first algorithm that can realize Diverse and Lifespan Age Transformation on human faces, where the diversity jointly manifests in the transformation of facial textures and shapes. Apart from the diversity mechanism embedded in the model, multiple consistency restrictions are leveraged to keep it away from counterfactual aging syntheses. Moreover, we propose a new metric to assess the rationality of Identity Deviation under Age Gaps (IDAG) between the input face and its series of age-transformed generations, which is based on statistical laws summarized from plenty of genuine face-aging data. Extensive experimental results demonstrate the uniqueness and effectiveness of our method in synthesizing diverse and perceptually reasonable faces across the whole lifetime.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14036"
    },
    {
        "doc_id": 35,
        "title": "Comparison of modularity-based approaches for nodes clustering in binary hypergraphs",
        "authors": [
            "Veronica Poda",
            "Catherine Matias"
        ],
        "subjects": [
            "Social and Information Networks",
            "Combinatorics",
            "Data Analysis, Statistics and Probability",
            "Applications"
        ],
        "abstract": "We conducted a comparative analysis of the performance of modularity-based methods for clustering nodes in binary hypergraphs. Statistical analysis and node clustering in hypergraphs constitute an emerging topic suffering from a lack of standardization. In contrast to the case of graphs, the concept of nodes' community in hypergraphs is not unique and encompasses various distinct situations. To address this, we begin by presenting, within a unified framework, the various hypergraph modularity criteria proposed in the literature, emphasizing their differences and respective focuses. Subsequently, we provide an overview of the state-of-the-art codes available to maximize hypergraph modularities for detecting node communities in binary hypergraphs. Through exploration of various simulation settings with controlled ground truth clustering, we offer a comparison of these methods using different quality measures, including true clustering recovery, running time, (local) maximization of the objective, and the number of clusters detected. Our contribution marks the first attempt to clarify the advantages and drawbacks of these newly available methods. This effort lays the foundation for a better understanding of the primary objectives of modularity-based node clustering methods for binary hypergraphs.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14028"
    },
    {
        "doc_id": 36,
        "title": "Semantic Ensemble Loss and Latent Refinement for High-Fidelity Neural Image Compression",
        "authors": [
            "Daxin Li",
            "Yuanchao Bai",
            "Kai Wang",
            "Junjun Jiang",
            "Xianming Liu"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ],
        "abstract": "Recent advancements in neural compression have surpassed traditional codecs in PSNR and MS-SSIM measurements. However, at low bit-rates, these methods can introduce visually displeasing artifacts, such as blurring, color shifting, and texture loss, thereby compromising perceptual quality of images. To address these issues, this study presents an enhanced neural compression method designed for optimal visual fidelity. We have trained our model with a sophisticated semantic ensemble loss, integrating Charbonnier loss, perceptual loss, style loss, and a non-binary adversarial loss, to enhance the perceptual quality of image reconstructions. Additionally, we have implemented a latent refinement process to generate content-aware latent codes. These codes adhere to bit-rate constraints, balance the trade-off between distortion and fidelity, and prioritize bit allocation to regions of greater importance. Our empirical findings demonstrate that this approach significantly improves the statistical fidelity of neural image compression. On CLIC2024 validation set, our approach achieves a 62% bitrate saving compared to MS-ILLM under FID metric.",
        "comments": "7 pages, 4 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14007"
    },
    {
        "doc_id": 37,
        "title": "Localization of Lindbladian Fermions",
        "authors": [
            "Foster Thompson",
            "Yi Huang",
            "Alex Kamenev"
        ],
        "subjects": [
            "Disordered Systems and Neural Networks"
        ],
        "abstract": "We study a Lindbladian generalization of the Anderson model of localization that describes disordered free fermions coupled to a disordered environment. From finite size scaling of both eigenvalue statistics and participation ratio, we identify localization transitions in both the non-Hermitian Lindbladian spectrum, which governs transient relaxation dynamics, and in the Hermitian stationary state density matrix. These localization transitions occur at different critical values of Hamiltonian and dissipative disorder strength, implying the existence of atypical phases with a mixture of localized and delocalized features. We find this phenomenon is robust to changes to the value of the dissipative spectral gap.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14006"
    },
    {
        "doc_id": 38,
        "title": "Evaluating the Determinants of Mode Choice Using Statistical and Machine Learning Techniques in the Indian Megacity of Bengaluru",
        "authors": [
            "Tanmay Ghosh",
            "Nithin Nagaraj"
        ],
        "subjects": [
            "Machine Learning",
            "General Economics"
        ],
        "abstract": "The decision making involved behind the mode choice is critical for transportation planning. While statistical learning techniques like discrete choice models have been used traditionally, machine learning (ML) models have gained traction recently among the transportation planners due to their higher predictive performance. However, the black box nature of ML models pose significant interpretability challenges, limiting their practical application in decision and policy making. This study utilised a dataset of $1350$ households belonging to low and low-middle income bracket in the city of Bengaluru to investigate mode choice decision making behaviour using Multinomial logit model and ML classifiers like decision trees, random forests, extreme gradient boosting and support vector machines. In terms of accuracy, random forest model performed the best ($0.788$ on training data and $0.605$ on testing data) compared to all the other models. This research has adopted modern interpretability techniques like feature importance and individual conditional expectation plots to explain the decision making behaviour using ML models. A higher travel costs significantly reduce the predicted probability of bus usage compared to other modes (a $0.66\\%$ and $0.34\\%$ reduction using Random Forests and XGBoost model for $10\\%$ increase in travel cost). However, reducing travel time by $10\\%$ increases the preference for the metro ($0.16\\%$ in Random Forests and 0.42% in XGBoost). This research augments the ongoing research on mode choice analysis using machine learning techniques, which would help in improving the understanding of the performance of these models with real-world data in terms of both accuracy and interpretability.",
        "comments": "65 pages, 26 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13977"
    },
    {
        "doc_id": 39,
        "title": "Sparse signal recovery and source localization via covariance learning",
        "authors": [
            "Esa Ollila"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "In the Multiple Measurements Vector (MMV) model, measurement vectors are connected to unknown, jointly sparse signal vectors through a linear regression model employing a single known measurement matrix (or dictionary). Typically, the number of atoms (columns of the dictionary) is greater than the number measurements and the sparse signal recovery problem is generally ill-posed. In this paper, we treat the signals and measurement noise as independent Gaussian random vectors with unknown signal covariance matrix and noise variance, respectively, and derive fixed point (FP) equation for solving the likelihood equation for signal powers, thereby enabling the recovery of the sparse signal support (sources with non-zero variances). Two practical algorithms, a block coordinate descent (BCD) and a cyclic coordinate descent (CCD) algorithms, that leverage on the FP characterization of the likelihood equation are then proposed. Additionally, a greedy pursuit method, analogous to popular simultaneous orthogonal matching pursuit (OMP), is introduced. Our numerical examples demonstrate effectiveness of the proposed covariance learning (CL) algorithms both in classic sparse signal recovery as well as in direction-of-arrival (DOA) estimation problems where they perform favourably compared to the state-of-the-art algorithms under a broad variety of settings.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13975"
    },
    {
        "doc_id": 40,
        "title": "Z-estimation system: a modular approach to asymptotic analysis",
        "authors": [
            "Jie Kate Hu"
        ],
        "subjects": [
            "Statistics Theory"
        ],
        "abstract": "Asymptotic analysis for related inference problems often involves similar steps and proofs. These intermediate results could be shared across problems if each of them is made self-contained and easily identified. However, asymptotic analysis using Taylor expansions is limited for result borrowing because it is a step-to-step procedural approach. This article introduces EEsy, a modular system for estimating finite and infinitely dimensional parameters in related inference problems. It is based on the infinite-dimensional Z-estimation theorem, Donsker and Glivenko-Cantelli preservation theorems, and weight calibration techniques. This article identifies the systematic nature of these tools and consolidates them into one system containing several modules, which can be built, shared, and extended in a modular manner. This change to the structure of method development allows related methods to be developed in parallel and complex problems to be solved collaboratively, expediting the development of new analytical methods. This article considers four related inference problems -- estimating parameters with random sampling, two-phase sampling, auxiliary information incorporation, and model misspecification. We illustrate this modular approach by systematically developing 9 parameter estimators and 18 variance estimators for the four related inference problems regarding semi-parametric additive hazards models. Simulation studies show the obtained asymptotic results for these 27 estimators are valid. In the end, I describe how this system can simplify the use of empirical process theory, a powerful but challenging tool to be adopted by the broad community of methods developers. I discuss challenges and the extension of this system to other inference problems.",
        "comments": "MSC Class:          62",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13948"
    },
    {
        "doc_id": 41,
        "title": "Is the age pension in Australia sustainable and fair? Evidence from forecasting the old-age dependency ratio using the Hamilton-Perry model",
        "authors": [
            "Sizhe Chen",
            "Han Lin Shang",
            "Yang Yang"
        ],
        "subjects": [
            "Applications",
            "Methodology"
        ],
        "abstract": "The age pension aims to assist eligible elderly Australians meet specific age and residency criteria in maintaining basic living standards. In designing efficient pension systems, government policymakers seek to satisfy the expectations of the overall aging population in Australia. However, the population's unique demographic characteristics at the state and territory level are often overlooked due to the lack of available data. We use the Hamilton-Perry model, which requires minimum input, to model and forecast the evolution of age-specific populations at the state level. We also integrate the obtained sub-national demographic information to determine sustainable pension ages up to 2051. We also investigate pension welfare distribution in all states and territories to identify disadvantaged residents under the current pension system. Using the sub-national mortality data for Australia from 1971 to 2021 obtained from AHMD (2023), we implement the Hamilton-Perry model with the help of functional time series forecasting techniques. With forecasts of age-specific population sizes for each state and territory, we compute the old age dependency ratio to determine the nationwide sustainable pension age.",
        "comments": "31 pages, 14 figures, 1 table",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13943"
    },
    {
        "doc_id": 42,
        "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
        "authors": [
            "Lucius E. J. Bynum",
            "Joshua R. Loftus",
            "Julia Stoyanovich"
        ],
        "subjects": [
            "Artificial Intelligence",
            "Computers and Society",
            "Machine Learning"
        ],
        "abstract": "Counterfactuals and counterfactual reasoning underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions -- like interventions on race -- may not translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13935"
    },
    {
        "doc_id": 43,
        "title": "Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics",
        "authors": [
            "Xingche Guo",
            "Donglin Zeng",
            "Yuanjia Wang"
        ],
        "subjects": [
            "Machine Learning",
            "Applications",
            "Methodology",
            "Machine Learning"
        ],
        "abstract": "Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) within the EMBARC study, we propose a novel RL-HMM framework for analyzing reward-based decision-making. Our model accommodates learning strategy switching between two distinct approaches under a hidden Markov model (HMM): subjects making decisions based on the RL model or opting for random choices. We account for continuous RL state space and allow time-varying transition probabilities in the HMM. We introduce a computationally efficient EM algorithm for parameter estimation and employ a nonparametric bootstrap for inference. We apply our approach to the EMBARC study to show that MDD patients are less engaged in RL compared to the healthy controls, and engagement is associated with brain activities in the negative affect circuitry during an emotional conflict task.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13929"
    },
    {
        "doc_id": 44,
        "title": "Scale-invariant Phenomena in Repeating Fast Radio Bursts and Glitching Pulsars",
        "authors": [
            "Chong-Yu Gao",
            "Jun-Jie Wei"
        ],
        "subjects": [
            "High Energy Astrophysical Phenomena"
        ],
        "abstract": "The recent discoveries of a remarkable glitch/antiglitch accompanied by fast radio burst (FRB)-like bursts from the Galactic magnetar SGR J1935+2154 have revealed the physical connection between the two. In this work, we study the statistical properties of radio bursts from the hyperactive repeating source FRB 20201124A and of glitches from the pulsar PSR B1737--30. For FRB 20201124A, we confirm that the probability density functions of fluctuations of energy, peak flux, duration, and waiting time well follow the Tsallis q-Gaussian distribution. The derived q values from q-Gaussian distribution keep approximately steady for different temporal interval scales, which indicate that there is a common scale-invariant structure in repeating FRBs. Similar scale-invariant property can be found in PSR B1737--30's glitches, implying an underlying association between the origins of repeating FRBs and pulsar glitches. These statistical features can be well understood within the same physical framework of self-organized criticality systems.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13916"
    },
    {
        "doc_id": 45,
        "title": "Spectral Clustering for Discrete Distributions",
        "authors": [
            "Zixiao Wang",
            "Dong Qiao",
            "Jicong Fan"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "Discrete distribution clustering (D2C) was often solved by Wasserstein barycenter methods. These methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. In this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and Wasserstein distance) for D2C. To improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. We provide theoretical guarantees for the success of the proposed methods in clustering distributions. Experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13913"
    },
    {
        "doc_id": 46,
        "title": "A Survey of Deep Learning and Foundation Models for Time Series Forecasting",
        "authors": [
            "John A. Miller",
            "Mohammed Aldosari",
            "Farah Saeed",
            "Nasid Habib Barna",
            "Subas Rana",
            "I. Budak Arpinar",
            "Ninghao Liu"
        ],
        "subjects": [
            "Machine Learning"
        ],
        "abstract": "Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available. Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to utilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling techniques are reviewed, and suggestions for further work are provided.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13912"
    },
    {
        "doc_id": 47,
        "title": "Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression",
        "authors": [
            "Siyu Lou",
            "Chengchun Liu",
            "Yuntian Chen",
            "Fanyang Mo"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Databases",
            "Applications"
        ],
        "abstract": "Thin-layer chromatography (TLC) is a crucial technique in molecular polarity analysis. Despite its importance, the interpretability of predictive models for TLC, especially those driven by artificial intelligence, remains a challenge. Current approaches, utilizing either high-dimensional molecular fingerprints or domain-knowledge-driven feature engineering, often face a dilemma between expressiveness and interpretability. To bridge this gap, we introduce Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical neural networks and symbolic regression. UHiSR automatically distills chemical-intuitive polarity indices, and discovers interpretable equations that link molecular structure to chromatographic behavior.",
        "comments": "33 pages, 6 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13904"
    },
    {
        "doc_id": 48,
        "title": "Discrete Hawkes process with flexible residual distribution and filtered historical simulation",
        "authors": [
            "Kyungsub Lee"
        ],
        "subjects": [
            "Statistical Finance",
            "Methodology"
        ],
        "abstract": "We introduce a new model which can be considered as a extended version of the Hawkes process in a discrete sense. This model enables the integration of various residual distributions while preserving the fundamental properties of the original Hawkes process. The rich nature of this model enables a filtered historical simulation which incorporate the properties of original time series more accurately. The process naturally extends to multi-variate models with easy implementations of estimation and simulation. We investigate the effect of flexible residual distribution on estimation of high frequency financial data compared with the Hawkes process.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13890"
    },
    {
        "doc_id": 49,
        "title": "Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation",
        "authors": [
            "Yixuan Zhang",
            "Qiaomin Xie"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Optimization and Control"
        ],
        "abstract": "Stochastic Approximation (SA) is a widely used algorithmic approach in various fields, including optimization and reinforcement learning (RL). Among RL algorithms, Q-learning is particularly popular due to its empirical success. In this paper, we study asynchronous Q-learning with constant stepsize, which is commonly used in practice for its fast convergence. By connecting the constant stepsize Q-learning to a time-homogeneous Markov chain, we show the distributional convergence of the iterates in Wasserstein distance and establish its exponential convergence rate. We also establish a Central Limit Theory for Q-learning iterates, demonstrating the asymptotic normality of the averaged iterates. Moreover, we provide an explicit expansion of the asymptotic bias of the averaged iterate in stepsize. Specifically, the bias is proportional to the stepsize up to higher-order terms and we provide an explicit expression for the linear coefficient. This precise characterization of the bias allows the application of Richardson-Romberg (RR) extrapolation technique to construct a new estimate that is provably closer to the optimal Q function. Numerical results corroborate our theoretical finding on the improvement of the RR extrapolation method.",
        "comments": "41 pages, 3 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13884"
    },
    {
        "doc_id": 50,
        "title": "Spontaneous stochasticity amplifies even thermal noise to the largest scales of turbulence in a few eddy turnover times",
        "authors": [
            "Dmytro Bandak",
            "Alexei Mailybaev",
            "Gregory L. Eyink",
            "Nigel Goldenfeld"
        ],
        "subjects": [
            "Fluid Dynamics"
        ],
        "abstract": "How predictable are turbulent flows? Here we use theoretical estimates and shell model simulations to argue that Eulerian spontaneous stochasticity, a manifestation of the non-uniqueness of the solutions to the Euler equation that is conjectured to occur in Navier-Stokes turbulence at high Reynolds numbers, leads to universal statistics at finite times, not just at infinite time as for standard chaos. These universal statistics are predictable, even though individual flow realizations are not. Any small-scale noise vanishing slowly enough with increasing Reynolds number can trigger spontaneous stochasticity and here we show that thermal noise alone, in the absence of any larger disturbances, would suffice. If confirmed for Navier-Stokes turbulence, our findings would imply that intrinsic stochasticity of turbulent fluid motions at all scales can be triggered even by unavoidable molecular noise, with implications for modeling in engineering, climate, astrophysics and cosmology.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13881"
    },
    {
        "doc_id": 51,
        "title": "Principal Component Regression to Study the Impact of Economic Factors on Disadvantaged Communities",
        "authors": [
            "Narmadha M. Mohankumar",
            "Milan Jain",
            "Heng Wan",
            "Sumitrra Ganguli",
            "Kyle D. Wilson",
            "David M. Anderson"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "The Council on Environmental Quality's Climate and Economic Justice Screening Tool defines \"disadvantaged communities\" (DAC) in the USA, highlighting census tracts where benefits of climate and energy investments are not accruing. We use a principal component generalized linear model, which addresses the intertwined nature of economic factors, income and employment and model their relationship to DAC status. Our study 1) identifies the most significant income groups and employment industries that impact DAC status, 2) provides the probability of DAC status across census tracts and compares the predictive accuracy with widely used machine learning approaches, 3) obtains historical predictions of the probability of DAC status, 4) obtains spatial downscaling of DAC status across block groups. Our study provides valuable insights for policymakers and stakeholders to develop strategies that promote sustainable development and address inequities in climate and energy investments in the USA.",
        "comments": "13 pages, 9 figures, 2 tables",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13880"
    },
    {
        "doc_id": 52,
        "title": "Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?",
        "authors": [
            "Huy Nguyen",
            "Pedram Akbarian",
            "Nhat Ho"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Dense-to-sparse gating mixture of experts (MoE) has recently become an effective alternative to a well-known sparse MoE. Rather than fixing the number of activated experts as in the latter model, which could limit the investigation of potential experts, the former model utilizes the temperature to control the softmax weight distribution and the sparsity of the MoE during training in order to stabilize the expert specialization. Nevertheless, while there are previous attempts to theoretically comprehend the sparse MoE, a comprehensive analysis of the dense-to-sparse gating MoE has remained elusive. Therefore, we aim to explore the impacts of the dense-to-sparse gate on the maximum likelihood estimation under the Gaussian MoE in this paper. We demonstrate that due to interactions between the temperature and other model parameters via some partial differential equations, the convergence rates of parameter estimations are slower than any polynomial rates, and could be as slow as $\\mathcal{O}(1/\\log(n))$, where $n$ denotes the sample size. To address this issue, we propose using a novel activation dense-to-sparse gate, which routes the output of a linear layer to an activation function before delivering them to the softmax function. By imposing linearly independence conditions on the activation function and its derivatives, we show that the parameter estimation rates are significantly improved to polynomial rates.",
        "comments": "53 pages",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13875"
    },
    {
        "doc_id": 53,
        "title": "A V2X-based Privacy Preserving Federated Measuring and Learning System",
        "authors": [
            "Levente Alekszejenk\u00f3",
            "Tadeusz Dobrowiecki"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Cryptography and Security",
            "Machine Learning"
        ],
        "abstract": "Future autonomous vehicles (AVs) will use a variety of sensors that generate a vast amount of data. Naturally, this data not only serves self-driving algorithms; but can also assist other vehicles or the infrastructure in real-time decision-making. Consequently, vehicles shall exchange their measurement data over Vehicle-to-Everything (V2X) technologies. Moreover, predicting the state of the road network might be beneficial too. With such a prediction, we might mitigate road congestion, balance parking lot usage, or optimize the traffic flow. That would decrease transportation costs as well as reduce its environmental impact.\n  In this paper, we propose a federated measurement and learning system that provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V) communication while also operating a federated learning (FL) scheme over the Vehicle-to-Network (V2N) link to create a predictive model of the transportation network. As we are yet to have real-world AV data, we model it with a non-IID (independent and identically distributed) dataset to evaluate the capabilities of the proposed system in terms of performance and privacy. Results indicate that the proposed FL scheme improves learning performance and prevents eavesdropping at the aggregator server side.",
        "comments": "8 pages, 5 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13848"
    },
    {
        "doc_id": 54,
        "title": "Machine learning for industrial sensing and control: A survey and practical perspective",
        "authors": [
            "Nathan P. Lawrence",
            "Seshu Kumar Damarla",
            "Jong Woo Kim",
            "Aditya Tulsyan",
            "Faraz Amjad",
            "Kai Wang",
            "Benoit Chachuat",
            "Jong Min Lee",
            "Biao Huang",
            "R. Bhushan Gopaluni"
        ],
        "subjects": [
            "Systems and Control",
            "Machine Learning"
        ],
        "abstract": "With the rise of deep learning, there has been renewed interest within the process industries to utilize data on large-scale nonlinear sensing and control problems. We identify key statistical and machine learning techniques that have seen practical success in the process industries. To do so, we start with hybrid modeling to provide a methodological framework underlying core application areas: soft sensing, process optimization, and control. Soft sensing contains a wealth of industrial applications of statistical and machine learning methods. We quantitatively identify research trends, allowing insight into the most successful techniques in practice.\n  We consider two distinct flavors for data-driven optimization and control: hybrid modeling in conjunction with mathematical programming techniques and reinforcement learning. Throughout these application areas, we discuss their respective industrial requirements and challenges.\n  A common challenge is the interpretability and efficiency of purely data-driven methods. This suggests a need to carefully balance deep learning techniques with domain knowledge. As a result, we highlight ways prior knowledge may be integrated into industrial machine learning applications. The treatment of methods, problems, and applications presented here is poised to inform and inspire practitioners and researchers to develop impactful data-driven sensing, optimization, and control solutions in the process industries.",
        "comments": "48 pages",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13836"
    },
    {
        "doc_id": 55,
        "title": "Generalized Free Cumulants for Quantum Chaotic Systems",
        "authors": [
            "Siddharth Jindal",
            "Pavan Hosur"
        ],
        "subjects": [
            "Statistical Mechanics",
            "High Energy Physics - Theory",
            "Quantum Physics"
        ],
        "abstract": "The eigenstate thermalization hypothesis (ETH) is the leading conjecture for the emergence of statistical mechanics in generic isolated quantum systems and is formulated in terms of the matrix elements of operators. An analog known as the ergodic bipartition (EB) describes entanglement and locality and is formulated in terms of the components of eigenstates. In this paper, we significantly generalize the EB and unify it with the ETH, extending the EB to study higher correlations and systems out of equilibrium. Our main result is a diagrammatic formalism that computes arbitrary correlations between eigenstates and operators based on a recently uncovered connection between the ETH and free probability theory. We refer to the connected components of our diagrams as generalized free cumulants. We apply our formalism in several ways. First, we focus on chaotic eigenstates and establish the so-called subsystem ETH and the Page curve as consequences of our construction. We also improve known calculations for thermal reduced density matrices and comment on an inherently free probabilistic aspect of the replica approach to entanglement entropy previously noticed in a calculation for the Page curve of an evaporating black hole. Next, we turn to chaotic quantum dynamics and demonstrate the ETH as a sufficient mechanism for thermalization, in general. In particular, we show that reduced density matrices relax to their equilibrium form and that systems obey the Page curve at late times. We also demonstrate that entanglement velocities, which govern the spreading of entanglement, are encoded in higher correlations of the EB. Lastly, we examine the chaotic structure of eigenstates and operators together and reveal previously overlooked correlations between them. Crucially, these correlations encode butterfly velocities, a well-known dynamical property of interacting quantum systems.",
        "comments": "44+14 pages, 19 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13829"
    },
    {
        "doc_id": 56,
        "title": "A Bayesian hierarchical mixture cure modelling framework to utilize multiple survival datasets for long-term survivorship estimates: A case study from previously untreated metastatic melanoma",
        "authors": [
            "Nathan Green",
            "Murat Kurt",
            "Andriy Moshyk",
            "James Larkin",
            "Gianluca Baio"
        ],
        "subjects": [
            "Applications",
            "Methodology"
        ],
        "abstract": "Time to an event of interest over a lifetime is a central measure of the clinical benefit of an intervention used in a health technology assessment (HTA). Within the same trial multiple end-points may also be considered. For example, overall and progression-free survival time for different drugs in oncology studies. A common challenge is when an intervention is only effective for some proportion of the population who are not clinically identifiable. Therefore, latent group membership as well as separate survival models for groups identified need to be estimated. However, follow-up in trials may be relatively short leading to substantial censoring. We present a general Bayesian hierarchical framework that can handle this complexity by exploiting the similarity of cure fractions between end-points; accounting for the correlation between them and improving the extrapolation beyond the observed data. Assuming exchangeability between cure fractions facilitates the borrowing of information between end-points. We show the benefits of using our approach with a motivating example, the CheckMate 067 phase 3 trial consisting of patients with metastatic melanoma treated with first line therapy.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13820"
    },
    {
        "doc_id": 57,
        "title": "Bayesian Analysis of the Beta Regression Model Subject to Linear Inequality Restrictions with Application",
        "authors": [
            "Solmaz Seifollahi",
            "Hossein Bevrani",
            "Kristofer Mansson"
        ],
        "subjects": [
            "Methodology",
            "Computation"
        ],
        "abstract": "ReRecent studies in machine learning are based on models in which parameters or state variables are bounded restricted. These restrictions are from prior information to ensure the validity of scientific theories or structural consistency based on physical phenomena. The valuable information contained in the restrictions must be considered during the estimation process to improve estimation accuracy. Many researchers have focused on linear regression models subject to linear inequality restrictions, but generalized linear models have received little attention. In this paper, the parameters of beta Bayesian regression models subjected to linear inequality restrictions are estimated. The proposed Bayesian restricted estimator, which is demonstrated by simulated studies, outperforms ordinary estimators. Even in the presence of multicollinearity, it outperforms the ridge estimator in terms of the standard deviation and the mean squared error. The results confirm that the proposed Bayesian restricted estimator makes sparsity in parameter estimating without using the regularization penalty. Finally, a real data set is analyzed by the new proposed Bayesian estimation method.",
        "comments": "16 pages, 7 tables",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13787"
    },
    {
        "doc_id": 58,
        "title": "Revisiting the memoryless property -- testing for the Pareto type I distribution",
        "authors": [
            "Lethani Ndwandwe",
            "James Allison",
            "Leonard Santana",
            "Jaco Visagie"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "We propose new goodness-of-fit tests for the Pareto type I distribution. These tests are based on a multiplicative version of the memoryless property which characterises this distribution. We present the results of a Monte Carlo power study demonstrating that the proposed tests are powerful compared to existing tests. As a result of independent interest, we demonstrate that tests specifically developed for the Pareto type I distribution substantially outperform tests for exponentiality applied to log-transformed data (since Pareto type I distributed values can be transformed to exponentiality via a simple log-transformation). Specifically, the newly proposed tests based on the multiplicative memoryless property of the Pareto distribution substantially outperform a test based on the memoryless property of the exponential distribution. The practical use of tests is illustrated by testing the hypothesis that two sets of observed golfers' earnings (those of the PGA and LIV tours) are realised from Pareto distributions.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13777"
    },
    {
        "doc_id": 59,
        "title": "Early Detection of Treatments Side Effect: A Sequential Approach",
        "authors": [
            "Jiayue Wang",
            "Ben Boukai"
        ],
        "subjects": [
            "Applications",
            "Statistics Theory"
        ],
        "abstract": "With the emergence and spread of infectious diseases with pandemic potential, such as COVID- 19, the urgency for vaccine development have led to unprecedented compressed and accelerated schedules that shortened the standard development timeline. In a relatively short time, the leading pharmaceutical companies1, received an Emergency Use Authorization (EUA) for vaccine\\prime s en-mass deployment To monitor the potential side effect(s) of the vaccine during the (initial) vaccination campaign, we developed an optimal sequential test that allows for the early detection of potential side effect(s). This test employs a rule to stop the vaccination process once the observed number of side effect incidents exceeds a certain (pre-determined) threshold. The optimality of the proposed sequential test is justified when compared with the (\u03b1, \u03b2) optimality of the non-randomized fixed-sample Uniformly Most Powerful (UMP) test. In the case of a single side effect, we study the properties of the sequential test and derive the exact expressions of the Average Sample Number (ASN) curve of the stopping time (and its variance) via the regularized incomplete beta function. Additionally, we derive the asymptotic distribution of the relative savings in ASN as compared to maximal sample size. Moreover, we construct the post-test parameter estimate and studied its sampling properties, including its asymptotic behavior under local-type alternatives. These limiting behavior results are the consistency and asymptotic normality of the post-test parameter estimator. We conclude the paper with a small simulation study illustrating the asymptotic performance of the point and interval estimation and provide a detailed example, based on COVID-19 side effect data (see Beatty et al. (2021)) of our suggested testing procedure.",
        "comments": "There are 21 pages, 8 pictures and 4 tables",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13760"
    },
    {
        "doc_id": 60,
        "title": "Assumptions and Bounds in the Instrumental Variable Model",
        "authors": [
            "Thomas S. Richardson",
            "James M. Robins"
        ],
        "subjects": [
            "Statistics Theory",
            "Artificial Intelligence"
        ],
        "abstract": "In this note we give proofs for results relating to the Instrumental Variable (IV) model with binary response $Y$ and binary treatment $X$, but with an instrument $Z$ that takes $K$ states that were originally stated in Richardson & Robins (2014), \"ACE Bounds; SEMS with Equilibrium Conditions,\" arXiv:1410.0470.",
        "comments": "27 pages, 1 figure, 1 table. Proofs of Theorems 1 and 2 stated in Richardson and Robins (2014), arXiv:1410.0470",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13758"
    },
    {
        "doc_id": 61,
        "title": "A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks",
        "authors": [
            "Charles Meyers",
            "Mohammad Reza Saleh Sedghpour",
            "Tommy L\u00f6fstedt",
            "Erik Elmroth"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a function of time and relate that to a novel metric that allows us to quickly determine whether or not the cost of training a model outweighs the cost of attacking it. Using this approach, we are able to approximate the expected failure rate using a small number of specially crafted samples rather than increasingly larger benchmark datasets. We demonstrate the efficacy of this technique on both the MNIST and CIFAR10 datasets using 8-, 16-, 32-, and 64-bit floating-point numbers, various data pre-processing techniques, and several attacks on five configurations of the ResNet model. Then, using empirical measurements, we examine the various trade-offs between cost, robustness, latency, and reliability to find that larger models do not significantly aid in adversarial robustness despite costing significantly more to train.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13751"
    },
    {
        "doc_id": 62,
        "title": "Conformal Prediction Sets Improve Human Decision Making",
        "authors": [
            "Jesse C. Cresswell",
            "Yi Sui",
            "Bhargava Kumar",
            "No\u00ebl Vouitsis"
        ],
        "subjects": [
            "Machine Learning",
            "Human-Computer Interaction",
            "Machine Learning"
        ],
        "abstract": "In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.",
        "comments": "Code available at https://github.com/layer6ai-labs/hitl-conformal-prediction",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13744"
    },
    {
        "doc_id": 63,
        "title": "The $M_{\\rm BH}-M_*$ relation up to $z\\sim2$ through decomposition of COSMOS-Web NIRCam images",
        "authors": [
            "Takumi S. Tanaka",
            "John D. Silverman",
            "Xuheng Ding",
            "Knud Jahnke",
            "Benny Trakhtenbrot",
            "Erini Lambrides",
            "Masafusa Onoue",
            "Irham Taufik Andika",
            "Angela Bongiorno",
            "Andreas L. Faisst",
            "Steven Gillman",
            "Christopher C. Hayward",
            "Michaela Hirschmann",
            "Anton Koekemoer",
            "Vasily Kokorev",
            "Zhaoxuan Liu",
            "Georgios E. Magdis",
            "Alvio Renzini",
            "Caitlin Casey",
            "Nicole E. Drakos",
            "Maximilien Franco",
            "Ghassem Gozaliasl",
            "Jeyhan Kartaltepe",
            "Daizhong Liu",
            "Henry Joy McCracken",
            "et al. (3 additional authors not shown)"
        ],
        "subjects": [
            "Astrophysics of Galaxies"
        ],
        "abstract": "Our knowledge of relations between supermassive black holes and their host galaxies at $z\\gtrsim1$ is still limited, even though being actively sought out to $z\\sim6$. Here, we use the high resolution and sensitivity of JWST to measure the host galaxy properties for 61 X-ray-selected type-I AGNs at $0.7<z<2.5$ with rest-frame optical/near-infrared imaging from COSMOS-Web and PRIMER. Black hole masses ($\\log\\left(M_{\\rm BH}/M_\\odot\\right)\\sim7.5-9.5$) are available from previous spectroscopic campaigns. We extract the host galaxy components from four NIRCam broadband images and the HST/ACS F814W image by applying a 2D image decomposition technique. We detect the host galaxy for $\\sim90\\%$ of the sample after subtracting the unresolved AGN emission. With host photometry free of AGN emission, we determine the stellar mass of the host galaxies to be $\\log\\left(M_*/M_\\odot\\right)\\sim10-11.5$ through SED fitting and measure the evolution of the mass relation between SMBHs and their host galaxies. Considering selection biases and measurement uncertainties, we find that the $M_\\mathrm{ BH}/M_*$ ratio evolves as $\\left(1+z\\right)^{0.37_{-0.60}^{+0.35}}$ thus remains essentially constant or exhibits mild evolution up to $z\\sim2.5$. We also see an amount of scatter ($\u03c3_\u03bc=0.28\\pm0.13$) is similar to the local relation and consistent with low-$z$ studies; this appears to not rule out non-causal cosmic assembly where mergers contribute to the statistical averaging towards the local relation. We highlight improvements to come with larger samples from JWST and, particularly, Euclid, which will exceed the statistical power of wide and deep surveys such as Subaru Hyper Suprime-Cam.",
        "comments": "31 pages, 19 figures, submitted to ApJ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13742"
    },
    {
        "doc_id": 64,
        "title": "Restoration of the Tully-Fisher Relation by Statistical Rectification",
        "authors": [
            "Hai Fu"
        ],
        "subjects": [
            "Astrophysics of Galaxies",
            "Instrumentation and Methods for Astrophysics"
        ],
        "abstract": "I employ the Lucy rectification algorithm to recover the inclination-corrected distribution of local disk galaxies in the plane of absolute magnitude ($M_i$) and \\HI\\ velocity width ($W_{20}$). By considering the inclination angle as a random variable with a known probability distribution, the novel approach eliminates one major source of uncertainty in studies of the Tully-Fisher relation: inclination angle estimation from axial ratio. Leveraging the statistical strength derived from the entire sample of 28,264 \\HI-selected disk galaxies at $z < 0.06$ from the Arecibo Legacy Fast ALFA (ALFALFA) survey, I show that the restored distribution follows a sharp correlation that is approximately a power law between $-16 > M_i > -22$: $M_i = M_0 - 2.5\u03b2\\ [\\log(W_{\\rm 20}/250 {\\rm km/s})]$, with $M_0 = -19.77\\pm0.04$ and $\u03b2= 4.39\\pm0.06$. At the brighter end ($M_i < -22$), the slope of the correlation decreases to $\u03b2\\approx 3.3$, confirming previous results. Because the method accounts for measurement errors, the intrinsic dispersion of the correlation is directly measured: $\u03c3(\\log W_{20}) \\approx 0.06$\\,dex between $-17 > M_i > -23$, while $\u03c3(M_i)$ decreases from $\\sim$0.8 in slow rotators to $\\sim$0.4 in fast rotators. The statistical rectification method holds significant potential, especially in the studies of intermediate-to-high-redshift samples, where limited spatial resolution hinders precise measurements of inclination angles.",
        "comments": "Resubmitted to ApJ Letters. Python notebook and data files are available at https://github.com/fuhaiastro/TFR_Lucy",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13738"
    },
    {
        "doc_id": 65,
        "title": "Is GN-z11 powered by a super-Eddington massive black hole?",
        "authors": [
            "Maulik Bhatt",
            "Simona Gallerani",
            "Andrea Ferrara",
            "Chiara Mazzucchelli",
            "Valentina D'Odorico",
            "Milena Valentini",
            "Tommaso Zana",
            "Emanuele Paolo Farina",
            "Srija Chakraborty"
        ],
        "subjects": [
            "Astrophysics of Galaxies",
            "Cosmology and Nongalactic Astrophysics"
        ],
        "abstract": "Observations of $z \\sim 6$ quasars powered by super-massive black holes (SMBHs, $M_{\\rm BH} \\sim 10^{8-10}\\, M_\\odot$) challenge our current understanding of early black hole formation and evolution. The advent of the James Webb Space Telescope (JWST) has enabled the study of massive black holes (MBHs, $M_{\\rm BH}\\sim 10^{6-7} \\ \\mathrm{M}_\\odot$) up to $z\\sim 11$, thus bridging the properties of $z\\sim 6$ quasars to their ancestors. JWST spectroscopic observations of GN-z11, a well-known $z=10.6$ star forming galaxy, have been interpreted with the presence of a super-Eddington (Eddington ratio $\\equiv \\,\u03bb_{\\rm Edd}\\sim 5.5$) accreting MBH. To test this hypothesis we use a zoom-in cosmological simulation of galaxy formation and BH co-evolution. We first test the simulation results against the observed probability distribution function (PDF) of $\u03bb_{\\rm Edd}$ found in $z\\sim 6$ quasars. Then, we select in the simulation those BHs that satisfy the following criteria: (a) $10 < z < 11 $, (b) $M_{\\rm BH} > 10^6 \\ \\mathrm{M}_\\odot$. Finally we apply the Extreme Value Statistics to the PDF of $\u03bb_{\\rm Edd}$ resulting from the simulation and find that the probability of observing a $z\\sim 10-11$ MBH, accreting with $\u03bb_{\\rm Edd} \\sim 5.5$, in the volume surveyed by JWST, is very low ($<0.5\\%$). We compare our predictions with those in the literature and further discuss the main limitations of our work. Our simulation cannot explain the JWST observations of GN-z11. This might be due to (i) missing physics in simulations, or (ii) uncertainties in the data analysis.",
        "comments": "8 pages, 2 figures; Submitted to A&A",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13733"
    },
    {
        "doc_id": 66,
        "title": "Origin of the Stochastic Gravitational Wave Background: First-Order Phase Transition vs. Black Hole Mergers",
        "authors": [
            "Martin Wolfgang Winkler",
            "Katherine Freese"
        ],
        "subjects": [
            "Cosmology and Nongalactic Astrophysics",
            "General Relativity and Quantum Cosmology",
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "The NANOGrav, Parkes and European Pulsar Timing Array (PTA) experiments have collected strong evidence for a stochastic gravitational wave background in the nHz-frequency band. In this work we perform a detailed statistical analysis of the signal in order to elucidate its physical origin. Specifically, we test the standard explanation in terms of supermassive black hole mergers against the prominent alternative explanation in terms of a first-order phase transition. By means of a frequentist hypothesis test we find that the observed gravitational wave spectrum prefers a first-order phase transition at $2-3\u03c3$ significance compared to black hole mergers (depending on the underlying black hole model). This mild preference is linked to the relatively large amplitude of the observed gravitational wave signal (above the typical expectation of black hole models) and to its spectral shape (which slightly favors the phase-transition spectrum over the predominantly single power-law spectrum predicted in black hole models). The best fit to the combined PTA data set is obtained for a phase transition which dominantly produces the gravitational wave signal by bubble collisions (rather than by sound waves). The best-fit (energy-density) spectrum features, within the frequency band of the PTA experiments, a crossover from a steeply rising power law (causality tail) to a softly rising power law; the peak frequency then falls slightly above the PTA-measured range. Such a spectrum can be obtained for a strong first-order phase transition in the thick-wall regime of vacuum tunneling which reheats the Universe to a temperature of $T_*\\sim \\text{GeV}$. A dark sector phase transition at the GeV-scale provides a comparably good fit.",
        "comments": "26 pages, 8 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13729"
    },
    {
        "doc_id": 67,
        "title": "Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare",
        "authors": [
            "Vibeke Binz Vallevik",
            "Aleksandar Babic",
            "Serena Elizabeth Marshall",
            "Severin Elvatun",
            "Helga Br\u00f8gger",
            "Sharmini Alagaratnam",
            "Bj\u00f8rn Edwin",
            "Narasimha Raghavan Veeraragavan",
            "Anne Kjersti Befring",
            "Jan Franz Nyg\u00e5rd"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence"
        ],
        "abstract": "Ensuring safe adoption of AI tools in healthcare hinges on access to sufficient data for training, testing and validation. In response to privacy concerns and regulatory requirements, using synthetic data has been suggested. Synthetic data is created by training a generator on real data to produce a dataset with similar statistical properties. Competing metrics with differing taxonomies for quality evaluation have been suggested, resulting in a complex landscape. Optimising quality entails balancing considerations that make the data fit for use, yet relevant dimensions are left out of existing frameworks. We performed a comprehensive literature review on the use of quality evaluation metrics on SD within the scope of tabular healthcare data and SD made using deep generative methods. Based on this and the collective team experiences, we developed a conceptual framework for quality assurance. The applicability was benchmarked against a practical case from the Dutch National Cancer Registry. We present a conceptual framework for quality assurance of SD for AI applications in healthcare that aligns diverging taxonomies, expands on common quality dimensions to include the dimensions of Fairness and Carbon footprint, and proposes stages necessary to support real-life applications. Building trust in synthetic data by increasing transparency and reducing the safety risk will accelerate the development and uptake of trustworthy AI tools for the benefit of patients. Despite the growing emphasis on algorithmic fairness and carbon footprint, these metrics were scarce in the literature review. The overwhelming focus was on statistical similarity using distance metrics while sequential logic detection was scarce. A consensus-backed framework that includes all relevant quality dimensions can provide assurance for safe and responsible real-life applications of SD.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13716"
    },
    {
        "doc_id": 68,
        "title": "Accelerating hyperbolic t-SNE",
        "authors": [
            "Martin Skrodzki",
            "Hunter van Geffen",
            "Nicolas F. Chaves-de-Plaza",
            "Thomas H\u00f6llt",
            "Elmar Eisemann",
            "Klaus Hildebrandt"
        ],
        "subjects": [
            "Human-Computer Interaction",
            "Artificial Intelligence",
            "Machine Learning",
            "Quantitative Methods",
            "Machine Learning"
        ],
        "abstract": "The need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. Hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. Subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. However, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. That is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. Furthermore, due to the non-linear nature of hyperbolic spaces, Euclidean acceleration structures cannot directly be translated to the hyperbolic setting. This paper introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. We compare our approach with existing methods and demonstrate that it computes embeddings of similar quality in significantly less time. Implementation and scripts for the experiments can be found at https://graphics.tudelft.nl/accelerating-hyperbolic-tsne.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13708"
    },
    {
        "doc_id": 69,
        "title": "Determinants of the Propensity for Innovation among Entrepreneurs in the Tourism Industry",
        "authors": [
            "Miguel Angel Montanes-Del-Rio",
            "Jose Aurelio Medina-Garrido"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "Tourism's increasing share of Gross Domestic Product throughout the world, its impact on employment and its continuous growth justifies the interest it raises amongst entrepreneurs and public authorities. However, this growth coexists with intense competition; as a result of which, tourism companies must continuously innovate in order to survive and grow. This is evident in the diversification of tourism products and destinations, the improvement of business processes and the incorporation of new technologies for intermediation, amongst other examples. This paper expounds on the factors that explain the propensity for innovation amongst tourism entrepreneurs and it may help governments to promote innovation that is based on those determining factors. The hypotheses are tested using a logistic regression on 699 international tourism entrepreneurs, taken from the 2014 Global Adult Population Survey of the Global Entrepreneurship Monitor project. The propensity for innovation amongst tourism entrepreneurs has a statistically significant relationship to gender, age, level of education and informal investments in previous businesses.",
        "comments": "Journal ref:        Sustainability 12:5003 (2020)",
        "date": "5 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.13679"
    },
    {
        "doc_id": 70,
        "title": "Analisis de la incidencia de la inversion extranjera directa y la inversion nacional, en el crecimiento economico de Chile",
        "authors": [
            "Alvear Guzman Katherine",
            "Campozano Buele Jenner",
            "Duran Canarte Paulette",
            "Holguin Cedeno Roger",
            "Mejia Crespin Fernando"
        ],
        "subjects": [
            "General Economics"
        ],
        "abstract": "The research aims to assess the impact of foreign direct investment (FDI) and domestic investment on Chile's economic growth. By elucidating the relationship between FDI and domestic investment, the study contributes valuable insights for economic policy formulation and future investments. The findings hold significance in shaping Chile's international perception as an investment destination, potentially influencing its standing in the global economic landscape. Demonstrating that FDI is a significant driver of economic growth could enhance confidence among foreign investors. The project's importance lies in contributing to economic knowledge and guiding strategic decisions for sustainable economic growth in Chile. Understanding the interplay of FDI and domestic investment allows for a balanced approach, promoting stable economic development and mitigating issues like excessive reliance on foreign investment. The study highlights the theory of internationalization as a conceptual framework for understanding the motives and strategies of multinational companies investing abroad. Leveraging data from sources like the Central Bank of Chile, the research analyzes variables such as Chile's economic growth (GDP), FDI, and domestic investment. The hypothesis posits a significant long-term causal relationship between FDI, National Investment (NI), and Chile's Economic Growth (GDP). Statistical analysis using the Eviews 6 software tool confirms that attracting foreign investments and promoting internal investment are imperative for sustainable economic growth in Chile.",
        "comments": "in Spanish language",
        "date": "13 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.13674"
    },
    {
        "doc_id": 71,
        "title": "Entrywise Inference for Causal Panel Data: A Simple and Instance-Optimal Approach",
        "authors": [
            "Yuling Yan",
            "Martin J. Wainwright"
        ],
        "subjects": [
            "Statistics Theory",
            "Econometrics",
            "Methodology",
            "Machine Learning"
        ],
        "abstract": "In causal inference with panel data under staggered adoption, the goal is to estimate and derive confidence intervals for potential outcomes and treatment effects. We propose a computationally efficient procedure, involving only simple matrix algebra and singular value decomposition. We derive non-asymptotic bounds on the entrywise error, establishing its proximity to a suitably scaled Gaussian variable. Despite its simplicity, our procedure turns out to be instance-optimal, in that our theoretical scaling matches a local instance-wise lower bound derived via a Bayesian Cram\u00e9r-Rao argument. Using our insights, we develop a data-driven procedure for constructing entrywise confidence intervals with pre-specified coverage guarantees. Our analysis is based on a general inferential toolbox for the SVD algorithm applied to the matrix denoising model, which might be of independent interest.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13665"
    },
    {
        "doc_id": 72,
        "title": "Navigating Multidimensional Ideologies with Reddit's Political Compass: Economic Conflict and Social Affinity",
        "authors": [
            "Ernesto Colacrai",
            "Federico Cinus",
            "Gianmarco De Francisci Morales",
            "Michele Starnini"
        ],
        "subjects": [
            "Social and Information Networks",
            "Computers and Society",
            "Physics and Society",
            "Applications"
        ],
        "abstract": "The prevalent perspective in quantitative research on opinion dynamics flattens the landscape of the online political discourse into a traditional left--right dichotomy. While this approach helps simplify the analysis and modeling effort, it also neglects the intrinsic multidimensional richness of ideologies. In this study, we analyze social interactions on Reddit, under the lens of a multi-dimensional ideological framework: the political compass. We examine over 8 million comments posted on the subreddits /r/PoliticalCompass and /r/PoliticalCompassMemes during 2020--2022. By leveraging their self-declarations, we disentangle the ideological dimensions of users into economic (left--right) and social (libertarian--authoritarian) axes. In addition, we characterize users by their demographic attributes (age, gender, and affluence).\n  We find significant homophily for interactions along the social axis of the political compass and demographic attributes. Compared to a null model, interactions among individuals of similar ideology surpass expectations by 6%. In contrast, we uncover a significant heterophily along the economic axis: left/right interactions exceed expectations by 10%. Furthermore, heterophilic interactions are characterized by a higher language toxicity than homophilic interactions, which hints at a conflictual discourse between every opposite ideology. Our results help reconcile apparent contradictions in recent literature, which found a superposition of homophilic and heterophilic interactions in online political discussions. By disentangling such interactions into the economic and social axes we pave the way for a deeper understanding of opinion dynamics on social media.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13656"
    },
    {
        "doc_id": 73,
        "title": "The limitations (and potential) of non-parametric morphology statistics for post-merger identification",
        "authors": [
            "Scott Wilkinson",
            "Sara L. Ellison",
            "Connor Bottrell",
            "Robert W. Bickley",
            "Shoshannah Byrne-Mamahit",
            "Leonardo Ferreira",
            "David R. Patton"
        ],
        "subjects": [
            "Astrophysics of Galaxies"
        ],
        "abstract": "Non-parametric morphology statistics have been used for decades to classify galaxies into morphological types and identify mergers in an automated way. In this work, we assess how reliably we can identify galaxy post-mergers with non-parametric morphology statistics. Low-redshift (z<0.2), recent (t_post-merger < 200 Myr), and isolated (r > 100 kpc) post-merger galaxies are drawn from the IllustrisTNG100-1 cosmological simulation. Synthetic r-band images of the mergers are generated with SKIRT9 and degraded to various image qualities, adding observational effects such as sky noise and atmospheric blurring. We find that even in perfect quality imaging, the individual non-parametric morphology statistics fail to recover more than 55% of the post-mergers, and that this number decreases precipitously with worsening image qualities. The realistic distributions of galaxy properties in IllustrisTNG allow us to show that merger samples assembled using individual morphology statistics are biased towards low mass, high gas fraction, and high mass ratio. However, combining all of the morphology statistics together using either a linear discriminant analysis or random forest algorithm increases the completeness and purity of the identified merger samples and mitigates bias with various galaxy properties. For example, we show that in imaging similar to that of the 10-year depth of the Legacy Survey of Space and Time (LSST), a random forest can identify 89% of mergers with a false positive rate of 17%. Finally, we conduct a detailed study of the effect of viewing angle on merger observability and find that there may be an upper limit to merger recovery due to the orientation of merger features with respect to the observer.",
        "comments": "32 pages, 21 figures Accepted for publication by MNRAS",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13654"
    },
    {
        "doc_id": 74,
        "title": "Unveiling homophily beyond the pool of opportunities",
        "authors": [
            "Sina Sajjadi",
            "Samuel Martin-Gutierrez",
            "Fariba Karimi"
        ],
        "subjects": [
            "Physics and Society"
        ],
        "abstract": "Unveiling individuals' preferences for connecting with similar others (choice homophily) beyond the structural factors determining the pool of opportunities, is a challenging task. Here, we introduce a robust methodology for quantifying and inferring choice homophily in a variety of social networks. Our approach employs statistical network ensembles to estimate and standardize homophily measurements. We control for group size imbalances and activity disparities by counting the number of possible network configurations with a given number of inter-group links using combinatorics. This method provides a principled measure of connection preferences and their confidence intervals. Our framework is versatile, suitable for undirected and directed networks, and applicable in scenarios involving multiple groups. To validate our inference method, we test it on synthetic networks and show that it outperforms traditional metrics. Our approach accurately captures the generative homophily used to build the networks, even when we include additional tie-formation mechanisms, such as preferential attachment and triadic closure. Results show that while triadic closure has some influence on the inference, its impact is small in homophilic networks. On the other hand, preferential attachment does not perturb the results of the inference method. We apply our method to real-world networks, demonstrating its effectiveness in unveiling underlying gender homophily. Our method aligns with traditional metrics in networks with balanced populations, but we obtain different results when the group sizes or degrees are imbalanced. This finding highlights the importance of considering structural factors when measuring choice homophily in social networks.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13642"
    },
    {
        "doc_id": 75,
        "title": "Can overfitted deep neural networks in adversarial training generalize? -- An approximation viewpoint",
        "authors": [
            "Zhongjie Shi",
            "Fanghui Liu",
            "Yuan Cao",
            "Johan A. K. Suykens"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Adversarial training is a widely used method to improve the robustness of deep neural networks (DNNs) over adversarial perturbations. However, it is empirically observed that adversarial training on over-parameterized networks often suffers from the \\textit{robust overfitting}: it can achieve almost zero adversarial training error while the robust generalization performance is not promising. In this paper, we provide a theoretical understanding of the question of whether overfitted DNNs in adversarial training can generalize from an approximation viewpoint. Specifically, our main results are summarized into three folds: i) For classification, we prove by construction the existence of infinitely many adversarial training classifiers on over-parameterized DNNs that obtain arbitrarily small adversarial training error (overfitting), whereas achieving good robust generalization error under certain conditions concerning the data quality, well separated, and perturbation level. ii) Linear over-parameterization (meaning that the number of parameters is only slightly larger than the sample size) is enough to ensure such existence if the target function is smooth enough. iii) For regression, our results demonstrate that there also exist infinitely many overfitted DNNs with linear over-parameterization in adversarial training that can achieve almost optimal rates of convergence for the standard generalization error. Overall, our analysis points out that robust overfitting can be avoided but the required model capacity will depend on the smoothness of the target function, while a robust generalization gap is inevitable. We hope our analysis will give a better understanding of the mathematical foundations of robustness in DNNs from an approximation view.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13624"
    },
    {
        "doc_id": 76,
        "title": "The assessment of replicability using the sum of p-values",
        "authors": [
            "Leonhard Held",
            "Samuel Pawel",
            "Charlotte Micheloud"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "Statistical significance of both the original and the replication study is a commonly used criterion to assess replication attempts, also known as the two-trials rule in drug development. However, replication studies are sometimes conducted although the original study is non-significant, in which case Type-I error rate control across both studies is no longer guaranteed. We propose an alternative method to assess replicability using the sum of p-values from the two studies. The approach provides a combined p-value and can be calibrated to control the overall Type-I error rate at the same level as the two-trials rule but allows for replication success even if the original study is non-significant. The unweighted version requires a less restrictive level of significance at replication if the original study is already convincing which facilitates sample size reductions of up to 10%. Downweighting the original study accounts for possible bias and requires a more stringent significance level and larger samples sizes at replication. Data from four large-scale replication projects are used to illustrate and compare the proposed method with the two-trials rule, meta-analysis and Fisher's combination method.",
        "comments": "6 figures, 0 tables, 1 box",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13615"
    },
    {
        "doc_id": 77,
        "title": "The Doob transform and the tree behind the forest, with application to near-critical dimers",
        "authors": [
            "Lucas Rey"
        ],
        "subjects": [
            "Probability"
        ],
        "abstract": "The Doob transform technique enables the study of a killed random walk (KRW) via a random walk (RW) with transition probabilities tilted by a discrete massive harmonic function. The main contribution of this paper is to transfer this powerful technique to statistical mechanics by relating two models, namely random rooted spanning forests (RSF) and random spanning trees (RST), and provide applications. More precisely, our first main theorem explicitly relates models on the level of partition functions, and probability measures, in the case of finite and infinite graphs. Then, in the planar case, we also rely on the dimer model: we introduce a killed and a drifted dimer model, extending to this general framework the models introduced in [Chh12,dT20]. Using Temperley's bijection between RST and dimers, this allows us to relate RSF to dimers and thus extend partially this bijection to RSF. As immediate applications, we give a short and transparent proof of Kenyon's result stating that the spectral curve of RSF is a Harnack curve, and provide a general setting to relate discrete massive holomorphic and harmonic functions. The other important application consists in proving universality of the convergence of the near-critical loop-erased RW, RST and dimer models by extending the results of [Chh12,CW21,HSB22] from the square lattice to any isoradial graphs: we introduce a loop erased RW, RST and dimer model on isoradial discretizations of any simply connected domain and prove convergence in the massive scaling limit towards continuous objects described by a massive version of SLE2.",
        "comments": "50 pages, 8 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13599"
    },
    {
        "doc_id": 78,
        "title": "WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition",
        "authors": [
            "Zhengyao Song",
            "Yongqiang Li",
            "Danni Yuan",
            "Li Liu",
            "Shaokui Wei",
            "Baoyuan Wu"
        ],
        "subjects": [
            "Cryptography and Security"
        ],
        "abstract": "This work explores an emerging security threat against deep neural networks (DNNs) based image classification, i.e., backdoor attack. In this scenario, the attacker aims to inject a backdoor into the model by manipulating training data, such that the backdoor could be activated by a particular trigger and bootstraps the model to make a target prediction at inference. Currently, most existing data poisoning-based attacks struggle to achieve success at low poisoning ratios, increasing the risk of being defended by defense methods. In this paper, we propose a novel frequency-based backdoor attack via Wavelet Packet Decomposition (WPD), WPD decomposes the original image signal to a spectrogram that contains frequency information with different semantic meanings. We leverage WPD to statistically analyze the frequency distribution of the dataset to infer the key frequency regions the DNNs would focus on, and the trigger information is only injected into the key frequency regions. Our method mainly includes three parts: 1) the selection of the poisoning frequency regions in spectrogram; 2) trigger generation; 3) the generation of the poisoned dataset. Our method is stealthy and precise, evidenced by the 98.12% Attack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio 0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can bypass most existing defense methods. Besides, we also provide visualization analyses to explain why our method works.",
        "comments": "13 pages, 21 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13578"
    },
    {
        "doc_id": 79,
        "title": "Rare event probability estimation for groundwater inverse problems with a two-stage Sequential Monte Carlo approach",
        "authors": [
            "Lea Friedli",
            "Niklas Linde"
        ],
        "subjects": [
            "Applications"
        ],
        "abstract": "Bayesian inversions followed by estimations of rare event probabilities are often needed to analyse groundwater hazards. Instead of focusing on the posterior distribution of model parameters, the main interest lies then in the distribution of a specific quantity of interest contingent upon these parameters. To address the associated methodological challenges, we introduce a two-stage Sequential Monte Carlo approach. In the first stage, it generates particles that approximate the posterior distribution; in the second stage, it employs subset sampling techniques to assess the probability of the rare event of interest. By considering two hydrogeological problems of increasing complexity, we showcase the efficiency and accuracy of the resulting PostRisk-SMC method for rare event probability estimation related to groundwater hazards. We compare the performance of the PostRisk-SMC method with a traditional Monte Carlo approach that relies on Markov chain Monte Carlo samples. We showcase that our estimates align with those of the traditional method, but the coefficients of variation are notably lower for the same computational budget when targeting more rare events. Furthermore, we highlight that the PostRisk-SMC method allows estimating rare event probabilities approaching one in a billion using less than one hundred thousand forward simulations. Even if the presented examples are related to groundwater hazards, the methodology is well-suited for addressing a wide range of topics in the geosciences and beyond.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13572"
    },
    {
        "doc_id": 80,
        "title": "Benchmarking the Fairness of Image Upsampling Methods",
        "authors": [
            "Mike Laszkiewicz",
            "Imant Daunhawer",
            "Julia E. Vogt",
            "Asja Fischer",
            "Johannes Lederer"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\\unicode{x2013}$inspired by their supervised fairness counterparts$\\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13555"
    },
    {
        "doc_id": 81,
        "title": "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?",
        "authors": [
            "Ri\u010dards Marcinkevi\u010ds",
            "Sonia Laguna",
            "Moritz Vandenhirtz",
            "Julia E. Vogt"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the practical utility of the proposed techniques, we apply them to deep chest X-ray classifiers and show that fine-tuned black boxes can be as intervenable and more performant than CBMs.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13544"
    },
    {
        "doc_id": 82,
        "title": "Functional weak convergence of stochastic integrals for moving averages and continuous-time random walks",
        "authors": [
            "Andreas S\u00f8jmark",
            "Fabrice Wunderlich"
        ],
        "subjects": [
            "Probability",
            "Statistics Theory"
        ],
        "abstract": "There is by now an extensive and well-developed theory of weak convergence for moving averages and continuous-time random walks (CTRWs) with respect to Skorokhod's M1 and J1 topologies. Here we address the fundamental question of how this translates into functional limit theorems, in the M1 or J1 topology, for stochastic integrals driven by these processes. As a key application, we provide weak approximation results for a general class of SDEs driven by time-changed L\u00e9vy processes. Such SDEs and their associated fractional Fokker--Planck--Kolmogorov equations are central to models of anomalous diffusion in statistical physics, and our results provide a rigorous functional characterisation of these as continuum limits of the corresponding models driven by CTRWs. In regard to strictly M1 convergent moving averages and so-called correlated CTRWs, it turns out that the convergence of stochastic integrals can fail markedly. Nevertheless, we are able to identify natural classes of integrand processes for which the convergence holds. We end by showing that these results are general enough to yield functional limit theorems, in the M1 topology, for certain stochastic delay differential equations driven by moving averages.",
        "comments": "43 pages",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13543"
    },
    {
        "doc_id": 83,
        "title": "Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models",
        "authors": [
            "Lukas Heinrich",
            "Tobias Golling",
            "Michael Kagan",
            "Samuel Klein",
            "Matthew Leigh",
            "Margarita Osadchy",
            "John Andrew Raine"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "Machine Learning",
            "High Energy Physics - Experiment",
            "Data Analysis, Statistics and Probability"
        ],
        "abstract": "We propose masked particle modeling (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capability of the model, showing that it can be adapted to tasks such as supervised and weakly supervised jet classification, and that the model can transfer efficiently with small fine-tuning data sets to new classes and new data domains.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13537"
    },
    {
        "doc_id": 84,
        "title": "Finetuning Foundation Models for Joint Analysis Optimization",
        "authors": [
            "Matthias Vigl",
            "Nicole Hartman",
            "Lukas Heinrich"
        ],
        "subjects": [
            "High Energy Physics - Experiment",
            "Machine Learning",
            "High Energy Physics - Phenomenology",
            "Data Analysis, Statistics and Probability"
        ],
        "abstract": "In this work we demonstrate that significant gains in performance and data efficiency can be achieved in High Energy Physics (HEP) by moving beyond the standard paradigm of sequential optimization or reconstruction and analysis components. We conceptually connect HEP reconstruction and analysis to modern machine learning workflows such as pretraining, finetuning, domain adaptation and high-dimensional embedding spaces and quantify the gains in the example usecase of searches of heavy resonances decaying via an intermediate di-Higgs system to four $b$-jets.",
        "comments": "13 pages, 12 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13536"
    },
    {
        "doc_id": 85,
        "title": "Depth Patterns",
        "authors": [
            "Annika Betken",
            "Alexander Schnurr"
        ],
        "subjects": [
            "Statistics Theory",
            "Probability"
        ],
        "abstract": "We establish a definition of ordinal patterns for multivariate time series data based on the concept of Tukey's halfspace depth. Given the definition of these \\emph{depth patterns}, we are interested in the probabilities of observing specific patterns in a time series. For this, we consider the relative frequency of depth patterns as natural estimators for their occurrence probabilities. Depending on the choice of reference distribution and the relation between reference and data distribution, we distinguish different settings that are considered separately. Within these settings we study statistical properties of ordinal pattern probabilities, establishing consistency and asymptotic normality under the assumption of weakly dependent time series data. Since our concept only depends on ordinal depth information, the resulting values are robust under small perturbations and measurement errors.",
        "comments": "MSC Class:          62M10; 62H10; 62H12; 60F05",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13532"
    },
    {
        "doc_id": 86,
        "title": "Detecting local perturbations of networks in a latent hyperbolic space",
        "authors": [
            "Alice Longhena",
            "Martin Guillemaud",
            "Mario Chavez"
        ],
        "subjects": [
            "Quantitative Methods",
            "Data Analysis, Statistics and Probability"
        ],
        "abstract": "Graph theoretical approaches have been proven to be effective in the characterization of connected systems, as well as in quantifying their dysfunction due to perturbation. In this paper, we show the advantage of a non-Euclidean (hyperbolic) representation of networks to identify local connectivity perturbations and to characterize the induced effects on a large scale. We propose two perturbation scores based on representations of the networks in a latent geometric space, obtained through an embedding onto the hyperbolic Poincar\u00e9 disk. We numerically demonstrate that these methods are able to localize perturbations in networks with homogeneous or heterogeneous degree connectivity. We apply this framework to identify the most perturbed brain areas in epileptic patients following surgery. This study is conceived in the effort of developing more powerful tools to represent and analyze brain networks, and it is the first to apply geometric network embedding techniques to the case of epilepsy.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13495"
    },
    {
        "doc_id": 87,
        "title": "Detection of Correlated Random Vectors",
        "authors": [
            "Dor Elimelech",
            "Wasim Huleihel"
        ],
        "subjects": [
            "Information Theory",
            "Machine Learning",
            "Statistics Theory"
        ],
        "abstract": "In this paper, we investigate the problem of deciding whether two standard normal random vectors $\\mathsf{X}\\in\\mathbb{R}^{n}$ and $\\mathsf{Y}\\in\\mathbb{R}^{n}$ are correlated or not. This is formulated as a hypothesis testing problem, where under the null hypothesis, these vectors are statistically independent, while under the alternative, $\\mathsf{X}$ and a randomly and uniformly permuted version of $\\mathsf{Y}$, are correlated with correlation $\u03c1$. We analyze the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$ and $\u03c1$. To derive our information-theoretic lower bounds, we develop a novel technique for evaluating the second moment of the likelihood ratio using an orthogonal polynomials expansion, which among other things, reveals a surprising connection to integer partition functions. We also study a multi-dimensional generalization of the above setting, where rather than two vectors we observe two databases/matrices, and furthermore allow for partial correlations between these two.",
        "comments": "34 pages",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13429"
    },
    {
        "doc_id": 88,
        "title": "Text Categorization Can Enhance Domain-Agnostic Stopword Extraction",
        "authors": [
            "Houcemeddine Turki",
            "Naome A. Etori",
            "Mohamed Ali Hadj Taieb",
            "Abdul-Hakeem Omotayo",
            "Chris Chinenye Emezue",
            "Mohamed Ben Aouicha",
            "Ayodele Awokoya",
            "Falalu Ibrahim Lawan",
            "Doreen Nixdorf"
        ],
        "subjects": [
            "Computation and Language",
            "Machine Learning"
        ],
        "abstract": "This paper investigates the role of text categorization in streamlining stopword extraction in natural language processing (NLP), specifically focusing on nine African languages alongside French. By leveraging the MasakhaNEWS, African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that text categorization effectively identifies domain-agnostic stopwords with over 80% detection success rate for most examined languages. Nevertheless, linguistic variances result in lower detection rates for certain languages. Interestingly, we find that while over 40% of stopwords are common across news categories, less than 15% are unique to a single category. Uncommon stopwords add depth to text but their classification as stopwords depends on context. Therefore combining statistical and linguistic approaches creates comprehensive stopword lists, highlighting the value of our hybrid method. This research enhances NLP for African languages and underscores the importance of text categorization in stopword extraction.",
        "comments": "A Project Report for the Masakhane Research Community",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13398"
    },
    {
        "doc_id": 89,
        "title": "An Ising Similarity Regression Model for Modeling Multivariate Binary Data",
        "authors": [
            "Zhi Yang Tho",
            "Francis K. C. Hui",
            "Tao Zou"
        ],
        "subjects": [
            "Methodology",
            "Statistics Theory",
            "Applications"
        ],
        "abstract": "Understanding the dependence structure between response variables is an important component in the analysis of correlated multivariate data. This article focuses on modeling dependence structures in multivariate binary data, motivated by a study aiming to understand how patterns in different U.S. senators' votes are determined by similarities (or lack thereof) in their attributes, e.g., political parties and social network profiles. To address such a research question, we propose a new Ising similarity regression model which regresses pairwise interaction coefficients in the Ising model against a set of similarity measures available/constructed from covariates. Model selection approaches are further developed through regularizing the pseudo-likelihood function with an adaptive lasso penalty to enable the selection of relevant similarity measures. We establish estimation and selection consistency of the proposed estimator under a general setting where the number of similarity measures and responses tend to infinity. Simulation study demonstrates the strong finite sample performance of the proposed estimator in terms of parameter estimation and similarity selection. Applying the Ising similarity regression model to a dataset of roll call voting records of 100 U.S. senators, we are able to quantify how similarities in senators' parties, businessman occupations and social network profiles drive their voting associations.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13379"
    },
    {
        "doc_id": 90,
        "title": "Mitigating System Bias in Resource Constrained Asynchronous Federated Learning Systems",
        "authors": [
            "Jikun Gao",
            "Ioannis Mavromatis",
            "Peizheng Li",
            "Pietro Carnelli",
            "Aftab Khan"
        ],
        "subjects": [
            "Machine Learning"
        ],
        "abstract": "Federated learning (FL) systems face performance challenges in dealing with heterogeneous devices and non-identically distributed data across clients. We propose a dynamic global model aggregation method within Asynchronous Federated Learning (AFL) deployments to address these issues. Our aggregation method scores and adjusts the weighting of client model updates based on their upload frequency to accommodate differences in device capabilities. Additionally, we also immediately provide an updated global model to clients after they upload their local models to reduce idle time and improve training efficiency. We evaluate our approach within an AFL deployment consisting of 10 simulated clients with heterogeneous compute constraints and non-IID data. The simulation results, using the FashionMNIST dataset, demonstrate over 10% and 19% improvement in global model accuracy compared to state-of-the-art methods PAPAYA and FedAsync, respectively. Our dynamic aggregation method allows reliable global model training despite limiting client resources and statistical data heterogeneity. This improves robustness and scalability for real-world FL deployments.",
        "comments": "6 pages, 5 figures. This work has been accepted by PerCom PerconAI workshop 2024",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13366"
    },
    {
        "doc_id": 91,
        "title": "Full Bayesian Significance Testing for Neural Networks",
        "authors": [
            "Zehua Liu",
            "Zimeng Li",
            "Jingyuan Wang",
            "Yue He"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "Significance testing aims to determine whether a proposition about the population distribution is the truth or not given observations. However, traditional significance testing often needs to derive the distribution of the testing statistic, failing to deal with complex nonlinear relationships. In this paper, we propose to conduct Full Bayesian Significance Testing for neural networks, called \\textit{n}FBST, to overcome the limitation in relationship characterization of traditional approaches. A Bayesian neural network is utilized to fit the nonlinear and multi-dimensional relationships with small errors and avoid hard theoretical derivation by computing the evidence value. Besides, \\textit{n}FBST can test not only global significance but also local and instance-wise significance, which previous testing methods don't focus on. Moreover, \\textit{n}FBST is a general framework that can be extended based on the measures selected, such as Grad-\\textit{n}FBST, LRP-\\textit{n}FBST, DeepLIFT-\\textit{n}FBST, LIME-\\textit{n}FBST. A range of experiments on both simulated and real data are conducted to show the advantages of our method.",
        "comments": "Published as a conference paper at AAAI 2024",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13335"
    },
    {
        "doc_id": 92,
        "title": "An Explicit Scheme for Pathwise XVA Computations",
        "authors": [
            "Lokman Abbas-Turki",
            "St\u00e9phane Cr\u00e9pey",
            "Botao Li",
            "Bouazza Saadeddine"
        ],
        "subjects": [
            "Risk Management",
            "Numerical Analysis",
            "Computational Finance",
            "Machine Learning"
        ],
        "abstract": "Motivated by the equations of cross valuation adjustments (XVAs) in the realistic case where capital is deemed fungible as a source of funding for variation margin, we introduce a simulation/regression scheme for a class of anticipated BSDEs, where the coefficient entails a conditional expected shortfall of the martingale part of the solution. The scheme is explicit in time and uses neural network least-squares and quantile regressions for the embedded conditional expectations and expected shortfall computations. An a posteriori Monte Carlo validation procedure allows assessing the regression error of the scheme at each time step. The superiority of this scheme with respect to Picard iterations is illustrated in a high-dimensional and hybrid market/default risks XVA use-case.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13314"
    },
    {
        "doc_id": 93,
        "title": "On distributional limit laws for recurrence",
        "authors": [
            "Mark Holland",
            "Mike Todd"
        ],
        "subjects": [
            "Dynamical Systems",
            "Probability"
        ],
        "abstract": "For a probability measure preserving dynamical system $(\\mathcal{X},f,\u03bc)$, the Poincar\u00e9 Recurrence Theorem asserts that $\u03bc$-almost every orbit is recurrent with respect to its initial condition. This motivates study of the statistics of the process $X_n(x)=\\text{dist}(f^n(x),x))$, and real-valued functions thereof. For a wide class of non-uniformly expanding dynamical systems, we show that the time-$n$ counting process $R_n(x)$ associated to the number recurrences below a certain radii sequence $r_n(\u03c4)$ follows an \\emph{averaged} Poisson distribution $G(\u03c4)$. Furthermore, we obtain quantitative results on almost sure rates for the recurrence statistics of the process $X_n$.",
        "comments": "MSC Class:          37A50; 37B20; 60G55; 37E05; 60G70",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13300"
    },
    {
        "doc_id": 94,
        "title": "Transport and information in open quantum systems",
        "authors": [
            "Kasper Poulsen"
        ],
        "subjects": [
            "Quantum Physics",
            "Mesoscale and Nanoscale Physics",
            "Statistical Mechanics"
        ],
        "abstract": "With the approaching second quantum revolution, the study of quantum thermodynamics, particularly heat flow, has become even more relevant for two main reasons. First, understanding heat and other types of noise is essential for protecting quantum information and preventing decoherence. Second, the ability to manufacture and control quantum systems developed for the quantum computer allows for experimental study of quantum thermodynamics in entirely new settings.\n  In this thesis, several systems involving quantum systems in contact with baths are studied theoretically in experimentally available settings. First, two rectification or diode setups for heat currents are proposed using a dark-state mechanism. In one system, the dark-state mechanism is imperfect but very robust. In the other system, the dark-state mechanism relies on quantum entanglement and is much better but more fragile towards decoherence. Next, a quantum version of the Wheatstone bridge is built using the same entanglement-powered dark state mechanism. After having studied several boundary-driven quantum systems, the lessons learned are generalized into resonance conditions using a general linear chain of weakly interacting chains of strongly interacting spins.\n  The final two chapters focus on the ability to study statistical physics in realizable quantum systems. First, a Maxwell's demon setup is proposed. A demon-controlled qutrit is coupled to two non-Markovian baths. The information back-flow from the non-Markovian baths allows the demon to more effectively transfer heat from the cold bath to the hot bath. Second, the Mott insulator to superfluid phase transition in a lattice of transmons is examined. The ground state has a variable particle number and is prepared using adiabatic state preparation. This allows for the exploration of the entire phase diagram.",
        "comments": "PhD thesis",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13292"
    },
    {
        "doc_id": 95,
        "title": "Diverse Science from VLT imagery and spectroscopy of PNe in the Galactic Bulge",
        "authors": [
            "Quentin Parker",
            "Shuyu Tan",
            "Andreas Ritter",
            "Albert Zijlstra"
        ],
        "subjects": [
            "Astrophysics of Galaxies",
            "Solar and Stellar Astrophysics"
        ],
        "abstract": "We have undertaken a deep investigation of a well defined sample of 136 PNe located in a 10x10 degree central region of the Galactic Bulge observed with the ESO VLT and supplemented by archival HST imagery. These studies have provided precise morphologies, major axes position angles and the most robust sample of consistently derived chemical abundances available to date. Using these data we have statistically confirmed, at 5-sigma, the precise PNe population that provides the PNe alignment of major axes previously suggested in the Galactic Bulge, revealed a partial solution to the sulfur anomaly and uncovered interesting morphological, abundance and kinematic features. We summarise the most significant findings here with detailed results appearing in a series of related publications.",
        "comments": "6 pages, 5 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13257"
    },
    {
        "doc_id": 96,
        "title": "Quantum natural gradient without monotonicity",
        "authors": [
            "Toi Sasaki",
            "Hideyuki Miyahara"
        ],
        "subjects": [
            "Quantum Physics",
            "Statistical Mechanics",
            "Information Theory",
            "Computational Physics",
            "Machine Learning"
        ],
        "abstract": "Natural gradient (NG) is an information-geometric optimization method that plays a crucial role, especially in the estimation of parameters for machine learning models like neural networks. To apply NG to quantum systems, the quantum natural gradient (QNG) was introduced and utilized for noisy intermediate-scale devices. Additionally, a mathematically equivalent approach to QNG, known as the stochastic reconfiguration method, has been implemented to enhance the performance of quantum Monte Carlo methods. It is worth noting that these methods are based on the symmetric logarithmic derivative (SLD) metric, which is one of the monotone metrics. So far, monotonicity has been believed to be a guiding principle to construct a geometry in physics. In this paper, we propose generalized QNG by removing the condition of monotonicity. Initially, we demonstrate that monotonicity is a crucial condition for conventional QNG to be optimal. Subsequently, we provide analytical and numerical evidence showing that non-monotone QNG outperforms conventional QNG based on the SLD metric in terms of convergence speed.",
        "comments": "6 pages, 3 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13237"
    },
    {
        "doc_id": 97,
        "title": "The role of carbon in red giant spectro-seismology",
        "authors": [
            "Kirsten A. Banks",
            "Sarah L. Martell",
            "C. G. Tinney",
            "Dennis Stello",
            "Marc Hon",
            "Claudia Reyes",
            "James Priest",
            "Sven Buder",
            "Benjamin T. Montet"
        ],
        "subjects": [
            "Solar and Stellar Astrophysics",
            "Astrophysics of Galaxies"
        ],
        "abstract": "Although red clump stars function as reliable standard candles, their surface characteristics (i.e. T$_{\\text{eff}}$, log~$g$, and [Fe/H]) overlap with those of red giant branch stars, which are not standard candles. Recent results have revealed that spectral features containing carbon (e.g. CN molecular bands) carry information correlating with the ''gold-standard'' asteroseismic classifiers that distinguish red clump from red giant branch stars. However, the underlying astrophysical processes driving the correlation between these spectroscopic and asteroseismic quantities in red giants remain inadequately explored. This study aims to enhance our understanding of this ''spectro-seismic'' effect, by refining the list of key spectral features predicting red giant evolutionary state. In addition, we conduct further investigation into those key spectral features to probe the astrophysical processes driving this connection. We employ the data-driven The Cannon algorithm to analyse high-resolution ($R\\sim80,000$) Veloce spectra from the Anglo-Australian Telescope for 301 red giant stars (where asteroseismic classifications from the TESS mission are known for 123 of the stars). The results highlight molecular spectroscopic features, particularly those containing carbon (e.g. CN), as the primary indicators of the evolutionary states of red giant stars. Furthermore, by investigating CN isotopic pairs (that is, $^{12}$C$^{14}$N and $^{13}$C$^{14}$N) we find statistically significant differences in the reduced equivalent widths of such lines, suggesting that physical processes that change the surface abundances and isotopic ratios in red giant stars, such as deep mixing, are the driving forces of the ''spectro-seismic'' connection of red giants.",
        "comments": "13 pages, 9 figures, submitted to MNRAS",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13235"
    },
    {
        "doc_id": 98,
        "title": "On the Statistical Mechanics of Mass Accommodation at Liquid-Vapor Interfaces",
        "authors": [
            "Kritanjan Polley",
            "Kevin R. Wilson",
            "David T. Limmer"
        ],
        "subjects": [
            "Chemical Physics",
            "Statistical Mechanics"
        ],
        "abstract": "We propose a framework for describing the dynamics associated with the adsorption of small molecules to liquid-vapor interfaces, using an intermediate resolution between traditional continuum theories that are bereft of molecular detail and molecular dynamics simulations that are replete with them. In particular, we develop an effective single particle equation of motion capable of describing the physical processes that determine thermal and mass accommodation probabilities. The effective equation is parameterized with quantities that vary through space away from the liquid-vapor interface. Of particular importance in describing the early time dynamics is the spatially dependent friction, for which we propose a numerical scheme to evaluate from molecular simulation. Taken together with potentials of mean force computable with importance sampling methods, we illustrate how to compute the mass accommodation coefficient and residence time distribution. Throughout, we highlight the case of ozone adsorption in aqueous solutions and its dependence on electrolyte composition.",
        "comments": "9 pages, 7 figures",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13234"
    },
    {
        "doc_id": 99,
        "title": "On Principled Local Optimization Methods for Federated Learning",
        "authors": [
            "Honglin Yuan"
        ],
        "subjects": [
            "Machine Learning",
            "Distributed, Parallel, and Cluster Computing",
            "Optimization and Control",
            "Machine Learning"
        ],
        "abstract": "Federated Learning (FL), a distributed learning paradigm that scales on-device learning collaboratively, has emerged as a promising approach for decentralized AI applications. Local optimization methods such as Federated Averaging (FedAvg) are the most prominent methods for FL applications. Despite their simplicity and popularity, the theoretical understanding of local optimization methods is far from clear. This dissertation aims to advance the theoretical foundation of local methods in the following three directions.\n  First, we establish sharp bounds for FedAvg, the most popular algorithm in Federated Learning. We demonstrate how FedAvg may suffer from a notion we call iterate bias, and how an additional third-order smoothness assumption may mitigate this effect and lead to better convergence rates. We explain this phenomenon from a Stochastic Differential Equation (SDE) perspective.\n  Second, we propose Federated Accelerated Stochastic Gradient Descent (FedAc), the first principled acceleration of FedAvg, which provably improves the convergence rate and communication efficiency. Our technique uses on a potential-based perturbed iterate analysis, a novel stability analysis of generalized accelerated SGD, and a strategic tradeoff between acceleration and stability.\n  Third, we study the Federated Composite Optimization problem, which extends the classic smooth setting by incorporating a shared non-smooth regularizer. We show that direct extensions of FedAvg may suffer from the \"curse of primal averaging,\" resulting in slow convergence. As a solution, we propose a new primal-dual algorithm, Federated Dual Averaging, which overcomes the curse of primal averaging by employing a novel inter-client dual averaging procedure.",
        "comments": "Stanford University Doctoral Dissertation",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13216"
    },
    {
        "doc_id": 100,
        "title": "Assessing Influential Observations in Pain Prediction using fMRI Data",
        "authors": [
            "Dongliang Zhang",
            "Masoud Asgharian",
            "Martin A. Lindquist"
        ],
        "subjects": [
            "Methodology"
        ],
        "abstract": "Influential diagnosis is an integral part of data analysis, of which most existing methodological frameworks presume a deterministic submodel and are designed for low-dimensional data (i.e., the number of predictors p smaller than the sample size n). However, the stochastic selection of a submodel from high-dimensional data where p exceeds n has become ubiquitous. Thus, methods for identifying observations that could exert undue influence on the choice of a submodel can play an important role in this setting. To date, discussion of this topic has been limited, falling short in two domains: (i) constrained ability to detect multiple influential points, and (ii) applicability only in restrictive settings. After describing the problem, we characterize and formalize the concept of influential observations on variable selection. Then, we propose a generalized diagnostic measure, extended from an available metric accommodating different model selectors and multiple influential observations, the asymptotic distribution of which is subsequently establish large p, thus providing guidelines to ascertain influential observations. A high-dimensional clustering procedure is further incorporated into our proposed scheme to detect multiple influential points. Simulation is conducted to assess the performances of various diagnostic approaches. The proposed procedure further demonstrates its value in improving predictive power when analyzing thermal-stimulated pain based on fMRI data.",
        "comments": "21 pages, 6 figures",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13208"
    },
    {
        "doc_id": 101,
        "title": "Nucleosynthesis in magnetorotational supernovae: impact of the magnetic field configuration",
        "authors": [
            "M. Reichert",
            "M. Bugli",
            "J. Guilet",
            "M. Obergaulinger",
            "M. \u00c1. Aloy",
            "A. Arcones"
        ],
        "subjects": [
            "High Energy Astrophysical Phenomena"
        ],
        "abstract": "The production of heavy elements is one of the main by-products of the explosive end of massive stars. A long sought goal is finding differentiated patterns in the nucleosynthesis yields, which could permit identifying a number of properties of the explosive core. Among them, the traces of the magnetic field topology are particularly important for \\emph{extreme} supernova explosions, most likely hosted by magnetorotational effects. We investigate the nucleosynthesis of five state-of-the-art magnetohydrodynamic models with fast rotation that have been previously calculated in full 3D and that involve an accurate neutrino transport (M1). One of the models does not contain any magnetic field and synthesizes elements around the iron group, in agreement with other CC-SNe models in literature. All other models host a strong magnetic field of the same intensity, but with different topology. For the first time, we investigate the nucleosynthesis of MR-SNe models with a quadrupolar magnetic field and a 90 degree tilted dipole. We obtain a large variety of ejecta compositions reaching from iron nuclei to nuclei up to the third r-process peak. We assess the robustness of our results by considering the impact of different nuclear physics uncertainties such as different nuclear masses, $\u03b2^{-}$-decays and $\u03b2^{-}$-delayed neutron emission probabilities, neutrino reactions, fission, and a feedback of nuclear energy on the temperature. We find that the qualitative results do not change with different nuclear physics input. The properties of the explosion dynamics and the magnetic field configuration are the dominant factors determining the ejecta composition.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14402"
    },
    {
        "doc_id": 102,
        "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes",
        "authors": [
            "Ege Ozguroglu",
            "Ruoshi Liu",
            "D\u00eddac Sur\u00eds",
            "Dian Chen",
            "Achal Dave",
            "Pavel Tokmakov",
            "Carl Vondrick"
        ],
        "subjects": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ],
        "abstract": "We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions. By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art. As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts. Experiments show that our approach outperforms supervised baselines on established benchmarks. Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions.",
        "comments": "Website: https://gestalt.cs.columbia.edu/",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14398"
    },
    {
        "doc_id": 103,
        "title": "Entanglement entropy and deconfined criticality: emergent SO(5) symmetry and proper lattice bipartition",
        "authors": [
            "Jonathan D'Emidio",
            "Anders W. Sandvik"
        ],
        "subjects": [
            "Strongly Correlated Electrons",
            "High Energy Physics - Lattice"
        ],
        "abstract": "We study the R\u00e9nyi entanglement entropy (EE) of the two-dimensional $J$-$Q$ model, the emblematic quantum spin model of deconfined criticality at the phase transition between antiferromagnetic and valence-bond-solid ground states. Quantum Monte Carlo simulations with an improved EE scheme reveal critical corner contributions that scale logarithmically with the system size, with a coefficient in remarkable agreement with the form expected from a large-$N$ conformal field theory with SO($N=5$) symmetry. However, details of the bipartition of the lattice are crucial in order to observe this behavior. If the subsystem for the reduced density matrix does not properly accommodate valence-bond fluctuations, logarithmic contributions appear even for corner-less bipartitions. We here use a $45^\\circ$ tilted cut on the square lattice. Beyond supporting an SO($5$) deconfined quantum critical point, our results for both the regular and tilted cuts demonstrate important microscopic aspects of the EE that are not captured by conformal field theory.",
        "comments": "5 pages, 3 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14396"
    },
    {
        "doc_id": 104,
        "title": "Summing up perturbation series around superintegrable point",
        "authors": [
            "A. Mironov",
            "A. Morozov",
            "A. Popolitov",
            "Sh. Shakirov"
        ],
        "subjects": [
            "High Energy Physics - Theory",
            "Mathematical Physics"
        ],
        "abstract": "We work out explicit formulas for correlators in the Gaussian matrix model perturbed by a logarithmic potential, i.e. by inserting Miwa variables. In this paper, we concentrate on the example of a single Miwa variable. The ordinary Gaussian model is superintegrable, i.e. the average of the Schur functions $S_Q$ is an explicit function of the Young diagram $Q$. The question is what happens to this property after perturbation. We show that the entire perturbation series can be nicely summed up into a kind of Borel transform of a universal exponential function, while the dependence on $R$ enters through a polynomial factor in front of this exponential. Moreover, these polynomials can be described explicitly through a single additional structure, which we call ``truncation'' of the Young diagram $Q$. It is unclear if one can call this an extended superintegrability, but at least it is a tremendously simple deformation of it. Moreover, the vanishing Gaussian correlators remain vanishing and, hence, are not deformed at all.",
        "comments": "15 pages + Appendix (7 pages)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14392"
    },
    {
        "doc_id": 105,
        "title": "Weakening of magnetic braking in cataclysmic variables explains the dearth of period bouncers",
        "authors": [
            "Arnab Sarkar",
            "Antonio C. Rodriguez",
            "Sivan Ginzburg",
            "Lev Yungelson",
            "Christopher A. Tout"
        ],
        "subjects": [
            "Solar and Stellar Astrophysics",
            "High Energy Astrophysical Phenomena"
        ],
        "abstract": "Period bouncers are cataclysmic variables (CVs) that have evolved past their orbital period minimum. The strong disagreement between theory and observations of the relative fraction of period bouncers is a severe shortcoming in the understanding of CV evolution. We test the implications of the hypothesis that magnetic braking (MB), which is suggested to be an additional angular momentum loss (AML) mechanism for CVs below the period gap ($P_\\mathrm{orb}\\lesssim 120$ min), weakens around their period minimum. We compute the evolution of CV donors below the period gap using the MESA code, assuming that the evolution of the system is driven by AML by gravitational wave radiation (GWR) and MB. We parametrize the MB strength as $\\mathrm{AML_{MB}}=\u03ba\\mathrm{AML_{GWR}}$. We compute two qualitatively different sets of models, one where $\u03ba$ is a constant and the other where $\u03ba$ depends on stellar parameters. We find that in the latter set of models, $\u03ba$ decreases as the CV approaches the period minimum ($P_\\mathrm{orb}\\approx80\\,$ min), beyond which $\u03ba\\approx0$. This stalls their evolution so that they spend a long time in the observed period minimum spike ($80\\lesssim P_\\mathrm{orb}/\\,\\mathrm{min}\\lesssim 86$). Here they become difficult to distinguish from pre-bounce systems in the spike. A strong decrease in mass-transfer rate makes them virtually undetectable as they evolve further. We also discuss the physical processes, such as dynamo action, white dwarf magnetism and dead zones, that may cause such a weakening of MB at short orbital periods. The weakening magnetic braking formalism solves the problem of the lack of period bouncers in CV observational surveys.",
        "comments": "Submitted to A&A Letters. Comments are welcome",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14389"
    },
    {
        "doc_id": 106,
        "title": "Entropic Quantum Central Limit Theorem and Quantum Inverse Sumset Theorem",
        "authors": [
            "Kaifeng Bu",
            "Weichen Gu",
            "Arthur Jaffe"
        ],
        "subjects": [
            "Quantum Physics",
            "Information Theory",
            "Mathematical Physics",
            "Probability"
        ],
        "abstract": "We establish an entropic, quantum central limit theorem and quantum inverse sumset theorem in discrete-variable quantum systems describing qudits or qubits. Both results are enabled by using our recently-discovered quantum convolution. We show that the exponential rate of convergence of the entropic central limit theorem is bounded by the magic gap. We also establish an ``quantum, entropic inverse sumset theorem,'' by introducing a quantum doubling constant. Furthermore, we introduce a ``quantum Ruzsa divergence'', and we pose a conjecture called ``convolutional strong subaddivity,'' which leads to the triangle inequality for the quantum Ruzsa divergence. A byproduct of this work is a magic measure to quantify the nonstabilizer nature of a state, based on the quantum Ruzsa divergence.",
        "comments": "23 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14385"
    },
    {
        "doc_id": 107,
        "title": "A Sum-of-Squares Hierarchy in the Absence of Pointwise Proofs I: Energy Certificates",
        "authors": [
            "J. S. Sandhu",
            "J. Shi"
        ],
        "subjects": [
            "Computational Complexity",
            "Mathematical Physics",
            "Classical Analysis and ODEs",
            "Optimization and Control",
            "Probability"
        ],
        "abstract": "We devise a parameterized family of distributions, the high-entropy step distributions (HES), which are expressive enough to capture near-optima of spherical spin glass models in the full Replica Symmetry Breaking (fRSB) regime and yet permit low-degree Sum-of-Squares (SoS) certificates that no such distribution can achieve value slightly larger than the true optimum. This yields a SoS optimization program and rounding scheme that attains near-optimal solutions for spherical spin glasses in the fRSB regime. In other regimes, the same results occur at the ALG value, which is a conjectured best-value attainable by any polynomial time algorithm. These SoS programs optimize over families of distributions of possible solutions, and circumvent the oft-cited impossibility of providing a low-degree SoS proof of concentration of measure by instead proving the same bounds only in expectation on solution distributions that can be produced by the chosen rounding algorithm. The new SoS hierarchy does not make any specific reference to the spherical spin glass problem, and we conjecture that it can be applied to a broad range of average-case problems to obtain value that is optimal among polynomial-time algorithms. We give evidence for this with examples of ensembles that provably fool certain local iterative algorithms but for which there is either proof or evidence that the SoS program is better. This opens the door to addressing a question posed by Barak about the possible optimality of SoS on average-case optimization problems, and by Schramm about reductions between different families of algorithms for average-case problems. In this paper, we give low-degree SoS proofs certifying key properties about HES distributions as well as the ALG threshold for spherical spin glasses. The rounding algorithm is introduced and analyzed in a companion paper.",
        "comments": "130 pages, 0 figures. First of two companion papers",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14383"
    },
    {
        "doc_id": 108,
        "title": "Electrotaxis of self-propelling artificial swimmers in microchannels",
        "authors": [
            "Carola M. Buness",
            "Avi Rana",
            "Corinna C. Maass",
            "Ranabir Dey"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Fluid Dynamics"
        ],
        "abstract": "Ciliated microswimmers and flagellated bacteria alter their swimming trajectories to follow the direction of an applied electric field exhibiting electrotaxis. Both for matters of application and physical modelling, it is instructive to study such behaviour in synthetic swimmers. We show here that under an external electric field, self-propelling active droplets autonomously modify their swimming trajectories in microchannels, even undergoing `U-turns', to exhibit robust electrotaxis. Depending on the relative initial orientations of the microswimmer and the external electric field, the active droplet can also navigate upstream of an external flow following a centre-line motion, instead of the oscillatory upstream trajectory observed in absence of electric field. Using a hydrodynamic theory model, we show that the electrically induced angular velocity and electrophoretic effects, along with the microswimmer motility and its hydrodynamic interactions with the microchannel walls, play crucial roles in dictating the electrotactic trajectories and dynamics. Specifically, the transformation in the trajectories during upstream swimming against an external flow under an electric field can be understood as a reverse Hopf bifurcation for a dynamical system. Our study provides a simple methodology and a systematic understanding of manoeuvring active droplets in microconfinements for micro-robotic applications especially in biotechnology.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14376"
    },
    {
        "doc_id": 109,
        "title": "Spatially Resolved Conductivity of Rectangular Interconnects considering Surface Scattering -- Part II: Circuit-Compatible Modeling",
        "authors": [
            "Xinkang Chen",
            "Sumeet Kumar Gupta"
        ],
        "subjects": [
            "Applied Physics"
        ],
        "abstract": "Interconnect conductivity modeling is a critical aspect for modern chip design. Surface scattering -- an important scattering mechanism in scaled interconnects is usually captured using Fuchs-Sondheimer (FS) model which offers the average behavior of the interconnect. However, to support the modern interconnect structures (such as tapered geometries), modeling spatial dependency of conductivity becomes important. In Part I of this work, we presented a spatially resolved FS (SRFS) model for rectangular interconnects derived from the fundamental FS approach. While the proposed SRFS model offers both spatial-dependency of conductivity and its direct relationship with the physical parameters, its complex expression is not suitable for incorporation in circuit simulations. In this part, we build upon our SRFS model to propose a circuit-compatible conductivity model for rectangular interconnects accounting for 2D surface scattering. The proposed circuit-compatible model offers spatial resolution of conductivity as well as explicit dependence on the physical parameters such as electron mean free path ($\u03bb_0$), specularity ($p$) and interconnect geometry. We validate our circuit-compatible model over a range of interconnect width/height (and $\u03bb_0$) and p values and show a close match with the physical SRFS model proposed in Part I (with error < 0.7%). We also compare our circuit-compatible model with a previous spatially resolved analytical model (appropriately modified for a fair comparison) and show that our model captures the spatial resolution of conductivity and the dependence on physical parameters more accurately. Finally, we present a semi-analytical equation for the average conductivity based on our circuit-compatible model.",
        "comments": "10 pages, 8 figures in process to submit to IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14374"
    },
    {
        "doc_id": 110,
        "title": "Modeling Global Surface Dust Deposition Using Physics-Informed Neural Networks",
        "authors": [
            "Constanza A. Molina Catricheo",
            "Fabrice Lambert",
            "Julien Salomon",
            "Elwin van 't Wout"
        ],
        "subjects": [
            "Geophysics"
        ],
        "abstract": "Paleoclimatic measurements serve to understand geophysical processes and evaluate climate model performances. However, their spatial coverage is generally sparse and unevenly distributed across the globe. Statistical interpolation methods are the prevalent techniques to grid such data, but these purely data-driven approaches sometimes produce results that are incoherent with our knowledge of the physical world. Physics-Informed Neural Networks (PINNs) follow an innovative approach to data analysis and physical modeling through machine learning, as they incorporate physical principles into the data-driven learning process. Here, we develop PINNs to reconstruct global maps of atmospheric dust surface deposition fluxes from measurement data in paleoclimatic archives, for the Holocene and Last Glacial Maximum periods. We design an advection-diffusion equation to consider dominant wind directions at various latitudes, which prevents dust particles from flowing upwind. Our PINN improves on standard kriging interpolation by allowing variable asymmetry around data points. The reconstructions display realistic dust plumes from continental sources towards ocean basins following prevailing winds.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14372"
    },
    {
        "doc_id": 111,
        "title": "Efficient Optimisation of Physical Reservoir Computers using only a Delayed Input",
        "authors": [
            "Enrico Picco",
            "Lina Jaurigue",
            "Kathy L\u00fcdge",
            "Serge Massar"
        ],
        "subjects": [
            "Emerging Technologies",
            "Artificial Intelligence",
            "Neural and Evolutionary Computing",
            "Optics"
        ],
        "abstract": "We present an experimental validation of a recently proposed optimization technique for reservoir computing, using an optoelectronic setup. Reservoir computing is a robust framework for signal processing applications, and the development of efficient optimization approaches remains a key challenge. The technique we address leverages solely a delayed version of the input signal to identify the optimal operational region of the reservoir, simplifying the traditionally time-consuming task of hyperparameter tuning. We verify the effectiveness of this approach on different benchmark tasks and reservoir operating conditions.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14371"
    },
    {
        "doc_id": 112,
        "title": "Anomalous localization in spin-chain with tilted interactions",
        "authors": [
            "Arindam Mallick",
            "Jakub Zakrzewski"
        ],
        "subjects": [
            "Disordered Systems and Neural Networks",
            "Quantum Gases",
            "Strongly Correlated Electrons",
            "High Energy Physics - Lattice",
            "Quantum Physics"
        ],
        "abstract": "The localization properties of a disorder-free spin chain with inhomogeneous interactions are studied. In particular, we consider interaction strength growing linearly along the chain for systems with different interaction ranges. Using exact diagonalization we find the participation ratio of all eigenstates which allows us to quantify the localization volume in the Hilbert space. Surprisingly the localization volume changes nonmonotonically with the interaction range. The model for the infinite interaction range resembles the Schwinger model of lattice gauge theory in staggered formalism. The model studied may be implemented in state-of-the-art cold atomic devices and could reveal hidden features in disorder-free confinement phenomena.",
        "comments": "11 pages, 9 figures. Comments are welcome",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14369"
    },
    {
        "doc_id": 113,
        "title": "Spectral Gaps of 2D and 3D Many-body Quantum Systems in the Thermodynamic Limit",
        "authors": [
            "Illya V. Lukin",
            "Andrii G. Sotnikov",
            "Jacob M. Leamer",
            "Alicia B. Magann",
            "Denys I. Bondar"
        ],
        "subjects": [
            "Quantum Physics",
            "Strongly Correlated Electrons"
        ],
        "abstract": "We present an expression for the spectral gap, opening up new possibilities for performing and accelerating spectral calculations of quantum many-body systems. We develop and demonstrate one such possibility in the context of tensor network simulations. Our approach requires only minor modifications of the widely used Simple Update method and is computationally lightweight relative to other approaches. We validate it by computing spectral gaps of the 2D and 3D transverse-field Ising models and find strong agreement with previously reported perturbation theory results.",
        "comments": "7 pages and 4 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14368"
    },
    {
        "doc_id": 114,
        "title": "Spatially Resolved Conductivity of Rectangular Interconnects considering Surface Scattering -- Part I: Physical Modeling",
        "authors": [
            "Xinkang Chen",
            "Sumeet Kumar Gupta"
        ],
        "subjects": [
            "Applied Physics"
        ],
        "abstract": "Accurate modeling of interconnect conductivity is important for performance evaluation of chips in advanced technologies. Surface scattering in interconnects is usually treated by using Fuchs-Sondheimer (FS) approach. While the FS model offer explicit inclusion of the physical parameters, it lacks spatial dependence of conductivity across the interconnect cross-section. To capture the space-dependency of conductivity, an empirical modeling approach based on \"cosh\" function has been proposed, but it lacks physical insights. In this work, we present a 2D spatially resolved FS (SRFS) model for rectangular interconnects derived from the Boltzmann transport equations. The proposed SRFS model for surface scattering offers both spatial dependence and explicit relation of conductivity to physical parameters such as mean free path and specularity of electrons and interconnect geometry. We highlight the importance of physics-based spatially resolved conductivity model by showing the differences in the spatial profiles between the proposed physical approach and the previous empirical approach. In Part II of this work, we build upon the SRFS approach to propose a compact model for spatially-resolved conductivity accounting for surface scattering in rectangular interconnects.",
        "comments": "12 pages, 8 figures, in process to submit to IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14366"
    },
    {
        "doc_id": 115,
        "title": "Impact of dynamical regionalization on precipitation biases and teleconnections over West Africa",
        "authors": [
            "I\u00f1igo G\u00f3mara",
            "Elsa Mohino",
            "Teresa Losada",
            "Marta Dom\u00ednguez",
            "Roberto Su\u00e1rez-Moreno",
            "Bel\u00e9n Rodr\u00edguez-Fonseca"
        ],
        "subjects": [
            "Atmospheric and Oceanic Physics",
            "Geophysics"
        ],
        "abstract": "West African societies are highly dependent on the West African Monsoon (WAM).Thus, a correct representation of the WAM in climate models is of paramount importance. In this article, the ability of 8 CMIP5 historical General Circulation Models (GCMs) and 4 CORDEX-Africa Regional Climate Models (RCMs) to characterize the WAM dynamics and variability is assessed for the period July-August-September 1979-2004. Simulations are compared with observations. Uncertainties in RCM performance and lateral boundary conditions are assessed individually. Results show that both GCMs and RCMs have trouble to simulate the northward migration of the Intertropical Convergence Zone in boreal summer. The greatest bias improvements are obtained after regionalization of the most inaccurate GCM simulations. To assess WAM variability, a Maximum Covariance Analysis is performed between Sea Surface Temperature and precipitation anomalies in observations, GCM and RCM simulations. The assessed variability patterns are: El Nino-Southern Oscillation (ENSO); the eastern Mediterranean (MED); and the Atlantic Equatorial Mode (EM). Evidence is given that regionalization of the ENSO-WAM teleconnection does not provide any added value. Unlike GCMs, RCMs are unable to precisely represent the ENSO impact on air subsidence over West Africa. Contrastingly, the simulation of the MED-WAM teleconnection is improved after regionalization. Humidity advection and convergence over the Sahel area are better simulated by RCMs. Finally, no robust conclusions can be determined for the EM-WAM teleconnection, which cannot be isolated for the 1979-2004 period. The novel results in this article will help to select the most appropriate RCM simulations to study WAM teleconnections.",
        "comments": "Journal ref:        Clim Dyn 50, 4481-4506 (2018)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14364"
    },
    {
        "doc_id": 116,
        "title": "Universal collective Larmor-Silin mode emerging in magnetized correlated Dirac fermions",
        "authors": [
            "Chuang Chen",
            "Yuan Da Liao",
            "Chengkang Zhou",
            "Gaopei Pan",
            "Zi Yang Meng",
            "Yang Qi"
        ],
        "subjects": [
            "Strongly Correlated Electrons",
            "Mesoscale and Nanoscale Physics",
            "Materials Science"
        ],
        "abstract": "Employing large-scale quantum Monte Carlo simulations, we find in magnetized interacting Dirac fermion model, there emerges a new and universal collective Larmor-Silin spin wave mode in the transverse dynamical spin susceptibility. Such mode purely originates from the interaction among Dirac fermions and distinguishes itself from the usual particle-hole continuum with finite lifetime and clear dispersion. Our unbiased numerical results offer the dynamic signature of this new collective excitations in interacting Dirac fermion systems, and provide experimental guidance for inelastic neutron scattering, electron spin resonance and other spectroscopic approaches in the investigation of such universal collective modes in quantum Moire materials, topological insulators and quantum spin liquid materials under magnetic field, with quintessential interaction nature beyond the commonly assumed noninteracting Dirac fermion or spinon approximations.",
        "comments": "11 pages, 5 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14358"
    },
    {
        "doc_id": 117,
        "title": "Near-field radiative heat transfer between shifted graphene gratings",
        "authors": [
            "Minggang Luo",
            "Youssef Jeyar",
            "Brahim Guizal",
            "Mauro Antezza"
        ],
        "subjects": [
            "Optics",
            "Mesoscale and Nanoscale Physics",
            "Applied Physics"
        ],
        "abstract": "We examine the near-field radiative heat transfer between finite-thickness planar fused silica slabs covered with graphene gratings, through the utilization of the exact Fourier modal method augmented with local basis functions (FMM-LBF), with focus on the lateral shift effect. To do so, we propose and validate a minor modification of the FMM-LBF theory to account for the lateral shift. This approach goes far beyond the effective medium approximation because this latter cannot account for the lateral shift. We show that the heat flux can exhibit significant oscillations with the lateral shift and, at short separation, it can experience up to a 60%-70% reduction compared to the aligned case. Such a lateral shift effect is found to be sensitive to the geometric factor $d/D$ (separation distance to grating period ratio). When $d/D>2$ (realized through large separation or small grating period), the two graphene gratings see each other as an effective whole rather than in detail, and thus the lateral shift effect on heat transfer becomes less important. Therefore, we can clearly distinguish two asymptotic regimes for radiative heat transfer: the LSE (Lateral Shift Effect) regime, where a significant lateral shift effect is observed, and the non-LSE regime, where this effect is negligible. Furthermore, regardless of the lateral shift, the radiative heat flux shows an obvious and non-monotonic dependence on the graphene chemical potential. That is, we can get an optimal radiative heat flux (peaking at about 0.3eV chemical potential) by $\\textit{in situ}$ modulating the chemical potential. This work has the potential to unveil new avenues for harnessing the lateral shift effect on radiative heat transfer in graphene-based nanodevices.",
        "comments": "13 pages, 13 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14357"
    },
    {
        "doc_id": 118,
        "title": "Exact surface energy of the Hubbard model with unparallel boundary magnetic fields",
        "authors": [
            "Pei Sun",
            "Yi Qiao",
            "Junpeng Cao",
            "Wen-Li Yang"
        ],
        "subjects": [
            "Mathematical Physics"
        ],
        "abstract": "In this paper, we study the exact physical quantities in the thermodynamic limit of the one-dimensional Hubbard model with unparallel boundary magnetic fields based on the off-diagonal Bethe ansatz solution. At the half-filling, we obtain the different patterns of Bethe roots of the reduced Bethe ansatz equations for the different boundary parameters. According to them, we obtain the densities of states, ground state energy density and surface energy. Our results show that the system has the stable boundary bound states when the boundary magnetic fields satisfy some constraints.",
        "comments": "13 pages, 3 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14356"
    },
    {
        "doc_id": 119,
        "title": "Initial data for Minkowski stability with arbitrary decay",
        "authors": [
            "Allen Juntao Fang",
            "J\u00e9r\u00e9mie Szeftel",
            "Arthur Touati"
        ],
        "subjects": [
            "Analysis of PDEs",
            "General Relativity and Quantum Cosmology",
            "Mathematical Physics"
        ],
        "abstract": "We construct and parametrize solutions to the constraint equations of general relativity in a neighborhood of Minkowski spacetime with arbitrary prescribed decay properties at infinity. We thus provide a large class of initial data for the results on stability of Minkowski which include a mass term in the asymptotics. Due to the symmetries of Minkowski, a naive linear perturbation fails. Our construction is based on a simplified conformal method, a reduction to transverse traceless perturbations and a nonlinear fixed point argument where we face linear obstructions coming from the cokernels of both the linearized constraint operator and the Laplace operator. To tackle these obstructions, we introduce a well-chosen truncated black hole around which to perturb. The control of the parameters of the truncated black hole is the most technical part of the proof, since its center of mass and angular momentum could be arbitrarily large.",
        "comments": "86 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14353"
    },
    {
        "doc_id": 120,
        "title": "Uncovering Heterogeneity of Solar Flare Mechanism With Mixture Models",
        "authors": [
            "Bach Viet Do",
            "Yang Chen",
            "XuanLong Nguyen",
            "Ward Manchester"
        ],
        "subjects": [
            "Solar and Stellar Astrophysics",
            "Applications",
            "Methodology"
        ],
        "abstract": "The physics of solar flares occurring on the Sun is highly complex and far from fully understood. However, observations show that solar eruptions are associated with the intense kilogauss fields of active regions, where free energies are stored with field-aligned electric currents. With the advent of high-quality data sources such as the Geostationary Operational Environmental Satellites (GOES) and Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager (HMI), recent works on solar flare forecasting have been focusing on data-driven methods. In particular, black box machine learning and deep learning models are increasingly adopted in which underlying data structures are not modeled explicitly. If the active regions indeed follow the same laws of physics, there should be similar patterns shared among them, reflected by the observations. Yet, these black box models currently used in the literature do not explicitly characterize the heterogeneous nature of the solar flare data, within and between active regions. In this paper, we propose two finite mixture models designed to capture the heterogeneous patterns of active regions and their associated solar flare events. With extensive numerical studies, we demonstrate the usefulness of our proposed method for both resolving the sample imbalance issue and modeling the heterogeneity for rare energetic solar flare events.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14345"
    },
    {
        "doc_id": 121,
        "title": "From the Choi Formalism in Infinite Dimensions to Unique Decompositions of Generators of Completely Positive Dynamical Semigroups",
        "authors": [
            "Frederik vom Ende"
        ],
        "subjects": [
            "Functional Analysis",
            "Mathematical Physics",
            "Quantum Physics"
        ],
        "abstract": "Given any separable complex Hilbert space, any trace-class operator $B$ which does not have purely imaginary trace, and any generator $L$ of a norm-continuous one-parameter semigroup of completely positive maps we prove that there exists a unique bounded operator $K$ and a unique completely positive map $\u03a6$ such that (i) $L=K(\\cdot)+(\\cdot)K^*+\u03a6$, (ii) the superoperator $\u03a6(B^*(\\cdot)B)$ is trace class and has vanishing trace, and (iii) ${\\rm tr}(B^*K)$ is a real number. Central to our proof is a modified version of the Choi formalism which relates completely positive maps to positive semi-definite operators. We characterize when this correspondence is injective and surjective, respectively, which in turn explains why the proof idea of our main result cannot extend to non-separable Hilbert spaces. In particular, we find examples of positive semi-definite operators which have empty pre-image under the Choi formalism as soon as the underlying Hilbert space is infinite-dimensional.",
        "comments": "25+3 pages. Generalizes arXiv:2310.04037 to infinite dimensions. To be submitted to J. Funct. Anal",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14344"
    },
    {
        "doc_id": 122,
        "title": "Probing Quantum Entanglement from Quantum Correction to Newtonian Potential Energy",
        "authors": [
            "A. Belhaj",
            "S. E. Ennadifi",
            "L. Jebli"
        ],
        "subjects": [
            "Quantum Physics",
            "Mathematical Physics"
        ],
        "abstract": "Inspired by string theory ideas, we probe quantum entanglement from the gravitational potential energy. Concretely, we reconsider the study of quantum corrections to the Newtonian potential energy by treating a massive two-particle system $m_{1}$ and $m_{2}$ with size dimensions $r_{1}$ ad $% r_{2}$ where the two particles separated by a distance $d$ are under only their mutual classical gravitational interaction $V_{r}\\left( r_{1}\\text{, }% r_{2}\\right) $. Exploring such a size-dependent gravitational behavior and taking the limit $r_{1}$, $r_{2}\\ll d$, we investigate the associated quantum biparticle state and express its evolution after an interaction time $\u03c4$. Among others, we show that the two masses cannot be separable due to the induced gravitational entanglement in terms of the accumulated quantum phase $\u03b4\u03c6=\u03b4V_{g}\u03c4/\\hbar $. By analogy with the classical gravity, we derive the expression of the resulting extremely weak entanglement force from the corresponding gravitational entanglement energy. Then, we provide certain entanglement diagnostics.",
        "comments": "13 Pages, Latex, 0 Figure, 0 Table, Hand conducted work. To appear in Physica Scripta (2024)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14342"
    },
    {
        "doc_id": 123,
        "title": "Quantum Variational Algorithms for the Allocation of Resources in a Cloud/Edge Architecture",
        "authors": [
            "Carlo Mastroianni",
            "Francesco Plastina",
            "Jacopo Settino",
            "Andrea Vinci"
        ],
        "subjects": [
            "Quantum Physics",
            "Disordered Systems and Neural Networks",
            "Other Condensed Matter"
        ],
        "abstract": "Modern Cloud/Edge architectures need to orchestrate multiple layers of heterogeneous computing nodes, including pervasive sensors/actuators, distributed Edge/Fog nodes, centralized data centers and quantum devices. The optimal assignment and scheduling of computation on the different nodes is a very difficult problem, with NP-hard complexity. In this paper, we explore the possibility of solving this problem with variational quantum algorithms, which can become a viable alternative to classical algorithms in the near future. In particular, we compare the performances, in terms of success probability, of two algorithms, i.e., Quantum Approximate Optimization Algorithm (QAOA) and Variational Quantum Eigensolver (VQE). The simulation experiments, performed for a set of simple problems, show that the VQE algorithm ensures better performances when it is equipped with appropriate circuit ansatzes that are able to restrict the search space. Moreover, experiments executed on real quantum hardware show that the execution time, when increasing the size of the problem, grows much more slowly than the trend obtained with classical computation, which is known to be exponential.",
        "comments": "14 pages, 13 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14339"
    },
    {
        "doc_id": 124,
        "title": "Momentum, energy and vorticity balances in deep-water surface gravity waves",
        "authors": [
            "Aidan Blaser",
            "Rapha\u00ebl Benamran",
            "A. Bia Villas B\u00f4as",
            "Luc Lenain",
            "Nick Pizzo"
        ],
        "subjects": [
            "Fluid Dynamics"
        ],
        "abstract": "The particle trajectories in irrotational, incompressible and inviscid deep-water surface gravity waves are open, leading to a net drift in the direction of wave propagation commonly referred to as the Stokes Drift, which is responsible for catalysing surface wave-induced mixing in the ocean and transporting marine debris. A balance between phase-averaged momentum density, kinetic energy density and vorticity for irrotational, monochromatic and periodic two-dimensional water waves is derived by working directly within the Lagrangian reference frame, which tracks particle trajectories as a function of their labels and time. This balance should be expected as all three of these quantities are conserved following particles in this system. Vorticity in particular is always conserved along particles in two-dimensional inviscid flow, and as such even in its absence it is the value of the vorticity which fundamentally sets the drift, which in the Lagrangian frame is identified as the phase-averaged momentum density of the system. A relationship between the drift and the geometric mean water level of particles is found at the surface and applications for potential new ways of inferring drift are discussed. Finally, an example of an initially quiescent fluid driven by a wavelike pressure disturbance is considered, showing how the net momentum and energy from the surface disturbance transfer to the wave field, recognizing the source of the mean Lagrangian drift as the net momentum required to generate an irrotational surface wave by any conservative force.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14334"
    },
    {
        "doc_id": 125,
        "title": "Rotating effects on the photoionization cross-section of a 2D quantum ring",
        "authors": [
            "Carlos Magno O. Pereira",
            "Frankbelson dos S. Azevedo",
            "Lu\u00eds Fernando C. Pereira",
            "Edilberto O. Silva"
        ],
        "subjects": [
            "Mesoscale and Nanoscale Physics",
            "Quantum Physics"
        ],
        "abstract": "In this letter, we investigate the nonrelativistic quantum motion of a charged particle within a rotating frame, taking into account the Aharonov-Bohm (AB) effect and a uniform magnetic field. Our analysis entails the derivation of the equation of motion and the corresponding radial equation to describe the system. Solving the resulting radial equation enables us to determine the eigenvalues and eigenfunctions, providing a clear expression for the energy levels. Furthermore, our numerical analysis highlights the substantial influence of rotation on both energy levels and optical properties. Specifically, we evaluate the photoionization cross-section (PCS) with and without the effects of rotation. To elucidate the impact of rotation on the photoionization process of the system, we present graphics that offer an appealing visualization of the intrinsic nature of the physics involved.",
        "comments": "6 pages, 3 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14333"
    },
    {
        "doc_id": 126,
        "title": "Tripartite entanglement and tripartite steering in three-qubit pure states induced by vacuum-one-photon superpositions",
        "authors": [
            "Jian Wang",
            "Huan Liu",
            "Xue-feng Zhan",
            "Xue-xiang Xu"
        ],
        "subjects": [
            "Quantum Physics"
        ],
        "abstract": "Utilizing a tritter with variable parameter $T$ and induced by vacuum-one-photon superpositions $\\left\\vert 0\\right\\rangle +\u03b1\\left\\vert 1\\right\\rangle $ with $\u03b1=\\left\\vert \u03b1\\right\\vert e^{i\u03c6}$, we generate a class of three-qubit pure states. These states take the form of $\\left\\vert \u03c8\\right\\rangle _{123}=c_{0}\\left\\vert 000\\right\\rangle +c_{1}\\left\\vert 100\\right\\rangle +c_{2}\\left\\vert 010\\right\\rangle +c_{3}\\left\\vert 001\\right\\rangle $. The coefficients ($ c_{0}$, $c_{1}$, $c_{2}$, and $c_{3}$) can be manipulated through interaction parameters ($\\left\\vert \u03b1\\right\\vert $, $\u03c6$, and $T$). In line with Xie and Eberly's work[Phys. Rev. Lett. 127, 040403 (2021)], we investigate the genuine tripartite entanglement for $\\left\\vert \u03c8\\right\\rangle _{123}$ using the concurrence triangle measure. Drawing on Hao et al.'s research [Phys. Rev. Lett. 128, 120402 (2021)], we examine tripartite steering for $\\left\\vert \u03c8\\right\\rangle _{123}$ under certain measurements based on the uncertainty relations criterion. We identify nine potential configurations exhibiting varying steerability across different parameter spaces. It is important to highlight that while the state $% \\left\\vert \u03c8\\right\\rangle _{123}$ exhibits entanglement, steering remains unattainable in a substantial portion of the parameter space.",
        "comments": "10 pages, 8 figures, comments are welcome",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14328"
    },
    {
        "doc_id": 127,
        "title": "Practical Phase-Space Electronic Hamiltonians for Ab Initio Dynamics",
        "authors": [
            "Zhen Tao",
            "Tian Qiu",
            "Mansi Bhati",
            "Xuezhi Bian",
            "Titouan Duston",
            "Jonathan Rawlinson",
            "Robert G. Littlejohn",
            "Joseph E. Subotnik"
        ],
        "subjects": [
            "Chemical Physics"
        ],
        "abstract": "Modern electronic structure theory is built around the Born-Oppenheimer approximation and the construction of an electronic Hamiltonian H_{el}(X) that depends on the nuclear position X (and not the nuclear momentum P). In this article, using the well-known theory of electron translation (Gamma') and rotational (Gamma'') factors to couple electronic transitions to nuclear motion, we construct a practical phase-space electronic Hamiltonian that depends on both nuclear position and momentum, H_{PS}(X,P). While classical Born-Oppenheimer dynamics that run along the eigensurfaces of the operator H_{el}(X) can recover many nuclear properties correctly, we present some evidence that motion along the eigensurfaces of H_{PS}(X,P) can better capture both nuclear and electronic properties (including the elusive electronic momentum studied by Nafie). Moreover, only the latter (as opposed to the former) conserves the total linear and angular momentum in general.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14327"
    },
    {
        "doc_id": 128,
        "title": "Radiative corrections to di-meson tau decays",
        "authors": [
            "Alejandro Miranda"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "High Energy Physics - Experiment"
        ],
        "abstract": "We review radiative corrections to tau decays into two mesons discussing their impact in new physics searches.",
        "comments": "5 pages, 1 figure. Accepted for publication in the proceedings of the HADRON 2023 Conference, Genova, Italy, 5-9 June 2023",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14326"
    },
    {
        "doc_id": 129,
        "title": "Invisible neutrino decay at long-baseline neutrino oscillation experiments",
        "authors": [
            "Christoph A. Ternes",
            "Giulia Pagliaroli"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "High Energy Physics - Experiment"
        ],
        "abstract": "We perform an updated analysis of long-baseline accelerator data in the framework of neutrino oscillations in presence of invisible neutrino decay. We analyze data from T2K, NOvA and MINOS/MINOS+ and show that the combined analysis of all experiments improves the previous bound from long-baseline data by approximately one order of magnitude.",
        "comments": "v1: 7 pages, 2 figures, 1 table",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14316"
    },
    {
        "doc_id": 130,
        "title": "MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor Fusion Perception Systems",
        "authors": [
            "Xinyu Gao",
            "Zhijie Wang",
            "Yang Feng",
            "Lei Ma",
            "Zhenyu Chen",
            "Baowen Xu"
        ],
        "subjects": [
            "Software Engineering"
        ],
        "abstract": "Multi-sensor fusion stands as a pivotal technique in addressing numerous safety-critical tasks and applications, e.g., self-driving cars and automated robotic arms. With the continuous advancement in data-driven artificial intelligence (AI), MSF's potential for sensing and understanding intricate external environments has been further amplified, bringing a profound impact on intelligent systems and specifically on their perception systems. Similar to traditional software, adequate testing is also required for AI-enabled MSF systems. Yet, existing testing methods primarily concentrate on single-sensor perception systems (e.g., image-/point cloud-based object detection systems). There remains a lack of emphasis on generating multi-modal test cases for MSF systems. To address these limitations, we design and implement MultiTest, a fitness-guided metamorphic testing method for complex MSF perception systems. MultiTest employs a physical-aware approach to synthesize realistic multi-modal object instances and insert them into critical positions of background images and point clouds. A fitness metric is designed to guide and boost the test generation process. We conduct extensive experiments with five SOTA perception systems to evaluate MultiTest from the perspectives of: (1) generated test cases' realism, (2) fault detection capabilities, and (3) performance improvement. The results show that MultiTest can generate realistic and modality-consistent test data and effectively detect hundreds of diverse faults of an MSF system under test. Moreover, retraining an MSF system on the test cases generated by MultiTest can improve the system's robustness.",
        "comments": "The first two authors contributed equally. To appear in the proceedings of the 46th International Conference on Software Engineering (ICSE 2024)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14314"
    },
    {
        "doc_id": 131,
        "title": "Temperature Separation in a Vortex Tube and Solar Convection",
        "authors": [
            "Haibin Chen",
            "Rong Wu"
        ],
        "subjects": [
            "Fluid Dynamics",
            "Astrophysics of Galaxies"
        ],
        "abstract": "Why does the temperature gradient within a vortex tube deviate significantly from the adiabatic gradient is an important but unresolved issue. A new theory from solar physics suggests that the vorticity gradient, like the temperature gradient, can suppress or promote convection depending on the conditions, causing the temperature gradient to deviate significantly from or approach the adiabatic gradient. The gas near the wall has a very high vorticity, which can provide a large buoyancy force, driving some fluid parcels to undergo multiple collisions and reach near the axis, achieving temperature separation.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14313"
    },
    {
        "doc_id": 132,
        "title": "Theory of Acoustic Polarons in the Two-Dimensional SSH Model Applied to the Layered Superatomic Semiconductor Re6Se8Cl2",
        "authors": [
            "Petra Shih",
            "Timothy C. Berkelbach"
        ],
        "subjects": [
            "Materials Science",
            "Chemical Physics"
        ],
        "abstract": "Layered superatomic semiconductors, whose buildings blocks are atomically precise molecular clusters, exhibit interesting electronic and vibrational properties. In recent work [Science 382, 438 (2023)], transient reflection microscopy revealed quasi-ballistic exciton dynamics in Re6Se8Cl2, which was attributed to the formation of polarons due to coupling with acoustic phonons. Here, we characterize the electronic, excitonic, and phononic properties with periodic density functional theory. We further parameterize a polaron Hamiltonian with nonlocal [Su-Schrieffer-Heeger (SSH)] coupling to acoustic phonon to study the polaron ground state binding energy and dispersion relation with variational wavefunctions. We calculate a polaron binding energy of about 10 meV at room temperature, and the maximum group velocity of our polaron dispersion relation is 1.5 km/s, which is similar to the experimentally observed exciton transport velocity.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14312"
    },
    {
        "doc_id": 133,
        "title": "Higher categories",
        "authors": [
            "Rune Haugseng"
        ],
        "subjects": [
            "Category Theory",
            "Algebraic Topology"
        ],
        "abstract": "Invited contribution to the Encyclopedia of Mathematical Physics. We give an introduction to the homotopical theory of higher categories, focused on motivating the definitions of the basic objects, namely $\\infty$-categories and $(\\infty,n)$-categories.",
        "comments": "33 pages; contribution to Encyclopedia of Mathematical Physics, 2nd ed",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14311"
    },
    {
        "doc_id": 134,
        "title": "Andr\u00e9-Quillen cohomology in the context of curved algebras",
        "authors": [
            "Joan Bellier-Mill\u00e8s",
            "Sinan Yalin"
        ],
        "subjects": [
            "Algebraic Topology",
            "Algebraic Geometry",
            "K-Theory and Homology",
            "Symplectic Geometry"
        ],
        "abstract": "The Andr\u00e9-Quillen cohomology of an algebra with coefficients in a module is defined by deriving a functor based on K\u00e4hler differential forms. It can be computed using a cofibrant resolution of the algebra in a model category structure where weak equivalences are quasi-isomorphisms. This construction works for algebras over an operad, providing a cohomology theory tailored for each type of algebra. For curved algebras however, the notion of quasi-isomorphism is meaningless. The occurrence and importance of curved structures in various research topics (symplectic topology, deformation theory, derived geometry, mathematical physics) motivate the development of their homotopy theory and Andr\u00e9-Quillen cohomology theory. To get a homotopical context with an appropriate notion of weak equivalence, we consider filtered complete modules with a predifferential inducing a differential on the associated graded. Curved algebras in such modules are algebras over a curved operad. In this article, we consider curved operads which are not necessarily augmented. Bar and cobar constructions adapted to these curved operads are developed, as well as Koszul duality theory. Consequently, we obtain homotopy versions of our curved algebras and make it explicit for interesting cases. Two main examples are the curved operads encoding curved unital associative algebras and curved complex Lie algebras. In particular, homotopy curved unital associative algebras describe the structure of Floer complexes of lagrangian submanifolds and Fukaya categories in symplectic topology. Bar and cobar constructions for curved algebras are also developed, and we obtain resolutions from which we compute their Andr\u00e9-Quillen cohomology with module coefficients. Our computations in the case of curved complex Lie algebras reveal an interesting link between their Andr\u00e9-Quillen cohomology and derived complex analytic geometry.",
        "comments": "78 pages, comments welcome",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14309"
    },
    {
        "doc_id": 135,
        "title": "The soaring kite: a tale of two punctured tori",
        "authors": [
            "Mathieu Giroux",
            "Andrzej Pokraka",
            "Franziska Porkert",
            "Yoann Sohnle"
        ],
        "subjects": [
            "High Energy Physics - Theory",
            "High Energy Physics - Phenomenology",
            "Mathematical Physics"
        ],
        "abstract": "We consider the 5-mass kite family of self-energy Feynman integrals and present a systematic approach for constructing an epsilon-form basis, along with its differential equation pulled back onto the moduli space of two tori. Each torus is associated with one of the two distinct elliptic curves this family depends on. We demonstrate how the locations of relevant punctures, which are required to parametrize the full image of the kinematic space onto this moduli space, can be extracted from integrals over maximal cuts. A boundary value is provided such that the differential equation is systematically solved in terms of iterated integrals over g-kernels and modular forms. Then, the numerical evaluation of the master integrals is discussed, and important challenges in that regard are emphasized. In an appendix, we introduce new relations between g-kernels.",
        "comments": "59 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14307"
    },
    {
        "doc_id": 136,
        "title": "$\u03bde\\to\u03bde$ scattering with massive Dirac or Majorana neutrinos and general interactions",
        "authors": [
            "Juan Manuel M\u00e1rquez",
            "Pablo Roig",
            "M\u00f3nica Salinas"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "We calculate the neutrino-electron elastic scattering cross section, extending the results previously obtained in arXiv:1702.05721v2, in the presence of generic new interactions that take into account all the effects caused by finite neutrino masses. We address the potential significance of a heavy neutrino sector during precision measurements, particularly for tau neutrinos scattering with masses in the MeV range, for which the existing upper bounds on $|U_{\u03c44}|^2$ would result in conceivably measurable contributions. Finally, we comment on the possibility to distinguish between Dirac and Majorana neutrinos, including the analysis of the new emerging parameters and its application to illustrative model-dependent scenarios.",
        "comments": "22 pages, 1 figure",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14305"
    },
    {
        "doc_id": 137,
        "title": "Correlation function and the inverse problem in the $BD$ interaction",
        "authors": [
            "Hai-Peng Li",
            "Jing-Yu Yi",
            "Chu-Wen Xiao",
            "De-Liang Yao",
            "Wei-Hong Liang",
            "Eulogio Oset"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "We carry a study of the correlation functions of the $B^0 D^+, B^+ D^0$ system, which develops a bound state by about $40$ MeV, using input consistent with the $T_{cc}(3875)$ state. Then we face the inverse problem of starting from these correlation functions to determine scattering observables related to the system, including the existence of the bound state and its molecular nature. The important output of the approach is the uncertainty by which these observables can be obtained, assuming errors in the $B^0 D^+, B^+ D^0$ correlation functions typical of current ones in present correlation functions. We observe that it is possible to obtain scattering lengths and effective ranges with relative high precision and the existence of a bound state. While the pole position is obtained with errors of the order of $50 \\%$ of the binding energy, the molecular probability of the state is obtained with a very small error of the order of $6\\%$. All these findings can serve as motivation to perform such measurements in future runs of high energy hadron collisions.",
        "comments": "16 pages, 3 figures, 7 tables",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14302"
    },
    {
        "doc_id": 138,
        "title": "Amorphous silicon detectors for proton beam monitoring in FLASH radiotherapy",
        "authors": [
            "Nicolas Wyrsch",
            "Luca Antognini",
            "Christophe Ballif",
            "Saverio Braccini",
            "Pierluigi Casolaro",
            "Sylvain Dunand",
            "Alexander Gottstein",
            "Matt Large",
            "Isidre Mateu",
            "Jonathan Thomet"
        ],
        "subjects": [
            "High Energy Physics - Experiment",
            "Materials Science"
        ],
        "abstract": "Ultra-high dose rate radiation therapy (FLASH) based on proton irradiation is of major interest for cancer treatments but creates new challenges for dose monitoring. Amorphous hydrogenated silicon is known to be one of the most radiation-hard semiconductors. In this study, detectors based on this material are investigated at proton dose rates similar to or exceeding those required for FLASH therapy. Tested detectors comprise two different types of contacts, two different thicknesses deposited either on glass or on polyimide substrates. All detectors exhibit excellent linear behaviour as a function of dose rate up to a value of 20 kGy/s. Linearity is achieved independently of the depletion condition of the device and remarkably in passive (unbiased) conditions. The degradation of the performance as a function of the dose rate and its recovery are also discussed.",
        "comments": "16 pages, 9 figures, presented at 29th Internation Conference on Solid-State Dosimetry, 2023",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14300"
    },
    {
        "doc_id": 139,
        "title": "The overlooked significance of the unbiased exponential phasefactor in the determination of the finite-density lattice QCD equation of state",
        "authors": [
            "Sabarnya Mitra"
        ],
        "subjects": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology",
            "Nuclear Experiment",
            "Nuclear Theory"
        ],
        "abstract": "Within the framework of (2+1)-flavor QCD at finite temperature and chemical potential, we present results using high statistics data and demonstrate how the phasefactor of low order unbiased exponential resummation offers excellent prediction, proving to be an alternative reliable estimator of the radius of convergence of the eighth order QCD Taylor series at finite baryon density measured using the ratio and the Merci-Roberts estimators. We construct a new non-trivial unbiased phasefactor for complex isospin chemical potentials $\\muI$ and highlight its novelty. We find that this new unbiased phasefactor is very much capable of indicating the onset of non-monotonicity in finite $\\muI$ thermodynamics, which we illustrate by comparing the phasefactor results with that of low order cumulants of $\\muI$ fluctuations for non-vanishing $\\muI$. We also furnish results establishing that this unbiased phasefactor is reliable in manifesting the beginning of the overlap problem for finite, real $\\muI$. The errorbars increase drastically across the indications provided by the phasefactor which becomes very apparent from the coincidence between the phasefactor and the maximum of the errorbar slopes.",
        "comments": "9 pages, 6 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14299"
    },
    {
        "doc_id": 140,
        "title": "Characterising the Haar measure on the $p$-adic rotation groups via inverse limits of measure spaces",
        "authors": [
            "Paolo Aniello",
            "Sonia L'Innocente",
            "Stefano Mancini",
            "Vincenzo Parisi",
            "Ilaria Svampa",
            "Andreas Winter"
        ],
        "subjects": [
            "Mathematical Physics",
            "Functional Analysis",
            "Group Theory",
            "Number Theory"
        ],
        "abstract": "We determine the Haar measure on the compact $p$-adic special orthogonal groups of rotations $\\mathrm{SO}(d)_p$ in dimension $d=2,3$, by exploiting the machinery of inverse limits of measure spaces, for every prime $p>2$. We characterise $\\mathrm{SO}(d)_p$ as inverse limits of finite groups, of which we provide parametrisations and orders, together with an equivalent description through a multivariable Hensel lifting. Supplying these finite groups with their normalised counting measures, we get an inverse family of Haar measure spaces for each $\\mathrm{SO}(d)_p$. Finally, we constructively prove the existence of the so-called inverse limit measure of these inverse families, which is explicitly computable, and prove that it gives the Haar measure on $\\mathrm{SO}(d)_p$. Our results pave the way towards the study of the irreducible projective unitary representations of the $p$-adic rotation groups, with potential applications to the recently proposed $p$-adic quantum information theory.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14298"
    },
    {
        "doc_id": 141,
        "title": "The three-pion $K$-matrix at NLO in ChPT",
        "authors": [
            "Jorge Baeza-Ballesteros",
            "Johan Bijnens",
            "Tom\u00e1\u0161 Husek",
            "Fernando Romero-L\u00f3pez",
            "Stephen R. Sharpe",
            "Mattias Sj\u00f6"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "High Energy Physics - Lattice",
            "Nuclear Theory"
        ],
        "abstract": "The three-particle $K$-matrix, $\\mathcal{K}_{\\mathrm{df},3}$, is a scheme-dependent quantity that parametrizes short-range three-particle interactions in the relativistic-field-theory three-particle finite-volume formalism. In this work, we compute its value for systems of three pions in all isospin channels through next-to-leading order in Chiral Perturbation Theory, generalizing previous work done at maximum isospin. We obtain analytic expressions through quadratic order (or cubic order, in the case of zero isospin) in the expansion about the three-pion threshold.",
        "comments": "44 pages, 8 figures, 10 tables",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14293"
    },
    {
        "doc_id": 142,
        "title": "Quantum Electrometer for Time-Resolved Material Science at the Atomic Lattice Scale",
        "authors": [
            "Gregor Pieplow",
            "Cem G\u00fcney Torun",
            "Joseph H. D. Munns",
            "Franziska Marie Herrmann",
            "Andreas Thies",
            "Tommaso Pregnolato",
            "Tim Schr\u00f6der"
        ],
        "subjects": [
            "Applied Physics",
            "Materials Science",
            "Quantum Physics"
        ],
        "abstract": "The detection of individual charges plays a crucial role in fundamental material science and the advancement of classical and quantum high-performance technologies that operate with low noise. However, resolving charges at the lattice scale in a time-resolved manner has not been achieved so far. Here, we present the development of an electrometer, leveraging on the spectroscopy of an optically-active spin defect embedded in a solid-state material with a non-linear Stark response. By applying our approach to diamond, a widely used platform for quantum technology applications, we successfully localize charge traps, quantify their impact on transport dynamics and noise generation, analyze relevant material properties, and develop strategies for material optimization.",
        "comments": "Main: 9 pages, 5 figures; Supplement: 14 pages, 10 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14290"
    },
    {
        "doc_id": 143,
        "title": "How far can we see back in time in high-energy collisions using charm quarks?",
        "authors": [
            "Laszlo Gyulai",
            "Gabor Biro",
            "Robert Vertesi",
            "Gergely Gabor Barnafoldi"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "Nuclear Theory"
        ],
        "abstract": "We use open charm production to estimate how far we can see back in time in high-energy hadron-hadron collisions. We analyze the transverse momentum distributions of the identified D mesons from pp, p-Pb and A-A collisions at the ALICE and STAR experiments covering the energy range from $\\sqrt{s_{\\rm NN}} = 200$ GeV up to 7 TeV. Within a non-extensive statistical framework, the common Tsallis parameters for D mesons represent higher temperature and more degrees of freedom than that of light-flavour hadrons. The production of D mesons corresponds to a significantly earlier proper time, $\u03c4_{\\rm D} = (0.18 \\pm 0.06) \u03c4_{\\rm LF}$.",
        "comments": "18 pages, 6 figures, 1 table",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14282"
    },
    {
        "doc_id": 144,
        "title": "Bifurcation of Dividing Surfaces Constructed from Period-Doubling Bifurcations of Periodic Orbits in a Caldera Potential Energy Surface",
        "authors": [
            "Matthaios Katsanikas",
            "Makrina Agaoglou",
            "Stephen Wiggins"
        ],
        "subjects": [
            "Chaotic Dynamics",
            "Dynamical Systems",
            "Chemical Physics"
        ],
        "abstract": "In this work we analyze the bifurcation of dividing surfaces that occurs as a result of two period-doubling bifurcations in a 2D caldera-type potential. We study the structure, the range, the minimum and maximum extents of the periodic orbit dividing surfaces before and after a subcritical period-doubling bifurcation of the family of the central minimum of the potential energy surface. Furthermore, we repeat the same study for the case of a supercritical perioddoubling bifurcation of the family of the central minimum of the potential energy surface. We will discuss and compare the results for the two cases of bifurcations of dividing surfaces.",
        "comments": "15 pages. arXiv admin note: text overlap with arXiv:2107.09623",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14275"
    },
    {
        "doc_id": 145,
        "title": "Viscoelasticty with physics-augmented neural networks: Model formulation and training methods without prescribed internal variables",
        "authors": [
            "Max Rosenkranz",
            "Karl A. Kalina",
            "J\u00f6rg Brummund",
            "WaiChing Sun",
            "Markus K\u00e4stner"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science"
        ],
        "abstract": "We present an approach for the data-driven modeling of nonlinear viscoelastic materials at small strains which is based on physics-augmented neural networks (NNs) and requires only stress and strain paths for training. The model is built on the concept of generalized standard materials and is therefore thermodynamically consistent by construction. It consists of a free energy and a dissipation potential, which can be either expressed by the components of their tensor arguments or by a suitable set of invariants. The two potentials are described by fully/partially input convex neural networks. For training of the NN model by paths of stress and strain, an efficient and flexible training method based on a recurrent cell, particularly a long short-term memory cell, is developed to automatically generate the internal variable(s) during the training process. The proposed method is benchmarked and thoroughly compared with existing approaches. These include a method that obtains the internal variable by integrating the evolution equation over the entire sequence, while the other method uses an an auxiliary feedforward neural network for the internal variable(s). Databases for training are generated by using a conventional nonlinear viscoelastic reference model, where 3D and 2D plane strain data with either ideal or noisy stresses are generated. The coordinate-based and the invariant-based formulation are compared and the advantages of the latter are demonstrated. Afterwards, the invariant-based model is calibrated by applying the three training methods using ideal or noisy stress data. All methods yield good results, but differ in computation time and usability for large data sets. The presented training method based on a recurrent cell turns out to be particularly robust and widely applicable and thus represents a promising approach for the calibration of other types of models as well.",
        "comments": "21 pages, 16 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14270"
    },
    {
        "doc_id": 146,
        "title": "Phenomenology of TMD parton distributions in Drell-Yan and $Z^0$ boson production in a hadron structure oriented approach",
        "authors": [
            "F. Aslan",
            "M. Boglione",
            "J. O. Gonzalez-Hernandez",
            "T. Rainaldi",
            "T. C. Rogers",
            "A. Simonelli"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "Nuclear Theory"
        ],
        "abstract": "We present a first practical implementation of a recently proposed hadron structure oriented (HSO) approach to TMD phenomenology applied to Drell-Yan like processes, including lepton pair production at moderate $Q^2$ and $Z^0$ boson production. We compare and contrast general features of our methodology with other common practices and emphasize the improvements derived from our approach that we view as essential for applications where extracting details of nonperturbative transverse hadron structure is a major goal. These include the HSO's preservation of a basic TMD parton-model-like framework even while accounting for full TMD factorization and evolution, explicit preservation of the integral relationship between TMD and collinear pdfs, and the ability to meaningfully compare different theoretical models of nonperturbative TMD parton distributions. In our examples, we show that there is significant sensitivity at moderate $Q^2$ to both the form of the nonperturbative transverse momentum dependence and the parametrization of collinear parton densities. However, we also find that evolving to $Q^2 = M_Z^2$, without fitting, results in a satisfactory postdiction of existing data for $Z^0$ production, nearly independently of the modeling of nonperturbative transverse momentum behavior. We argue that this demonstrates that moderate $Q$ measurements should be given greater weight than high $Q$ measurements in extractions of nonperturbative transverse momentum dependence. We also obtain new extractions of the nonperturbative Collins-Soper kernel within the HSO approach. We discuss its features and compare with some earlier extractions.",
        "comments": "33 pages, 9 figures, 2 tables",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14266"
    },
    {
        "doc_id": 147,
        "title": "Existence and characterization of edge states in an acoustic trimer Su-Schrieffer-Heeger model",
        "authors": [
            "I. Ioannou Sougleridis",
            "A. Anastasiadis",
            "O. Richoux",
            "V. Achilleos",
            "G. Theocharis",
            "V. Pagneux",
            "F. K. Diakonos"
        ],
        "subjects": [
            "Applied Physics"
        ],
        "abstract": "We report on a direct mapping of acoustic slender waveguides to the one dimensional trimer Su- Schrieffer-Heeger model, with neither chiral nor mirror symmetry. Importantly, we can choose to perform this mapping for either the acoustic velocity or pressure. We demonstrate that, for finite systems, this choice is necessarily linked to the boundary conditions. It allows for the unveiling of the edge states of the acoustic system through an edge state phase diagram. An experimental realization of our setup in the audible regime corroborates our theoretical predictions.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14264"
    },
    {
        "doc_id": 148,
        "title": "Room temperature nonlocal detection of charge-spin interconversion in a topological insulator",
        "authors": [
            "Anamul Md. Hoque",
            "Lars Sj\u00f6str\u00f6m",
            "Dmitrii Khokhriakov",
            "Bing Zhao",
            "Saroj P. Dash"
        ],
        "subjects": [
            "Mesoscale and Nanoscale Physics"
        ],
        "abstract": "Topological insulators (TIs) are emerging materials for next-generation low-power nanoelectronic and spintronic device applications. TIs possess non-trivial spin-momentum locking features in the topological surface states in addition to the spin-Hall effect (SHE), and Rashba states due to high spin-orbit coupling (SOC) properties. These phenomena are vital for observing the charge-spin conversion (CSC) processes for spin-based memory, logic and quantum technologies. Although CSC has been observed in TIs by potentiometric measurements, reliable nonlocal detection has so far been limited to cryogenic temperatures up to T = 15 K. Here, we report nonlocal detection of CSC and its inverse effect in the TI compound Bi1.5Sb0.5Te1.7Se1.3 at room temperature using a van der Waals heterostructure with a graphene spin-valve device. The lateral nonlocal device design with graphene allows observation of both spin-switch and Hanle spin precession signals for generation, injection and detection of spin currents by the TI. Detailed bias- and gate-dependent measurements in different geometries prove the robustness of the CSC effects in the TI. These findings demonstrate the possibility of using topological materials to make all-electrical room-temperature spintronic devices.",
        "comments": "12 pages, 4 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14262"
    },
    {
        "doc_id": 149,
        "title": "Conservation laws and the foundations of quantum mechanics",
        "authors": [
            "Yakir Aharonov",
            "Sandu Popescu",
            "Daniel Rohrlich"
        ],
        "subjects": [
            "Quantum Physics"
        ],
        "abstract": "In a recent paper, PNAS, 118, e1921529118 (2021), it was argued that while the standard definition of conservation laws in quantum mechanics, which is of a statistical character, is perfectly valid, it misses essential features of nature and it can and must be revisited to address the issue of conservation/non-conservation in individual cases. Specifically, in the above paper an experiment was presented in which it can be proven that in some individual cases energy is not conserved, despite being conserved statistically. It was felt however that this is worrisome, and that something must be wrong if there are individual instances in which conservation doesn't hold, even though this is not required by the standard conservation law. Here we revisit that experiment and show that although its results are correct, there is a way to circumvent them and ensure individual case conservation in that situation. The solution is however quite unusual, challenging one of the basic assumptions of quantum mechanics, namely that any quantum state can be prepared, and it involves a time-holistic, double non-conservation effect. Our results bring new light on the role of the preparation stage of the initial state of a particle and on the interplay of conservation laws and frames of reference. We also conjecture that when such a full analysis of any conservation experiment is performed, conservation is obeyed in every individual case.",
        "comments": "Journal ref:        PNAS, 120 (41) e2220810120 (2023)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14261"
    },
    {
        "doc_id": 150,
        "title": "Stability and Transport of Gyrokinetic Critical Pedestals",
        "authors": [
            "J. F. Parisi",
            "A. O. Nelson",
            "W. Guttenfelder",
            "R. Gaur",
            "J. W. Berkery",
            "S. M. Kaye",
            "K. Barada",
            "C. Clauser",
            "A. Diallo",
            "D. R. Hatch",
            "A. Kleiner",
            "M. Lampert",
            "T. Macwan",
            "J. E. Menard"
        ],
        "subjects": [
            "Plasma Physics"
        ],
        "abstract": "A gyrokinetic threshold model for pedestal width-height scaling prediction is applied to multiple devices and to a shaping and aspect-ratio scan giving $\u0394_{\\mathrm{ped}} = 0.92 A^{1.04} \u03ba^{-1.24} 0.38^\u03b4 \u03b2_{\u03b8,\\mathrm{ped}}^{1.05}$ for pedestal width $\u0394_{\\mathrm{ped}}$, aspect-ratio $A$, elongation $\u03ba$, triangularity $\u03b4$, and normalized pedestal height $\u03b2_{\u03b8,\\mathrm{ped}}$. We also find a width-transport scaling $\u0394_{\\mathrm{ped} } = 0.028 \\left(q_e/\u0393_e - 1.7 \\right)^{1.5} \\sim \u03b7_e ^{1.5}$ where $q_e$ and $\u0393_e$ are turbulent electron heat and particle fluxes and $\u03b7_e = \\nabla \\ln T_e / \\nabla \\ln n_e$ for electron temperature $T_e$ and density $n_e$. Pedestals close to those limited by kinetic-ballooning-modes (KBMs) have modified turbulent transport properties compared to strongly driven KBMs. The role of flow shear is studied as a width-height scaling constraint and pedestal saturation mechanism for a standard and wide pedestal discharge.",
        "comments": "34 pages, 16 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14260"
    },
    {
        "doc_id": 151,
        "title": "Mpemba effects in nonequilibrium open quantum systems",
        "authors": [
            "Xuanhua Wang",
            "Jin Wang"
        ],
        "subjects": [
            "Quantum Physics",
            "Statistical Mechanics",
            "Chemical Physics"
        ],
        "abstract": "Originally, the Mpemba effect (MPE) is referred to the faster icing of a higher-temperature system than a system of a lower temperature. This concept was later generalized to anomalous decays of certain system quantities to the equilibrium states. In this study, we investigate the scenario when a system has no such equilibrium state to approach. Instead, the system is put in contact with two different baths, and only a nonequilibrium state exists, sustained by constant energy injection from the surrounding thermal baths. Firstly, we show that the nonequilibrium conditions can dramatically enlarge the parameter regimes where the MPE emerges. Secondly, we demonstrate that the anomalous MPEs and inverse MPEs emerge in the evolution of quantum correlations in the two-site fermionic system and that nonequilibrium conditions can expedite or delay the MPEs. Thirdly, we show that the nonequilibrium-induced quantum coherence can have considerable contributions to the emergence of the MPE which the conventional Lindbladian dynamics fails to capture.",
        "comments": "9 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14259"
    },
    {
        "doc_id": 152,
        "title": "On the vapour compression in cavitation bubbles",
        "authors": [
            "Davide Bernardo Preso",
            "Daniel Fuster",
            "Armand Baptiste Sieber",
            "Danail Obreschkow",
            "Mohamed Farhat"
        ],
        "subjects": [
            "Fluid Dynamics"
        ],
        "abstract": "The composition of the gaseous phase of cavitation bubbles and its role on the collapse remains to date poorly understood. In this work, experiments of single cavitation bubbles in aqueous ammonia serve as a novel approach to investigate the effect of the vapour contained in a bubble on its collapse. We find that the higher vapour pressure of more concentrated aqueous ammonia acts as a resistance to the collapse, reducing the total energy dissipation. In line with visual observation, acoustic measurements, and luminescence recordings, it is also observed that higher vapour pressures contribute to a more spherical collapse, likely hindering the growth of interface instabilities by decreasing the collapse velocities and accelerations. Remarkably, we evidence a strong difference between the effective damping and the energy of the shock emission, suggesting that the latter is not the dominant dissipation mechanism at collapse as predicted from classical correction models accounting for slightly compressible liquids. Furthermore, our results suggest that the vapour inside collapsing bubbles gets compressed, consistently with previous studies performed in the context of single bubble sonoluminescence, addressing the question about the ability of vapours to readily condense during a bubble collapse in similar regimes. These findings provide insights into the identification of the influence of the bubble content and the energy exchanges of the bubble with its surrounding media, eventually paving the way to a more efficient use of cavitation in engineering and biomedical applications.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14253"
    },
    {
        "doc_id": 153,
        "title": "Variational Neural and Tensor Network Approximations of Thermal States",
        "authors": [
            "Sirui Lu",
            "Giacomo Giudice",
            "J. Ignacio Cirac"
        ],
        "subjects": [
            "Quantum Physics",
            "Disordered Systems and Neural Networks",
            "Strongly Correlated Electrons"
        ],
        "abstract": "We introduce a variational Monte Carlo algorithm for approximating finite-temperature quantum many-body systems, based on the minimization of a modified free energy. We employ a variety of trial states -- both tensor networks as well as neural networks -- as variational ans\u00e4tze for our numerical optimization. We benchmark and compare different constructions in the above classes, both for one- and two-dimensional problems, with systems made of up to \\(N=100\\) spins. Despite excellent results in one dimension, our results suggest that the numerical ans\u00e4tze employed have certain expressive limitations for tackling more challenging two-dimensional systems.",
        "comments": "7+9 pages, 3+4 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14243"
    },
    {
        "doc_id": 154,
        "title": "Short-Time Infrequent Metadynamics for Improved Kinetics Inference",
        "authors": [
            "Ofir Blumer",
            "Shlomi Reuveni",
            "Barak Hirshberg"
        ],
        "subjects": [
            "Chemical Physics"
        ],
        "abstract": "Infrequent Metadynamics is a popular method to obtain the rate of long timescale processes from accelerated simulations. The inference procedure is based on rescaling the first-passage times of Metadynamics trajectories using a bias-dependent acceleration factor. While useful in many cases, it is limited to Poisson kinetics, and a reliable estimation of the unbiased rate requires slow bias deposition and prior knowledge of efficient collective variables. Here, we propose an improved inference scheme, which is based on two key observations: 1) The time-independent rate of Poisson processes can be estimated using short trajectories only. 2) Short trajectories experience minimal bias, and their rescaled first-passage times follow the unbiased distribution even for relatively high deposition rates and suboptimal collective variables. Therefore, by limiting the inference procedure to short timescales, we obtain an improved tradeoff between speedup and accuracy at no additional computational cost, especially when employing suboptimal collective variables. We demonstrate the improved inference scheme for a model system and two molecular systems.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14237"
    },
    {
        "doc_id": 155,
        "title": "Unraveling how winds and surface heat fluxes control the Atlantic Ocean's meridional heat transport",
        "authors": [
            "Dhruv Bhagtani",
            "Andrew McC. Hogg",
            "Ryan M. Holmes",
            "Navid C. Constantinou"
        ],
        "subjects": [
            "Atmospheric and Oceanic Physics"
        ],
        "abstract": "The North Atlantic Ocean circulation, fueled by winds and surface buoyancy fluxes, carries 1.25$\\,$PettaWatts of heat poleward in the subtropics, and plays an important role in regulating global weather and climate patterns. Using a series of simulations with perturbed surface forcing, we study how winds and surface heat flux gradients affect the Atlantic meridional heat transport. We decompose the Atlantic meridional heat transport into contributions from circulation cells at warm and cold temperatures (resembling a subtropical gyre and the dense overturning circulation respectively), and a mixed circulation that contains water masses traversing both these cells. Variations in wind stress initially alter the amount of heat carried by the warm and mixed cells, but on long time scales ($>$10 years), changes in the temperature distribution restore the heat transport to equilibrium. Changes in surface buoyancy forcing control the cold cell's circulation, and its associated meridional heat flux, through high-latitude processes.",
        "comments": "13 pages, 3 figures, submitted to the Geophysical Research Letters",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14230"
    },
    {
        "doc_id": 156,
        "title": "Exploring the time axis within medium-modified jets",
        "authors": [
            "Liliana Apolin\u00e1rio",
            "Pablo Guerrero-Rodr\u00edguez",
            "Korinna Zapp"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "In this manuscript, we illustrate how to use the newly proposed $\u03c4$ re-clustering algorithm to select jets with different degrees of quenching without biasing their initial transverse momentum spectrum. Our study is based on Z+jet simulated events using the JEWEL Monte Carlo event generator to account for jet quenching effects. We apply the $\u03c4$ re-clustering algorithm to extract a proxy for a time axis (formation time) within the evolving medium. This information allows us to label jets according to their fragmentation pattern and select populations with enhanced sensitivity to quenching effects. Our results illustrate the potential of jets as precision tools for QGP tomography. Further, we show that the discussed method minimizes the biases stemming from $p_{T}$-, $dR$- or mass-based jet selection.",
        "comments": "12 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14229"
    },
    {
        "doc_id": 157,
        "title": "Periodically Forced Nonlinear Oscillatory Acoustic Vacuum",
        "authors": [
            "Makrina Agaoglou",
            "Michal Feckan",
            "Michal Pospisil",
            "Vassilis M. Rothos",
            "Alexander F. Vakakis"
        ],
        "subjects": [
            "Mathematical Physics"
        ],
        "abstract": "In this work, we study the in-plane oscillations of a finite lattice of particles coupled by linear springs under distributed harmonic excitation. Melnikov-type analysis is applied for the persistence of periodic oscillations of a reduced system.",
        "comments": "11 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14227"
    },
    {
        "doc_id": 158,
        "title": "On the well-posedness of inverse problems under information field theory: application to model-form error detection",
        "authors": [
            "Alex Alberts",
            "Ilias Bilionis"
        ],
        "subjects": [
            "Mathematical Physics"
        ],
        "abstract": "We derive properties of information field theory (IFT) as applied to inverse problems. The results here can be extended to methodologies which can be seen as limiting cases of IFT, such as Gaussian process regression and physics-informed machine learning. We first define the concept of a well-posed inverse problem within the context of IFT, and pose a few useful theorems for conditions in which an inverse problem becomes well-posed. Using the Gaussian random field interpretation of IFT, we show how identifying parameters of a covariance kernel becomes a well-posed inverse problem under certain conditions. An expression for the Hessian of the inverse problem log posterior is derived to construct the results. A specific focus is placed on the inverse problem of detecting model-form error. We provide an example where the physics are assumed to be the Poisson equation and prove conditions for which identifying model-form error in this case becomes a well-posed inverse problem under IFT.",
        "comments": "16 pages. Will be presented at SIAM Conference on Uncertainty Quantification 2024",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14224"
    },
    {
        "doc_id": 159,
        "title": "Application of performance portability solutions for GPUs and many-core CPUs to track reconstruction kernels",
        "authors": [
            "Ka Hei Martin Kwok",
            "Matti Kortelainen",
            "Giuseppe Cerati",
            "Alexei Strelchenko",
            "Oliver Gutsche",
            "Allison Reinsvold Hall",
            "Steve Lantz",
            "Michael Reid",
            "Daniel Riley",
            "Sophie Berkman",
            "Seyong Lee",
            "Hammad Ather",
            "Boyana Norris",
            "Cong Wang"
        ],
        "subjects": [
            "Accelerator Physics"
        ],
        "abstract": "Next generation High-Energy Physics (HEP) experiments are presented with significant computational challenges, both in terms of data volume and processing power. Using compute accelerators, such as GPUs, is one of the promising ways to provide the necessary computational power to meet the challenge. The current programming models for compute accelerators often involve using architecture-specific programming languages promoted by the hardware vendors and hence limit the set of platforms that the code can run on. Developing software with platform restrictions is especially unfeasible for HEP communities as it takes significant effort to convert typical HEP algorithms into ones that are efficient for compute accelerators. Multiple performance portability solutions have recently emerged and provide an alternative path for using compute accelerators, which allow the code to be executed on hardware from different vendors. We apply several portability solutions, such as Kokkos, SYCL, C++17 std::execution::par and Alpaka, on two mini-apps extracted from the mkFit project: p2z and p2r. These apps include basic kernels for a Kalman filter track fit, such as propagation and update of track parameters, for detectors at a fixed z or fixed r position, respectively. The two mini-apps explore different memory layout formats.\n  We report on the development experience with different portability solutions, as well as their performance on GPUs and many-core CPUs, measured as the throughput of the kernels from different GPU and CPU vendors such as NVIDIA, AMD and Intel.",
        "comments": "26th Intl Conf Computing High Energy & Nuclear Phys (CHEP 2023)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14221"
    },
    {
        "doc_id": 160,
        "title": "Shocks, clouds and atomic outflows in active galactic nuclei hosting relativistic jets",
        "authors": [
            "Manel Perucho"
        ],
        "subjects": [
            "High Energy Astrophysical Phenomena",
            "Astrophysics of Galaxies"
        ],
        "abstract": "A number of observations have revealed atomic and/or molecular lines in active galaxies hosting jets and outflows. Line widths indicate outward motions of hundreds to few thousands of kilometers per second. They appear associated to the presence of radio emission in Gigahert-peaked spectrum (GPS) and compact steep spectrum (CSS) sources, with linear sizes < 10 kpc. Numerical simulations have shown that the bow shocks triggered by relativistic jets in their host galaxies drive ionisation and turbulence in the interstellar medium (ISM). However, the presence of atomic lines requires rapid recombination of ionised gas, which seems to be hard to explain from the physical conditions revealed so far by numerical simulations of powerful jets. The aim of this paper is to provide a global frame to explain the presence of lines in terms of jet and shock evolution, and fix the parameter space in which the atomic and molecular outflows might occur. This parameter space is inspired by numerical simulations and basic analytical models of jet evolution as a background. Our results show that a plausible, general explanation involves momentum transfer and heating to the interstellar medium gas by jet triggered shocks within the inner kiloparsecs. The presence of post-shock atomic gas is possible in the case of shocks interacting with dense clouds that remain relatively stable after the shock passage. According to our results, current numerical simulations cannot reproduce the physical conditions to explain the presence of atomic and molecular outflows in young radio-sources. However, I show that these outflows might occur in low-power jets at all scales, and predict a trend towards powerful jets showing lines at CSS scales, when clouds have cooled to recombination temperatures.",
        "comments": "Accepted for publication in Astronomy & Astrophysics",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14218"
    },
    {
        "doc_id": 161,
        "title": "The explicit form of the unitary representation of the Poincar\u00e9 group for vector-valued wave functions (massive and massless), with applications to photon's localization and position operators",
        "authors": [
            "Arkadiusz Jadczyk"
        ],
        "subjects": [
            "Quantum Physics"
        ],
        "abstract": "We geometrically derive the explicit form of the Unitary representation of the Poincare group and use it to apply speed-of-light boosts to simple polarization basis to end up with Hawton-Baylis photon position operator with commuting components. We give explicit formulas for other photon boost eigenmodes. We investigate the underlying affine connections on the light cone in momentum space and find that while Pryce connection is metric semi-symmetric, the flat Hawton-Baylis connection is not semi-symmetric. Finally we discuss localizability of photon states localized on closed loops and show that photon states on the circle, both unnormalized improper states and finite norm wave packet smeared over washer-like regions are strictly localized with respect to Hawton-Baylis operators with commuting components and also with respect to the noncommutative Jauch-Piron-Amrein POV measure.",
        "comments": "30 pages, 3 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14217"
    },
    {
        "doc_id": 162,
        "title": "At the junction between deep learning and statistics of extremes: formalizing the landslide hazard definition",
        "authors": [
            "Ashok Dahal",
            "Rapha\u00ebl Huser",
            "Luigi Lombardo"
        ],
        "subjects": [
            "Machine Learning",
            "Geophysics",
            "Applications",
            "Machine Learning"
        ],
        "abstract": "The most adopted definition of landslide hazard combines spatial information about landslide location (susceptibility), threat (intensity), and frequency (return period). Only the first two elements are usually considered and estimated when working over vast areas. Even then, separate models constitute the standard, with frequency being rarely investigated. Frequency and intensity are intertwined and depend on each other because larger events occur less frequently and vice versa. However, due to the lack of multi-temporal inventories and joint statistical models, modelling such properties via a unified hazard model has always been challenging and has yet to be attempted. Here, we develop a unified model to estimate landslide hazard at the slope unit level to address such gaps. We employed deep learning, combined with a model motivated by extreme-value theory to analyse an inventory of 30 years of observed rainfall-triggered landslides in Nepal and assess landslide hazard for multiple return periods. We also use our model to further explore landslide hazard for the same return periods under different climate change scenarios up to the end of the century. Our results show that the proposed model performs excellently and can be used to model landslide hazard in a unified manner. Geomorphologically, we find that under both climate change scenarios (SSP245 and SSP885), landslide hazard is likely to increase up to two times on average in the lower Himalayan regions while remaining the same in the middle Himalayan region whilst decreasing slightly in the upper Himalayan region areas.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14210"
    },
    {
        "doc_id": 163,
        "title": "Quantum information recovery from black hole with projective measurement",
        "authors": [
            "Ran Li",
            "Jin Wang"
        ],
        "subjects": [
            "General Relativity and Quantum Cosmology",
            "High Energy Physics - Theory",
            "Quantum Physics"
        ],
        "abstract": "We studied the Hayden-Preskill thought experiment with the local projective measurement. Compared to the original model, the measurement is applied on the Hawking radiation that was emitted after throwing the quantum diary into the black hole. Within this setup, we explored various aspects of this model, including the information recovery from the black hole, the relation to the black hole final state proposal, the relation between the Yoshida-Kitaev protocol and Petz recovery map, the effects of the decoherence, and the quantum simulations of the decoding protocols. These aspects may provide us new insights into the non-perturbative nature of quantum black holes.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14207"
    },
    {
        "doc_id": 164,
        "title": "Experimental investigations of underwater and airborne noises produced by a large hovercraft in Ural River estuary",
        "authors": [
            "A. I. Vedenev",
            "O. Yu. Kochetov",
            "A. A. Lunkov",
            "A. S. Shurup",
            "S. S. Kassymbekova"
        ],
        "subjects": [
            "Atmospheric and Oceanic Physics"
        ],
        "abstract": "Simultaneous measurements of underwater and airborne noises produced by Griffon Hoverwork BHT130 hovercraft were carried out in environmentally sensitive area - wildlife preserve in the area of the Ural River estuary near the Caspian Sea shelf. Measurements were organized to assess the possible negative impact of noise from hovercraft on fish and birds in wildlife preserve. The particle velocity of underwater noise was estimated by using a gradient-type vector receiver. That was a distinctive aspect of the underwater noise studies since the majority of fish perceives the sound in terms of vibration of particles, and only a few as the pressure. Using synchronous recording of underwater and airborne noises, the mutual correlation of these data was investigated. The obtained correlation levels between underwater and airborne noises produced by hovercraft can be used for simplified estimation of the upper boundary of underwater noise level by measuring levels of airborne noise. The measured and estimated maximal levels of underwater noises of hovercraft are considerably lower than noises from conventional vessels with underwater engines, that makes hovercraft attractive alternative for use in locations with high underwater noise requirements, such as Ural River estuary and Caspian Sea shelf.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14204"
    },
    {
        "doc_id": 165,
        "title": "Investigating Organic Carbon and Thermal History of CM Carbonaceous Chondrites Using Spectroscopy and Laboratory Techniques",
        "authors": [
            "Safoura Tanbakouei",
            "Rui-Lin Cheng",
            "Binlong Ye",
            "Josep Ryan Michalski",
            "Ashley J. King"
        ],
        "subjects": [
            "Earth and Planetary Astrophysics",
            "Instrumentation and Methods for Astrophysics",
            "Geophysics"
        ],
        "abstract": "The CM chondrites are characterized as primary accretionary rocks which originate from primitive water-rich asteroids formed during the early Solar System. Here, we study the mineralogy and organic characteristics of right CM and one ungrouped chondrite to better understand their alteration history; Queen Alexandra Range 93005 (QUE 93005), Murchison, LaPaz Icefield 02333 (LAP 02333), Miller Range (MIL 13005), Mackay Glacier 05231 (MCY 05231), Northwest Africa 8534 (NWA 8534), Northwest Africa 3340 (NWA 3340), Yamato 86695 (Y-86695), and the ungrouped carbonaceous chondrite Belgica 7904 (B-7904). Raman spectroscopy has been employed to detect the presence of organic carbon in the samples, specifically through the G band at approximately 1580 cm-1 and D band at around 1350 cm-1. The properties of organic matter in meteorites serve as valuable indicators for characterizing the structure and crystallinity of carbonaceous materials and estimating their thermal metamorphism degree. The R1 parameter, defined as the peak height ratio of the D and G bands, provides a quantifiable measure of this structural organization. Raman spectra are used to show the general mineralogy, thermal history and heating stage of CM and ungrouped chondrites. X-ray diffraction patterns further indicate the mineralogical compositions of the samples. Visible to near-infrared (VNIR) and attenuated total reflection (ATR) reflectance spectra illustrate the trends related to their mineralogy and furthermore infer aqueous alteration, thermal history of CM carbonaceous chondrites, formation and evolution of their parent bodies.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14201"
    },
    {
        "doc_id": 166,
        "title": "Speeding up Fermionic Lattice Calculations with Photonic Accelerated Inverters",
        "authors": [
            "Felipe Attanasio",
            "Marc Bauer",
            "Jelle Dijkstra",
            "Timoteo Lee",
            "Jan M. Pawlowski",
            "Wolfram Pernice"
        ],
        "subjects": [
            "High Energy Physics - Lattice",
            "Applied Physics",
            "Computational Physics"
        ],
        "abstract": "Lattice field theory (LFT) is the standard non-perturbative method to perform numerical calculations of quantum field theory. However, the typical bottleneck of fermionic lattice calculations is the inversion of the Dirac matrix. This inversion is solved by iterative methods, like the conjugate gradient algorithm, where matrix-vector multiplications (MVMs) are the main operation. Photonic integrated circuits excel in performing quick and energy-efficient MVMs, but at the same time, they are known to have low accuracy. This can be overcome by using mixed precision methods. In this paper, we explore the idea of using photonic technology to fulfil the demand for computational power of fermionic lattice calculations. These methods have the potential to reduce computation costs by one order of magnitude. Because of the hybrid nature of these methods, we call these 'photonic accelerated inverters (PAIs)'.",
        "comments": "10 pages, 8 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14200"
    },
    {
        "doc_id": 167,
        "title": "Deep Learning to Improve the Sensitivity of Di-Higgs Searches in the $4b$ Channel",
        "authors": [
            "Cheng-Wei Chiang",
            "Feng-Yang Hsieh",
            "Shih-Chieh Hsu",
            "Ian Low"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "The study of di-Higgs events, both resonant and non-resonant, plays a crucial role in understanding the fundamental interactions of the Higgs boson. In this work we consider di-Higgs events decaying into four $b$-quarks and propose to improve the experimental sensitivity by utilizing a novel machine learning algorithm known as Symmetry Preserving Attention Network (\\textsc{Spa-Net}) -- a neural network structure whose architecture is designed to incorporate the inherent symmetries in particle reconstruction tasks. We demonstrate that the \\textsc{Spa-Net} can enhance the experimental reach over baseline methods such as the cut-based and the Deep Neural Networks (DNN)-based analyses. At the Large Hadron Collider, with a 14-TeV centre-of-mass energy and an integrated luminosity of 300 fb$^{-1}$, the \\textsc{Spa-Net} allows us to establish 95\\% C.L. upper limits in resonant production cross-sections that are 10\\% to 45\\% stronger than baseline methods. For non-resonant di-Higgs production, \\textsc{Spa-Net} enables us to constrain the self-coupling that is 9\\% more stringent than the baseline method.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14198"
    },
    {
        "doc_id": 168,
        "title": "Sensitivity of two-mode SRF cavity to generic electromagnetic interactions of ultralight dark matter",
        "authors": [
            "Chang-Jie Dai",
            "Tong Li",
            "Rui-Jia Zhang"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "The ultralight dark matter (ULDM) such as axion or wavelike scalar plays as a plausible DM candidate. Recently, the possible non-standard ULDM couplings draw much attention. In this work we investigate the detection of electromagnetic couplings in a few benchmark models of ULDM. For illustration, we consider the generic axion electrodynamics including CP violating coupling as well as the newly proposed axion electromagnetodynamics. The superconducting radio frequency (SRF) cavity with two-mode has more advantages than the traditional cavity approach with static background field. We utilize the two-mode SRF cavity to probe the generic couplings of ULDM with frequency lower than GHz. The choices of the transverse electromagnetic modes are explicitly specified for the detection. We show the sensitivity of the SRF cavity to the axion couplings in the above frameworks.",
        "comments": "26 pages, 3 figures, 2 tables",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14195"
    },
    {
        "doc_id": 169,
        "title": "Interactions of the Pseudoscalar Meson Octet and the Baryon Decuplet in the Continuum and a Finite Volume",
        "authors": [
            "Teng Ji",
            "Xiang-Kun Dong",
            "Ulf-G. Mei\u00dfner"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "High Energy Physics - Experiment",
            "High Energy Physics - Lattice",
            "Nuclear Theory"
        ],
        "abstract": "This study focuses on the interaction of the pseudoscalar meson octet and the baryon decuplet. In the continuum, it is observed that several $J^{P}=\\frac32^-$ baryon resonances can be produced by the Weinberg-Tomozawa interaction in unitarized chiral perturbation theory, including the $N(1875)$, $\u03a3(1670)$, $\u03a3(1910)$, $\u039e(1820)$ and $\u03a9(2012)$. Among them, the $\u039e(1820)$ and $\u03a3(1670)$ may exhibit a potential two-pole structures. The unitarized chiral perturbation approach is then applied as the underlying theory to predict the energy levels of these systems in a finite volume. These energy levels are well described by the $K$-matrix parameterization constrained by flavor SU(3) symmetry. With the parameters from the best fits, the poles extracted from the $K$-matrix parameterization closely correspond to those derived from the underlying chiral effective field theory, as long as they are close to physical region and not significantly higher than the lowest relevant threshold.",
        "comments": "11 pages, 6 figures and 2 tables",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14188"
    },
    {
        "doc_id": 170,
        "title": "Non-symmetrical sparking may hint \"zits'' on a pulsar surface",
        "authors": [
            "Zhengli Wang",
            "Jiguang Lu",
            "Jingchen Jiang",
            "Shunshun Cao",
            "Weiyang Wang",
            "Enwei Liang",
            "Renxin Xu"
        ],
        "subjects": [
            "High Energy Astrophysical Phenomena"
        ],
        "abstract": "Pulsar electrodynamics could be relevant to the physics of stellar surface, which remains poorly understood for more than half a centenary and is difficult to probe due to the absence of direct and clear observational evidence. Nevertheless, highly-sensitive telescopes (e.g., China's Five-hundred-meter Aperture Spherical radio Telescope, FAST) may play an essential role in solving the problem since the predicted surface condition would have quite different characteristics in some models of pulsar structure, especially after the establishment of the standard model of particle physics. For instance, small hills (or ``zit'') may exist on solid strangeon star surface with rigidity, preferential discharge, i.e., gap sparking, may occur around the hills in the polar cap region. In this work, with the 110-min polarization observation of PSR B0950+08 targeted by FAST, we report that the gap sparking is significantly non-symmetrical to the meridian plane on which the rotational and magnetic axes lie. It is then speculated that this asymmetry could be the result of preferential sparking around zits which might rise randomly on pulsar surface. Some polarization features of both single pulses and the mean pulse, as well as the cross-correlation function of different emission regions, have also been presented.",
        "comments": "Accepted for publication in Astronomische Nachrichten",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14181"
    },
    {
        "doc_id": 171,
        "title": "Magnetic fields of protoplanetary disks",
        "authors": [
            "Sergey A. Khaibrakhmanov"
        ],
        "subjects": [
            "Solar and Stellar Astrophysics",
            "Earth and Planetary Astrophysics",
            "Plasma Physics"
        ],
        "abstract": "We review the current status of studies on accretion and protoplanetary disks of young stars with large-scale magnetic fields. Observational data on magnetic fields of the disks are compiled and analysed. Modern analytical and numerical MHD models of protoplanetary disks are discussed. The mechanisms of angular momentum transport via turbulence, magnetic tensions and outflows are outlined. We consider the influence of Ohmic dissipation, magnetic ambipolar diffusion, magnetic buoyancy, and the Hall effect on the evolution of the magnetic flux in disks. Modern MHD models of accretion disks show that the magnetic field can influence the structure of protoplanetary disks. We argue that the available observational data on the magnetic fields in protoplanetary disks can be interpreted within the framework of fossil magnetic field theory. We summarize the problems of the modern theory of accretion and protoplanetary disks with magnetic fields and also outline the prospects for further research.",
        "comments": "31 pages, 1 table, 2 figures, accepted to Astronomical and Astrophysical Transactions",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14180"
    },
    {
        "doc_id": 172,
        "title": "Deep Neural Networks as Variational Solutions for Correlated Open Quantum Systems",
        "authors": [
            "Johannes Mellak",
            "Enrico Arrigoni",
            "Wolfgang von der Linden"
        ],
        "subjects": [
            "Quantum Physics",
            "Disordered Systems and Neural Networks"
        ],
        "abstract": "In this work we apply deep neural networks to find the non-equilibrium steady state solution to correlated open quantum many-body systems. Motivated by the ongoing search to find more powerful representations of (mixed) quantum states, we design a simple prototypical convolutional neural network and show that parametrizing the density matrix directly with more powerful models can yield better variational ansatz functions and improve upon results reached by neural density operator based on the restricted Boltzmann machine. Hereby we give up the explicit restriction to positive semi-definite density matrices. However, this is fulfilled again to good approximation by optimizing the parameters. The great advantage of this approach is that it opens up the possibility of exploring more complex network architectures that can be tailored to specific physical properties. We show how translation invariance can be enforced effortlessly and reach better results with fewer parameters. We present results for the dissipative one-dimensional transverse-field Ising model and a two-dimensional dissipative Heisenberg model compared to exact values.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14179"
    },
    {
        "doc_id": 173,
        "title": "Multicasting Optical Reconfigurable Switch",
        "authors": [
            "Niyazi Ulas Dinc",
            "Mustafa Yildirim",
            "Christophe Moser",
            "Demetri Psaltis"
        ],
        "subjects": [
            "Optics",
            "Networking and Internet Architecture"
        ],
        "abstract": "Artificial Intelligence (AI) demands large data flows within datacenters, heavily relying on multicasting data transfers. As AI models scale, the requirement for high-bandwidth and low-latency networking compounds. The common use of electrical packet switching faces limitations due to its optical-electrical-optical conversion bottleneck. Optical switches, while bandwidth-agnostic and low-latency, suffer from having only unicast or non-scalable multicasting capability. This paper introduces an optical switching technique addressing the scalable multicasting challenge. Our approach enables arbitrarily programmable simultaneous unicast and multicast connectivity, eliminating the need for optical splitters that hinder scalability due to optical power loss. We use phase modulation in multiple planes, tailored to implement any multicast connectivity map. Using phase modulation enables wavelength selectivity on top of spatial selectivity, resulting in an optical switch that implements space-wavelength routing. We conducted simulations and experiments to validate our approach. Our results affirm the concept's feasibility and effectiveness, as a multicasting switch.",
        "comments": "12 pages, 3 figures, article",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14173"
    },
    {
        "doc_id": 174,
        "title": "No-go guide for the Hubble tension: late-time or local-scale new physics",
        "authors": [
            "Lu Huang",
            "Shao-Jiang Wang",
            "Wang-Wei Yu"
        ],
        "subjects": [
            "Cosmology and Nongalactic Astrophysics"
        ],
        "abstract": "The standard model of modern cosmology might be cracked by the recent persistent hot debate on the Hubble-constant ($H_0$) tension, which manifests itself as the sound-horizon ($r_s$) tension or absolute-magnitude ($M_B$) tension if deeming the origin of the Hubble tension from modifying the early or late Universe, respectively. In this Letter, we achieve a fully model-independent constraint (fitting a model-independent global parameterization to a model-independent inverse distant ladder with a model-independent high-redshift calibration) on late-time models with strong evidence against homogeneous new physics over the $\u039b$-cold-dark ($\u039b$CDM) model. Further using this model-independent constraint to calibrate sufficiently local supernovae with corresponding late-time models extrapolated below the homogeneity scale, we find surprisingly that, although both $H_0$ tension and $M_B$ tension are absent in our local Universe, a combination of $H_0$ and $M_B$ as the intercept $a_B$ of the magnitude-redshift relation exhibits $3\\sim 7\u03c3$ tension even for the $\u039b$CDM model. This $a_B$ tension seems to call for local-scale inhomogeneous new physics disguised as local observational systematics.",
        "comments": "5 pages + references, 1 table and 3 figures, codes can be found at https://github.com/huanglu37/No-go-guide-for-the-Hubble-tension-late-time-or-local-scale-new-physics and chains can be found at https://zenodo.org/records/10559728",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14170"
    },
    {
        "doc_id": 175,
        "title": "Runaway Electron Dynamics in ITER Disruptions with Shattered Pellet Injections",
        "authors": [
            "Oskar Vallhagen",
            "Lise Hanebring",
            "Javier Artola",
            "Michael Lehnen",
            "Eric Nardon",
            "T\u00fcnde F\u00fcl\u00f6p",
            "Mathias Hoppe",
            "Sarah Newton",
            "Istvan Pusztai"
        ],
        "subjects": [
            "Plasma Physics"
        ],
        "abstract": "This study systematically explores the parameter space of disruption mitigation through shattered pellet injection in ITER with a focus on runaway electron dynamics, using the disruption modelling tool DREAM. The physics fidelity is considerably increased compared to previous studies, by e.g., using realistic magnetic geometry, resistive wall configuration, thermal quench onset criteria, as well as including additional effects, such as ion transport and enhanced runaway electron transport during the thermal quench. The work aims to provide a fairly comprehensive coverage of experimentally feasible scenarios, considering plasmas representative of both non-activated and high-performance DT operation, different thermal quench onset criteria and transport levels, a wide range of hydrogen and neon quantities injected in one or two stages, and pellets with various characteristic shard sizes. Using a staggered injection scheme, with a pure hydrogen injection preceding a mixed hydrogen-neon injection, we find injection parameters leading to acceptable runaway electron currents in all investigated discharges without activated runaway sources. Dividing the injection into two stages is found to significantly enhance the assimilation and minimize runaway electron generation due to the hot-tail mechanism. However, while a staggered injection outperforms a single stage injection also in cases with radioactive runaway electron sources, no cases with acceptable runaway electron currents are found for a DT-plasma with a 15 MA plasma current.",
        "comments": "16 pages, 7 figures, submitted to Nuclear Fusion",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14167"
    },
    {
        "doc_id": 176,
        "title": "QCD analysis of the $P$-wave charmonium electromagnetic Dalitz decays $h_{c}\\rightarrow\u03b7^{(\\prime)}\\ell^{+}\\ell^{-}$",
        "authors": [
            "Chao-Jie Fan",
            "Jun-Kang He"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology"
        ],
        "abstract": "The $P$-wave charmonium electromagnetic Dalitz decays $h_{c}\\rightarrow\u03b7^{(\\prime)}\\ell^{+}\\ell^{-}$ $(\\ell=e, \u03bc)$ with large recoil momentum are investigated in the framework of perturbative QCD, and the soft contributions from the small recoil momentum region are described by the overlap of soft wave functions. The transition form factors $f_{h_{c}\u03b7^{(\\prime)}}(q^{2})$ and the normalized transition form factors $F_{h_{c} \u03b7^{(\\prime)}}(q^{2})$ in full kinematic region are derived for the first time. It is noticed that there are no extra IR divergences at the one-loop level and the tree level, and the transition form factors in which the relativistic corrections from the internal momentum of $h_{c}$ are taken into account are insensitive to both the shapes of $\u03b7^{(\\prime)}$ distribution amplitudes and the invariant mass of the lepton pair in the large recoil momentum region. Furthermore, we find that the contributions from the soft mechanism and those from hard mechanism are comparable with each other in the branching ratios $\\mathcal{B}(h_{c}\\rightarrow\u03b7^{(\\prime)}\\ell^{+}\\ell^{-})$. By employing the obtained $F_{h_{c} \u03b7^{(\\prime)}}(q^{2})$, we give the predictions of the branching ratios $\\mathcal{B}(h_{c}\\rightarrow\u03b7^{(\\prime)}\\ell^{+}\\ell^{-})$, which may come within the range of measurement of present or near-future experiments.",
        "comments": "32 pages, 6 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14152"
    },
    {
        "doc_id": 177,
        "title": "Polarized and bright telecom C-band single-photon source from InP-based quantum dots coupled to elliptical Bragg gratings",
        "authors": [
            "Zhenxuan Ge",
            "Tunghsun Chung",
            "Yu-Ming He",
            "Mohamed Benyoucef",
            "Yongheng Huo"
        ],
        "subjects": [
            "Quantum Physics"
        ],
        "abstract": "Bright, polarized, and high-purity single-photon sources in telecom wavelengths are crucial components in long-distance quantum communication, optical quantum computation and quantum networks. Semiconductor InAs/InP quantum dots (QDs) combined with photonic cavities provide a competitive path leading to optimal single-photon sources in this range. Here, we demonstrate a bright and polarized single-photon source operating in the telecom C-band based on an elliptical Bragg grating (EBG) cavity. With a significant Purcell enhancement of 5.25$\\pm$0.05, the device achieves a polarization ratio of 0.986, single-photon purity of g^2 (0)=0.078$\\pm$0.016 and single-polarized photon collection efficiency of ~ 24% at the first lens (NA=0.65) without blinking. These findings suggest that C-band QD-based single-photon sources are potential candidates for advancing quantum communication.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14150"
    },
    {
        "doc_id": 178,
        "title": "Mathematical Tri-State Model for Bee Shimmering Propagation Dynamics",
        "authors": [
            "Navin Patel",
            "Henri Huijberts",
            "Kaspar Althoefer",
            "Ketao Zhang"
        ],
        "subjects": [
            "Adaptation and Self-Organizing Systems",
            "Dynamical Systems",
            "Biological Physics"
        ],
        "abstract": "Bees undergo a self-organised process known as shimmering, where they form emergent patterns when they interact with each other on the nest surface as a defence mechanism in response to predator attacks. Many experimental studies have empirically investigated how the transfer of information to neighbouring bees propagates in various shimmering processes by measuring shimmering wave strength. However, there is no analytical modelling of the collective defence mechanism in nature. Here we introduce the first analytical tri-state Inactive-Active-Relapse (IAR) model to formulate the intrinsic process of bee shimmering. The major shimmering behaviour is shown to emerge under theoretical conditions which is demonstrated numerically and visually by simulating 1,000,000 bee agents, while the number of agents is scalable. Furthermore, we elaborate on these mathematical results to construct a wave strength function to demonstrate the accuracy of shimmering dynamics. The constructed wave strength function can be adapted to peak between 50-150ms which supports the experimental studies. Our results provide a foundation for further theoretical understanding of bee shimmering wave dynamics and could serve as inspiration for modelling other self-organised phenomena across scientific applications.",
        "comments": "20 pages, 7 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14145"
    },
    {
        "doc_id": 179,
        "title": "Efficient photon-pair generation empowered by dual quasi-bound states in the continuum",
        "authors": [
            "Tingting Liu",
            "Meibao Qin",
            "Siqi Feng",
            "Xu Tu",
            "Tianjing Guo",
            "Feng Wu",
            "Shuyuan Xiao"
        ],
        "subjects": [
            "Optics"
        ],
        "abstract": "Here we demonstrate the efficient photon-pair generation via spontaneous parametric down conversion from a semiconductor metasurface supporting dual quasi-bound states in the continuum (quasi-BICs). In a simple metasurface design composed of AlGaAs ellipse nano-cyclinders, the two high-$Q$ quasi-BIC resonances that coincide with the generated signal and idler frequencies significantly boost the local electric field. This leads to a substantial enhancement in the reverse classical nonlinear process of sum frequency generation and subsequently the remarkable high generation rate of photon pairs under the quantum-classical correspondence principle. Within a narrowband wavelength regime around the quasi-BIC resonances, the rate of pair production is enhanced up to $\\sim10^{4}$ Hz, two orders of magnitude larger than that in the Mie resonant AlGaAs nanoantennas. Moreover, the photon pair emission is mainly concentrated in the normal direction with respect to the metasurface, and shows tunable rate with the $Q$ factor by engineering the rotation angle of nano-cylinders. The presented work enables nanoscale sources of high-quality entangled photons which will find applications in advanced quantum imaging and communications.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14140"
    },
    {
        "doc_id": 180,
        "title": "Micro and Nano 3D investigation of complex gut alterations-dementia interplay",
        "authors": [
            "F. Palermo",
            "N. Marrocco",
            "L. Dacom",
            "E. Grisafi",
            "M. Musella",
            "A. Sanna",
            "L. Massimi",
            "I. Bukreeva",
            "O. Junemann",
            "M. Eckermann",
            "P. Cloetens",
            "T. Weitkamp",
            "N. Kerlero de Rosbo",
            "C. Balducci",
            "A. Cedola"
        ],
        "subjects": [
            "Applied Physics"
        ],
        "abstract": "Alzheimer's disease (AD), a debilitating neurodegenerative disorder, remains one of the foremost public health challenges of our time. Despite decades of research, its etiology largely remains enigmatic. Recently, attention has turned to the gut-brain axis, a complex network of communication between the gastrointestinal tract and the brain, as a potential player in the pathogenesis of AD. Here we exploited X-ray Phase Contrast Tomography to provide an in-depth analysis of the link between the gut condition and AD, exploring gut anatomy and structure in murine models. We conducted a comprehensive analysis by comparing the outcomes in various mouse models of cognitive impairment, including AD, frail mice, and frontotemporal dementia (FTD) affected mice. We discovered an association between substantial changes in the gut structure and the presence of amyloid-beta (A\\b{eta}) in the brain. We found that the most important gut alterations are related to A\\b{eta} occurrence in the brain. In particular, we investigated the gut morphology, the distribution of enteric micro-processes and neurons in the ileum. Understanding the intricate interplay between gut condition and dementia may open new avenues for early AD diagnosis and treatment offering hope for a future where these diseases may be more effectively addressed.",
        "comments": "9 pages and 5 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14139"
    },
    {
        "doc_id": 181,
        "title": "Homoclinic chaos in a pair of parametrically-driven coupled SQUIDs",
        "authors": [
            "M. Agaoglou",
            "V. M. Rothos",
            "H. Susanto"
        ],
        "subjects": [
            "Chaotic Dynamics",
            "Mathematical Physics"
        ],
        "abstract": "An rf superconducting quantum interference device (SQUID) consists of a superconducting ring interrupted by a Josephson junction (JJ). When driven by an alternating magnetic field, the induced supercurrents around the ring are determined by the JJ through the celebrated Josephson relations. This system exhibits rich nonlinear behavior, including chaotic effects. We study the dynamics of a pair of parametrically-driven coupled SQUIDs arranged in series. We take advantage of the weak damping that characterizes these systems to perform a multiple-scales analysis and obtain amplitude equations, describing the slow dynamics of the system. This picture allows us to expose the existence of homoclinic orbits in the dynamics of the integrable part of the slow equations of motion. Using high-dimensional Melnikov theory, we are able to obtain explicit parameter values for which these orbits persist in the full system, consisting of both Hamiltonian and non-Hamiltonian perturbations, to form so-called Silnikov orbits, indicating a loss of integrability and the existence of chaos.",
        "comments": "4 pages. arXiv admin note: text overlap with arXiv:1007.3939 by other authors",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14128"
    },
    {
        "doc_id": 182,
        "title": "Spatially Resolved High Voltage Kelvin Probe Force Microcopy: A Novel Avenue for Examining Electrical Phenomena at Nanoscale",
        "authors": [
            "Conor J. McCluskey",
            "Niyorjyoti Sharma",
            "Jesi R. Maguire",
            "Serene Pauly",
            "Andrew Rogers",
            "TJ Lindsay",
            "Kristina M. Holsgrove",
            "Brian J. Rodriguez",
            "Navneet Soin",
            "John Marty Gregg",
            "Raymond G. P. McQuaid",
            "Amit Kumar"
        ],
        "subjects": [
            "Materials Science",
            "Applied Physics"
        ],
        "abstract": "Kelvin probe microscopy (KPFM) is a well-established scanning probe technique, used to measure surface potential accurately; it has found extensive use in the study of a range of materials phenomena. In its conventional form, KPFM frustratingly precludes imaging samples or scenarios where large surface potential exists or large surface potential gradients are created outside the typical +/-10V window. If the potential regime measurable via KPFM could be expanded, to enable precise and reliable metrology, through a high voltage KPFM (HV-KPFM) adaptation, it could open up pathways towards a range of novel experiments, where the detection limit of regular KPFM has so far prevented the use of the technique. In this work, HV-KPFM has been realised and shown to be capable of measuring large surface potential and potential gradients with accuracy and precision. The technique has been employed to study a range of materials (positive temperature coefficient of resistivity ceramics, charge storage fluoropolymers and pyroelectrics) where accurate spatially resolved mapping of surface potential within high voltage regime facilitates novel physical insight. The results demonstrate that HV-KPFM can be used as an effective tool to fill in existing gaps in surface potential measurements while also opening routes for novel studies in materials physics.",
        "comments": "Main Text: 16 pages, 5 figures Supplementary information:4 pages, 2 tables and 2 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14124"
    },
    {
        "doc_id": 183,
        "title": "Heavy baryon decays into light meson and dark baryon within LCSR",
        "authors": [
            "Yu-Ji Shi",
            "Ye Xing",
            "Zhi-Peng Xing"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "High Energy Physics - Experiment"
        ],
        "abstract": "We studied the decays of Heavy baryon into a pseudoscalar meson and a dark baryon in the recently developed $B$-Mesogenesis scenario, where the two types of effective Lagrangians proposed by the scenario are both considered. The decay amplitudes of $\u039b_b^0$ are calculated by light-cone sum rules using its light-cone distribution amplitudes. The decay amplitudes of $\u039e_b^{0,\\pm}$ are related with those of $\u039b_b^0$ through a flavor SU(3) analysis. The uncertainties of threshold parameter and the Borel parameter are both considered in the numerical calculation. The values of effective coupling constants in the $B$-Mesogenesis are taken as their upper limits that obtained from our previous study on the inclusive decay. The upper limits of the decay branching fractions are presented as functions of the dark baryon mass.",
        "comments": "21 pages, 9 figures, 3 tables",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14120"
    },
    {
        "doc_id": 184,
        "title": "Pseudoscalar mesons from a PNJL model at zero temperature",
        "authors": [
            "R. M. Aguirre",
            "O. Louren\u00e7o"
        ],
        "subjects": [
            "High Energy Physics - Phenomenology",
            "Nuclear Theory"
        ],
        "abstract": "We study pseudoscalar $\u03c0$, $K$ and $\u03b7$ meson properties, such as masses and couplings, in dense matter at zero temperature. We use a recently proposed phenomenological quark model, known as the PNJL0, which takes into account the confinement/deconfinement phase transition by means of the traced Polyakov loop ($\u03a6$) which serves as an order parameter at zero temperature. We consider two different scenarios, namely, symmetric quark matter with equal chemical potentials for all the flavors, and the beta equilibrated matter. In the latter case the hadron-quark phase transition is implemented by a two model approach. For the hadron side we use a relativistic mean-field model with density dependent couplings. We show that $\u03a6$ induces abrupt changes in the mesons properties with gap sizes regulated by the phenomenological gluonic sector of the model.",
        "comments": "14 pages, 12 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14119"
    },
    {
        "doc_id": 185,
        "title": "Uniqueness of photon sphere for Reissner-Nordstr\u00f6m electric-magnetic system",
        "authors": [
            "Marek Rogatko"
        ],
        "subjects": [
            "General Relativity and Quantum Cosmology",
            "High Energy Physics - Theory"
        ],
        "abstract": "Uniqueness of static, asymptotically flat, non-extremal {\\it photon sphere} in Einstein-Maxwell spacetime with electric and magnetic charges has been proved. Using conformal positive energy theorem, as well as, the positive mass theorem and adequate conformal transformations, we envisage the two alternative ways of proving that the exterior region of a certain radius of the studied static {\\it photon sphere}, is characterized by ADM mass, electric and magnetic charges.",
        "comments": "22 pages, RevTex, to be published in Phys.Rev.D15",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14116"
    },
    {
        "doc_id": 186,
        "title": "Development of a Silicon Drift Detector Array to Search for keV-scale Sterile Neutrinos with the KATRIN Experiment",
        "authors": [
            "Daniel Siegmann",
            "Frank Edzards",
            "Christina Bruch",
            "Matteo Biassoni",
            "Marco Carminati",
            "Martin Descher",
            "Carlo Fiorini",
            "Christian Forstner",
            "Andrew Gavin",
            "Matteo Gugiatti",
            "Roman Hiller",
            "Dominic Hinz",
            "Thibaut Houdy",
            "Anton Huber",
            "Pietro King",
            "Peter Lechner",
            "Steffen Lichter",
            "Danilo Mie\u00dfner",
            "Andrea Nava",
            "Anthony Onillon",
            "David C. Radford",
            "Daniela Spreng",
            "Markus Steidl",
            "Paolo Trigilio",
            "Korbinian Urban",
            "et al. (3 additional authors not shown)"
        ],
        "subjects": [
            "Instrumentation and Detectors"
        ],
        "abstract": "Sterile neutrinos in the keV mass range present a viable candidate for dark matter. They can be detected through single $\u03b2$ decay, where they cause small spectral distortions. The Karlsruhe Tritium Neutrino (KATRIN) experiment aims to search for keV-scale sterile neutrinos with high sensitivity. To achieve this, the KATRIN beamline will be equipped with a novel multi-pixel silicon drift detector focal plane array named TRISTAN. In this study, we present the performance of a TRISTAN detector module, a component of the eventual 9-module system. Our investigation encompasses spectroscopic aspects such as noise performance, energy resolution, linearity, and stability.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14114"
    },
    {
        "doc_id": 187,
        "title": "CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks",
        "authors": [
            "Andrei Tomut",
            "Saeed S. Jahromi",
            "Sukhbinder Singh",
            "Faysal Ishtiaq",
            "Cesar Mu\u00f1oz",
            "Prabdeep Singh Bajaj",
            "Ali Elborady",
            "Gianni del Bimbo",
            "Mehrazin Alizadeh",
            "David Montero",
            "Pablo Martin-Ramiro",
            "Muhammad Ibrahim",
            "Oussama Tahiri Alaoui",
            "John Malcolm",
            "Samuel Mugel",
            "Roman Orus"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning",
            "Quantum Physics"
        ],
        "abstract": "Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined and interpretable model compression. Our method is versatile and can be implemented with - or on top of - other compression techniques. As a benchmark, we demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model to only $30\\%$ of its original size while recovering over $90\\%$ of the original accuracy after a brief distributed retraining.",
        "comments": "4 pages, 3 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14109"
    },
    {
        "doc_id": 188,
        "title": "Travelling waves in nonlinear magneto-inductive lattices",
        "authors": [
            "M. Agaoglou",
            "M. Feckan",
            "M. Pospisil",
            "V. M. Rothos",
            "H. Susanto"
        ],
        "subjects": [
            "Mathematical Physics"
        ],
        "abstract": "We consider a lattice equation modelling one-dimensional metamaterials formed by a discrete array of nonlinear resonators. We focus on periodic travelling waves due to the presence of a periodic force. The existence and uniqueness results of periodic travelling waves of the system are presented. Our analytical results are found to be in good agreement with direct numerical computations",
        "comments": "21 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14108"
    },
    {
        "doc_id": 189,
        "title": "Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement",
        "authors": [
            "Aaqib Saeed",
            "Dimitris Spathis",
            "Jungwoo Oh",
            "Edward Choi",
            "Ali Etemad"
        ],
        "subjects": [
            "Machine Learning",
            "Signal Processing"
        ],
        "abstract": "Wearable technologies enable continuous monitoring of various health metrics, such as physical activity, heart rate, sleep, and stress levels. A key challenge with wearable data is obtaining quality labels. Unlike modalities like video where the videos themselves can be effectively used to label objects or events, wearable data do not contain obvious cues about the physical manifestation of the users and usually require rich metadata. As a result, label noise can become an increasingly thorny issue when labeling such data. In this paper, we propose a novel solution to address noisy label learning, entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially learns a seed model using weak labels. Next, it fine-tunes the seed model using a handful of expert corrections. Finally, it achieves better generalizability and robustness by merging the seed and fine-tuned models via weighted parameter averaging. We evaluate our approach on four challenging tasks and datasets, and compare it against eight competitive baselines designed to deal with noisy labels. We show that FHLR achieves significantly better performance when learning from noisy labels and achieves state-of-the-art by a large margin, with up to 19% accuracy improvement under symmetric and asymmetric noise. Notably, we find that FHLR is particularly robust to increased label noise, unlike prior works that suffer from severe performance degradation. Our work not only achieves better generalization in high-stakes health sensing benchmarks but also sheds light on how noise affects commonly-used models.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14107"
    },
    {
        "doc_id": 190,
        "title": "Inverse source problem for discrete Helmholtz equation",
        "authors": [
            "Roman Novikov",
            "Basant Lal Sharma"
        ],
        "subjects": [
            "Analysis of PDEs",
            "Mathematical Physics"
        ],
        "abstract": "We consider multi-frequency inverse source problem for the discrete Helmholtz operator on the square lattice $\\mathbb{Z}^d$, $d \\ge 1$. We consider this problem for the cases with and without phase information. We prove uniqueness results and present examples of non-uniqueness for this problem for the case of compactly supported source function. Relations with inverse scattering problem for the discrete Schr\u00f6dinger operators in the Born approximation are also provided.",
        "comments": "12 pages",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14103"
    },
    {
        "doc_id": 191,
        "title": "Few-magnon excitations in a frustrated spin-$S$ ferromagnetic chain",
        "authors": [
            "Jiawei Li",
            "Ye Cao",
            "Ning Wu"
        ],
        "subjects": [
            "Strongly Correlated Electrons",
            "Quantum Physics"
        ],
        "abstract": "We study few-magnon excitations in a finite-size spin-$S$ ferromagnetic nearest-neighbor (NN) XXZ chain with additional antiferromagnetic next-nearest-neighbor (NNN) interaction $J'$ and single-ion (SI) anisotropy $D$. Using a set of exact two-magnon Bloch states, the two-magnon problem is mapped to a single-particle one on an effective open chain with both NN and NNN hoppings. For the commensurate momentum $k=-\u03c0$, the effective chain is decoupled into two NN open chains that can be exactly solved via a plane-wave ansatz. Based on this, we identify in the $\u0394'-D/|J'|$ plane (with $\u0394'$ the anisotropy parameter for the NNN coupling) the regions supporting the SI or NNN exchange two-magnon bound states near the edge of the band. We prove that there always exists a lower-energy NN exchange two-magnon bound state near the band edge. For $S=1/2$, we numerically calculate the $n$-magnon spectra for $n\\leq5$ by using a spin-operator matrix element method. The corresponding $n$-magnon commensurate instability regions are determined for finite chains and consistent results with prior literature are observed.",
        "comments": "10 pages, 9 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14101"
    },
    {
        "doc_id": 192,
        "title": "Carry Your Fault: A Fault Propagation Attack on Side-Channel Protected LWE-based KEM",
        "authors": [
            "Suparna Kundu",
            "Siddhartha Chowdhury",
            "Sayandeep Saha",
            "Angshuman Karmakar",
            "Debdeep Mukhopadhyay",
            "Ingrid Verbauwhede"
        ],
        "subjects": [
            "Cryptography and Security"
        ],
        "abstract": "Post-quantum cryptographic (PQC) algorithms, especially those based on the learning with errors (LWE) problem, have been subjected to several physical attacks in the recent past. Although the attacks broadly belong to two classes - passive side-channel attacks and active fault attacks, the attack strategies vary significantly due to the inherent complexities of such algorithms. Exploring further attack surfaces is, therefore, an important step for eventually securing the deployment of these algorithms. Also, it is important to test the robustness of the already proposed countermeasures in this regard. In this work, we propose a new fault attack on side-channel secure masked implementation of LWE-based key-encapsulation mechanisms (KEMs) exploiting fault propagation. The attack typically originates due to an algorithmic modification widely used to enable masking, namely the Arithmetic-to-Boolean (A2B) conversion. We exploit the data dependency of the adder carry chain in A2B and extract sensitive information, albeit masking (of arbitrary order) being present. As a practical demonstration of the exploitability of this information leakage, we show key recovery attacks of Kyber, although the leakage also exists for other schemes like Saber. The attack on Kyber targets the decapsulation module and utilizes Belief Propagation (BP) for key recovery. To the best of our knowledge, it is the first attack exploiting an algorithmic component introduced to ease masking rather than only exploiting the randomness introduced by masking to obtain desired faults (as done by Delvaux). Finally, we performed both simulated and electromagnetic (EM) fault-based practical validation of the attack for an open-source first-order secure Kyber implementation running on an STM32 platform.",
        "comments": "ACM Class:          E.3.3",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14098"
    },
    {
        "doc_id": 193,
        "title": "Evaluating User Experience and Data Quality in a Gamified Data Collection for Appearance-Based Gaze Estimation",
        "authors": [
            "Mingtao Yue",
            "Tomomi Sayuda",
            "Miles Pennington",
            "Yusuke Sugano"
        ],
        "subjects": [
            "Human-Computer Interaction"
        ],
        "abstract": "Appearance-based gaze estimation, which uses only a regular camera to estimate human gaze, is important in various application fields. While the technique faces data bias issues, data collection protocol is often demanding, and collecting data from a wide range of participants is difficult. It is an important challenge to design opportunities that allow a diverse range of people to participate while ensuring the quality of the training data. To tackle this challenge, we introduce a novel gamified approach for collecting training data. In this game, two players communicate words via eye gaze through a transparent letter board. Images captured during gameplay serve as valuable training data for gaze estimation models. The game is designed as a physical installation that involves communication between players, and it is expected to attract the interest of diverse participants. We assess the game's significance on data quality and user experience through a comparative user study.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14095"
    },
    {
        "doc_id": 194,
        "title": "GQHAN: A Grover-inspired Quantum Hard Attention Network",
        "authors": [
            "Ren-Xin Zhao",
            "Jinjing Shi",
            "Xuelong Li"
        ],
        "subjects": [
            "Quantum Physics",
            "Artificial Intelligence"
        ],
        "abstract": "Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy in discerning the significance of quantum data, resulting in diminished efficacy when handling extensive quantum datasets. Hard Attention Mechanism (HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters the substantial challenge of non-differentiability, consequently constraining its extensive applicability. In response to the dilemma of HAM and QML, a Grover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a Flexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed. Notably, the FO is designed to surmount the non-differentiable issue by executing the activation or masking of Discrete Primitives (DPs) with Flexible Control (FC) to weave various discrete destinies. Based on this, such discrete choice can be visualized with a specially defined Quantum Hard Attention Score (QHAS). Furthermore, a trainable ADO is devised to boost the generality and flexibility of GQHAM. At last, a Grover-inspired Quantum Hard Attention Network (GQHAN) based on QGHAM is constructed on PennyLane platform for Fashion MNIST binary classification. Experimental findings demonstrate that GQHAN adeptly surmounts the non-differentiability hurdle, surpassing the efficacy of extant quantum soft self-attention mechanisms in accuracies and learning ability. In noise experiments, GQHAN is robuster to bit-flip noise in accuracy and amplitude damping noise in learning performance. Predictably, the proposal of GQHAN enriches the Quantum Attention Mechanism (QAM), lays the foundation for future quantum computers to process large-scale data, and promotes the development of quantum computer vision.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14089"
    },
    {
        "doc_id": 195,
        "title": "Spatially localized scalar structures on hyperscaling violating geometries",
        "authors": [
            "I. Andrade",
            "M. A. Marques",
            "R. Menezes",
            "D. C. Moreira"
        ],
        "subjects": [
            "High Energy Physics - Theory",
            "General Relativity and Quantum Cosmology",
            "Pattern Formation and Solitons"
        ],
        "abstract": "In this work, we investigate probe scalar field models preserving covariance on fixed, static background geometries that present hyperscaling violation properties. We develop a first-order framework that rises from restrictions on the dynamical and hyperscaling violating exponents. The results show that stable, analytical kink-like solutions and their respective energy densities can be obtained for a general class of models. In the canonical model, in particular, these solutions minimize the energy of the system.",
        "comments": "12 pages, 2 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14082"
    },
    {
        "doc_id": 196,
        "title": "Accelerating Fractional PINNs using Operational Matrices of Derivative",
        "authors": [
            "Tayebeh Taheri",
            "Alireza Afzal Aghaei",
            "Kourosh Parand"
        ],
        "subjects": [
            "Machine Learning",
            "Numerical Analysis"
        ],
        "abstract": "This paper presents a novel operational matrix method to accelerate the training of fractional Physics-Informed Neural Networks (fPINNs). Our approach involves a non-uniform discretization of the fractional Caputo operator, facilitating swift computation of fractional derivatives within Caputo-type fractional differential problems with $0<\u03b1<1$. In this methodology, the operational matrix is precomputed, and during the training phase, automatic differentiation is replaced with a matrix-vector product. While our methodology is compatible with any network, we particularly highlight its successful implementation in PINNs, emphasizing the enhanced accuracy achieved when utilizing the Legendre Neural Block (LNB) architecture. LNB incorporates Legendre polynomials into the PINN structure, providing a significant boost in accuracy. The effectiveness of our proposed method is validated across diverse differential equations, including Delay Differential Equations (DDEs) and Systems of Differential Algebraic Equations (DAEs). To demonstrate its versatility, we extend the application of the method to systems of differential equations, specifically addressing nonlinear Pantograph fractional-order DDEs/DAEs. The results are supported by a comprehensive analysis of numerical outcomes.",
        "comments": "19 pages, 11 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14081"
    },
    {
        "doc_id": 197,
        "title": "Light-induced photodissociation on the lowest three electronic states of NaH molecule",
        "authors": [
            "Otabek Umarov",
            "Andr\u00e1s Csehi",
            "P\u00e9ter Badank\u00f3",
            "G\u00e1bor J. Hal\u00e1sz",
            "\u00c1gnes Vib\u00f3k"
        ],
        "subjects": [
            "Chemical Physics"
        ],
        "abstract": "It has been known that electronic conical intersections in a molecular system can also be created by laser light even in diatomics. The direct consequence of these light-induced degeneracies is the appearance of a strong mixing between the electronic and vibrational motions, which has a strong fingerprint on the ultrafast nuclear dynamics. In the present work, pump and probe numerical simulations have been performed with the NaH molecule involving the first three singlet electronic states (X1\u03a3+(X), A1\u03a3+(A) and B1\u03a0(B)) and several light-induced degeneracies in the numerical description. To demonstrate the impact of the multiple light-induced non-adiabatic effects together with the molecular rotation on the dynamical properties of the molecule, the dissociation probabilities, kinetic energy release spectra (KER) and the angular distributions of the photofragments were calculated by discussing the role of the permanent dipole moment as well.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14080"
    },
    {
        "doc_id": 198,
        "title": "Generation of High-Brilliance Polarized $\u03b3$-Rays via Vacuum Birefringence",
        "authors": [
            "Chong Lv",
            "Feng Wan",
            "Yousef I. Salamin",
            "Qian Zhao",
            "Mamutjan Ababekri",
            "Ruirui Xu",
            "Jian-Xing Li"
        ],
        "subjects": [
            "Plasma Physics"
        ],
        "abstract": "High-brilliance circularly polarized $\u03b3$-photon beams are of great significance for a wide range of applications. However, their generation through nonlinear Compton scattering must require a high-density longitudinally-spin-polarized electron beam and consequently is still a great challenge. Here, we put forward a novel method to generate such $\u03b3$-photon beams via the vacuum dichroism (VD)-assisted vacuum birefringence (VB) effect, only utilizing a well-established unpolarized electron beam. We split a linearly polarized (LP) laser pulse into two subpulses with the first one colliding with a dense unpolarized electron beam to generate an LP $\u03b3$-photon beam (via nonlinear Compton scattering), which then further collides with the second subpulse and is transformed into a circularly polarized one via the VB effect. We find that by manipulating the relative polarization of two subpulses, one can ``purify'' the polarization of the $\u03b3$-photon beam via the VD effect, thereby significantly enhancing the circular polarization of the $\u03b3$-photon beam. Due to the VD assistance, the VB effect reaches optimal when the relative polarization is nearly $30^\\circ$, not the widely used $45^\\circ$ in the common VB detection methods. Our results show that one can obtain a circularly polarized $\u03b3$-photon beam with degree of about 30% (43%) for energies above 500 (1000) MeV and brilliance of about $10^{24}~(10^{23})~\\mathrm{photons / (s \\cdot mm^2 \\cdot mrad^{2} \\cdot 0.1\\%BW)}$ at $500~(1000)$ MeV by using a currently feasible laser with a peak intensity of about $10^{22}~\\mathrm{W/cm^2}$. And, it can be further improved to above 60% (75%) by increasing the laser pulse duration. Moreover, our method can also be used to efficiently confirm the well-known VB effect itself, which has been predicted a very long time ago but has not been directly observed in experiments yet.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14075"
    },
    {
        "doc_id": 199,
        "title": "Optical phase encoding in pulsed approach to reservoir computing",
        "authors": [
            "Johan Henaff",
            "Matthieu Ansquer",
            "Miguel C Soriano",
            "Roberta Zambrini",
            "Nicolas Treps",
            "Valentina Parigi"
        ],
        "subjects": [
            "Quantum Physics",
            "Optics"
        ],
        "abstract": "The exploitation of the full structure of multimode light fields enables compelling capabilities in many fields including classical and quantum information science. We exploit data-encoding on the optical phase of the pulses of a femtosecond laser source for a photonic implementation of a reservoir computing protocol. Rather than intensity detection, data-reading is done via homodyne detection that accesses combinations of amplitude and phase of the field. Numerical and experimental results on NARMA tasks and laser dynamic predictions are shown. We discuss perspectives for quantum enhanced protocols.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14073"
    },
    {
        "doc_id": 200,
        "title": "Gamma rays from dark matter spikes in EAGLE simulations",
        "authors": [
            "J. Aschersleben",
            "G. Bertone",
            "D. Horns",
            "E. Moulin",
            "R. F. Peletier",
            "M. Vecchi"
        ],
        "subjects": [
            "High Energy Astrophysical Phenomena",
            "Cosmology and Nongalactic Astrophysics"
        ],
        "abstract": "Intermediate Mass Black Holes (IMBHs) with a mass range between $100 \\, \\text{M}_\\odot$ and $10^6 \\, \\text{M}_\\odot$ are expected to be surrounded by high dark matter densities, so-called dark matter spikes. The high density of self-annihilating WIMPs in these spikes leads to copious gamma-ray production. Sufficiently nearby IMBHs could therefore appear as unidentified gamma-ray sources. However, the number of IMBHs and their distribution within our own Milky Way is currently unknown. In this work, we provide a mock catalogue of IMBHs and their dark matter spikes obtained from the EAGLE simulations, in which black holes with a mass of $10^5 \\, \\text{M}_\\odot/h$ are seeded into the centre of halos greater than $10^{10} \\, \\text{M}_\\odot/h$ to model black hole feedback influencing the formation of galaxies. The catalogue contains the coordinates and dark matter spike parameters for over 8700 IMBHs present in about 400 Milky Way-like galaxies. We expect about $19^{+13}_{-8}$ IMBHs within our own galaxy, mainly distributed in the Galactic Centre and the Galactic Plane. In the most optimistic scenario, we find that current and future gamma-ray observatories, such as Fermi-LAT, H.E.S.S. and CTA, would be sensitive enough to probe the cross section of dark matter self-annihilation around IMBHs down to many orders of magnitude below the thermal relic cross section for dark matter particles with masses from GeV to TeV. We have made the IMBH mock catalogue and the source code for our analysis publicly available, providing the resources to study dark matter self-annihilation around IMBHs with current and upcoming gamma-ray observatories.",
        "comments": "25 pages, 10 figures, submitted to Journal of Cosmology and Astroparticle Physics",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14072"
    },
    {
        "doc_id": 201,
        "title": "Health Digital Twins Supported by Artificial Intelligence-based Algorithms and Extended Reality in Cardiology",
        "authors": [
            "Zofia Rudnicka",
            "Klaudia Proniewska",
            "Mark Perkins",
            "Agnieszka Pregowska"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Recently, significant efforts have been made to create Health Digital Twins (HDTs), digital twins for clinical applications. Heart modeling is one of the fastest-growing fields, which favors the effective application of HDTs. The clinical application of HDTs will be increasingly widespread in the future of healthcare services and has a huge potential to form part of the mainstream in medicine. However, it requires the development of both models and algorithms for the analysis of medical data, and advances in Artificial Intelligence (AI) based algorithms have already revolutionized image segmentation processes. Precise segmentation of lesions may contribute to an efficient diagnostics process and a more effective selection of targeted therapy. In this paper, a brief overview of recent achievements in HDT technologies in the field of cardiology, including interventional cardiology was conducted. HDTs were studied taking into account the application of Extended Reality (XR) and AI, as well as data security, technical risks, and ethics-related issues. Special emphasis was put on automatic segmentation issues. It appears that improvements in data processing will focus on automatic segmentation of medical imaging in addition to three-dimensional (3D) pictures to reconstruct the anatomy of the heart and torso that can be displayed in XR-based devices. This will contribute to the development of effective heart diagnostics. The combination of AI, XR, and an HDT-based solution will help to avoid technical errors and serve as a universal methodology in the development of personalized cardiology. Additionally, we describe potential applications, limitations, and further research directions.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14208"
    },
    {
        "doc_id": 202,
        "title": "Validation of Golden Gate assemblies using highly multiplexed Nanopore amplicon sequencing",
        "authors": [
            "Adan A. Ramirez Rojas",
            "Cedric K. Brinkmann",
            "Daniel Schindler"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "Golden Gate cloning has revolutionized synthetic biology. Its concept of modular, highly characterized libraries of parts that can be combined into higher order assemblies allows engineering principles to be applied to biological systems. The basic parts, typically stored in level 0 plasmids, are sequence validated by the method of choice and can be combined into higher order assemblies on demand. Higher order assemblies are typically transcriptional units, and multiple transcriptional units can be assembled into multi-gene constructs. Higher order Golden Gate assembly based on defined and validated parts usually does not introduce sequence changes. Therefore, simple validation of the assemblies, e.g. by colony PCR or restriction digest pattern analysis, is sufficient. However, in many experimental setups, researchers do not use defined parts, but rather part libraries, resulting in assemblies of high combinatorial complexity where sequencing again becomes mandatory. Here we present a detailed protocol for the use of a highly multiplexed dual barcode amplicon sequencing using the Nanopore sequencing platform for in-house sequence validation. The workflow, called DuBA.flow, is a start-to-finish procedure that provides all necessary steps from a single colony to the final easy-to-interpret sequencing report.",
        "comments": "25 pages, 3 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14191"
    },
    {
        "doc_id": 203,
        "title": "Dual-trigger release of berberine chloride from the Gelatin/Perfluorohexane core-shell structure",
        "authors": [
            "Mahshid Givarian",
            "Fathollah Moztarzadeh",
            "Maryam Ghaffari",
            "AmirHossein Bahmanpour",
            "Maryam Mollazadeh-Bajestani",
            "Manijhe Mokhtari-Dizaji",
            "Fatemeh Mehradnia"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "The development of smart nanocarriers that enable controlled drug release in response to internal and external triggers is an emerging approach for targeted therapy. This study focused on designing pH-sensitive, ultrasound-responsive gelatin/perfluorohexane (PFH) nanodroplets loaded with berberine chloride as a model drug. The nanodroplets were prepared using an emulsion technique and optimized by varying process parameters like homogenization rate, polymer concentration, surfactant, drug, and perfluorocarbon content. The optimal formulation yielded nanodroplets with a particle size of 281.7 nm, a drug encapsulation efficiency of 66.8, and a passive drug release of 15.4 within 24 hours. Characterization confirmed successful encapsulation and pH-responsive behavior. Ultrasound stimulation significantly enhanced drug release, with 150 kHz being more effective than 1 MHz in triggering acoustic droplet vaporization while minimizing heat generation. After 10 minutes of radiation, the optimal formulation showed 89.4% cumulative drug release. The nanodroplets displayed stability over one month at 4\u00b0C. Overall, the dual-triggered nanodroplets demonstrate excellent potential for controlled delivery and targeted release of berberine chloride.",
        "comments": "39 pages and 5 figures, to appear in Applied Biochemistry and Biotechnology journal",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14172"
    },
    {
        "doc_id": 204,
        "title": "Label-free detection of exosomes from different cellular sources based on surface-enhanced Raman spectroscopy combined with machine learning models",
        "authors": [
            "Yang Lia",
            "Xiaoming Lyu",
            "Kuo Zhan",
            "Haoyu Ji",
            "Lei Qin",
            "JianAn Huang"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "Exosomes are significant facilitators of inter-cellular communication that can unveil cell-cell interactions, signaling pathways, regulatory mechanisms and disease diagnostics. Nonetheless, current analysis required large amount of data for exosome identification that it hampers efficient and timely mechanism study and diagnostics. Here, we used a machine-learning assisted Surface-enhanced Raman spectroscopy (SERS) method to detect exosomes derived from six distinct cell lines (HepG2, Hela, 143B, LO-2, BMSC, and H8) with small amount of data. By employing sodium borohydride-reduced silver nanoparticles and sodium borohydride solution as an aggregating agent, 100 SERS spectra of the each types of exosomes were collected and then subjected to multivariate and machine learning analysis. By integrating Principal Component Analysis with Support Vector Machine (PCA-SVM) models, our analysis achieved a high accuracy rate of 94.4% in predicting exosomes originating from various cellular sources. In comparison to other machine learning analysis, our method used small amount of SERS data to allow a simple and rapid exosome detection, which enables a timely subsequent study of cell-cell interactions, communication mechanisms, and disease mechanisms in life sciences.",
        "comments": "5 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14104"
    },
    {
        "doc_id": 205,
        "title": "Left/Right Brain, human motor control and the implications for robotics",
        "authors": [
            "Jarrad Rinaldo",
            "Levin Kuhlmann",
            "Jason Friedman",
            "Gideon Kowadlo"
        ],
        "subjects": [
            "Robotics",
            "Artificial Intelligence",
            "Machine Learning",
            "Neural and Evolutionary Computing",
            "Neurons and Cognition"
        ],
        "abstract": "Neural Network movement controllers promise a variety of advantages over conventional control methods however they are not widely adopted due to their inability to produce reliably precise movements. This research explores a bilateral neural network architecture as a control system for motor tasks. We aimed to achieve hemispheric specialisation similar to what is observed in humans across different tasks; the dominant system (usually the right hand, left hemisphere) excels at tasks involving coordination and efficiency of movement, and the non-dominant system performs better at tasks requiring positional stability. Specialisation was achieved by training the hemispheres with different loss functions tailored toward the expected behaviour of the respective hemispheres. We compared bilateral models with and without specialised hemispheres, with and without inter-hemispheric connectivity (representing the biological Corpus Callosum), and unilateral models with and without specialisation. The models were trained and tested on two tasks common in the human motor control literature: the random reach task, suited to the dominant system, a model with better coordination, and the hold position task, suited to the non-dominant system, a model with more stable movement. Each system out-performed the non-favoured system in its preferred task. For both tasks, a bilateral model outperforms the 'non-preferred' hand, and is as good or better than the 'preferred' hand. The Corpus Callosum tends to improve performance, but not always for the specialised models.",
        "comments": "ACM Class:          I.2.6; I.2.9",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14057"
    },
    {
        "doc_id": 206,
        "title": "Radical Realism",
        "authors": [
            "Nicol\u00e1s Hinrichs",
            "Noah Guzm\u00e1n"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "The ontogeny of cognitive neuroscience has emerged within the hegemony of substance ontology. Persistent physicalist influences are described through three developmental hallmarks that yielded epistemic attractors - promoters and perpetuators of material-discursive practices oriented toward reification and self-vindication across the interdisciplinary spectrum which, as a whole, has been driven away from its pretensions to scientific realism. In virtue of a desire for a radical return thereto, we adopt a metaphysic stance akin to pragmatism, and briefly make the case that such concerns have sociopolitical implications extending far beyond the realm of mere philosophical interest.",
        "comments": "19 pages, 3 figures",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14049"
    },
    {
        "doc_id": 207,
        "title": "DNA Sequence Classification with Compressors",
        "authors": [
            "\u015e\u00fckr\u00fc Ozan"
        ],
        "subjects": [
            "Genomics",
            "Machine Learning"
        ],
        "abstract": "Recent studies in DNA sequence classification have leveraged sophisticated machine learning techniques, achieving notable accuracy in categorizing complex genomic data. Among these, methods such as k-mer counting have proven effective in distinguishing sequences from varied species like chimpanzees, dogs, and humans, becoming a staple in contemporary genomic research. However, these approaches often demand extensive computational resources, posing a challenge in terms of scalability and efficiency. Addressing this issue, our study introduces a novel adaptation of Jiang et al.'s compressor-based, parameter-free classification method, specifically tailored for DNA sequence analysis. This innovative approach utilizes a variety of compression algorithms, such as Gzip, Brotli, and LZMA, to efficiently process and classify genomic sequences. Not only does this method align with the current state-of-the-art in terms of accuracy, but it also offers a more resource-efficient alternative to traditional machine learning methods. Our comprehensive evaluation demonstrates the proposed method's effectiveness in accurately classifying DNA sequences from multiple species. We present a detailed analysis of the performance of each algorithm used, highlighting the strengths and limitations of our approach in various genomic contexts. Furthermore, we discuss the broader implications of our findings for bioinformatics, particularly in genomic data processing and analysis. The results of our study pave the way for more efficient and scalable DNA sequence classification methods, offering significant potential for advancements in genomic research and applications.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14025"
    },
    {
        "doc_id": 208,
        "title": "Temperature Compensation through Kinetic Regulation in Biochemical Oscillators",
        "authors": [
            "Haochen Fu",
            "Chenyi Fei",
            "Qi Ouyang",
            "Yuhai Tu"
        ],
        "subjects": [
            "Molecular Networks",
            "Biological Physics"
        ],
        "abstract": "Nearly all circadian clocks maintain a period that is insensitive to temperature changes, a phenomenon known as temperature compensation (TC). Yet, it is unclear whether there is any common feature among different systems that exhibit TC. From a general timescale invariance, we show that TC relies on existence of certain period-lengthening reactions wherein the period of the system increases strongly with the rates in these reactions. By studying several generic oscillator models, we show that this counter-intuitive dependence is nonetheless a common feature of oscillators in the nonlinear (far-from-onset) regime where the oscillation can be separated into fast and slow phases. The increase of the period with the period-lengthening reaction rates occurs when the amplitude of the slow phase in the oscillation increases with these rates while the progression-speed in the slow phase is controlled by other rates of the system. The positive dependence of the period on the period-lengthening rates balances its inverse dependence on other kinetic rates in the system, which gives rise to robust TC in a wide range of parameters. We demonstrate the existence of such period-lengthening reactions and their relevance for TC in all four model systems we considered. Theoretical results for a model of the Kai system are supported by experimental data. A study of the energy dissipation also shows that better TC performance requires higher energy consumption. Our study unveils a general mechanism by which a biochemical oscillator achieves TC by operating at regimes far from the onset where period-lengthening reactions exist.",
        "comments": "19 pages, 11 figures (main text + supplementary information)",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13960"
    },
    {
        "doc_id": 209,
        "title": "Towards 3D Molecule-Text Interpretation in Language Models",
        "authors": [
            "Sihang Li",
            "Zhiyuan Liu",
            "Yanchen Luo",
            "Xiang Wang",
            "Xiangnan He",
            "Kenji Kawaguchi",
            "Tat-Seng Chua",
            "Qi Tian"
        ],
        "subjects": [
            "Machine Learning",
            "Information Retrieval",
            "Biomolecules"
        ],
        "abstract": "Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks, including molecule-text retrieval, molecule captioning, and more challenging open-text molecular QA tasks, especially focusing on 3D-dependent properties.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13923"
    },
    {
        "doc_id": 210,
        "title": "Inverse Molecular Design with Multi-Conditional Diffusion Guidance",
        "authors": [
            "Gang Liu",
            "Jiaxin Xu",
            "Tengfei Luo",
            "Meng Jiang"
        ],
        "subjects": [
            "Machine Learning",
            "Biomolecules"
        ],
        "abstract": "Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We introduce multi-conditional diffusion guidance. The proposed Transformer-based denoising model has a condition encoder that learns the representations of numerical and categorical conditions. The denoising model, consisting of a structure encoder-decoder, is trained for denoising under the representation of conditions. The diffusion process becomes graph-dependent to accurately estimate graph-related noise in molecules, unlike the previous models that focus solely on the marginal distributions of atoms or bonds. We extensively validate our model for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution learning to condition control for molecular properties. An inverse polymer design task for gas separation with feedback from domain experts further demonstrates its practical utility.",
        "comments": "20 pages, 8 figures, 7 tables",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13858"
    },
    {
        "doc_id": 211,
        "title": "Engineering Yeast Cells to Facilitate Information Exchange",
        "authors": [
            "Nikolaos Ntetsikas",
            "Styliana Kyriakoudi",
            "Antonis Kirmizis",
            "Bige Deniz Unluturk",
            "Andreas Pitsillides",
            "Ian F. Akyildiz",
            "Marios Lestas"
        ],
        "subjects": [
            "Emerging Technologies",
            "Information Theory",
            "Molecular Networks"
        ],
        "abstract": "Although continuous advances in theoretical modelling of Molecular Communications (MC) are observed, there is still an insuperable gap between theory and experimental testbeds, especially at the microscale. In this paper, the development of the first testbed incorporating engineered yeast cells is reported. Different from the existing literature, eukaryotic yeast cells are considered for both the sender and the receiver, with \u03b1-factor molecules facilitating the information transfer. The use of such cells is motivated mainly by the well understood biological mechanism of yeast mating, together with their genetic amenability. In addition, recent advances in yeast biosensing establish yeast as a suitable detector and a neat interface to in-body sensor networks. The system under consideration is presented first, and the mathematical models of the underlying biological processes leading to an end-to-end (E2E) system are given. The experimental setup is then described and used to obtain experimental results which validate the developed mathematical models. Beyond that, the ability of the system to effectively generate output pulses in response to repeated stimuli is demonstrated, reporting one event per two hours. However, fast RNA fluctuations indicate cell responses in less than three minutes, demonstrating the potential for much higher rates in the future.",
        "comments": "18 pages, 9 figures (2 of which are not colored) all .png, recently accepted for publication at TMBMC",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13712"
    },
    {
        "doc_id": 212,
        "title": "Accelerating hyperbolic t-SNE",
        "authors": [
            "Martin Skrodzki",
            "Hunter van Geffen",
            "Nicolas F. Chaves-de-Plaza",
            "Thomas H\u00f6llt",
            "Elmar Eisemann",
            "Klaus Hildebrandt"
        ],
        "subjects": [
            "Human-Computer Interaction",
            "Artificial Intelligence",
            "Machine Learning",
            "Quantitative Methods",
            "Machine Learning"
        ],
        "abstract": "The need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. Hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. Subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. However, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. That is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. Furthermore, due to the non-linear nature of hyperbolic spaces, Euclidean acceleration structures cannot directly be translated to the hyperbolic setting. This paper introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. We compare our approach with existing methods and demonstrate that it computes embeddings of similar quality in significantly less time. Implementation and scripts for the experiments can be found at https://graphics.tudelft.nl/accelerating-hyperbolic-tsne.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13708"
    },
    {
        "doc_id": 213,
        "title": "A generic model of consciousness",
        "authors": [
            "Mark J. Hadley"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "This is a model of consciousness. The hard problem of consciousness, what it feels like, is answered. The work builds on medical research analyzing the source and mechanisms associated with our feelings. It goes further by describing a generic model with wide applicability. The model is fully consistent with medical pathways in humans, but easily extends to animals and AI. The essence of the model is the interplay between associative memory and physiology. The model is a clear and concrete counterexample to the famous philosophical objections to a scientific explanation.",
        "comments": "Journal ref:        Journal of Artificial Intelligence and Consciousness 10 (2):291--308 (2023)",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13690"
    },
    {
        "doc_id": 214,
        "title": "Detecting local perturbations of networks in a latent hyperbolic space",
        "authors": [
            "Alice Longhena",
            "Martin Guillemaud",
            "Mario Chavez"
        ],
        "subjects": [
            "Quantitative Methods",
            "Data Analysis, Statistics and Probability"
        ],
        "abstract": "Graph theoretical approaches have been proven to be effective in the characterization of connected systems, as well as in quantifying their dysfunction due to perturbation. In this paper, we show the advantage of a non-Euclidean (hyperbolic) representation of networks to identify local connectivity perturbations and to characterize the induced effects on a large scale. We propose two perturbation scores based on representations of the networks in a latent geometric space, obtained through an embedding onto the hyperbolic Poincar\u00e9 disk. We numerically demonstrate that these methods are able to localize perturbations in networks with homogeneous or heterogeneous degree connectivity. We apply this framework to identify the most perturbed brain areas in epileptic patients following surgery. This study is conceived in the effort of developing more powerful tools to represent and analyze brain networks, and it is the first to apply geometric network embedding techniques to the case of epilepsy.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13495"
    },
    {
        "doc_id": 215,
        "title": "Unified neural field theory of brain dynamics underlying oscillations in Parkinson's disease and generalized epilepsies",
        "authors": [
            "Eli J M\u00fcller",
            "Sacha J van Albada",
            "Jong-Won Kim",
            "Peter A Robinson"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "The mechanisms underlying pathologically synchronized neural oscillations in Parkinson's disease (PD) and generalized epilepsies are jointly explored via a neural field model of the corticothalamic-basal ganglia (CTBG) system. The basal ganglia (BG) are approximated as a single effective population and their roles in modulating oscillatory corticothalamic (CT) dynamics and vice versa are analyzed. Besides normal EEG rhythms, enhanced activity around 4 Hz and 20 Hz exists in the model, consistent with characteristic frequencies in PD. These rhythms result from resonances in loops between the BG and CT populations, analogous to those underlying epileptic oscillations in a previous CT model. Dopamine depletion is argued to weaken the dampening of these resonances in PD, and network connections explain the significant coherence between BG, thalamic, and cortical activity around 4-8 Hz and 20 Hz. Parallels between the afferent and efferent connection sites of the thalamic reticular nucleus (TRN) and BG predict low dopamine to correspond to a reduced likelihood of tonic-clonic (grand mal) seizures, agreeing with experimental findings. Further, the model predicts an increased likelihood of absence (petit mal) seizure resulting from low dopamine levels matching experimental findings. Suppression of absence seizure activity is shown when afferent and efferent BG connections to the CT system are strengthened, consistent with other CTBG modeling studies. The BG are demonstrated to suppress activity of the CTBG system near tonic-clonic seizure states, providing insight into the reported efficacy of current treatments in BG circuits. Sleep states of the TRN are also found to suppress pathological PD activity matching observations. Overall, the findings demonstrate strong parallels between coherent oscillations in generalized epilepsies and PD, and provide insights into possible comorbidities.",
        "comments": "Journal ref:        Journal of Theoretical Biology (2017) 428, 132-146",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13467"
    },
    {
        "doc_id": 216,
        "title": "TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled Zero-shot Genome Classification",
        "authors": [
            "Sathyanarayanan Aakur",
            "Vishalini R. Laguduva",
            "Priyadharsini Ramamurthy",
            "Akhilesh Ramachandran"
        ],
        "subjects": [
            "Genomics",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "A species' genetic code or genome encodes valuable evolutionary, biological, and phylogenetic information that aids in species recognition, taxonomic classification, and understanding genetic predispositions like drug resistance and virulence. However, the vast number of potential species poses significant challenges in developing a general-purpose whole genome classification tool. Traditional bioinformatics tools have made notable progress but lack scalability and are computationally expensive. Machine learning-based frameworks show promise but must address the issue of large classification vocabularies with long-tail distributions. In this study, we propose addressing this problem through zero-shot learning using TEPI, Taxonomy-aware Embedding and Pseudo-Imaging. We represent each genome as pseudo-images and map them to a taxonomy-aware embedding space for reasoning and classification. This embedding space captures compositional and phylogenetic relationships of species, enabling predictions in extensive search spaces. We evaluate TEPI using two rigorous zero-shot settings and demonstrate its generalization capabilities qualitatively on curated, large-scale, publicly sourced data.",
        "comments": "Accepted to IEEE JBHI",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13219"
    },
    {
        "doc_id": 217,
        "title": "Single NV in nanodiamond for quantum sensing of protein dynamics in an ABEL trap",
        "authors": [
            "Ivan Perez",
            "Anke Krueger",
            "Joerg Wrachtrup",
            "Fedor Jelezko",
            "Michael B\u00f6rsch"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "Enzymes are cellular protein machines using a variety of conformational changes to power fast biochemical catalysis. Our goal is to exploit the single-spin properties of the luminescent NV (nitrogen-vacancy) center in nanodiamonds to reveal the dynamics of an active enzyme complex at physiological conditions with the highest spatio-temporal resolution. Specifically attached to the membrane enzyme FoF1-ATP synthase, the NV sensor will report the adenosine triphosphate (ATP)-driven full rotation of Fo motor subunits in ten consecutive 36\u00b0 steps. Conformational dynamics are monitored using either a double electron-electron resonance scheme or NV- magnetometry with optical readout or using NV- relaxometry with a superparamagnetic nanoparticle as the second marker attached to the same enzyme. First, we show how all photophysical parameters like individual size, charge, brightness, spectral range of fluorescence and fluorescence lifetime can be determined for the NV- center in a single nanodiamond held in aqueous solution by a confocal anti-Brownian electrokinetic trap (ABEL trap). Stable photon count rates of individual nanodiamonds and the absence of blinking allow for observation times of single nanodiamonds in solution exceeding hundreds of seconds. For the proposed quantum sensing of nanometer-sized distance changes within an active enzyme, we show that local magnetic field fluctuations can be detected all-optically by analyzing fluorescence lifetime changes of the NV- center in each nanodiamond in solution.",
        "comments": "14 pages, 5 figures",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13180"
    },
    {
        "doc_id": 218,
        "title": "Enabling Global Image Data Sharing in the Life Sciences",
        "authors": [
            "Peter Bajcsy",
            "Sreenivas Bhattiprolu",
            "Katy Borner",
            "Beth Cimini",
            "Lucy Collinson",
            "Jan Ellenberg",
            "Reto Fiolka",
            "Maryellen Giger",
            "Wojtek Goscinski",
            "Matthew Hartley",
            "Nathan Hotaling",
            "Rick Horwitz",
            "Florian Jug",
            "Anna Kreshuk",
            "Emma Lundberg",
            "Aastha Mathur",
            "Kedar Narayan",
            "Shuichi Onami",
            "Anne L. Plant",
            "Fred Prior",
            "Jason Swedlow",
            "Adam Taylor",
            "Antje Keppler"
        ],
        "subjects": [
            "Other Quantitative Biology"
        ],
        "abstract": "Coordinated collaboration is essential to realize the added value of and infrastructure requirements for global image data sharing in the life sciences. In this White Paper, we take a first step at presenting some of the most common use cases as well as critical/emerging use cases of (including the use of artificial intelligence for) biological and medical image data, which would benefit tremendously from better frameworks for sharing (including technical, resourcing, legal, and ethical aspects). In the second half of this paper, we paint an ideal world scenario for how global image data sharing could work and benefit all life sciences and beyond. As this is still a long way off, we conclude by suggesting several concrete measures directed toward our institutions, existing imaging communities and data initiatives, and national funders, as well as publishers. Our vision is that within the next ten years, most researchers in the world will be able to make their datasets openly available and use quality image data of interest to them for their research and benefit. This paper is published in parallel with a companion White Paper entitled Harmonizing the Generation and Pre-publication Stewardship of FAIR Image Data, which addresses challenges and opportunities related to producing well-documented and high-quality image data that is ready to be shared. The driving goal is to address remaining challenges and democratize access to everyday practices and tools for a spectrum of biomedical researchers, regardless of their expertise, access to resources, and geographical location.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13023"
    },
    {
        "doc_id": 219,
        "title": "Harmonizing the Generation and Pre-publication Stewardship of FAIR Image Data",
        "authors": [
            "Nikki Bialy",
            "Frank Alber",
            "Brenda Andrews",
            "Michael Angelo",
            "Brian Beliveau",
            "Lacramioara Bintu",
            "Alistair Boettiger",
            "Ulrike Boehm",
            "Claire M. Brown",
            "Mahmoud Bukar Maina",
            "James J. Chambers",
            "Beth Cimini",
            "Kevin Eliceiri",
            "Rachel Errington",
            "Orestis Faklaris",
            "Nathalie Gaudreault",
            "Ronald N. Germain",
            "Wojtek Goscinski",
            "David Grunwald",
            "Michael Halter",
            "Dorit Hanein",
            "John W. Hickey",
            "Judith Lacoste",
            "Alex Laude",
            "Emma Lundberg",
            "et al. (22 additional authors not shown)"
        ],
        "subjects": [
            "Other Quantitative Biology"
        ],
        "abstract": "Together with the molecular knowledge of genes and proteins, biological images promise to significantly enhance the scientific understanding of complex cellular systems and to advance predictive and personalized therapeutic products for human health. For this potential to be realized, quality-assured image data must be shared among labs at a global scale to be compared, pooled, and reanalyzed, thus unleashing untold potential beyond the original purpose for which the data was generated. There are two broad sets of requirements to enable image data sharing in the life sciences. One set of requirements is articulated in the companion White Paper entitled Enabling Global Image Data Sharing in the Life Sciences, which is published in parallel and addresses the need to build the cyberinfrastructure for sharing the digital array data. In this White Paper, we detail a broad set of requirements, which involves collecting, managing, presenting, and propagating contextual information essential to assess the quality, understand the content, interpret the scientific implications, and reuse image data in the context of the experimental details. We start by providing an overview of the main lessons learned to date through international community activities, which have recently made considerable progress toward generating community standard practices for imaging Quality Control (QC) and metadata. We then provide a clear set of recommendations for amplifying this work. The driving goal is to address remaining challenges and democratize access to everyday practices and tools for a spectrum of biomedical researchers, regardless of their expertise, access to resources, and geographical location.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13022"
    },
    {
        "doc_id": 220,
        "title": "How norms shape the evolution of prosocial behavior. Compassion, Universalizability, Reciprocity, Equity: A C.U.R.E for social dilemmas",
        "authors": [
            "Brian Mintz",
            "Feng Fu"
        ],
        "subjects": [
            "Physics and Society",
            "Populations and Evolution"
        ],
        "abstract": "How cooperation evolves and particularly maintains at a large scale remains an open problem for improving humanity across domains ranging from climate change to pandemic response. To shed light on how behavioral norms can resolve the social dilemma of cooperation, here we present a formal mathematical model of individuals' decision making under general social norms, encompassing a variety of concerns and motivations an individual may have beyond simply maximizing their own payoffs. Using the canonical game of the Prisoner's Dilemma, we compare four different norms: compassion, universalizability, reciprocity, and equity, to determine which social forces can facilitate the evolution of cooperation, if any. We analyze our model through a variety of limiting cases, including weak selection, low mutation, and large population sizes. This is complemented by computer simulations of population dynamics via a Fisher process, which confirm our theoretical results. We find that the first two norms lead to the emergence of cooperation in a wide range of games, but the latter two do not on their own. Due to its generality, our framework can be used to investigate many more norms, as well as how norms themselves emerge and evolve. Our work complements recent work on fair-minded learning dynamics and provides a useful bottom-up perspective into understanding the impact of top-down social norms on collective cooperative intelligence.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13015"
    },
    {
        "doc_id": 221,
        "title": "Ready for climate change? The importance of adaptive thermoregulatory flexibility for the Malagasy bat species Triaenops menamena",
        "authors": [
            "Sina Remmers"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "The balance between energy intake and expenditure is essential and crucial for survival for all organisms. The energy management is closely linked to the ecology. Thus, changes in environmental conditions can be challenging, especially for the animals physiology. Different strategies of thermoregulation have evolved and heterothermy seems to be the most efficient way for saving energy. Daily torpor, a temporally controlled reduction of the metabolic rate and body temperature, is one form of heterothermy and recent studies revealed that this physiological strategy is used by many tropical and subtropical species. Yet, little is known about torpor in bats and their intraspecific thermoregulatory flexibility. Therefore, three populations of the Malagasy bat species Triaenops menamena were investigated, to examine their metabolic rate, skin temperature and related energy expenditure during normothermic and torpid states in context of different microclimatic conditions. This study exposed significant physiological differences among these three populations along a gradient of fluctuation in environmental conditions. The greater the fluctuations in ambient temperature and humidity, the higher was the general resting metabolic rate and the rate of its reduction, but the lower was the torpid metabolic rate. This species shows a highly adaptive flexibility in their physiology and are able to cope with unfavorable environmental conditions by using different strategies of thermoregulation and hypometabolism, which is beneficial regarding ongoing climatic changes.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13012"
    },
    {
        "doc_id": 222,
        "title": "SegmentAnyBone: A Universal Model that Segments Any Bone at Any Location on MRI",
        "authors": [
            "Hanxue Gu",
            "Roy Colglazier",
            "Haoyu Dong",
            "Jikai Zhang",
            "Yaqian Chen",
            "Zafer Yildiz",
            "Yuwen Chen",
            "Lin Li",
            "Jichen Yang",
            "Jay Willhite",
            "Alex M. Meyer",
            "Brian Guo",
            "Yashvi Atul Shah",
            "Emily Luo",
            "Shipra Rajput",
            "Sally Kuehn",
            "Clark Bulleit",
            "Kevin A. Wu",
            "Jisoo Lee",
            "Brandon Ramirez",
            "Darui Lu",
            "Jay M. Levin",
            "Maciej A. Mazurowski"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Quantitative Methods"
        ],
        "abstract": "Magnetic Resonance Imaging (MRI) is pivotal in radiology, offering non-invasive and high-quality insights into the human body. Precise segmentation of MRIs into different organs and tissues would be highly beneficial since it would allow for a higher level of understanding of the image content and enable important measurements, which are essential for accurate diagnosis and effective treatment planning. Specifically, segmenting bones in MRI would allow for more quantitative assessments of musculoskeletal conditions, while such assessments are largely absent in current radiological practice. The difficulty of bone MRI segmentation is illustrated by the fact that limited algorithms are publicly available for use, and those contained in the literature typically address a specific anatomic area. In our study, we propose a versatile, publicly available deep-learning model for bone segmentation in MRI across multiple standard MRI locations. The proposed model can operate in two modes: fully automated segmentation and prompt-based segmentation. Our contributions include (1) collecting and annotating a new MRI dataset across various MRI protocols, encompassing over 300 annotated volumes and 8485 annotated slices across diverse anatomic regions; (2) investigating several standard network architectures and strategies for automated segmentation; (3) introducing SegmentAnyBone, an innovative foundational model-based approach that extends Segment Anything Model (SAM); (4) comparative analysis of our algorithm and previous approaches; and (5) generalization analysis of our algorithm across different anatomical locations and MRI sequences, as well as an external dataset. We publicly release our model at https://github.com/mazurowski-lab/SegmentAnyBone.",
        "comments": "15 pages, 15 figures",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12974"
    },
    {
        "doc_id": 223,
        "title": "Analysis of a detailed multi-stage model of stochastic gene expression using queueing theory and model reduction",
        "authors": [
            "Muhan Ma",
            "Juraj Szavits-Nossan",
            "Abhyudai Singh",
            "Ramon Grima"
        ],
        "subjects": [
            "Molecular Networks",
            "Quantitative Methods",
            "Subcellular Processes"
        ],
        "abstract": "We introduce a biologically detailed, stochastic model of gene expression describing the multiple rate-limiting steps of transcription, nuclear pre-mRNA processing, nuclear mRNA export, cytoplasmic mRNA degradation and translation of mRNA into protein. The processes in sub-cellular compartments are described by an arbitrary number of processing stages, thus accounting for a significantly finer molecular description of gene expression than conventional models such as the telegraph, two-stage and three-stage models of gene expression. We use two distinct tools, queueing theory and model reduction using the slow-scale linear-noise approximation, to derive exact or approximate analytic expressions for the moments or distributions of nuclear mRNA, cytoplasmic mRNA and protein fluctuations, as well as lower bounds for their Fano factors in steady-state conditions. We use these to study the phase diagram of the stochastic model; in particular we derive parametric conditions determining three types of transitions in the properties of mRNA fluctuations: from sub-Poissonian to super-Poissonian noise, from high noise in the nucleus to high noise in the cytoplasm, and from a monotonic increase to a monotonic decrease of the Fano factor with the number of processing stages. In contrast, protein fluctuations are always super-Poissonian and show weak dependence on the number of mRNA processing stages. Our results delineate the region of parameter space where conventional models give qualitatively incorrect results and provide insight into how the number of processing stages, e.g. the number of rate-limiting steps in initiation, splicing and mRNA degradation, shape stochastic gene expression by modulation of molecular memory.",
        "comments": "49 pages, 4 figures",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12661"
    },
    {
        "doc_id": 224,
        "title": "The stability and instability of the language control network: a longitudinal resting-state functional magnetic resonance imaging study",
        "authors": [
            "Zilong Li",
            "Cong Liu",
            "Xin Pan",
            "Guosheng Ding",
            "Ruiming Wanga"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "The language control network is vital among language-related networks responsible for solving the problem of multiple language switching. Researchers have expressed concerns about the instability of the language control network when exposed to external influences (e.g., Long-term second language learning). However, some studies have suggested that the language control network is stable. Therefore, whether the language control network is stable or not remains unclear. In the present study, we directly evaluated the stability and instability of the language control network using resting-state functional magnetic resonance imaging (rs-fMRI). We employed cohorts of Chinese first-year college students majoring in English who underwent second language (L2) acquisition courses at a university and those who did not. Two resting-state fMRI scans were acquired approximately 1 year apart. We found that the language control network was both moderately stable and unstable. We further investigated the morphological coexistence patterns of stability and instability within the language control network. First, we extracted connections representing stability and plasticity from the entire network. We then evaluated whether the coexistence patterns were modular (stability and instability involve different brain regions) or non-modular (stability and plasticity involve the same brain regions but have unique connectivity patterns). We found that both stability and instability coexisted in a non-modular pattern. Compared with the non-English major group, the English major group has a more non-modular coexistence pattern.. These findings provide preliminary evidence of the coexistence of stability and instability in the language control network.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12616"
    },
    {
        "doc_id": 225,
        "title": "Experiencing an elongated limb in virtual reality modifies the tactile distance perception of the corresponding real limb",
        "authors": [
            "Fran\u00e7ois Le Jeune",
            "Marco D'Alonzo",
            "Valeria Piombino",
            "Alessia Noccaro",
            "Domenico Formica",
            "Giovanni Di Pino"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "In measurement, a reference frame is needed to compare the measured object to something already known. This raises the neuroscientific question of which reference frame is used by humans when exploring the environment. Previous studies suggested that, in touch, the body employed as measuring tool also serves as reference frame. Indeed, an artificial modification of the perceived dimensions of the body changes the tactile perception of external object dimensions. However, it is unknown if such a change in tactile perception would occur when the body schema is modified through the illusion of owning an limb altered in size. Therefore, employing a virtual hand illusion paradigm with an elongated forearm of different lengths, we systematically tested the subjective perception of distance between two points (tactile distance perception task, TDP task) on the corresponding real forearm following the illusion. Thus, TDP task is used as a proxy to gauge changes in the body schema. Embodiment of the virtual arm was found significantly greater after the synchronous visuo-tactile stimulation condition compared to the asynchronous one, and the forearm elongation significantly increased the TDP. However, we did not find any link between the visuo-tactile induced ownership over the elongated arm and TDP variation, suggesting that vision plays the main role in the modification of the body schema. Additionally, significant effect of elongation found on TDP but not on proprioception suggests that these are affected differently by body schema modifications. These findings confirm the body schema malleability and its role as reference frame in touch.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12601"
    },
    {
        "doc_id": 226,
        "title": "A robust balancing mechanism for spiking neural networks",
        "authors": [
            "Antonio Politi",
            "Alessandro Torcini"
        ],
        "subjects": [
            "Disordered Systems and Neural Networks",
            "Neurons and Cognition"
        ],
        "abstract": "Dynamical balance of excitation and inhibition is usually invoked to explain the irregular low firing activity observed in the cortex. We propose a robust nonlinear balancing mechanism for a random network of spiking neurons, which works also in absence of strong external currents. Biologically, the mechanism exploits the plasticity of excitatory-excitatory synapses induced by short-term depression. Mathematically, the nonlinear response of the synaptic activity is the key ingredient responsible for the emergence of a stable balanced regime. Our claim is supported by a simple self-consistent analysis accompanied by extensive simulations performed for increasing network sizes. The observed regime is essentially fluctuation driven and characterized by highly irregular spiking dynamics of all neurons.",
        "comments": "9 pages, 4 figures",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12559"
    },
    {
        "doc_id": 227,
        "title": "Understanding Cellular Noise with Optical Perturbation and Deep Learning",
        "authors": [
            "Chuanbo Liu",
            "Yu Fu",
            "Lu Lin",
            "Elliot L. Elson",
            "Jin Wang"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "Noise plays a crucial role in the regulation of cellular and organismal function and behavior.\n  Exploring noise's impact is key to understanding fundamental biological processes, such as gene expression, signal transduction, and the mechanisms of development and evolution.\n  Currently, a comprehensive method to quantify dynamical behavior of cellular noise within these biochemical systems is lacking.\n  In this study, we introduce an optically-controlled perturbation system utilizing the light-sensitive Phytochrome B (PhyB) from \\textit{Arabidopsis thaliana}, which enables precise noise modulation with high spatial-temporal resolution.\n  Our system exhibits exceptional sensitivity to light, reacting consistently to pulsed light signals, distinguishing it from other photoreceptor-based promoter systems that respond to a single light wavelength.\n  To characterize our system, we developed a stochastic model for phytochromes that accounts for photoactivation/deactivation, thermal reversion, and the dynamics of the light-activated gene promoter system.\n  To precisely control our system, we determined the rate constants for this model using an omniscient deep neural network that can directly map rate constant combinations to time-dependent state joint distributions.\n  By adjusting the activation rates through light intensity and degradation rates via N-terminal mutagenesis, we illustrate that out optical-controlled perturbation can effectively modulate molecular expression level as well as noise.\n  Our results highlight the potential of employing an optically-controlled gene perturbation system as a noise-controlled stimulus source.\n  This approach, when combined with the analytical capabilities of a sophisticated deep neural network, enables the accurate estimation of rate constants from observational data in a broad range of biochemical reaction networks.",
        "comments": "33 pages, 4 figures",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12498"
    },
    {
        "doc_id": 228,
        "title": "Modular Control of Biological Networks",
        "authors": [
            "David Murrugarra",
            "Alan Veliz-Cuba",
            "Elena Dimitrova",
            "Claus Kadelka",
            "Matthew Wheeler",
            "Reinhard Laubenbacher"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "The concept of control is central to understanding and applications of biological network models. Some of their key structural features relate to control functions, through gene regulation, signaling, or metabolic mechanisms, and computational models need to encode these. Applications of models often focus on model-based control, such as in biomedicine or metabolic engineering. This paper presents an approach to model-based control that exploits two common features of biological networks, namely their modular structure and canalizing features of their regulatory mechanisms. The paper focuses on intracellular regulatory networks, represented by Boolean network models. A main result of this paper is that control strategies can be identified by focusing on one module at a time. This paper also presents a criterion based on canalizing features of the regulatory rules to identify modules that do not contribute to network control and can be excluded. For even moderately sized networks, finding global control inputs is computationally very challenging. The modular approach presented here leads to a highly efficient approach to solving this problem. This approach is applied to a published Boolean network model of blood cancer large granular lymphocyte (T-LGL) leukemia to identify a minimal control set that achieves a desired control objective.",
        "comments": "15 pages, 5 figures. arXiv admin note: text overlap with arXiv:2206.04217",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12477"
    },
    {
        "doc_id": 229,
        "title": "A dynamic model to study the potential TB infections and assessment of control strategies in China",
        "authors": [
            "Chuanqing Xu",
            "Kedeng Cheng",
            "Songbai Guo",
            "Dehui Yuan",
            "Xiaoyu Zhao"
        ],
        "subjects": [
            "Populations and Evolution",
            "Dynamical Systems"
        ],
        "abstract": "China is one of the countries with a high burden of tuberculosis, and although the number of new cases of tuberculosis has been decreasing year by year, the number of new infections per year has remained high and the diagnosis rate of tuberculosis-infected patients has remained low. Based on the analysis of TB infection data, we develop a model of TB transmission dynamics that include potentially infected individuals and BCG vaccination, fit the model parameters to the data on new TB cases, calculate the basic reproduction number \\mathcal{R}_v= 0.4442. A parametric sensitivity analysis of \\mathcal{R}_v is performed, and we obtained the correlation coefficients of BCG vaccination rate and effectiveness rate with \\mathcal{R}_v as -0.810, -0.825. According to the model, we estimate that there are 614,186 (95% CI [562,631,665,741]) potentially infected TB cases in China, accounting for about 39.5% of the total number of TB cases. We assess the feasibility of achieving the goals of the WHO strategy to end tuberculosis in China and find that reducing the number of new cases by 90 per cent by 2035 is very difficult with the current tuberculosis control measures. However, with an effective combination of control measures such as increased detection of potentially infected persons, improved drug treatment, and reduction of overall exposure to tuberculosis patients, it is feasible to reach the WHO strategic goal of ending tuberculosis by 2035.",
        "comments": "20 pages, 10 figures, 33 conference",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12462"
    },
    {
        "doc_id": 230,
        "title": "Hypochaos prevents tragedy of the commons in discrete-time eco-evolutionary game dynamics",
        "authors": [
            "Samrat Sohel Mondal",
            "Avishuman Ray",
            "Sagar Chakraborty"
        ],
        "subjects": [
            "Populations and Evolution",
            "Adaptation and Self-Organizing Systems"
        ],
        "abstract": "While quite a few recent papers have explored game-resource feedback using the framework of evolutionary game theory, almost all the studies are confined to using time-continuous dynamical equations. Moreover, in such literature, the effect of ubiquitous chaos in the resulting eco-evolutionary dynamics is rather missing. Here, we present a deterministic eco-evolutionary discrete-time dynamics in generation-wise non-overlapping population of two types of harvesters, one harvesting at a faster rate than the other, consuming a self-renewing resource capable of showing chaotic dynamics. In the light of our finding that sometimes chaos is confined exclusively to either the dynamics of the resource or that of the consumer fractions, an interesting scenario is realized: The resource state can keep oscillating chaotically, and hence, it does not vanish to result in the tragedy of the commons, extinction of the resource due to selfish indiscriminate exploitation, and yet the consumer population, whose dynamics depends directly on the state of the resource, may end up being composed exclusively of defectors, i.e., high harvesters. This appears non-intuitive because it is well known that prevention of tragedy of the commons usually requires substantial cooperation to be present.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12245"
    },
    {
        "doc_id": 231,
        "title": "A distribution-guided Mapper algorithm",
        "authors": [
            "Yuyang Tao",
            "Shufei Ge"
        ],
        "subjects": [
            "Algebraic Topology",
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "Motivation: The Mapper algorithm is an essential tool to explore shape of data in topology data analysis. With a dataset as an input, the Mapper algorithm outputs a graph representing the topological features of the whole dataset. This graph is often regarded as an approximation of a reeb graph of data. The classic Mapper algorithm uses fixed interval lengths and overlapping ratios, which might fail to reveal subtle features of data, especially when the underlying structure is complex.\n  Results: In this work, we introduce a distribution guided Mapper algorithm named D-Mapper, that utilizes the property of the probability model and data intrinsic characteristics to generate density guided covers and provides enhanced topological features. Our proposed algorithm is a probabilistic model-based approach, which could serve as an alternative to non-prababilistic ones. Moreover, we introduce a metric accounting for both the quality of overlap clustering and extended persistence homology to measure the performance of Mapper type algorithm. Our numerical experiments indicate that the D-Mapper outperforms the classical Mapper algorithm in various scenarios. We also apply the D-Mapper to a SARS-COV-2 coronavirus RNA sequences dataset to explore the topological structure of different virus variants. The results indicate that the D-Mapper algorithm can reveal both vertical and horizontal evolution processes of the viruses.\n  Availability: Our package is available at https://github.com/ShufeiGe/D-Mapper.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12237"
    },
    {
        "doc_id": 232,
        "title": "Machine Learning Modeling Of SiRNA Structure-Potency Relationship With Applications Against Sars-Cov-2 Spike Gene",
        "authors": [
            "Damilola Oshunyinka"
        ],
        "subjects": [
            "Biomolecules",
            "Machine Learning"
        ],
        "abstract": "The pharmaceutical Research and development (R&D) process is lengthy and costly, taking nearly a decade to bring a new drug to the market. However, advancements in biotechnology, computational methods, and machine learning algorithms have the potential to revolutionize drug discovery, speeding up the process and improving patient outcomes. The COVID-19 pandemic has further accelerated and deepened the recognition of the potential of these techniques, especially in the areas of drug repurposing and efficacy predictions. Meanwhile, non-small molecule therapeutic modalities such as cell therapies, monoclonal antibodies, and RNA interference (RNAi) technology have gained importance due to their ability to target specific disease pathways and/or patient populations. In the field of RNAi, many experiments have been carried out to design and select highly efficient siRNAs. However, the established patterns for efficient siRNAs are sometimes contradictory and unable to consistently determine the most potent siRNA molecules against a target mRNA. Thus, this paper focuses on developing machine learning models based on the cheminformatics representation of the nucleotide composition (i.e. AUTGC) of siRNA to predict their potency and aid the selection of the most efficient siRNAs for further development. The PLS (Partial Least Square) and SVR (Support Vector Regression) machine learning models built in this work outperformed previously published models. These models can help in predicting siRNA potency and aid in selecting the best siRNA molecules for experimental validation and further clinical development. The study has demonstrated the potential of AI/machine learning models to help expedite siRNA-based drug discovery including the discovery of potent siRNAs against SARS-CoV-2.",
        "comments": "Master's thesis",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12232"
    },
    {
        "doc_id": 233,
        "title": "Biological species delimitation based on genetic and spatial dissimilarity: a comparative study",
        "authors": [
            "Gabriele d'Angella",
            "Christian Hennig"
        ],
        "subjects": [
            "Populations and Evolution",
            "Applications",
            "Methodology"
        ],
        "abstract": "The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. This paper investigates the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Various approaches are compared (some of which already exist in the literature) based on simulated metapopulations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Approaches involve partial Mantel testing, maximum likelihood mixed-effects models with a population effect, and jackknife-based homogeneity tests. A key challenge is that most tests perform on genetic and geographical distance data, violating standard independence assumptions. Simulations showed that partial Mantel tests and mixed-effects models have larger power than jackknife-based methods, but tend to display type-I-error rates slightly above the significance level. Moreover, a multiple regression model neglecting the dependence in the dissimilarities did not show inflated type-I-error rate. An application on brassy ringlets concludes the paper.",
        "comments": "paper of 23 pages with 4 figures; appendix of 11 pages with 4 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12126"
    },
    {
        "doc_id": 234,
        "title": "Matching biomolecular structures by registration of point clouds",
        "authors": [
            "Michael Habeck",
            "Andreas Kr\u00f6pelin",
            "Nima Vakili"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "Motivation: Assessing the match between two biomolecular structures is at the heart of structural analyses such as superposition, alignment and docking. These tasks are typically solved with specialized structure-matching techniques implemented in software for protein structural alignment, rigid-body docking, or rigid fitting into cryo-EM maps. Results: We present a unifying framework to compare biomolecular structures by applying ideas from computer vision. The structures are represented as three-dimensional point clouds and compared by quantifying their overlap. We use the kernel correlation to measure point cloud overlap, and discuss local and global optimization strategies for maximizing the kernel correlation over the space of rigid transformations. We derive a majorization-minimization procedure that can be used to register two point clouds without establishing a point-to-point correspondence. We demonstrate that the majorization-minimization algorithms outperform the commonly used Iterative Closest Point registration algorithm. Furthermore, we discuss and benchmark a randomization strategy for globally optimizing the kernel correlation. We illustrate the approach on various 3D fitting problems such as the comparison of circularly permuted structures and rigid fitting of cryo-EM maps or bead models from small-angle scattering.",
        "comments": "18 pages (main text), 7 figures (main text)",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12082"
    },
    {
        "doc_id": 235,
        "title": "DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI",
        "authors": [
            "Sergio Morell-Ortega",
            "Marina Ruiz-Perez",
            "Marien Gadea",
            "Roberto Vivo-Hernando",
            "Gregorio Rubio",
            "Fernando Aparici",
            "Maria de la Iglesia-Vaya",
            "Gwenaelle Catheline",
            "Pierrick Coup\u00e9",
            "Jos\u00e9 V. Manj\u00f3n"
        ],
        "subjects": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Neurons and Cognition"
        ],
        "abstract": "This paper introduces a novel multimodal and high-resolution human brain cerebellum lobule segmentation method. Unlike current tools that operate at standard resolution ($1 \\text{ mm}^{3}$) or using mono-modal data, the proposed method improves cerebellum lobule segmentation through the use of a multimodal and ultra-high resolution ($0.125 \\text{ mm}^{3}$) training dataset. To develop the method, first, a database of semi-automatically labelled cerebellum lobules was created to train the proposed method with ultra-high resolution T1 and T2 MR images. Then, an ensemble of deep networks has been designed and developed, allowing the proposed method to excel in the complex cerebellum lobule segmentation task, improving precision while being memory efficient. Notably, our approach deviates from the traditional U-Net model by exploring alternative architectures. We have also integrated deep learning with classical machine learning methods incorporating a priori knowledge from multi-atlas segmentation, which improved precision and robustness. Finally, a new online pipeline, named DeepCERES, has been developed to make available the proposed method to the scientific community requiring as input only a single T1 MR image at standard resolution.",
        "comments": "20 pages",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12074"
    },
    {
        "doc_id": 236,
        "title": "Approximating a linear dynamical system from non-sequential data",
        "authors": [
            "Cliff Stein",
            "Pratik Worah"
        ],
        "subjects": [
            "Genomics"
        ],
        "abstract": "Given non-sequential snapshots from instances of a dynamical system, we design a compressed sensing based algorithm that reconstructs the dynamical system. We formally prove that successful reconstruction is possible under the assumption that we can construct an approximate clock from a subset of the coordinates of the underlying system.\n  As an application, we argue that our assumption is likely true for genomic datasets, and we recover the underlying nuclear receptor networks and predict pathways, as opposed to genes, that may differentiate phenotypes in some publicly available datasets.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11858"
    },
    {
        "doc_id": 237,
        "title": "The NOSTRA model: coherent estimation of infection sources in the case of possible nosocomial transmission",
        "authors": [
            "David J Pascall",
            "Chris Jackson",
            "Stephanie Evans",
            "Theodore Gouliouris",
            "Chris Illingworth",
            "Stefan Piatek",
            "Julie V Robotham",
            "Oliver Stirrup",
            "Ben Warne",
            "Judith Breuer",
            "Daniela De Angelis"
        ],
        "subjects": [
            "Applications",
            "Quantitative Methods"
        ],
        "abstract": "Nosocomial infections have important consequences for patients and hospital staff: they worsen patient outcomes and their management stresses already overburdened health systems. Accurate judgements of whether an infection is nosocomial helps staff make appropriate choices to protect other patients within the hospital. Nosocomiality cannot be properly assessed without considering whether the infected patient came into contact with high risk potential infectors within the hospital. We developed a Bayesian model that integrates epidemiological, contact and pathogen genetic data to determine how likely an infection is to be nosocomial and the probability of given infection candidates being the source of the infection.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11837"
    },
    {
        "doc_id": 238,
        "title": "Full-dimensional characterisation of time-warped spike-time stimulus-response distribution geometries",
        "authors": [
            "James B Isbister"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Characterising the representation of sensory stimuli in the brain is a fundamental scientific endeavor, which can illuminate principles of information coding. Most characterizations reduce the dimensionality of neural data by converting spike trains to firing rates or binned spike counts, applying explicitly named methods of ``dimensionality reduction'', or collapsing trial-to-trial variability. Characterisation of the full-dimensional geometry of timing-based representations may provide unexpected insights into how complex high-dimensional information is encoded. Recent research shows that the distribution of representations elicited over trials of a single stimulus can be geometrically characterized without the application of dimensionality reduction, maintaining the temporal spiking information of individual neurons in a cell assembly and illuminating rich geometric structure. We extend these results, showing that precise spike time patterns for larger cell assemblies are time-warped (i.e. stretched or compressed) on each trial. Moreover, by geometrically characterizing distributions of large spike time patterns, our analysis supports the hypothesis that the degree to which a spike time pattern is time-warped depends on the cortical area's background activity level on a single trial. Finally, we suggest that the proliferation of large electrophysiology datasets and the increasing concentration of ``neural geometrists'', creates ideal conditions for characterization of full-dimensional spike time representations, in complement to dimensionality reduction approaches.",
        "comments": "Accepted as an extended abstract at the NeurReps workshop at NeurIPS 2023. The workshop doesn't publish extended abstracts so submitting here",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11784"
    },
    {
        "doc_id": 239,
        "title": "Impact of temporal interaction on the evolution of cooperation",
        "authors": [
            "Yujie He",
            "Tianyu Ren",
            "Junjun Zheng",
            "Huawen Liang"
        ],
        "subjects": [
            "Physics and Society",
            "Social and Information Networks",
            "Populations and Evolution"
        ],
        "abstract": "This research investigates the impact of dynamic interactions with time-varying topologies on the evolution of cooperative behaviours in social dilemmas. Traditional research has focused on deterministic rules governing pairwise interactions, yet the impact of interaction frequency and synchronicity on cooperation remains underexplored. Addressing this gap, our work introduces two temporal interaction mechanisms to model the stochastic or periodic participation of individuals in these games, acknowledging real-life variances due to exogenous temporal factors and geographical time differences. We consider that the interaction state significantly influences both game payoff calculations and the strategy updating process, offering new insights into the emergence and sustainability of cooperation. Our results indicate that maximum game participation frequency is suboptimal under a stochastic interaction mechanism. Instead, an intermediate region of activation probability yields the highest cooperation level, especially under strong dilemma conditions. This suggests that a balance between inactivity security and interaction frequency is crucial. Furthermore, local synchronization of interactions within specific areas is shown to be beneficial, as time differences hinder the spread of cross-structures but promote the formation of dense cooperative clusters with smoother boundaries. Our findings provide an intuitive understanding of node-based temporality and probabilistic interactions, contributing to the broader discourse on resolving social dilemmas.",
        "comments": "7 pages, 6 figures",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11782"
    },
    {
        "doc_id": 240,
        "title": "Combining oligo pools and Golden Gate cloning to create protein variant libraries or guide RNA libraries for CRISPR applications",
        "authors": [
            "Alicia Maci\u00e1 Valero",
            "Rianne C. Prins",
            "Thijs de Vroet",
            "Sonja Billerbeck"
        ],
        "subjects": [
            "Quantitative Methods",
            "Biomolecules"
        ],
        "abstract": "Oligo pools are array-synthesized, user-defined mixtures of single-stranded oligonucleotides that can be used as a source of synthetic DNA for library cloning. While currently offering the most affordable source of synthetic DNA, oligo pools also come with limitations such as a maximum synthesis length (approximately 350 bases), a higher error rate compared to alternative synthesis methods, and the presence of truncated molecules in the pool due to incomplete synthesis. Here, we provide users with a comprehensive protocol that details how oligo pools can be used in combination with Golden Gate cloning to create user-defined protein mutant libraries, as well as single guide RNA libraries for CRISPR applications. Our methods are optimized to work within the Yeast Toolkit Golden Gate scheme, but are in principle compatible with any other Golden Gate-based modular cloning toolkit and extendable to other restriction enzyme-based cloning methods beyond Golden Gate. Our methods yield high-quality, affordable, in-house variant libraries.",
        "comments": " ",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11746"
    },
    {
        "doc_id": 241,
        "title": "Evolutionary dynamics of any multiplayer game on regular graphs",
        "authors": [
            "Chaoqian Wang",
            "Matja\u017e Perc",
            "Attila Szolnoki"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Statistical Mechanics",
            "Computational Complexity",
            "Cellular Automata and Lattice Gases",
            "Populations and Evolution"
        ],
        "abstract": "Multiplayer games on graphs are at the heart of theoretical descriptions of key evolutionary processes that govern vital social and natural systems. However, a comprehensive theoretical framework for solving multiplayer games with an arbitrary number of strategies on graphs is still missing. Here, we solve this by drawing an analogy with the Ball-and-Box problem, based on which we show that the local configuration of multiplayer games on graphs is equivalent to distributing $k$ identical co-players among $n$ distinct strategies. We use this to derive the replicator equation for any $n$-strategy multiplayer game under weak selection, which can be solved in polynomial time. As an example, we revisit the second-order free-riding problem, where costly punishment cannot truly resolve social dilemmas in a well-mixed population. Yet, in structured populations, we derive an accurate threshold for the punishment strength, beyond which punishment can either lead to the extinction of defection or transform the system into a rock-paper-scissors-like cycle. The analytical solution also qualitatively agrees with the phase diagrams that were previously obtained for non-marginal selection strengths. Our framework thus allows an exploration of any multi-strategy multiplayer game on regular graphs.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11686"
    },
    {
        "doc_id": 242,
        "title": "Accelerating Seed Location Filtering in DNA Read Mapping Using a Commercial Compute-in-SRAM Architecture",
        "authors": [
            "Courtney Golden",
            "Dan Ilan",
            "Nicholas Cebry",
            "Christopher Batten"
        ],
        "subjects": [
            "Hardware Architecture",
            "Genomics"
        ],
        "abstract": "DNA sequence alignment is an important workload in computational genomics. Reference-guided DNA assembly involves aligning many read sequences against candidate locations in a long reference genome. To reduce the computational load of this alignment, candidate locations can be pre-filtered using simpler alignment algorithms like edit distance. Prior work has explored accelerating filtering on simulated compute-in-DRAM, due to the massive parallelism of compute-in-memory architectures. In this paper, we present work-in-progress on accelerating filtering using a commercial compute-in-SRAM accelerator. We leverage the recently released Gemini accelerator platform from GSI Technology, which is the first, to our knowledge, commercial-scale compute-in-SRAM system. We accelerate the Myers' bit-parallel edit distance algorithm, producing average speedups of 14.1x over single-core CPU performance. Individual query/candidate alignments produce speedups of up to 24.1x. These early results suggest this novel architecture is well-suited to accelerating the filtering step of sequence-to-sequence DNA alignment.",
        "comments": "Journal ref:        5th Workshop on Accelerator Architecture in Computational Biology and Bioinformatics (AACBB), June 2023",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11685"
    },
    {
        "doc_id": 243,
        "title": "Modern approaches to improving phase contrast electron microscopy",
        "authors": [
            "Jeremy J. Axelrod",
            "Jessie T. Zhang",
            "Petar N. Petrov",
            "Robert M. Glaeser",
            "Holger Mueller"
        ],
        "subjects": [
            "Quantitative Methods",
            "Optics",
            "Biomolecules"
        ],
        "abstract": "Although defocus can be used to generate partial phase contrast in transmission electron microscope images, cryo-electron microscopy (cryo-EM) can be further improved by the development of phase plates which increase contrast by applying a phase shift to the unscattered part of the electron beam. Many approaches have been investigated, including the ponderomotive interaction between light and electrons. We review the recent successes achieved with this method in high-resolution, single-particle cryo-EM. We also review the status of using pulsed or near-field enhanced laser light as alternatives, along with approaches that use scanning transmission electron microscopy (STEM) with a segmented detector rather than a phase plate.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11678"
    },
    {
        "doc_id": 244,
        "title": "Enhancing selectivity using Wasserstein distance based reweighing",
        "authors": [
            "Pratik Worah"
        ],
        "subjects": [
            "Machine Learning",
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "Given two labeled data-sets $\\mathcal{S}$ and $\\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\\mathcal{T}$.\n  On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.\n  Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1 (a highly similar protein). We tune the algorithm's parameter so that overall change in holdout loss is negligible, but the selectivity, i.e., the fraction of top 100 MNK2 binders that are MNK1 non-binders, increases from 54\\% to 95\\%, as a result of our reweighing. Of the 43 distinct small molecules predicted to be most selective from the enamine catalog, 2 small molecules were experimentally verified to be selective, i.e., they reduced the enzyme activity of MNK2 below 50\\% but not MNK1, at 10$\u03bc$M -- a 5\\% success rate.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11562"
    },
    {
        "doc_id": 245,
        "title": "Understanding Hepatitis B Virus Infection through Hepatocyte Proliferation and Capsid Recycling",
        "authors": [
            "Rupchand Sutradhar",
            "D C Dalal"
        ],
        "subjects": [
            "Populations and Evolution",
            "Dynamical Systems"
        ],
        "abstract": "Proliferation of uninfected as well as infected hepatocytes and recycling of DNA-containing\n  capsids are two major mechanisms playing significant roles in the clearance of hepatitis B\n  virus (HBV) infection. In this study, the temporal dynamics of this infection are investigated\n  through two in silico bio-mathematical models considering both proliferation of hepatocytes\n  and the recycling of capsids. Both models are formulated on the basis of a key finding in the existing literature: mitosis of infected yields in two uninfected progenies. In the first model,\n  we examine regular proliferation (occurs continuously), while the second model deals with the\n  irregular proliferation (happens when the total number of liver cells decreases to less than 70%\n  of its initial volume). The models are calibrated with the experimental data obtained from\n  an adult chimpanzee. Results of this study suggest that when both hepatocytes proliferate\n  with equal rate, proliferation aids the individual in a rapid recovery from the acute infection\n  whereas in the case of chronic infection, the severity of the infection increases if the proliferation\n  occurs frequently. On the other hand, if the infected cells proliferate at a slower rate than uninfected cells, the proliferation of uninfected hepatocytes contributes to increase the infection,\n  but the proliferation of infected hepatocytes acts to reduce the infection from the long-term\n  perspective. Furthermore, it is also observed that the differences between the outcomes of\n  regular and irregular proliferations are substantial and noteworthy.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11481"
    },
    {
        "doc_id": 246,
        "title": "Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis",
        "authors": [
            "Yin Li",
            "Yu Xiong",
            "Wenxin Fan",
            "Kai Wang",
            "Qingqing Yu",
            "Liping Si",
            "Patrick van der Smagt",
            "Jun Tang",
            "Nutan Chen"
        ],
        "subjects": [
            "Machine Learning",
            "Quantitative Methods"
        ],
        "abstract": "Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT.\n  Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities.\n  Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\\,\\%$ to $72\\%$, and for LSTM models, it is $66\\,\\%$ to $84\\,\\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for LSTM models it is between $1.09$ and $1.77$. Notably, these RMSEs are significantly lower than the random prediction error of $4.55$.\n  Conclusion: We creatively apply sequential models in the long-term management of SCIT with promising accuracy in the prediction of SCIT nonadherence in Allergic Rhinitis (AR) patients. While LSTM outperforms SLAC in adherence prediction, SLAC excels in score prediction for patients undergoing SCIT for AR. The state-action-based SLAC adds flexibility, presenting a novel and effective approach for managing long-term AIT.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11447"
    },
    {
        "doc_id": 247,
        "title": "MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks via Text Prompts",
        "authors": [
            "Haoqiang Guo",
            "Sendong Zhao",
            "Haochun Wang",
            "Yanrui Du",
            "Bing Qin"
        ],
        "subjects": [
            "Machine Learning",
            "Computation and Language",
            "Biomolecules"
        ],
        "abstract": "Deep learning is now widely used in drug discovery, providing significant acceleration and cost reduction. As the most fundamental building block, molecular representation is essential for predicting molecular properties to enable various downstream applications. Most existing methods attempt to incorporate more information to learn better representations. However, not all features are equally important for a specific task. Ignoring this would potentially compromise the training efficiency and predictive accuracy. To address this issue, we propose a novel approach, which treats language models as an agent and molecular pretraining models as a knowledge base. The agent accentuates task-relevant features in the molecular representation by understanding the natural language description of the task, just as a tailor customizes clothes for clients. Thus, we call this approach MolTailor. Evaluations demonstrate MolTailor's superior performance over baselines, validating the efficacy of enhancing relevance for molecular representation learning. This illustrates the potential of language model guided optimization to better exploit and unleash the capabilities of existing powerful molecular representation methods. Our codes and appendix are available at https://github.com/SCIR-HI/MolTailor.",
        "comments": "Accepted by AAAI 2024",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11403"
    },
    {
        "doc_id": 248,
        "title": "PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding",
        "authors": [
            "Ruochi Zhang",
            "Haoran Wu",
            "Chang Liu",
            "Huaping Li",
            "Yuqian Wu",
            "Kewei Li",
            "Yifan Wang",
            "Yifan Deng",
            "Jiahui Chen",
            "Fengfeng Zhou",
            "Xin Gao"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "Recent advances in protein language models have catalyzed significant progress in peptide sequence representation. Despite extensive exploration in this field, pre-trained models tailored for peptide-specific needs remain largely unaddressed due to the difficulty in capturing the complex and sometimes unstable structures of peptides. This study introduces a novel multi-view contrastive learning framework PepHarmony for the sequence-based peptide encoding task. PepHarmony innovatively combines both sequence- and structure-level information into a sequence-level encoding module through contrastive learning. We carefully select datasets from the Protein Data Bank (PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences and structures. The experimental data highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures compared with the baseline and fine-tuned models. The robustness of our model is confirmed through extensive ablation studies, which emphasize the crucial roles of contrastive loss and strategic data sorting in enhancing predictive performance. The proposed PepHarmony framework serves as a notable contribution to peptide representations, and offers valuable insights for future applications in peptide drug discovery and peptide engineering. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony or http://www.healthinformaticslab.org/supp/.",
        "comments": "25 pages, 5 figures, 3 tables",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11360"
    },
    {
        "doc_id": 249,
        "title": "Sensory adaptation in a continuum model of bacterial chemotaxis -- working range, cost-accuracy relation, and coupled systems",
        "authors": [
            "Vansh Kharbanda",
            "Benedikt Sabass"
        ],
        "subjects": [
            "Cell Behavior",
            "Soft Condensed Matter"
        ],
        "abstract": "Sensory adaptation enables organisms to adjust their perception in a changing environment. A paradigm is bacterial chemotaxis, where the output activity of chemoreceptors is adapted to different baseline concentrations via receptor methylation. The range of internal receptor states limits the stimulus magnitude to which these systems can adapt. Here, we employ a highly idealized, Langevin-equation based model to study how the finite range of state variables affects the adaptation accuracy and the energy dissipation in individual and coupled systems. Maintaining an adaptive state requires constant energy dissipation. We show that the steady-state dissipation rate increases approximately linearly with the adaptation accuracy for varying stimulus magnitudes in the so-called perfect adaptation limit. This result complements the well-known logarithmic cost-accuracy relationship for varying chemical driving. Next, we study linearly coupled pairs of sensory units. We find that the interaction reduces the dissipation rate per unit and affects the overall cost-accuracy relationship. A coupling of the slow methylation variables results in a better accuracy than a coupling of activities. Overall, the findings highlight the significance of both the working range and collective operation mode as crucial design factors that impact the accuracy and energy expenditure of molecular adaptation networks.",
        "comments": " ",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11341"
    },
    {
        "doc_id": 250,
        "title": "Uncertainty quantification of receptor ligand binding sites prediction",
        "authors": [
            "Nanjie Chen",
            "Dongliang Yu",
            "Dmitri Beglov",
            "Mark Kon",
            "Julio Enrique Castrillon-Candas"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "Recent advancements in protein docking site prediction have highlighted the limitations of traditional rigid docking algorithms, like PIPER, which often neglect critical stochastic elements such as solvent-induced fluctuations. These oversights can lead to inaccuracies in identifying viable docking sites due to the complexity of high-dimensional, stochastic energy manifolds with low regularity. To address this issue, our research introduces a novel model where the molecular shapes of ligands and receptors are represented using multi-variate Karhunen-Lo `eve (KL) expansions. This method effectively captures the stochastic nature of energy manifolds, allowing for a more accurate representation of molecular interactions.Developed as a plugin for PIPER, our scientific computing software enhances the platform, delivering robust uncertainty measures for the energy manifolds of ranked binding sites. Our results demonstrate that top-ranked binding sites, characterized by lower uncertainty in the stochastic energy manifold, align closely with actual docking sites. Conversely, sites with higher uncertainty correlate with less optimal docking positions. This distinction not only validates our approach but also sets a new standard in protein docking predictions, offering substantial implications for future molecular interaction research and drug development.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11312"
    },
    {
        "doc_id": 251,
        "title": "Seasonality of primary productivity affects coastal species more than its magnitude",
        "authors": [
            "Carlota Muniz",
            "Christopher McQuaid",
            "Nicolas Weidberg"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "While the importance of extreme conditions is recognised, patterns in species abundances are often interpreted through average environmental conditions within their distributional range. For marine species with pelagic larvae, temperature and phytoplankton concentration are key variables. Along the south coast of South Africa, conspicuous spatial patterns in recruitment rates and the abundances of different mussel species exist, with focal areas characterized by large populations. We studied 15 years of sea surface temperature (SST) and chlorophyll-a (chl-a) satellite data, using spectral analyses to partition their temporal variability over ecologically relevant time periods, including seasonal (101 to 365 days) and intra-seasonal cycles (20 to 100 days). Adult cover and mussel recruitment were measured at 10 sites along the south coast and regression models showed that about 70 percent of the variability in recruitment and adult cover was explained by seasonal variability in chl-a, while mean annual chl-a and SST only explained 30 percent of the recruitment, with no significant effect for adult cover. SST and chl-a at two upwelling centres showed less predictable seasonal cycles during the second half of the study period with a significant cooling trend during austral autumn, coinciding with one of the mussel reproductive peaks. This likely reflects recent changes in the Agulhas Current, the world largest western boundary current, which affects coastal ecosystems by driving upwelling.",
        "comments": "Journal ref:        Science of the Total Environment, 757:143740, 2021",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11289"
    },
    {
        "doc_id": 252,
        "title": "Smart Drug-Delivery Systems for Cancer Nanotherapy",
        "authors": [
            "Paola Sanchez-Moreno",
            "Juan Luis Ortega-Vinuesa",
            "Jose Manuel Peula-Garcia",
            "Juan Antonio Marchal",
            "Houria Boulaiz"
        ],
        "subjects": [
            "Tissues and Organs",
            "Mesoscale and Nanoscale Physics",
            "Applied Physics",
            "Biological Physics"
        ],
        "abstract": "Despite all the advances achieved in the field of tumor-biology research, in most cases conventional therapies including chemotherapy are still the leading choices. The main disadvantage of these treatments, in addition to the low solubility of many antitumor drugs, is their lack of specificity, which explains the frequent occurrence of serious side effects due to nonspecific drug uptake by healthy cells. Progress in nanotechnology and its application in medicine have provided new opportunities and different smart systems. Such systems can improve the intracellular delivery of the drugs due to their multifunctionality and targeting potential. The purpose of this manuscript is to review and analyze the recent progress made in nanotherapy applied to cancer treatment. First, we provide a global overview of cancer and different smart nanoparticles currently used in oncology. Then, we analyze in detail the development of drug-delivery strategies in cancer therapy, focusing mainly on the intravenously administered smart nanoparticles with protein corona to avoid immune-system clearance. Finally, we discuss the challenges, clinical trials, and future directions of the nanoparticle-based therapy in cancer.",
        "comments": "Preprint version, 25 pages, 7 figures, 3 tables. Authors thank to Bentham Science the posibility of deposit the ACCEPTED VERSION of the peer-reviewed article after 12 months of publication on journal web site on arXiv repository. The published manuscript is available at EurekaSelect via https://www.eurekaselect.com/openurl/content.php?genre=article&doi=10.2174/1389450117666160527142544",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11192"
    },
    {
        "doc_id": 253,
        "title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
        "authors": [
            "Chih-Hsuan Wei",
            "Alexis Allot",
            "Po-Ting Lai",
            "Robert Leaman",
            "Shubo Tian",
            "Ling Luo",
            "Qiao Jin",
            "Zhizheng Wang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "subjects": [
            "Computation and Language",
            "Quantitative Methods"
        ],
        "abstract": "PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art AI techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11048"
    },
    {
        "doc_id": 254,
        "title": "Equivariant Graph Neural Operator for Modeling 3D Dynamics",
        "authors": [
            "Minkai Xu",
            "Jiaqi Han",
            "Aaron Lou",
            "Jean Kossaifi",
            "Arvind Ramanathan",
            "Kamyar Azizzadenesheli",
            "Jure Leskovec",
            "Stefano Ermon",
            "Anima Anandkumar"
        ],
        "subjects": [
            "Machine Learning",
            "Numerical Analysis",
            "Quantitative Methods"
        ],
        "abstract": "Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stacking the Fourier layers over equivariant networks. EGNO is the first operator learning framework that is capable of modeling solution dynamics functions over time while retaining 3D equivariance. Comprehensive experiments in multiple domains, including particle simulations, human motion capture, and molecular dynamics, demonstrate the significantly superior performance of EGNO against existing methods, thanks to the equivariant temporal modeling.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11037"
    },
    {
        "doc_id": 255,
        "title": "Clustering Molecular Energy Landscapes by Adaptive Network Embedding",
        "authors": [
            "Paula Mercurio",
            "Di Liu"
        ],
        "subjects": [
            "Biomolecules",
            "Statistical Mechanics",
            "Machine Learning"
        ],
        "abstract": "In order to efficiently explore the chemical space of all possible small molecules, a common approach is to compress the dimension of the system to facilitate downstream machine learning tasks. Towards this end, we present a data driven approach for clustering potential energy landscapes of molecular structures by applying recently developed Network Embedding techniques, to obtain latent variables defined through the embedding function. To scale up the method, we also incorporate an entropy sensitive adaptive scheme for hierarchical sampling of the energy landscape, based on Metadynamics and Transition Path Theory. By taking into account the kinetic information implied by a system's energy landscape, we are able to interpret dynamical node-node relationships in reduced dimensions. We demonstrate the framework through Lennard-Jones (LJ) clusters and a human DNA sequence.",
        "comments": "19 pages, 10 figures",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10972"
    },
    {
        "doc_id": 256,
        "title": "Homogenisation of nonlinear blood flow in periodic networks: the limit of small haematocrit heterogeneity",
        "authors": [
            "Y. Ben-Ami",
            "B. D. Wood",
            "J. M. Pitt-Francis",
            "P. K. Maini",
            "H. M. Byrne"
        ],
        "subjects": [
            "Tissues and Organs",
            "Soft Condensed Matter",
            "Biological Physics"
        ],
        "abstract": "In this work we develop a homogenisation methodology to upscale mathematical descriptions of microcirculatory blood flow from the microscale (where individual vessels are resolved) to the macroscopic (or tissue) scale. Due to the assumed two-phase nature of blood and specific features of red blood cells (RBCs), mathematical models for blood flow in the microcirculation are highly nonlinear, coupling the flow and RBC concentrations (haematocrit). In contrast to previous works which accomplished blood-flow homogenisation by assuming that the haematocrit level remains constant, here we allow for spatial heterogeneity in the haematocrit concentration and thus begin with a nonlinear microscale model. We simplify the analysis by considering the limit of small haematocrit heterogeneity which prevails when variations in haematocrit concentration between neighbouring vessels are small. Homogenisation results in a system of coupled, nonlinear partial differential equations describing the flow and haematocrit transport at the macroscale, in which a nonlinear Darcy-type model relates the flow and pressure gradient via a haematocrit-dependent permeability tensor. During the analysis we obtain further that haematocrit transport at the macroscale is governed by a purely advective equation. Applying the theory to particular examples of two- and three-dimensional geometries of periodic networks, we calculate the effective permeability tensor associated with blood flow in these vascular networks. We demonstrate how the statistical distribution of vessel lengths and diameters, together with the average haematocrit level, affect the statistical properties of the macroscopic permeability tensor. These data can be used to simulate blood flow and haematocrit transport at the macroscale.",
        "comments": "34 pages, 8 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10932"
    },
    {
        "doc_id": 257,
        "title": "A Chaotic Associative Memory",
        "authors": [
            "Nurani Rajagopal Rohan",
            "Sayan Gupta",
            "V. Srinivasa Chakravarthy"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Chaotic Dynamics"
        ],
        "abstract": "We propose a novel Chaotic Associative Memory model using a network of chaotic Rossler systems and investigate the storage capacity and retrieval capabilities of this model as a function of increasing periodicity and chaos. In early models of associate memory networks, memories were modeled as fixed points, which may be mathematically convenient but has poor neurobiological plausibility. Since brain dynamics is inherently oscillatory, attempts have been made to construct associative memories using nonlinear oscillatory networks. However, oscillatory associative memories are plagued by the problem of poor storage capacity, though efforts have been made to improve capacity by adding higher order oscillatory modes. The chaotic associative memory proposed here exploits the continuous spectrum of chaotic elements and has higher storage capacity than previously described oscillatory associate memories.",
        "comments": "10 pages, 8 Figures, Submitted to \"Chaos: An Interdisciplinary Journal of Nonlinear Science\"",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10922"
    },
    {
        "doc_id": 258,
        "title": "Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior",
        "authors": [
            "Jason Toy",
            "Josh MacAdam",
            "Phil Tabor"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Artificial Intelligence"
        ],
        "abstract": "Recent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face challenges such as limited context windows and difficulties in generalization. In this paper, we introduce a metacognition module for generative agents, enabling them to observe their own thought processes and actions. This metacognitive approach, designed to emulate System 1 and System 2 cognitive processes, allows agents to significantly enhance their performance by modifying their strategy. We tested the metacognition module on a variety of scenarios, including a situation where generative agents must survive a zombie apocalypse, and observe that our system outperform others, while agents adapt and improve their strategies to complete tasks over time.",
        "comments": "9 pages, 4 figures",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10910"
    },
    {
        "doc_id": 259,
        "title": "Novel community data in ecology -- properties and prospects",
        "authors": [
            "Florian Hartig",
            "Nerea Abrego",
            "Alex Bush",
            "Jonathan M. Chase",
            "Gurutzeta Guillera-Arroita",
            "Mathew A. Leibold",
            "Otso Ovaskainen",
            "Lo\u00efc Pellissier",
            "Maximilian Pichler",
            "Giovanni Poggiato",
            "Laura Pollock",
            "Sara Si-Moussi",
            "Wilfried Thuiller",
            "Duarte S. Viana",
            "David I. Warton",
            "Damaris Zurell",
            "Douglas W. Yu"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "New technologies for acquiring biological information such as eDNA, acoustic or optical sensors, make it possible to generate spatial community observations at unprecedented scales. The potential of these novel community data to standardize community observations at high spatial, temporal, and taxonomic resolution and at large spatial scale ('many rows and many columns') has been widely discussed, but so far, there has been little integration of these data with ecological models and theory. Here, we review these developments and highlight emerging solutions, focusing on statistical methods for analyzing novel community data, in particular joint species distribution models; the new ecological questions that can be answered with these data; and the potential implications of these developments for policy and conservation.",
        "comments": "Journal ref:        Trends in Ecology & Evolution, 2024",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10860"
    },
    {
        "doc_id": 260,
        "title": "Exploring the role of structure in a time constrained decision task",
        "authors": [
            "Naomi Chaix-Eichel",
            "Gautham Venugopal",
            "Thomas Boraud",
            "Nicolas P. Rougier"
        ],
        "subjects": [
            "Neural and Evolutionary Computing",
            "Neurons and Cognition"
        ],
        "abstract": "The structure of the basal ganglia is remarkably similar across a number of species (often described in terms of direct, indirect and hyperdirect pathways) and is deeply involved in decision making and action selection. In this article, we are interested in exploring the role of structure when solving a decision task while avoiding to make any strong assumption regarding the actual structure. To do so, we exploit the echo state network paradigm that allows to solve complex task based on a random architecture. Considering a temporal decision task, the question is whether a specific structure allows for better performance and if so, whether this structure shares some similarity with the basal ganglia. Our results highlight the advantage of having a slow (direct) and a fast (hyperdirect) pathway that allows to deal with late information during a decision making task.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10849"
    },
    {
        "doc_id": 261,
        "title": "Neural Population Decoding and Imbalanced Multi-Omic Datasets For Cancer Subtype Diagnosis",
        "authors": [
            "Charles Theodore Kent",
            "Leila Bagheriye",
            "Johan Kwisthout"
        ],
        "subjects": [
            "Neural and Evolutionary Computing",
            "Machine Learning",
            "Genomics",
            "Quantitative Methods",
            "Applications"
        ],
        "abstract": "Recent strides in the field of neural computation has seen the adoption of Winner Take All (WTA) circuits to facilitate the unification of hierarchical Bayesian inference and spiking neural networks as a neurobiologically plausible model of information processing. Current research commonly validates the performance of these networks via classification tasks, particularly of the MNIST dataset. However, researchers have not yet reached consensus about how best to translate the stochastic responses from these networks into discrete decisions, a process known as population decoding. Despite being an often underexamined part of SNNs, in this work we show that population decoding has a significanct impact on the classification performance of WTA networks. For this purpose, we apply a WTA network to the problem of cancer subtype diagnosis from multi omic data, using datasets from The Cancer Genome Atlas (TCGA). In doing so we utilise a novel implementation of gene similarity networks, a feature encoding technique based on Kohoens self organising map algorithm. We further show that the impact of selecting certain population decoding methods is amplified when facing imbalanced datasets.",
        "comments": "This paper has been accepted in BIOINFORMATICS 2024 (BIOSTEC 2024)",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10844"
    },
    {
        "doc_id": 262,
        "title": "DeepRLI: A Multi-objective Framework for Universal Protein--Ligand Interaction Prediction",
        "authors": [
            "Haoyu Lin",
            "Shiwei Wang",
            "Jintao Zhu",
            "Yibo Li",
            "Jianfeng Pei",
            "Luhua Lai"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "Protein (receptor)--ligand interaction prediction is a critical component in computer-aided drug design, significantly influencing molecular docking and virtual screening processes. Despite the development of numerous scoring functions in recent years, particularly those employing machine learning, accurately and efficiently predicting binding affinities for protein--ligand complexes remains a formidable challenge. Most contemporary methods are tailored for specific tasks, such as binding affinity prediction, binding pose prediction, or virtual screening, often failing to encompass all aspects. In this study, we put forward DeepRLI, a novel protein--ligand interaction prediction architecture. It encodes each protein--ligand complex into a fully connected graph, retaining the integrity of the topological and spatial structure, and leverages the improved graph transformer layers with cosine envelope as the central module of the neural network, thus exhibiting superior scoring power. In order to equip the model to generalize to conformations beyond the confines of crystal structures and to adapt to molecular docking and virtual screening tasks, we propose a multi-objective strategy, that is, the model outputs three scores for scoring and ranking, docking, and screening, and the training process optimizes these three objectives simultaneously. For the latter two objectives, we augment the dataset through a docking procedure, incorporate suitable physics-informed blocks and employ an effective contrastive learning approach. Eventually, our model manifests a balanced performance across scoring, ranking, docking, and screening, thereby demonstrating its ability to handle a range of tasks. Overall, this research contributes a multi-objective framework for universal protein--ligand interaction prediction, augmenting the landscape of structure-based drug design.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10806"
    },
    {
        "doc_id": 263,
        "title": "FIMBA: Evaluating the Robustness of AI in Genomics via Feature Importance Adversarial Attacks",
        "authors": [
            "Heorhii Skovorodnikov",
            "Hoda Alkhzaimi"
        ],
        "subjects": [
            "Machine Learning",
            "Cryptography and Security",
            "Genomics"
        ],
        "abstract": "With the steady rise of the use of AI in bio-technical applications and the widespread adoption of genomics sequencing, an increasing amount of AI-based algorithms and tools is entering the research and production stage affecting critical decision-making streams like drug discovery and clinical outcomes. This paper demonstrates the vulnerability of AI models often utilized downstream tasks on recognized public genomics datasets. We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance. Further, we enhance our approach by generating poisoned data using a variational autoencoder-based model. Our empirical findings unequivocally demonstrate a decline in model performance, underscored by diminished accuracy and an upswing in false positives and false negatives. Furthermore, we analyze the resulting adversarial samples via spectral analysis yielding conclusions for countermeasures against such attacks.",
        "comments": "15 pages, core code available at: https://github.com/HeorhiiS/fimba-attack",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10657"
    },
    {
        "doc_id": 264,
        "title": "Exact analytical algorithm for solvent accessible surface area and derivatives in implicit solvent molecular simulations on GPUs",
        "authors": [
            "Xin Cao",
            "Michelle H. Hummel",
            "Yuzhang Wang",
            "Carlos Simmerling",
            "Evangelos A. Coutsias"
        ],
        "subjects": [
            "Biomolecules"
        ],
        "abstract": "In this paper, we present dSASA (differentiable SASA), an exact geometric method to calculate solvent accessible surface area (SASA) analytically along with atomic derivatives on GPUs. The atoms in a molecule are first assigned to tetrahedra in groups of four atoms by Delaunay tetrahedrization adapted for efficient GPU implementation and the SASA values for atoms and molecules are calculated based on the tetrahedrization information and inclusion-exclusion method. The SASA values from the numerical icosahedral-based method can be reproduced with more than 98% accuracy for both proteins and RNAs. Having been implemented on GPUs and incorporated into the software Amber, we can apply dSASA to implicit solvent molecular dynamics simulations with inclusion of this nonpolar term. The current GPU version of GB/SA simulations has been accelerated up to nearly 20-fold compared to the CPU version and it outperforms LCPO as the system size increases. The performance and importance of the nonpolar part in implicit solvent modeling are demonstrated in GB/SA simulations of proteins and accurate SASA calculation of nucleic acids.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10462"
    },
    {
        "doc_id": 265,
        "title": "Ecosystem models cannot predict the consequences of conservation decisions",
        "authors": [
            "Larissa Lubiana Botelho",
            "Cailan Jeynes-Smith",
            "Sarah Vollert",
            "Michael Bode"
        ],
        "subjects": [
            "Populations and Evolution",
            "Quantitative Methods"
        ],
        "abstract": "Ecosystem models are often used to predict the consequences of management decisions in applied ecology, including fisheries management and threatened species conservation. These models are high-dimensional, parameter-rich, and nonlinear, yet limited data is available to calibrate them, and they are rarely tested or validated. Consequently, the accuracy of their forecasts, and their utility as decision-support tools is a matter of debate. In this paper, we calibrate ecosystem models to time-series data from 110 different experimental microcosm ecosystems, each containing between three and five interacting species. We then assess how often these calibrated models offer accurate and useful predictions about how the ecosystem will respond to a set of standard management interventions. Our results show that for each timeseries dataset, a large number of very different parameter sets offer equivalent, good fits. However, these calibrated ecosystem models have poor predictive accuracy when forecasting future dynamics and offer ambiguous predictions about how species in the ecosystem will respond to management interventions. Closer inspection reveals that the ecosystem models fail because calibration cannot determine the types of interactions that occur within the ecosystem. Our findings call into question claims that ecosystem modelling can support applied ecological decision-making when they are calibrated against real-world datasets.",
        "comments": "23 pages (main text + supplementary material) 9 figures (main text + supplementary material)",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10439"
    },
    {
        "doc_id": 266,
        "title": "Diffusion of intrinsically disordered proteins within viscoelastic membraneless droplets",
        "authors": [
            "Fuga Watanabe",
            "Takuma Akimoto",
            "Robert B. Best",
            "Kresten Lindorff-Larsen",
            "Ralf Metzler",
            "Eiji Yamamoto"
        ],
        "subjects": [
            "Soft Condensed Matter",
            "Statistical Mechanics",
            "Biological Physics",
            "Computational Physics",
            "Biomolecules"
        ],
        "abstract": "In living cells, intrinsically disordered proteins (IDPs), such as FUS and DDX4, undergo phase separation, forming biomolecular condensates. Using molecular dynamics simulations, we investigate their behavior in their respective homogenous droplets. We find that the proteins exhibit transient subdiffusion due to the viscoelastic nature and confinement effects in the droplets. The conformation and the instantaneous diffusivity of the proteins significantly vary between the interior and the interface of the droplet, resulting in non-Gaussianity in the displacement distributions. This study highlights key aspects of IDP behavior in biomolecular condensates.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10438"
    },
    {
        "doc_id": 267,
        "title": "Exploring General Intelligence via Gated Graph Transformer in Functional Connectivity Studies",
        "authors": [
            "Gang Qu",
            "Anton Orlichenko",
            "Junqi Wang",
            "Gemeng Zhang",
            "Li Xiao",
            "Aiying Zhang",
            "Zhengming Ding",
            "Yu-Ping Wang"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Artificial Intelligence"
        ],
        "abstract": "Functional connectivity (FC) as derived from fMRI has emerged as a pivotal tool in elucidating the intricacies of various psychiatric disorders and delineating the neural pathways that underpin cognitive and behavioral dynamics inherent to the human brain. While Graph Neural Networks (GNNs) offer a structured approach to represent neuroimaging data, they are limited by their need for a predefined graph structure to depict associations between brain regions, a detail not solely provided by FCs. To bridge this gap, we introduce the Gated Graph Transformer (GGT) framework, designed to predict cognitive metrics based on FCs. Empirical validation on the Philadelphia Neurodevelopmental Cohort (PNC) underscores the superior predictive prowess of our model, further accentuating its potential in identifying pivotal neural connectivities that correlate with human cognitive processes.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10348"
    },
    {
        "doc_id": 268,
        "title": "DrugAssist: A Large Language Model for Molecule Optimization",
        "authors": [
            "Geyan Ye",
            "Xibao Cai",
            "Houtim Lai",
            "Xing Wang",
            "Junhong Huang",
            "Longyue Wang",
            "Wei Liu",
            "Xiangxiang Zeng"
        ],
        "subjects": [
            "Quantitative Methods",
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning"
        ],
        "abstract": "Recently, the impressive performance of large language models (LLMs) on a wide range of tasks has attracted an increasing number of attempts to apply LLMs in drug discovery. However, molecule optimization, a critical task in the drug discovery pipeline, is currently an area that has seen little involvement from LLMs. Most of existing approaches focus solely on capturing the underlying patterns in chemical structures provided by the data, without taking advantage of expert feedback. These non-interactive approaches overlook the fact that the drug discovery process is actually one that requires the integration of expert experience and iterative refinement. To address this gap, we propose DrugAssist, an interactive molecule optimization model which performs optimization through human-machine dialogue by leveraging LLM's strong interactivity and generalizability. DrugAssist has achieved leading results in both single and multiple property optimization, simultaneously showcasing immense potential in transferability and iterative optimization. In addition, we publicly release a large instruction-based dataset called MolOpt-Instructions for fine-tuning language models on molecule optimization tasks. We have made our code and data publicly available at https://github.com/blazerye/DrugAssist, which we hope to pave the way for future research in LLMs' application for drug discovery.",
        "comments": "Geyan Ye and Xibao Cai are equal contributors; Longyue Wang is corresponding author",
        "date": "28 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10334"
    },
    {
        "doc_id": 269,
        "title": "Fine scale depth regulation of invertebrate larvae around coastal fronts",
        "authors": [
            "Nicolas Weidberg",
            "Wayne Goschen",
            "Jennifer M. Jackson",
            "Paula Pattrick",
            "Christopher D. McQuaid",
            "Francesca Porri"
        ],
        "subjects": [
            "Quantitative Methods"
        ],
        "abstract": "Vertical migrations of zooplankters have been widely described, but their active movements through shallow, highly dynamic water columns within the inner shelf may be more complex and difficult to characterize. In this study, invertebrate larvae, currents, and hydrographic variables were sampled at different depths during and after the presence of fronts on three different cruises off the southern coast of South Africa. Internal wave dynamics were observed in the hydrographic data set but also through satellite imagery, although strong surface convergent currents were absent and thermal stratification was weak. During the first two cruises, fronts were more conspicuous and they preceded strong onshore currents at depth which developed with the rising tide. Vertical distributions of larvae changed accordingly, with higher abundances at these deep layers once the front disappeared. The third cruise was carried out during slack tides, the front was not conspicuous, deep strong onshore currents did not occur afterward and larval distributions did not change consistently through time. Overall, the vertical distributions of many larval taxa matched the vertical profiles of shoreward currents and multivariate analyses revealed that these flows structured the larval community, which was neither influenced by temperature nor chlorophyll. Thus, the ability to regulate active vertical positioning may enhance shoreward advection and determine nearshore larval distributions.",
        "comments": "Journal ref:        Limnology and Oceanography. 64 - 2, pp. 785 - 802, 2019",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10303"
    },
    {
        "doc_id": 270,
        "title": "Forecasting dengue outbreaks with uncertainty using seasonal weather patterns",
        "authors": [
            "Piumi Chathurangika",
            "Sanjeewa Perera",
            "Kushani De Silva"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "Dengue is a vector-borne disease transmitted to humans by vectors of genus Aedes and is a global threat with health, social, and economic impact in many of the tropical countries including Sri Lanka. The virus transmission is significantly impacted by environmental conditions, with a notable contribution from elevated per-capita vector density. These conditions are dynamic in nature and specially having the tropical climate, Sri Lanka experiences seasonal weather patterns dominated by monsoons. In this work, we investigate the dynamic influence of environmental conditions on dengue emergence in Colombo district where dengue is extremely prevalent in Sri Lanka. A novel approach leveraging the Markov chain Monte Carlo simulations has been employed to identify seasonal patterns of dengue disease emergence, utilizing the dynamics of weather patterns governing in the region. The newly developed algorithm allows us to estimate the timing of dengue outbreaks with uncertainty, enabling accurate forecasts of upcoming disease emergence patterns for better preparedness.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10295"
    },
    {
        "doc_id": 271,
        "title": "Mechanisms of nearshore retention and offshore export of mussel larvae over the Agulhas Bank",
        "authors": [
            "Nicolas Weidberg",
            "Francesca Porri",
            "Charles von der Meden",
            "Jennifer M. Jackson",
            "Wayne Goschen",
            "Christopher McQuaid"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "Ecological connectivity is critical for population dynamics but in many benthic species it is complicated by a planktonic larval phase, whose dispersal remains poorly understood. Using a plankton pump, we examine the distribution of intertidal mussel larvae along three axes: alongshore, cross-shelf and by depth during a large scale (600 km) cruise over the Agulhas Bank off southern Africa in August/September 2010. As a general pattern, higher veliger abundances were found close to the coast. Our analyses of the nearshore flow, estimated from ADCP data and the vertical distribution of larvae, show that onshore larval retention may be mediated by active vertical swimming through the water column guided by light and wind-induced turbulence. A massive offshore export of larvae off St Francis Bay was, however, observed during an Agulhas Current meander which influenced inner shelf waters. We hypothesize that, by increasing and homogenizing flow, the Agulhas Current may erase the effects of larval vertical positioning on onshore retention and transport larvae offshore. Our study highlights the need to integrate the effects of complex, region-specific physical dynamics with the swimming behaviour of larvae in order to explain their spatial distribution, population connectivity and the consequences for population dynamics.",
        "comments": "Journal ref:        Journal of Plankton Research. 37 - 6, pp. 1166 - 1180. Oxford Journals, 11/2015",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10292"
    },
    {
        "doc_id": 272,
        "title": "Analyzing Brain Activity During Learning Tasks with EEG and Machine Learning",
        "authors": [
            "Ryan Cho",
            "Mobasshira Zaman",
            "Kyu Taek Cho",
            "Jaejin Hwang"
        ],
        "subjects": [
            "Signal Processing",
            "Machine Learning",
            "Neurons and Cognition"
        ],
        "abstract": "This study aimed to analyze brain activity during various STEM activities, exploring the feasibility of classifying between different tasks. EEG brain data from twenty subjects engaged in five cognitive tasks were collected and segmented into 4-second clips. Power spectral densities of brain frequency waves were then analyzed. Testing different k-intervals with XGBoost, Random Forest, and Bagging Classifier revealed that Random Forest performed best, achieving a testing accuracy of 91.07% at an interval size of two. When utilizing all four EEG channels, cognitive flexibility was most recognizable. Task-specific classification accuracy showed the right frontal lobe excelled in mathematical processing and planning, the left frontal lobe in cognitive flexibility and mental flexibility, and the left temporoparietal lobe in connections. Notably, numerous connections between frontal and temporoparietal lobes were observed during STEM activities. This study contributes to a deeper understanding of implementing machine learning in analyzing brain activity and sheds light on the brain's mechanisms.",
        "comments": "20 pages, 7 figures",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10285"
    },
    {
        "doc_id": 273,
        "title": "EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model",
        "authors": [
            "Yuqi Chen",
            "Kan Ren",
            "Kaitao Song",
            "Yansen Wang",
            "Yifan Wang",
            "Dongsheng Li",
            "Lili Qiu"
        ],
        "subjects": [
            "Signal Processing",
            "Artificial Intelligence",
            "Machine Learning",
            "Multimedia",
            "Neurons and Cognition"
        ],
        "abstract": "Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance on various downstream tasks but also provide interpretable outcomes of the useful patterns within the data. To validate the effectiveness of our model, we extensively evaluate it on various downstream tasks and assess the performance under different transfer settings. Furthermore, we demonstrate how the learned model exhibits transferable anomaly detection performance and provides valuable interpretability of the acquired patterns via self-supervised learning.",
        "comments": "A preprint version of an ongoing work",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10278"
    },
    {
        "doc_id": 274,
        "title": "Evolving Diploid Boolean and Multi-Valued Gene Networks",
        "authors": [
            "Larry Bull"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "Boolean networks have been widely used to explore aspects of gene regulation, traditionally with a single network. A modified form of the model to explore the effects of increasing the number of gene states has also recently been introduced. In this paper, these discrete dynamical networks are evolved as diploids within rugged fitness landscapes to explore their behaviour. Results suggest the general properties of haploid networks in similar circumstances remain for diploids. The previously proposed inherent fitness landscape smoothing properties of eukaryotic sex are shown to be exhibited in these dynamical systems, as is their propensity to change in size based upon the characteristics of the network and fitness landscape.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2302.01694",
        "date": "19 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10237"
    },
    {
        "doc_id": 275,
        "title": "Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products",
        "authors": [
            "Shengjie Luo",
            "Tianlang Chen",
            "Aditi S. Krishnapriyan"
        ],
        "subjects": [
            "Machine Learning",
            "Materials Science",
            "Group Theory",
            "Chemical Physics",
            "Biomolecules"
        ],
        "abstract": "Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations increases significantly as higher-order tensors are used. In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps. We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics. Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics. This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis. Consequently, the multiplication between spherical functions represented by a 2D Fourier basis can be efficiently computed via the convolution theorem and Fast Fourier Transforms. This transformation reduces the complexity of full tensor products of irreps from $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^3)$, where $L$ is the max degree of irreps. Leveraging this approach, we introduce the Gaunt Tensor Product, which serves as a new method to construct efficient equivariant operations across different model architectures. Our experiments on the Open Catalyst Project and 3BPA datasets demonstrate both the increased efficiency and improved performance of our approach.",
        "comments": "36 pages; ICLR 2024 (Spotlight Presentation); Code: https://github.com/lsj2408/Gaunt-Tensor-Product",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10216"
    },
    {
        "doc_id": 276,
        "title": "Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation",
        "authors": [
            "Zhengyi Li",
            "Menglu Li",
            "Lida Zhu",
            "Wen Zhang"
        ],
        "subjects": [
            "Quantitative Methods",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "abstract": "Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context sequence information, and motif generated by aligning all context sequences of PTM sites assists the prediction. Extensive experiments on three datasets show that PTM-CMGMS outperforms the state-of-the-art methods.",
        "comments": " ",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10211"
    },
    {
        "doc_id": 277,
        "title": "Exploiting Hierarchical Interactions for Protein Surface Learning",
        "authors": [
            "Yiqun Lin",
            "Liang Pan",
            "Yi Li",
            "Ziwei Liu",
            "Xiaomeng Li"
        ],
        "subjects": [
            "Biomolecules",
            "Machine Learning"
        ],
        "abstract": "Predicting interactions between proteins is one of the most important yet challenging problems in structural bioinformatics. Intrinsically, potential function sites in protein surfaces are determined by both geometric and chemical features. However, existing works only consider handcrafted or individually learned chemical features from the atom type and extract geometric features independently. Here, we identify two key properties of effective protein surface learning: 1) relationship among atoms: atoms are linked with each other by covalent bonds to form biomolecules instead of appearing alone, leading to the significance of modeling the relationship among atoms in chemical feature learning. 2) hierarchical feature interaction: the neighboring residue effect validates the significance of hierarchical feature interaction among atoms and between surface points and atoms (or residues). In this paper, we present a principled framework based on deep learning techniques, namely Hierarchical Chemical and Geometric Feature Interaction Network (HCGNet), for protein surface analysis by bridging chemical and geometric features with hierarchical interactions. Extensive experiments demonstrate that our method outperforms the prior state-of-the-art method by 2.3% in site prediction task and 3.2% in interaction matching task, respectively. Our code is available at https://github.com/xmed-lab/HCGNet.",
        "comments": "Accepted to J-BHI",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10144"
    },
    {
        "doc_id": 278,
        "title": "Correlating fluorescence microscopy, optical and magnetic tweezers to study single chiral biopolymers, tested on DNA plectoneme formation dynamics",
        "authors": [
            "Jack W Shepherd",
            "Sebastien Guilbaud",
            "Zhaokun Zhou",
            "Jamieson Howard",
            "Matthew Burman",
            "Charley Schaefer",
            "Adam Kerrigan",
            "Clare Steele-King",
            "Agnes Noy",
            "Mark C Leake"
        ],
        "subjects": [
            "Biological Physics",
            "Biomolecules"
        ],
        "abstract": "Biopolymer topology is critical for determining interactions inside cell environments, exemplified by DNA where its response to mechanical perturbation is as important as biochemical properties to its cellular roles. The dynamic structures of chiral biopolymers exhibit complex dependence with extension and torsion, however the physical mechanisms underpinning the emergence of structural motifs upon physiological twisting and stretching are poorly understood due to technological limitations in correlating force, torque and spatial localization information. We present COMBI-Tweez (Combined Optical and Magnetic BIomolecule TWEEZers), a transformative tool that overcomes these challenges by integrating optical trapping, time-resolved electromagnetic tweezers, and fluorescence microscopy, demonstrated on single DNA molecules, that can controllably form and visualise higher order structural motifs including plectonemes. This technology combined with cutting-edge MD simulations provides quantitative insight into complex dynamic structures relevant to DNA cellular processes and can be adapted to study a range of filamentous biopolymers.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10087"
    },
    {
        "doc_id": 279,
        "title": "GPU Acceleration of a Conjugate Exponential Model for Cancer Tissue Heterogeneity",
        "authors": [
            "Anik Chaudhuri",
            "Anwoy Mohanty",
            "Manoranjan Satpathy"
        ],
        "subjects": [
            "Distributed, Parallel, and Cluster Computing",
            "Quantitative Methods"
        ],
        "abstract": "Heterogeneity in the cell population of cancer tissues poses many challenges in cancer diagnosis and treatment. Studying the heterogeneity in cell populations from gene expression measurement data in the context of cancer research is a problem of paramount importance. In addition, reducing the computation time of the algorithms that deal with high volumes of data has its obvious merits. Parallelizable models using Markov chain Monte Carlo methods are typically slow. This paper shows a novel, computationally efficient, and parallelizable model to analyze heterogeneity in cancer tissues using GPUs. Because our model is parallelizable, the input data size does not affect the computation time much, provided the hardware resources are not exhausted. Our model uses qPCR (quantitative polymerase chain reaction) gene expression measurements to study heterogeneity in cancer tissue. We compute the cell proportion breakup by accelerating variational methods on a GPU. We test this model on synthetic and real-world gene expression data collected from fibroblasts and compare the performance of our algorithm with those of MCMC and Expectation Maximization. Our new model is computationally less complex and faster than existing Bayesian models for cancer tissue heterogeneity.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10068"
    },
    {
        "doc_id": 280,
        "title": "Cardiac Digital Twin Pipeline for Virtual Therapy Evaluation",
        "authors": [
            "Julia Camps",
            "Zhinuo Jenny Wang",
            "Ruben Doste",
            "Maxx Holmes",
            "Brodie Lawson",
            "Jakub Tomek",
            "Kevin Burrage",
            "Alfonso Bueno-Orovio",
            "Blanca Rodriguez"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Tissues and Organs"
        ],
        "abstract": "Cardiac digital twins are computational tools capturing key functional and anatomical characteristics of patient hearts for investigating disease phenotypes and predicting responses to therapy. When paired with large-scale computational resources and large clinical datasets, digital twin technology can enable virtual clinical trials on virtual cohorts to fast-track therapy development. Here, we present an automated pipeline for personalising ventricular anatomy and electrophysiological function based on routinely acquired cardiac magnetic resonance (CMR) imaging data and the standard 12-lead electrocardiogram (ECG). Using CMR-based anatomical models, a sequential Monte-Carlo approximate Bayesian computational inference method is extended to infer electrical activation and repolarisation characteristics from the ECG. Fast simulations are conducted with a reaction-Eikonal model, including the Purkinje network and biophysically-detailed subcellular ionic current dynamics for repolarisation. For each patient, parameter uncertainty is represented by inferring a population of ventricular models rather than a single one, which means that parameter uncertainty can be propagated to therapy evaluation. Furthermore, we have developed techniques for translating from reaction-Eikonal to monodomain simulations, which allows more realistic simulations of cardiac electrophysiology. The pipeline is demonstrated in a healthy female subject, where our inferred reaction-Eikonal models reproduced the patient's ECG with a Pearson's correlation coefficient of 0.93, and the translated monodomain simulations have a correlation coefficient of 0.89. We then apply the effect of Dofetilide to the monodomain population of models for this subject and show dose-dependent QT and T-peak to T-end prolongations that are in keeping with large population drug response data.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10029"
    },
    {
        "doc_id": 281,
        "title": "An optimization-based equilibrium measure describes non-equilibrium steady state dynamics: application to edge of chaos",
        "authors": [
            "Junbin Qiu",
            "Haiping Huang"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Statistical Mechanics",
            "Neural and Evolutionary Computing"
        ],
        "abstract": "Understanding neural dynamics is a central topic in machine learning, non-linear physics and neuroscience. However, the dynamics is non-linear, stochastic and particularly non-gradient, i.e., the driving force can not be written as gradient of a potential. These features make analytic studies very challenging. The common tool is to use path integral approach or dynamical mean-field theory, but the drawback is one has to solve the integro-differential or dynamical mean-field equations, which is computationally expensive and has no closed form solutions in general. From the aspect of associated Fokker-Planck equation, the steady state solution is generally unknown. Here, we treat searching for the steady state as an optimization problem, and construct an approximate potential closely related to the speed of the dynamics, and find that searching for the ground state of this potential is equivalent to running a stochastic gradient dynamics. The resultant stationary state follows exactly the canonical Boltzmann measure. Within this framework, the quenched disorder intrinsic in the neural networks can be averaged out by applying the replica method. Our theory reproduces the well-known result of edge-of-chaos, and further the order parameters characterizing the continuous transition are derived, and different scaling behavior with respect to inverse temperature in both sides of the transition is also revealed. Our method opens the door to analytically study the steady state landscape of the deterministic or stochastic high dimensional dynamics.",
        "comments": "16 pages, 7 figures",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10009"
    },
    {
        "doc_id": 282,
        "title": "Artificial Intelligence-based algorithms in medical image scan seg-mentation and intelligent visual-content generation -- a concise overview",
        "authors": [
            "Zofia Rudnicka",
            "Janusz Szczepanski",
            "Agnieszka Pregowska"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Recently, Artificial Intelligence (AI)-based algorithms have revolutionized the medical image segmentation processes. Thus, the precise segmentation of organs and their lesions may contribute to an efficient diagnostics process and a more effective selection of targeted therapies as well as increasing the effectiveness of the training process. In this context, AI may contribute to the automatization of the image scan segmentation process and increase the quality of the resulting 3D objects, which may lead to the generation of more realistic virtual objects. In this paper, we focus on the AI-based solutions applied in the medical image scan segmentation, and intelligent visual-content generation, i.e. computer-generated three-dimensional (3D) images in the context of Extended Reality (XR). We consider different types of neural networks used with a special emphasis on the learning rules applied, taking into account algorithm accuracy and performance, as well as open data availability. This paper attempts to summarize the current development of AI-based segmentation methods in medical imaging and intelligent visual content generation that are applied in XR. It concludes also with possible developments and open challenges in AI application in Extended Reality-based solutions. Finally, the future lines of research and development directions of Artificial Intelligence applications both in medical image segmentation and Extended Reality-based medical solutions are discussed",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09857"
    },
    {
        "doc_id": 283,
        "title": "FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction",
        "authors": [
            "Alexander Telepov",
            "Artem Tsypin",
            "Kuzma Khrabrov",
            "Sergey Yakukhnov",
            "Pavel Strashnov",
            "Petr Zhilyaev",
            "Egor Rumiantsev",
            "Daniel Ezhov",
            "Manvel Avetisian",
            "Olga Popova",
            "Artur Kadurin"
        ],
        "subjects": [
            "Biomolecules",
            "Machine Learning",
            "Machine Learning"
        ],
        "abstract": "A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (arXiv:2110.01219). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches.",
        "comments": "37 pages, 10 figures, to be published in TMLR journal (https://www.jmlr.org/tmlr/)",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09840"
    },
    {
        "doc_id": 284,
        "title": "The impact of Covid-19 vaccination in Aotearoa New Zealand: a modelling study",
        "authors": [
            "Samik Datta",
            "Giorgia Vattiato",
            "Oliver J Maclaren",
            "Ning Hua",
            "Andrew Sporle",
            "Michael J Plank"
        ],
        "subjects": [
            "Populations and Evolution",
            "Physics and Society"
        ],
        "abstract": "Aotearoa New Zealand implemented a Covid-19 elimination strategy in 2020 and 2021, which enabled a large majority of the population to be vaccinated before being exposed to the virus. This strategy delivered one of the lowest pandemic mortality rates in the world. However, quantitative estimates of the population-level health benefits of vaccination are lacking. Here, we use a validated mathematical model to investigate counterfactual scenarios with differing levels of vaccine coverage in different age and ethnicity groups. The model builds on earlier research by adding age- and time-dependent case ascertainment, the effect of antiviral medications, improved hospitalisation rate estimates, and the impact of relaxing control measures. The model was used for scenario analysis and policy advice for the New Zealand Government in 2022 and 2023. We compare the number of Covid-19 hospitalisations, deaths, and years of life lost in each counterfactual scenario to a baseline scenario that is fitted to epidemiological data between January 2022 and June 2023. Our results estimate that vaccines saved 6650 (95% credible interval [4424, 10180]) lives, and prevented 74500 [51000, 115400] years of life lost and 45100 [34400, 55600] hospitalisations during this 18-month period. Making the same comparison before the benefit of antiviral medications is accounted for, the estimated number of lives saved by vaccines increases to 7604 [5080, 11942]. Due to inequities in the vaccine rollout, vaccination rates among M\u0101ori were lower than in people of European ethnicity. Our results show that, if vaccination rates had been equitable, an estimated 11-26% of the 292 M\u0101ori Covid-19 deaths that were recorded in this time period could have been prevented. We conclude that Covid-19 vaccination greatly reduced health burden in New Zealand and that equity needs to be a key focus of future vaccination programmes.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09679"
    },
    {
        "doc_id": 285,
        "title": "Functional Linear Non-Gaussian Acyclic Model for Causal Discovery",
        "authors": [
            "Tian-Le Yang",
            "Kuang-Yao Lee",
            "Kun Zhang",
            "Joe Suzuki"
        ],
        "subjects": [
            "Machine Learning",
            "Statistics Theory",
            "Neurons and Cognition",
            "Methodology"
        ],
        "abstract": "In causal discovery, non-Gaussianity has been used to characterize the complete configuration of a Linear Non-Gaussian Acyclic Model (LiNGAM), encompassing both the causal ordering of variables and their respective connection strengths. However, LiNGAM can only deal with the finite-dimensional case. To expand this concept, we extend the notion of variables to encompass vectors and even functions, leading to the Functional Linear Non-Gaussian Acyclic Model (Func-LiNGAM). Our motivation stems from the desire to identify causal relationships in brain-effective connectivity tasks involving, for example, fMRI and EEG datasets. We demonstrate why the original LiNGAM fails to handle these inherently infinite-dimensional datasets and explain the availability of functional data analysis from both empirical and theoretical perspectives. {We establish theoretical guarantees of the identifiability of the causal relationship among non-Gaussian random vectors and even random functions in infinite-dimensional Hilbert spaces.} To address the issue of sparsity in discrete time points within intrinsic infinite-dimensional functional data, we propose optimizing the coordinates of the vectors using functional principal component analysis. Experimental results on synthetic data verify the ability of the proposed framework to identify causal relationships among multivariate functions using the observed samples. For real data, we focus on analyzing the brain connectivity patterns derived from fMRI data.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09641"
    },
    {
        "doc_id": 286,
        "title": "Molecular causality in the advent of foundation models",
        "authors": [
            "Sebastian Lobentanzer",
            "Pablo Rodriguez-Mier",
            "Stefan Bauer",
            "Julio Saez-Rodriguez"
        ],
        "subjects": [
            "Molecular Networks"
        ],
        "abstract": "Correlation is not causation. As simple as this widely agreed-upon statement may seem, scientifically defining causality and using it to drive our modern biomedical research is immensely challenging. In this perspective, we attempt to synergise the partly disparate fields of systems biology, causal reasoning, and machine learning, to inform future approaches in the field of systems biology and molecular networks.",
        "comments": "22 pages, 0 figures, 87 references; submitted to MSB",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09558"
    },
    {
        "doc_id": 287,
        "title": "Dimensional Neuroimaging Endophenotypes: Neurobiological Representations of Disease Heterogeneity Through Machine Learning",
        "authors": [
            "Junhao Wen",
            "Mathilde Antoniades",
            "Zhijian Yang",
            "Gyujoon Hwang",
            "Ioanna Skampardoni",
            "Rongguang Wang",
            "Christos Davatzikos"
        ],
        "subjects": [
            "Machine Learning",
            "Image and Video Processing",
            "Quantitative Methods"
        ],
        "abstract": "Machine learning has been increasingly used to obtain individualized neuroimaging signatures for disease diagnosis, prognosis, and response to treatment in neuropsychiatric and neurodegenerative disorders. Therefore, it has contributed to a better understanding of disease heterogeneity by identifying disease subtypes that present significant differences in various brain phenotypic measures. In this review, we first present a systematic literature overview of studies using machine learning and multimodal MRI to unravel disease heterogeneity in various neuropsychiatric and neurodegenerative disorders, including Alzheimer disease, schizophrenia, major depressive disorder, autism spectrum disorder, multiple sclerosis, as well as their potential in transdiagnostic settings. Subsequently, we summarize relevant machine learning methodologies and discuss an emerging paradigm which we call dimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological heterogeneity of neuropsychiatric and neurodegenerative disorders into a low dimensional yet informative, quantitative brain phenotypic representation, serving as a robust intermediate phenotype (i.e., endophenotype) largely reflecting underlying genetics and etiology. Finally, we discuss the potential clinical implications of the current findings and envision future research avenues.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09517"
    },
    {
        "doc_id": 288,
        "title": "Is the Emergence of Life an Expected Phase Transition in the Evolving Universe?",
        "authors": [
            "Stuart Kauffman",
            "Andrea Roli"
        ],
        "subjects": [
            "Populations and Evolution",
            "Biological Physics"
        ],
        "abstract": "We propose a novel definition of life in terms of which its emergence in the universe is expected, and its ever-creative open-ended evolution is entailed by no law. Living organisms are Kantian Wholes that achieve Catalytic Closure, Constraint Closure, and Spatial Closure. We here unite for the first time two established mathematical theories, namely Collectively Autocatalytic Sets and the Theory of the Adjacent Possible. The former establishes that a first-order phase transition to molecular reproduction is expected in the chemical evolution of the universe where the diversity and complexity of molecules increases; the latter posits that, under loose hypotheses, if the system starts with a small number of beginning molecules, each of which can combine with copies of itself or other molecules to make new molecules, over time the number of kinds of molecules increases slowly but then explodes upward hyperbolically. Together these theories imply that life is expected as a phase transition in the evolving universe. The familiar distinction between software and hardware loses its meaning in living cells. We propose new ways to study the phylogeny of metabolisms, new astronomical ways to search for life on exoplanets, new experiments to seek the emergence of the most rudimentary life, and the hint of a coherent testable pathway to prokaryotes with template replication and coding.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09514"
    },
    {
        "doc_id": 289,
        "title": "Role of Upwelling on Larval Dispersal and Productivity of Gooseneck Barnacle Populations in the Cantabrian Sea: Management Implications",
        "authors": [
            "Antonella Rivera",
            "Nicolas Weidberg",
            "Antonio F. Pardi\u00f1as",
            "Ricardo Gonzalez-Gil",
            "Luc\u0131a Garc\u0131a- Florez",
            "Jose Luis Acu\u00f1a"
        ],
        "subjects": [
            "Populations and Evolution"
        ],
        "abstract": "The effect of coastal upwelling on the recruitment and connectivity of coastal marine populations has rarely been characterized to a level of detail to be included into sound fishery management strategies. The gooseneck barnacle (Pollicipes pollicipes) fishery at the Cantabrian Coast (Northern Spain) is located at the fringes of the NW Spanish Upwelling system. This fishery is being co-managed through a fine-scale, interspersed set of protected rocks where each rock receives a distinct level of protection. Such interspersion is potentially beneficial, but the extent to which such spacing is consistent with mean larval dispersal distances is as yet unknown. We have simulated the spread of gooseneck barnacle larvae in the Central Cantabrian Coast using a high-resolution time-series of current profiles measured at a nearshore location. During a year of high upwelling activity (2009), theoretical recruitment success was 94% with peak recruitment predicted 56 km west of the emission point. However, for a year of low upwelling activity (2011) theoretical recruitment success dropped to 15.4% and peak recruitment was expected 13 km east of the emission point. This is consistent with a positive correlation between catch rates and the Integrated Upwelling Index, using a 4-year lag to allow recruits to reach commercial size. Furthermore, a net long-term westward larval transport was estimated by means of mitochondrial cytochrome c oxidase subunit I (COI) sequences for five populations in the Cantabrian Sea. Our results call into question the role of long distance dispersal, driven by the mesoscale processes in the area, in gooseneck barnacle populations and point to the prevalent role of small-scale, asymmetric connectivity more consistent with the typical scale of the co-management process in this fishery.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09513"
    },
    {
        "doc_id": 290,
        "title": "A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation",
        "authors": [
            "Nianzu Yang",
            "Kaipeng Zeng",
            "Haotian Lu",
            "Yexin Wu",
            "Zexin Yuan",
            "Shengdian Jiang",
            "Jiaxiang Wu",
            "Yimin Wang",
            "Junchi Yan"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "Neuronal morphology is essential for studying brain functioning and understanding neurodegenerative disorders. As the acquiring of real-world morphology data is expensive, computational approaches especially learning-based ones e.g. MorphVAE for morphology generation were recently studied, which are often conducted in a way of randomly augmenting a given authentic morphology to achieve plausibility. Under such a setting, this paper proposes \\textbf{MorphGrower} which aims to generate more plausible morphology samples by mimicking the natural growth mechanism instead of a one-shot treatment as done in MorphVAE. Specifically, MorphGrower generates morphologies layer by layer synchronously and chooses a pair of sibling branches as the basic generation block, and the generation of each layer is conditioned on the morphological structure of previous layers and then generate morphologies via a conditional variational autoencoder with spherical latent space. Extensive experimental results on four real-world datasets demonstrate that MorphGrower outperforms MorphVAE by a notable margin. Our code will be publicly available to facilitate future research.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09500"
    },
    {
        "doc_id": 291,
        "title": "Gene-associated Disease Discovery Powered by Large Language Models",
        "authors": [
            "Jiayu Chang",
            "Shiyu Wang",
            "Chen Ling",
            "Zhaohui Qin",
            "Liang Zhao"
        ],
        "subjects": [
            "Quantitative Methods",
            "Information Retrieval"
        ],
        "abstract": "The intricate relationship between genetic variation and human diseases has been a focal point of medical research, evidenced by the identification of risk genes regarding specific diseases. The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment. To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes. This framework aims to automate the labor-intensive process of sifting through medical literature for evidence linking genetic variations to diseases, thereby enhancing the efficiency of disease identification. Our approach involves using LLMs to conduct literature searches, summarize relevant findings, and pinpoint diseases related to specific genes. This paper details the development and application of our LLM-powered framework, demonstrating its potential in streamlining the complex process of literature retrieval and summarization to identify diseases associated with specific genetic variations.",
        "comments": "This is the official paper accepted by AAAI 2024 Workshop on Large Language Models for Biological Discoveries",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09490"
    },
    {
        "doc_id": 292,
        "title": "The Interplay Between Logical Phenomena and the Cognitive System of the Mind",
        "authors": [
            "Kazem Haghnejad Azar"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "In this article, we employ mathematical concepts as a tool to examine the phenomenon of consciousness experience and logical phenomena. Through our investigation, we aim to demonstrate that our experiences, while not confined to limitations, cannot be neatly encapsulated within a singular collection. Our conscious experience emerges as a result of the developmental and augmentative trajectory of our cognitive system. As our cognitive abilities undergo refinement and advancement, our capacity for logical thinking likewise evolves, thereby manifesting a heightened level of conscious experience. The primary objective of this article is to embark upon a profound exploration of the concept of logical experience, delving into the intricate process by which these experiences are derived from our mind.",
        "comments": " ",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09465"
    },
    {
        "doc_id": 293,
        "title": "Diffusion-Driven Generative Framework for Molecular Conformation Prediction",
        "authors": [
            "Bobin Yang",
            "Jie Deng",
            "Zhenghan Chen",
            "Ruoxue Wu"
        ],
        "subjects": [
            "Biomolecules",
            "Artificial Intelligence",
            "Machine Learning",
            "Chemical Physics"
        ],
        "abstract": "The task of deducing three-dimensional molecular configurations from their two-dimensional graph representations holds paramount importance in the fields of computational chemistry and pharmaceutical development. The rapid advancement of machine learning, particularly within the domain of deep generative networks, has revolutionized the precision of predictive modeling in this context. Traditional approaches often adopt a two-step strategy: initially estimating interatomic distances and subsequently refining the spatial molecular structure by solving a distance geometry problem. However, this sequential approach occasionally falls short in accurately capturing the intricacies of local atomic arrangements, thereby compromising the fidelity of the resulting structural models. Addressing these limitations, this research introduces a cutting-edge generative framework named \\method{}. This framework is grounded in the principles of diffusion observed in classical non-equilibrium thermodynamics. \\method{} views atoms as discrete entities and excels in guiding the reversal of diffusion, transforming a distribution of stochastic noise back into coherent molecular structures through a process akin to a Markov chain. This transformation commences with the initial representation of a molecular graph in an abstract latent space, culminating in the realization of three-dimensional structures via a sophisticated bilevel optimization scheme meticulously tailored to meet the specific requirements of the task. One of the formidable challenges in this modeling endeavor involves preserving roto-translational invariance to ensure that the generated molecular conformations adhere to the laws of physics. Extensive experimental evaluations confirm the efficacy of the proposed \\method{} in comparison to state-of-the-art methods.",
        "comments": "arXiv admin note: text overlap with arXiv:2105.07246 by other authors",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09451"
    },
    {
        "doc_id": 294,
        "title": "Regenerative Medicine for Tendon/Ligament Injuries: De Novo Equine Tendon/Ligament Neotissue Generation and Application",
        "authors": [
            "Takashi Taguchi"
        ],
        "subjects": [
            "Tissues and Organs"
        ],
        "abstract": "Tendon and ligament injuries are debilitating conditions across species. Poor regenerative capacities of these tissues limit restoration of original functions. The first study of this dissertation evaluated the effect of cellular administration on tendon/ligament injuries in horses using meta-analysis. The findings led to the second study that engineered implantable de novo tendon neotissue using equine adipose-derived multipotent stromal cells and collagen type I. The neotendon was evaluated for its biocompatibility and therapeutic potential in the third study using immunocompetent and immunocompromised rat bilateral calcaneal tendon elongation model. The fourth study investigated the therapeutic effects of neotendon in surgically-induced non-terminal equine accessory ligament of deep digital flexor tendon injury model.",
        "comments": " ",
        "date": "24 October, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.09423"
    },
    {
        "doc_id": 295,
        "title": "PERMUTOOLS: A MATLAB Package for Multivariate Permutation Testing",
        "authors": [
            "Michael J. Crosse",
            "John J. Foxe",
            "Sophie Molholm"
        ],
        "subjects": [
            "Methodology",
            "Quantitative Methods",
            "Computation"
        ],
        "abstract": "Statistical hypothesis testing and effect size measurement are routine parts of quantitative research. Advancements in computer processing power have greatly improved the capability of statistical inference through the availability of resampling methods. However, many of the statistical practices used today are based on traditional, parametric methods that rely on assumptions about the underlying population. These assumptions may not always be valid, leading to inaccurate results and misleading interpretations. Permutation testing, on the other hand, generates the sampling distribution empirically by permuting the observed data, providing distribution-free hypothesis testing. Furthermore, this approach lends itself to a powerful method for multiple comparison correction - known as max correction - which is less prone to type II errors than conventional correction methods. Parametric methods have also traditionally been utilized for estimating the confidence interval of various test statistics and effect size measures. However, these too can be estimated empirically using permutation or bootstrapping techniques. Whilst resampling methods are generally considered preferable, many popular programming languages and statistical software packages lack efficient implementations. Here, we introduce PERMUTOOLS, a MATLAB package for multivariate permutation testing and effect size measurement.",
        "comments": "7 pages, 2 figures, for PERMUTOOLS toolbox, see https://github.com/mickcrosse/PERMUTOOLS",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09401"
    },
    {
        "doc_id": 296,
        "title": "Graph-based vulnerability assessment of resting-state functional brain networks in full-term neonates",
        "authors": [
            "Mahshid Fouladivanda",
            "Kamran Kazemi",
            "Habibollah Danyali",
            "Ardalan Aarabi"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Quantitative Methods"
        ],
        "abstract": "Network disruption during early brain development can result in long-term cognitive impairments. In this study, we investigated rich-club organization in resting-state functional brain networks in full-term neonates using a multiscale connectivity analysis. We further identified the most influential nodes, also called spreaders, having higher impacts on the flow of information throughout the network. The network vulnerability to damage to rich-club (RC) connectivity within and between resting-state networks was also assessed using a graph-based vulnerability analysis. Our results revealed a rich club organization and small-world topology for resting-state functional brain networks in full term neonates, regardless of the network size. Interconnected mostly through short-range connections, functional rich-club hubs were confined to sensory-motor, cognitive-attention-salience (CAS), default mode, and language-auditory networks with an average cross-scale overlap of 36%, 20%, 15% and 12%, respectively. The majority of the functional hubs also showed high spreading potential, except for several non-RC spreaders within CAS and temporal networks. The functional networks exhibited high vulnerability to loss of RC nodes within sensorimotor cortices, resulting in a significant increase and decrease in network segregation and integration, respectively. The network vulnerability to damage to RC nodes within the language-auditory, cognitive-attention-salience, and default mode networks was also significant but relatively less prominent. Our findings suggest that the network integration in neonates can be highly compromised by damage to RC connectivity due to brain immaturity.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09255"
    },
    {
        "doc_id": 297,
        "title": "Reproducibility via neural fields of visual illusions induced by localized stimuli",
        "authors": [
            "Cyprien Tamekue",
            "Dario Prandi",
            "Yacine Chitour"
        ],
        "subjects": [
            "Neurons and Cognition",
            "Analysis of PDEs",
            "Numerical Analysis",
            "Pattern Formation and Solitons"
        ],
        "abstract": "This paper investigates the replication of experiments by Billock and Tsou [PNAS, 2007] using the controllability of neural fields of Amari-type modelling the cortical activity in the primary visual cortex (V1), focusing on a regular funnel pattern localised in the fovea or the peripheral visual field. The aim is to understand and model the visual phenomena observed in these experiments, emphasising their nonlinear nature. The study involves designing sensory inputs simulating the visual stimuli from Billock and Tsou's experiments. The after-images induced by these inputs are then theoretically and numerically studied to determine their capacity to replicate the experimentally observed visual effects. A key aspect of this research is investigating the effects induced by the nonlinear nature of neural responses. In particular, by highlighting the importance of both excitatory and inhibitory neurons in the emergence of certain visual phenomena, this study suggests that an interplay of both types of neuronal activities plays an essential role in visual processes, challenging the assumption that the latter is mainly driven by excitatory activities alone.",
        "comments": "MSC Class:          92C20; 35B36; 45A05; 45G15; 45K05; 65R20",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09108"
    },
    {
        "doc_id": 298,
        "title": "A hybrid tau-leap for simulating chemical kinetics with applications to parameter estimation",
        "authors": [
            "Thomas Trigo Trindade",
            "Konstantinos C. Zygalakis"
        ],
        "subjects": [
            "Molecular Networks",
            "Numerical Analysis",
            "Computation"
        ],
        "abstract": "We consider the problem of efficiently simulating stochastic models of chemical kinetics. The Gillespie Stochastic Simulation algorithm (SSA) is often used to simulate these models, however, in many scenarios of interest, the computational cost quickly becomes prohibitive. This is further exasperated in the Bayesian inference context when estimating parameters of chemical models, as the intractability of the likelihood requires multiple simulations of the underlying system. To deal with issues of computational complexity in this paper, we propose a novel hybrid $\u03c4$-leap algorithm for simulating well-mixed chemical systems. In particular, the algorithm uses $\u03c4$-leap when appropriate (high population densities), and SSA when necessary (low population densities, when discrete effects become non-negligible). In the intermediate regime, a combination of the two methods, which leverages the properties of the underlying Poisson formulation, is employed. As illustrated through a number of numerical experiments the hybrid $\u03c4$ offers significant computational savings when compared to SSA without however sacrificing the overall accuracy. This feature is particularly welcomed in the Bayesian inference context, as it allows for parameter estimation of stochastic chemical kinetics at reduced computational cost.",
        "comments": "25 pages, 8 figures",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09097"
    },
    {
        "doc_id": 299,
        "title": "Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction",
        "authors": [
            "Ziyang Yu",
            "Wenbing Huang",
            "Yang Liu"
        ],
        "subjects": [
            "Machine Learning",
            "Biomolecules"
        ],
        "abstract": "The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we propose a novel learning-based method called ElliDock, which predicts an elliptic paraboloid to represent the protein-protein docking interface. To be specific, our model estimates elliptic paraboloid interfaces for the two input proteins respectively, and obtains the roto-translation transformation for docking by making two interfaces coincide. By its design, ElliDock is independently equivariant with respect to arbitrary rotations/translations of the proteins, which is an indispensable property to ensure the generalization of the docking process. Experimental evaluations show that ElliDock achieves the fastest inference time among all compared methods and is strongly competitive with current state-of-the-art learning-based models such as DiffDock-PP and Multimer particularly for antibody-antigen docking.",
        "comments": "ICLR 2024",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08986"
    },
    {
        "doc_id": 300,
        "title": "From Physics to Sentience: Deciphering the Semantics of the Free-Energy Principle and Evaluating its Claims",
        "authors": [
            "Zahra Sheikhbahaee",
            "Adam Safron",
            "Casper Hesp",
            "Guillaume Dumas"
        ],
        "subjects": [
            "Neurons and Cognition"
        ],
        "abstract": "The Free-Energy Principle (FEP) [1-3] has been adopted in a variety of ambitious proposals that aim to characterize all adaptive, sentient, and cognitive systems within a unifying framework. Judging by the amount of attention it has received from the scientific community, the FEP has gained significant traction in these pursuits. The current target article represents an important iteration of this research paradigm in formally describing emergent dynamics rather than merely (quasi-)steady states. This affords more in-depth considerations of the spatio-temporal complexities of cross-scale causality - as we have encouraged and built towards in previous publications (e.g., [4-9]). In this spirit of constructive feedback, we submit a few technical comments on some of the matters that appear to require further attention, in order to improve the clarity, rigour, and applicability of this framework.",
        "comments": " ",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08873"
    },
    {
        "doc_id": 301,
        "title": "Higher order approximation of option prices in Barndorff-Nielsen and Shephard models",
        "authors": [
            "\u00c1lvaro Guinea Juli\u00e1",
            "Alet Roux"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "We present an approximation method based on the mixing formula (Hull & White 1987, Romano & Touzi 1997) for pricing European options in Barndorff-Nielsen and Shephard models. This approximation is based on a Taylor expansion of the option price. It is implemented using a recursive algorithm that allows us to obtain closed form approximations of the option price of any order (subject to technical conditions on the background driving L\u00e9vy process). This method can be used for any type of Barndorff-Nielsen and Shephard stochastic volatility model. Explicit results are presented in the case where the stationary distribution of the background driving L\u00e9vy process is inverse Gaussian or gamma. In both of these cases, the approximation compares favorably to option prices produced by the characteristic function. In particular, we also perform an error analysis of the approximation, which is partially based on the results of Das & Langren\u00e9 (2022). We obtain asymptotic results for the error of the $N^{\\text{th}}$ order approximation and error bounds when the variance process satisfies an inverse Gaussian Ornstein-Uhlenbeck process or a gamma Ornstein-Uhlenbeck process.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14390"
    },
    {
        "doc_id": 302,
        "title": "MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning",
        "authors": [
            "Junwei Su",
            "Shan Wu",
            "Jinhui Li"
        ],
        "subjects": [
            "Machine Learning",
            "General Economics",
            "Trading and Market Microstructure"
        ],
        "abstract": "In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies.",
        "comments": " ",
        "date": "25 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.14199"
    },
    {
        "doc_id": 303,
        "title": "Discrete Hawkes process with flexible residual distribution and filtered historical simulation",
        "authors": [
            "Kyungsub Lee"
        ],
        "subjects": [
            "Statistical Finance",
            "Methodology"
        ],
        "abstract": "We introduce a new model which can be considered as a extended version of the Hawkes process in a discrete sense. This model enables the integration of various residual distributions while preserving the fundamental properties of the original Hawkes process. The rich nature of this model enables a filtered historical simulation which incorporate the properties of original time series more accurately. The process naturally extends to multi-variate models with easy implementations of estimation and simulation. We investigate the effect of flexible residual distribution on estimation of high frequency financial data compared with the Hawkes process.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13890"
    },
    {
        "doc_id": 304,
        "title": "The impact of Hong Kong's anti-ELAB movement on political related firms",
        "authors": [
            "Ziqi Wang"
        ],
        "subjects": [
            "General Finance",
            "General Economics"
        ],
        "abstract": "Hong Kong's anti-ELAB movement had a significant impact on the stock market the stock price of listed companies. Using the number of protestors as the measurement of daily protesting intensity from 2019/6/6 to 2020/1/17, this paper documents that the stock price of listed companies associated with the pan-democratic parties were more negatively affected by protesting than other companies. Furthermore, this paper finds that after the implementation of the anti-mask law, protesting had a positive impact on red chips but a negative impact on companies related to pan-democracy parties. Therefore, this paper believes that after the central government and the HKSAR government adopted strict measures to stop violence and chaos, the value of the political connection of red chips became positive while the value of the connection with pan-democracy parties became negative.",
        "comments": "34 pages, 13 tables",
        "date": "28 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.13676"
    },
    {
        "doc_id": 305,
        "title": "Real-time Risk Metrics for Programmatic Stablecoin Crypto Asset-Liability Management (CALM)",
        "authors": [
            "Marcel Bluhm",
            "Adrian Cachinero Vasiljevi\u0107",
            "S\u00e9bastien Derivaux",
            "S\u00f8ren Terp H\u00f8rl\u00fcck Jessen"
        ],
        "subjects": [
            "Risk Management",
            "Cryptography and Security",
            "General Finance"
        ],
        "abstract": "Stablecoins have turned out to be the \"killer\" use case of the growing digital asset space. However, risk management frameworks, including regulatory ones, have been largely absent. In this paper, we address the critical question of measuring and managing risk in stablecoin protocols, which operate on public blockchain infrastructure. The on-chain environment makes it possible to monitor risk and automate its management via transparent smart-contracts in real-time. We propose two risk metrics covering capitalization and liquidity of stablecoin protocols. We then explore in a case-study type analysis how our risk management framework can be applied to DAI, the biggest decentralized stablecoin by market capitalisation to-date, governed by MakerDAO. Based on our findings, we recommend that the protocol explores implementing automatic capital buffer adjustments and dynamic maturity gap matching. Our analysis demonstrates the practical benefits for scalable (prudential) risk management stemming from real-time availability of high-quality, granular, tamper-resistant on-chain data in the digital asset space. We name this approach Crypto Asset-Liability Management (CALM).",
        "comments": "The authors would like to thank Professor Moorad Choudhry for review comments on an earlier draft. Submitted for the SNB-CIF Conference on Cryptoassets and Financial Innovation, 24 May 2024",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13399"
    },
    {
        "doc_id": 306,
        "title": "An Explicit Scheme for Pathwise XVA Computations",
        "authors": [
            "Lokman Abbas-Turki",
            "St\u00e9phane Cr\u00e9pey",
            "Botao Li",
            "Bouazza Saadeddine"
        ],
        "subjects": [
            "Risk Management",
            "Numerical Analysis",
            "Computational Finance",
            "Machine Learning"
        ],
        "abstract": "Motivated by the equations of cross valuation adjustments (XVAs) in the realistic case where capital is deemed fungible as a source of funding for variation margin, we introduce a simulation/regression scheme for a class of anticipated BSDEs, where the coefficient entails a conditional expected shortfall of the martingale part of the solution. The scheme is explicit in time and uses neural network least-squares and quantile regressions for the embedded conditional expectations and expected shortfall computations. An a posteriori Monte Carlo validation procedure allows assessing the regression error of the scheme at each time step. The superiority of this scheme with respect to Picard iterations is illustrated in a high-dimensional and hybrid market/default risks XVA use-case.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13314"
    },
    {
        "doc_id": 307,
        "title": "Optimizing Transition Strategies for Small to Medium Sized Portfolios",
        "authors": [
            "Nakul Upadhya",
            "Alexandre Granzer-Guay"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "This work discusses the benefits of constrained portfolio turnover strategies for small to medium-sized portfolios. We propose a dynamic multi-period model that aims to minimize transaction costs and maximize terminal wealth levels whilst adhering to strict portfolio turnover constraints. Our results demonstrate that using our framework in combination with a reasonable forecast, can lead to higher portfolio values and lower transaction costs on average when compared to a naive, single-period model. Such results were maintained given different problem cases, such as, trading horizon, assets under management, wealth levels, etc. In addition, the proposed model lends itself to a reformulation that makes use of the column generation algorithm which can be strategically leveraged to reduce complexity and solving times.",
        "comments": "All of the discussed experiments and presented results can be reproduced using our code at https://github.com/upadhyan/Portfolio-Changeover-Optimization",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.13126"
    },
    {
        "doc_id": 308,
        "title": "Reference-dependent asset pricing with a stochastic consumption-dividend ratio",
        "authors": [
            "Luca De Gennaro Aquino",
            "Xuedong He",
            "Moris Simon Strub",
            "Yuting Yang"
        ],
        "subjects": [
            "Mathematical Finance",
            "General Finance"
        ],
        "abstract": "We study a discrete-time consumption-based capital asset pricing model under expectations-based reference-dependent preferences. More precisely, we consider an endowment economy populated by a representative agent who derives utility from current consumption and from gains and losses in consumption with respect to a forward-looking, stochastic reference point. First, we consider a general model in which the agent's preferences include both contemporaneous gain-loss utility, that is, utility from the difference between current consumption and previously held expectations about current consumption, and prospective gain-loss utility, that is, utility from the difference between intertemporal beliefs about future consumption. A semi-closed form solution for equilibrium asset prices is derived for this case. We then specialize to a model in which the agent derives contemporaneous gain-loss utility only, obtaining equilibrium asset prices in closed form. Extensive numerical experiments show that, with plausible values of risk aversion and loss aversion, our models can generate equity premia that match empirical estimates. Interestingly, the models turn out to be consistent with some well-known empirical facts, namely procyclical variation in the price-dividend ratio and countercyclical variation in the conditional expected equity premium and in the conditional volatility of the equity premium. Furthermore, we find that prospective gain-loss utility is necessary for the model to predict reasonable values of the price-dividend ratio.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12856"
    },
    {
        "doc_id": 309,
        "title": "New approximate stochastic dominance approaches for Enhanced Indexation models",
        "authors": [
            "Francesco Cesarone",
            "Justo Puerto"
        ],
        "subjects": [
            "Portfolio Management",
            "Computational Finance",
            "General Finance"
        ],
        "abstract": "In this paper, we discuss portfolio selection strategies for Enhanced Indexation (EI), which are based on stochastic dominance relations. The goal is to select portfolios that stochastically dominate a given benchmark but that, at the same time, must generate some excess return with respect to a benchmark index. To achieve this goal, we propose a new methodology that selects portfolios using the ordered weighted average (OWA) operator, which generalizes previous approaches based on minimax selection rules and still leads to solving linear programming models. We also introduce a new type of approximate stochastic dominance rule and show that it implies the almost Second-order Stochastic Dominance (SSD) criterion proposed by Lizyayev and Ruszczynski (2012). We prove that our EI model based on OWA selects portfolios that dominate a given benchmark through this new form of stochastic dominance criterion. We test the performance of the obtained portfolios in an extensive empirical analysis based on real-world datasets. The computational results show that our proposed approach outperforms several SSD-based strategies widely used in the literature, as well as the global minimum variance portfolio.",
        "comments": " ",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12669"
    },
    {
        "doc_id": 310,
        "title": "From Numbers to Words: Multi-Modal Bankruptcy Prediction Using the ECL Dataset",
        "authors": [
            "Henri Arno",
            "Klaas Mulier",
            "Joke Baeck",
            "Thomas Demeester"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Computational Finance"
        ],
        "abstract": "In this paper, we present ECL, a novel multi-modal dataset containing the textual and numerical data from corporate 10K filings and associated binary bankruptcy labels. Furthermore, we develop and critically evaluate several classical and neural bankruptcy prediction models using this dataset. Our findings suggest that the information contained in each data modality is complementary for bankruptcy prediction. We also see that the binary bankruptcy prediction target does not enable our models to distinguish next year bankruptcy from an unhealthy financial situation resulting in bankruptcy in later years. Finally, we explore the use of LLMs in the context of our task. We show how GPT-based models can be used to extract meaningful summaries from the textual data but zero-shot bankruptcy prediction results are poor. All resources required to access and update the dataset or replicate our experiments are available on github.com/henriarnoUG/ECL.",
        "comments": "Presented at the 6th Workshop on Financial Technology and Natural Language Processing (FinNLP) @ IJCNLP-AACL 2023 in Bali, Indonesia",
        "date": "23 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12652"
    },
    {
        "doc_id": 311,
        "title": "Are Charter Value and Supervision Aligned? A Segmentation Analysis",
        "authors": [
            "Juan Aparicio",
            "Miguel A. Duran",
            "Ana Lozano-Vivas",
            "Jesus T. Pastor"
        ],
        "subjects": [
            "Risk Management",
            "General Economics"
        ],
        "abstract": "Previous work suggests that the charter value hypothesis is theoretically grounded and empirically supported, but not universally. Accordingly, this paper aims to perform an analysis of the relations between charter value, risk taking, and supervision, taking into account the relations' complexity. Specifically, using the CAMELS rating system as a general framework for supervision, we study how charter value relates to risk and supervision by means of classification and regression tree analysis. The sample covers the period 2005-2016 and consists of listed banks in countries that were members of the Eurozone when it came into existence, along with Greece. To evaluate the crisis consequences, we also separately analyze four subperiods and countries that required financial aid from third parties and those that did not so, along with large and small banks. Our results reflect the complexity of the relations between charter value, supervision, and risk. Indeed, supervision and charter value seem aligned regarding only some types of risk",
        "comments": "46 pages, 4 tables, 5 figures, accepted version of a paper published in the Journal of Financial Stability",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.12274"
    },
    {
        "doc_id": 312,
        "title": "General duality and dual attainment for adapted transport",
        "authors": [
            "Daniel Kr\u0161ek",
            "Gudmund Pammer"
        ],
        "subjects": [
            "Probability",
            "Optimization and Control",
            "Mathematical Finance"
        ],
        "abstract": "We investigate duality and existence of dual optimizers for several adapted optimal transport problems under minimal assumptions. This includes the causal and bicausal transport, the barycenter problem, and a general multimarginal problem incorporating causality constraints. Moreover, we discuss applications of our results in robust finance. We consider a non-dominated model of several financial markets where stocks are traded dynamically, but the joint stock dynamics are unknown. We show that a no-arbitrage assumption in a quasi-sure sense naturally leads to sets of multicausal couplings. Consequently, computing the robust superhedging price is equivalent to solving an adapted transport problem, and finding a superhedging strategy means solving the corresponding dual.",
        "comments": "32 pages",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11958"
    },
    {
        "doc_id": 313,
        "title": "Forecasting and Backtesting Gradient Allocations of Expected Shortfall",
        "authors": [
            "Takaaki Koike",
            "Cathy W. S. Chen",
            "Edward M. H. Lin"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "Capital allocation is a procedure for quantifying the contribution of each source of risk to aggregated risk. The gradient allocation rule, also known as the Euler principle, is a prevalent rule of capital allocation under which the allocated capital captures the diversification benefit of the marginal risk as a component of overall risk. This research concentrates on Expected Shortfall (ES) as a regulatory standard and focuses on the gradient allocations of ES, also called ES contributions. We achieve the comprehensive treatment of backtesting the tuple of ES contributions in the framework of the traditional and comparative backtests based on the concepts of joint identifiability and multi-objective elicitability. For robust forecast evaluation against the choice of scoring function, we further develop Murphy diagrams for ES contributions as graphical tools to check whether one forecast dominates another under a class of scoring functions. Finally, leveraging the recent concept of multi-objective elicitability, we propose a novel semiparametric model for forecasting dynamic ES contributions based on a compositional regression model. In an empirical analysis of stock returns we evaluate and compare a variety of models for forecasting dynamic ES contributions and demonstrate the outstanding performance of the proposed model.",
        "comments": "MSC Class:          62F07; 62P05; 91B30",
        "date": "22 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11701"
    },
    {
        "doc_id": 314,
        "title": "A Novel Decision Ensemble Framework: Customized Attention-BiLSTM and XGBoost for Speculative Stock Price Forecasting",
        "authors": [
            "Riaz Ud Din",
            "Salman Ahmed",
            "Saddam Hussain Khan"
        ],
        "subjects": [
            "Statistical Finance",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "Forecasting speculative stock prices is essential for effective investment risk management that drives the need for the development of innovative algorithms. However, the speculative nature, volatility, and complex sequential dependencies within financial markets present inherent challenges which necessitate advanced techniques. This paper proposes a novel framework, CAB-XDE (customized attention BiLSTM-XGB decision ensemble), for predicting the daily closing price of speculative stock Bitcoin-USD (BTC-USD). CAB-XDE framework integrates a customized bi-directional long short-term memory (BiLSTM) with the attention mechanism and the XGBoost algorithm. The customized BiLSTM leverages its learning capabilities to capture the complex sequential dependencies and speculative market trends. Additionally, the new attention mechanism dynamically assigns weights to influential features, thereby enhancing interpretability, and optimizing effective cost measures and volatility forecasting. Moreover, XGBoost handles nonlinear relationships and contributes to the proposed CAB-XDE framework robustness. Additionally, the weight determination theory-error reciprocal method further refines predictions. This refinement is achieved by iteratively adjusting model weights. It is based on discrepancies between theoretical expectations and actual errors in individual customized attention BiLSTM and XGBoost models to enhance performance. Finally, the predictions from both XGBoost and customized attention BiLSTM models are concatenated to achieve diverse prediction space and are provided to the ensemble classifier to enhance the generalization capabilities of CAB-XDE. The proposed CAB-XDE framework is empirically validated on volatile Bitcoin market, sourced from Yahoo Finance and outperforms state-of-the-art models with a MAPE of 0.0037, MAE of 84.40, and RMSE of 106.14.",
        "comments": "30 pages, 16 Figures, 4 Tables",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11621"
    },
    {
        "doc_id": 315,
        "title": "The geometry of multi-curve interest rate models",
        "authors": [
            "Claudio Fontana",
            "Giacomo Lanaro",
            "Agatha Murgoci"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "We study the problems of consistency and of the existence of finite-dimensional realizations for multi-curve interest rate models of Heath-Jarrow-Morton type, generalizing the geometric approach developed by T. Bj\u00f6rk and co-authors in the classical single-curve setting. We characterize when a multi-curve interest rate model is consistent with a given parameterized family of forward curves and spreads and when a model can be realized by a finite-dimensional state process. We illustrate the general theory in a number of model classes and examples, providing explicit constructions of finite-dimensional realizations. Based on these theoretical results, we perform the calibration of a three-curve Hull-White model to market data and analyse the stability of the estimated parameters.",
        "comments": "28 pages, 2 figures",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11619"
    },
    {
        "doc_id": 316,
        "title": "Functional Limit Theorems for Hawkes Processes",
        "authors": [
            "Ulrich Horst",
            "Wei Xu"
        ],
        "subjects": [
            "Probability",
            "Statistics Theory",
            "Mathematical Finance"
        ],
        "abstract": "We prove that the long-run behavior of Hawkes processes is fully determined by the average number and the dispersion of child events. For subcritical processes we provide FLLNs and FCLTs under minimal conditions on the kernel of the process with the precise form of the limit theorems depending strongly on the dispersion of child events. For a critical Hawkes process with weakly dispersed child events, functional central limit theorems do not hold. Instead, we prove that the rescaled intensity processes and rescaled Hawkes processes behave like CIR-processes without mean-reversion, respectively integrated CIR-processes. We provide the rate of convergence by establishing an upper bound on the Wasserstein distance between the distributions of rescaled Hawkes process and the corresponding limit process. By contrast, critical Hawkes process with heavily dispersed child events share many properties of subcritical ones. In particular, functional limit theorems hold. However, unlike subcritical processes critical ones with heavily dispersed child events display long-range dependencies.",
        "comments": "59 pages; Keywords and phrases: Hawkes process, functional limit theorem, regular variation, convergence rate",
        "date": "21 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11495"
    },
    {
        "doc_id": 317,
        "title": "PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding",
        "authors": [
            "Ruochi Zhang",
            "Haoran Wu",
            "Chang Liu",
            "Huaping Li",
            "Yuqian Wu",
            "Kewei Li",
            "Yifan Wang",
            "Yifan Deng",
            "Jiahui Chen",
            "Fengfeng Zhou",
            "Xin Gao"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "Recent advances in protein language models have catalyzed significant progress in peptide sequence representation. Despite extensive exploration in this field, pre-trained models tailored for peptide-specific needs remain largely unaddressed due to the difficulty in capturing the complex and sometimes unstable structures of peptides. This study introduces a novel multi-view contrastive learning framework PepHarmony for the sequence-based peptide encoding task. PepHarmony innovatively combines both sequence- and structure-level information into a sequence-level encoding module through contrastive learning. We carefully select datasets from the Protein Data Bank (PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences and structures. The experimental data highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures compared with the baseline and fine-tuned models. The robustness of our model is confirmed through extensive ablation studies, which emphasize the crucial roles of contrastive loss and strategic data sorting in enhancing predictive performance. The proposed PepHarmony framework serves as a notable contribution to peptide representations, and offers valuable insights for future applications in peptide drug discovery and peptide engineering. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony or http://www.healthinformaticslab.org/supp/.",
        "comments": "25 pages, 5 figures, 3 tables",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11360"
    },
    {
        "doc_id": 318,
        "title": "Data-driven Option Pricing",
        "authors": [
            "Min Dai",
            "Hanqing Jin",
            "Xi Yang"
        ],
        "subjects": [
            "Pricing of Securities"
        ],
        "abstract": "We propose an innovative data-driven option pricing methodology that relies exclusively on the dataset of historical underlying asset prices. While the dataset is rooted in the objective world, option prices are commonly expressed as discounted expectations of their terminal payoffs in a risk-neutral world. Bridging this gap motivates us to identify a pricing kernel process, transforming option pricing into evaluating expectations in the objective world. We recover the pricing kernel by solving a utility maximization problem, and evaluate the expectations in terms of a functional optimization problem. Leveraging the deep learning technique, we design data-driven algorithms to solve both optimization problems over the dataset. Numerical experiments are presented to demonstrate the efficiency of our methodology.",
        "comments": "15 pages, 3 figures",
        "date": "20 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11158"
    },
    {
        "doc_id": 319,
        "title": "BioFinBERT: Finetuning Large Language Models (LLMs) to Analyze Sentiment of Press Releases and Financial Text Around Inflection Points of Biotech Stocks",
        "authors": [
            "Valentina Aparicio",
            "Daniel Gordon",
            "Sebastian G. Huayamares",
            "Yuhuai Luo"
        ],
        "subjects": [
            "General Finance",
            "Computational Finance",
            "Trading and Market Microstructure"
        ],
        "abstract": "Large language models (LLMs) are deep learning algorithms being used to perform natural language processing tasks in various fields, from social sciences to finance and biomedical sciences. Developing and training a new LLM can be very computationally expensive, so it is becoming a common practice to take existing LLMs and finetune them with carefully curated datasets for desired applications in different fields. Here, we present BioFinBERT, a finetuned LLM to perform financial sentiment analysis of public text associated with stocks of companies in the biotechnology sector. The stocks of biotech companies developing highly innovative and risky therapeutic drugs tend to respond very positively or negatively upon a successful or failed clinical readout or regulatory approval of their drug, respectively. These clinical or regulatory results are disclosed by the biotech companies via press releases, which are followed by a significant stock response in many cases. In our attempt to design a LLM capable of analyzing the sentiment of these press releases,we first finetuned BioBERT, a biomedical language representation model designed for biomedical text mining, using financial textual databases. Our finetuned model, termed BioFinBERT, was then used to perform financial sentiment analysis of various biotech-related press releases and financial text around inflection points that significantly affected the price of biotech stocks.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.11011"
    },
    {
        "doc_id": 320,
        "title": "Forecasting Cryptocurrency Staking Rewards",
        "authors": [
            "Sauren Gupta",
            "Apoorva Hathi Katharaki",
            "Yifan Xu",
            "Bhaskar Krishnamachari",
            "Rajarshi Gupta"
        ],
        "subjects": [
            "Statistical Finance",
            "Cryptography and Security",
            "Machine Learning"
        ],
        "abstract": "This research explores a relatively unexplored area of predicting cryptocurrency staking rewards, offering potential insights to researchers and investors. We investigate two predictive methodologies: a) a straightforward sliding-window average, and b) linear regression models predicated on historical data. The findings reveal that ETH staking rewards can be forecasted with an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day look-aheads respectively, using a 7-day sliding-window average approach. Additionally, we discern diverse prediction accuracies across various cryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is identified as superior to the moving-window average for perdicting in the short term for XTZ and ATOM. The results underscore the generally stable and predictable nature of staking rewards for most assets, with MATIC presenting a noteworthy exception.",
        "comments": "9 pages, 18 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10931"
    },
    {
        "doc_id": 321,
        "title": "Application of Machine Learning in Stock Market Forecasting: A Case Study of Disney Stock",
        "authors": [
            "Dengxin Huang"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning",
            "Applications"
        ],
        "abstract": "This document presents a stock market analysis conducted on a dataset consisting of 750 instances and 16 attributes donated in 2014-10-23. The analysis includes an exploratory data analysis (EDA) section, feature engineering, data preparation, model selection, and insights from the analysis. The Fama French 3-factor model is also utilized in the analysis. The results of the analysis are presented, with linear regression being the best-performing model.",
        "comments": "9 pages, 7 figures",
        "date": "31 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10903"
    },
    {
        "doc_id": 322,
        "title": "Stylized Facts and Market Microstructure: An In-Depth Exploration of German Bond Futures Market",
        "authors": [
            "Hamza Bodor",
            "Laurent Carlier"
        ],
        "subjects": [
            "Statistical Finance",
            "Trading and Market Microstructure"
        ],
        "abstract": "This paper presents an in-depth analysis of stylized facts in the context of futures on German bonds. The study examines four futures contracts on German bonds: Schatz, Bobl, Bund and Buxl, using tick-by-tick limit order book datasets. It uncovers a range of stylized facts and empirical observations, including the distribution of order sizes, patterns of order flow, and inter-arrival times of orders. The findings reveal both commonalities and unique characteristics across the different futures, thereby enriching our understanding of these markets. Furthermore, the paper introduces insightful realism metrics that can be used to benchmark market simulators. The study contributes to the literature on financial stylized facts by extending empirical observations to this class of assets, which has been relatively underexplored in existing research. This work provides valuable guidance for the development of more accurate and realistic market simulators.",
        "comments": " ",
        "date": "19 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10722"
    },
    {
        "doc_id": 323,
        "title": "Dynamic Programming: Finite States",
        "authors": [
            "Thomas J. Sargent",
            "John Stachurski"
        ],
        "subjects": [
            "General Economics",
            "Optimization and Control"
        ],
        "abstract": "This book is about dynamic programming and its applications in economics, finance, and adjacent fields. It brings together recent innovations in the theory of dynamic programming and provides applications and code that can help readers approach the research frontier. The book is aimed at graduate students and researchers, although most chapters are accessible to undergraduate students with solid quantitative backgrounds.",
        "comments": "MSC Class:          90C39",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10473"
    },
    {
        "doc_id": 324,
        "title": "Deep Generative Modeling for Financial Time Series with Application in VaR: A Comparative Review",
        "authors": [
            "Lars Ericson",
            "Xuejun Zhu",
            "Xusi Han",
            "Rao Fu",
            "Shuang Li",
            "Steve Guo",
            "Ping Hu"
        ],
        "subjects": [
            "Computational Finance",
            "Machine Learning",
            "Risk Management",
            "Statistical Finance"
        ],
        "abstract": "In the financial services industry, forecasting the risk factor distribution conditional on the history and the current market environment is the key to market risk modeling in general and value at risk (VaR) model in particular. As one of the most widely adopted VaR models in commercial banks, Historical simulation (HS) uses the empirical distribution of daily returns in a historical window as the forecast distribution of risk factor returns in the next day. The objectives for financial time series generation are to generate synthetic data paths with good variety, and similar distribution and dynamics to the original historical data. In this paper, we apply multiple existing deep generative methods (e.g., CGAN, CWGAN, Diffusion, and Signature WGAN) for conditional time series generation, and propose and test two new methods for conditional multi-step time series generation, namely Encoder-Decoder CGAN and Conditional TimeVAE. Furthermore, we introduce a comprehensive framework with a set of KPIs to measure the quality of the generated time series for financial modeling. The KPIs cover distribution distance, autocorrelation and backtesting. All models (HS, parametric and neural networks) are tested on both historical USD yield curve data and additional data simulated from GARCH and CIR processes. The study shows that top performing models are HS, GARCH and CWGAN models. Future research directions in this area are also discussed.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10370"
    },
    {
        "doc_id": 325,
        "title": "Interplay between Cryptocurrency Transactions and Online Financial Forums",
        "authors": [
            "Ana Fern\u00e1ndez Vilas",
            "Rebeca P. D\u00edaz Redondo",
            "Daniel Couto Cancela",
            "Alejandro Torrado Pazos"
        ],
        "subjects": [
            "General Finance",
            "Computers and Society",
            "Machine Learning"
        ],
        "abstract": "Cryptocurrencies are a type of digital money meant to provide security and anonymity while using cryptography techniques. Although cryptocurrencies represent a breakthrough and provide some important benefits, their usage poses some risks that are a result of the lack of supervising institutions and transparency. Because disinformation and volatility is discouraging for personal investors, cryptocurrencies emerged hand-in-hand with the proliferation of online users' communities and forums as places to share information that can alleviate users' mistrust. This research focuses on the study of the interplay between these cryptocurrency forums and fluctuations in cryptocurrency values. In particular, the most popular cryptocurrency Bitcoin (BTC) and a related active discussion community, Bitcointalk, are analyzed. This study shows that the activity of Bitcointalk forum keeps a direct relationship with the trend in the values of BTC, therefore analysis of this interaction would be a perfect base to support personal investments in a non-regulated market and, to confirm whether cryptocurrency forums show evidences to detect abnormal behaviors in BTC values as well as to predict or estimate these values. The experiment highlights that forum data can explain specific events in the financial field. It also underlines the relevance of quotes (regular mechanism to response a post) at periods: (1) when there is a high concentration of posts around certain topics; (2) when peaks in the BTC price are observed; and, (3) when the BTC price gradually shifts downwards and users intend to sell.",
        "comments": "Journal ref:        Mathematics 2021, 9(4), 411;",
        "date": "27 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.10238"
    },
    {
        "doc_id": 326,
        "title": "An Exploration to the Correlation Structure and Clustering of Macroeconomic Variables (MEV)",
        "authors": [
            "Garvit Arora",
            "Shubhangi Shubhangi",
            "Ying Wu",
            "Xuan Mei"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "As a quantitative characterization of the complicated economy, Macroeconomic Variables (MEVs), including GDP, inflation, unemployment, income, spending, interest rate, etc., are playing a crucial role in banks' portfolio management and stress testing exercise. In recent years, especially during the COVID-19 period and the current high inflation environment, people are frequently talking about the changing \"correlation structure\" of MEVs. In this paper, we use a principal component based algorithm to better understand MEVs' correlation structure in a given period. We also demonstrate how this method can be used to visualize historical MEVs pattern changes between 2000 and 2022. Further, we use this method to compare different hypothetical or historical macroeconomic scenarios and present our key findings.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10162"
    },
    {
        "doc_id": 327,
        "title": "Cardiac Digital Twin Pipeline for Virtual Therapy Evaluation",
        "authors": [
            "Julia Camps",
            "Zhinuo Jenny Wang",
            "Ruben Doste",
            "Maxx Holmes",
            "Brodie Lawson",
            "Jakub Tomek",
            "Kevin Burrage",
            "Alfonso Bueno-Orovio",
            "Blanca Rodriguez"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Tissues and Organs"
        ],
        "abstract": "Cardiac digital twins are computational tools capturing key functional and anatomical characteristics of patient hearts for investigating disease phenotypes and predicting responses to therapy. When paired with large-scale computational resources and large clinical datasets, digital twin technology can enable virtual clinical trials on virtual cohorts to fast-track therapy development. Here, we present an automated pipeline for personalising ventricular anatomy and electrophysiological function based on routinely acquired cardiac magnetic resonance (CMR) imaging data and the standard 12-lead electrocardiogram (ECG). Using CMR-based anatomical models, a sequential Monte-Carlo approximate Bayesian computational inference method is extended to infer electrical activation and repolarisation characteristics from the ECG. Fast simulations are conducted with a reaction-Eikonal model, including the Purkinje network and biophysically-detailed subcellular ionic current dynamics for repolarisation. For each patient, parameter uncertainty is represented by inferring a population of ventricular models rather than a single one, which means that parameter uncertainty can be propagated to therapy evaluation. Furthermore, we have developed techniques for translating from reaction-Eikonal to monodomain simulations, which allows more realistic simulations of cardiac electrophysiology. The pipeline is demonstrated in a healthy female subject, where our inferred reaction-Eikonal models reproduced the patient's ECG with a Pearson's correlation coefficient of 0.93, and the translated monodomain simulations have a correlation coefficient of 0.89. We then apply the effect of Dofetilide to the monodomain population of models for this subject and show dose-dependent QT and T-peak to T-end prolongations that are in keeping with large population drug response data.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.10029"
    },
    {
        "doc_id": 328,
        "title": "Consistent asset modelling with random coefficients and switches between regimes",
        "authors": [
            "Felix L. Wolf",
            "Griselda Deelstra",
            "Lech A. Grzelak"
        ],
        "subjects": [
            "Pricing of Securities",
            "Computational Finance",
            "Risk Management"
        ],
        "abstract": "We explore a stochastic model that enables capturing external influences in two specific ways. The model allows for the expression of uncertainty in the parametrisation of the stochastic dynamics and incorporates patterns to account for different behaviours across various times or regimes. To establish our framework, we initially construct a model with random parameters, where the switching between regimes can be dictated either by random variables or deterministically. Such a model is highly interpretable. We further ensure mathematical consistency by demonstrating that the framework can be elegantly expressed through local volatility models taking the form of standard jump diffusions. Additionally, we consider a Markov-modulated approach for the switching between regimes characterised by random parameters. For all considered models, we derive characteristic functions, providing a versatile tool with wide-ranging applications. In a numerical experiment, we apply the framework to the financial problem of option pricing. The impact of parameter uncertainty is analysed in a two-regime model, where the asset process switches between periods of high and low volatility imbued with high and low uncertainty, respectively.",
        "comments": "MSC Class:          91G20 91G30",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09955"
    },
    {
        "doc_id": 329,
        "title": "Cross-Domain Behavioral Credit Modeling: transferability from private to central data",
        "authors": [
            "O. Didkovskyi",
            "N. Jean",
            "G. Le Pera",
            "C. Nordio"
        ],
        "subjects": [
            "Risk Management",
            "Statistical Finance"
        ],
        "abstract": "This paper introduces a credit risk rating model for credit risk assessment in quantitative finance, aiming to categorize borrowers based on their behavioral data. The model is trained on data from Experian, a widely recognized credit bureau, to effectively identify instances of loan defaults among bank customers. Employing state-of-the-art statistical and machine learning techniques ensures the model's predictive accuracy. Furthermore, we assess the model's transferability by testing it on behavioral data from the Bank of Italy, demonstrating its potential applicability across diverse datasets during prediction. This study highlights the benefits of incorporating external behavioral data to improve credit risk assessment in financial institutions.",
        "comments": "25 pages, 15 figures",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09778"
    },
    {
        "doc_id": 330,
        "title": "Neural Hawkes: Non-Parametric Estimation in High Dimension and Causality Analysis in Cryptocurrency Markets",
        "authors": [
            "Timoth\u00e9e Fabre",
            "Ioane Muni Toke"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Mathematical Finance"
        ],
        "abstract": "We propose a novel approach to marked Hawkes kernel inference which we name the moment-based neural Hawkes estimation method. Hawkes processes are fully characterized by their first and second order statistics through a Fredholm integral equation of the second kind. Using recent advances in solving partial differential equations with physics-informed neural networks, we provide a numerical procedure to solve this integral equation in high dimension. Together with an adapted training pipeline, we give a generic set of hyperparameters that produces robust results across a wide range of kernel shapes. We conduct an extensive numerical validation on simulated data. We finally propose two applications of the method to the analysis of the microstructure of cryptocurrency markets. In a first application we extract the influence of volume on the arrival rate of BTC-USD trades and in a second application we analyze the causality relationships and their directions amongst a universe of 15 cryptocurrency pairs in a centralized exchange.",
        "comments": " ",
        "date": "18 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09361"
    },
    {
        "doc_id": 331,
        "title": "A closer look at the chemical potential of an ideal agent system",
        "authors": [
            "Christoph J. B\u00f6rner",
            "Ingo Hoffmann",
            "John H. Stiebel"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "Models for spin systems known from statistical physics are used in econometrics in the form of agent-based models. Econophysics research in econometrics is increasingly developing general market models that describe exchange phenomena and use the chemical potential $\u03bc$ known from physics in the context of particle number changes. In statistical physics, equations of state are known for the chemical potential, which take into account the respective model framework and the corresponding state variables. A simple transfer of these equations of state to problems in econophysics appears difficult. To the best of our knowledge, the equation of state for the chemical potential is currently missing even for the simplest conceivable model of an ideal agent system. In this paper, this research gap is closed and the equation of state for the chemical potential is derived from the econophysical model assumptions of the ideal agent system. An interpretation of the equation of state leads to fundamental relationships that could also have been guessed, but are shown here by the theory.",
        "comments": "11 Pages, 0 Figures, Working Paper, Theoretical Contribution",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09233"
    },
    {
        "doc_id": 332,
        "title": "Mean-Field SDEs driven by $G$-Brownian Motion",
        "authors": [
            "Karl-Wilhelm Georg Bollweg",
            "Thilo Meyer-Brandis"
        ],
        "subjects": [
            "Probability",
            "Mathematical Finance"
        ],
        "abstract": "We extend the notion of mean-field SDEs to SDEs driven by $G$-Brownian motion. More precisely, we consider a $G$-SDE where the coefficients depend not only on time and the current state but also on the solution as random variable.",
        "comments": " ",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09113"
    },
    {
        "doc_id": 333,
        "title": "AI Thrust: Ranking Emerging Powers for Tech Startup Investment in Latin America",
        "authors": [
            "Abraham Ramos Torres",
            "Laura N Montoya"
        ],
        "subjects": [
            "General Economics",
            "Risk Management"
        ],
        "abstract": "Artificial intelligence (AI) is rapidly transforming the global economy, and Latin America is no exception. In recent years, there has been a growing interest in AI development and implementation in the region. This paper presents a ranking of Latin American (LATAM) countries based on their potential to become emerging powers in AI. The ranking is based on three pillars: infrastructure, education, and finance. Infrastructure is measured by the availability of electricity, high-speed internet, the quality of telecommunications networks, and the availability of supercomputers. Education is measured by the quality of education and the research status. Finance is measured by the cost of investments, history of investments, economic metrics, and current implementation of AI.\n  While Brazil, Chile, and Mexico have established themselves as major players in the AI industry in Latin America, our ranking demonstrates the new emerging powers in the region. According to the results, Argentina, Colombia, Uruguay, Costa Rica, and Ecuador are leading as new emerging powers in AI in Latin America. These countries have strong education systems, well-developed infrastructure, and growing financial resources. The ranking provides a useful tool for policymakers, investors, and businesses interested in AI development in Latin America. It can help to identify emerging LATAM countries with the greatest potential for AI growth and success.",
        "comments": "9 pages, 4 tables, 9 figures",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09056"
    },
    {
        "doc_id": 334,
        "title": "On continuity of state-dependent utilities",
        "authors": [
            "Edoardo Berton",
            "Alessandro Doldi",
            "Marco Maggis"
        ],
        "subjects": [
            "Mathematical Finance",
            "Theoretical Economics"
        ],
        "abstract": "State-dependent preferences for a general Savage's state space were shown in Wakker and Zank (1999) to admit a numerical representation in the form of the integral of a state-dependent utility, as soon as pointwise continuity of the preference ordering is assumed. In this paper we prove that such a state-dependent function inherits pointwise continuity from the preference ordering, providing in this way a positive answer to a conjecture posed in the aforementioned seminal work. We further apply this result to obtain an explicit representation of conditional Chisini means in the form of a conditional certainty equivalent.",
        "comments": "MSC Class:          91B06; 91B08; 60A05",
        "date": "17 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.09054"
    },
    {
        "doc_id": 335,
        "title": "Spurious Default Probability Projections in Credit Risk Stress Testing Models",
        "authors": [
            "Bernd Engelmann"
        ],
        "subjects": [
            "Risk Management"
        ],
        "abstract": "Credit risk stress testing has become an important risk management device which is used both by banks internally and by regulators. Stress testing is complex because it essentially means projecting a bank's full balance sheet conditional on a macroeconomic scenario over multiple years. Part of the complexity stems from using a wide range of model parameters for, e.g., rating transition, write-off rules, prepayment, or origination of new loans. A typical parameterization of a credit risk stress test model specifies parameters linked to an average economic, the through-the-cycle, state. These parameters are transformed to a stressed state by utilizing a macroeconomic model. It will be shown that the model parameterization implies a unique through-the-cycle portfolio which is unrelated to a bank's current portfolio. Independent of the stress imposed to the model, the current portfolio will have a tendency to propagate towards the through-the-cycle portfolio. This could create unwanted spurious effects on projected portfolio default rates especially when a stress test model's parameterization is inconsistent with a bank's current portfolio.",
        "comments": "15 pages, 4 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08892"
    },
    {
        "doc_id": 336,
        "title": "Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities and Risks",
        "authors": [
            "Xihan Xiong",
            "Zhipeng Wang",
            "Xi Chen",
            "William Knottenbelt",
            "Michael Huth"
        ],
        "subjects": [
            "General Finance",
            "Cryptography and Security"
        ],
        "abstract": "Lido, the leading Liquid Staking Derivative (LSD) provider on Ethereum, allows users to stake an arbitrary amount of ETH to receive stETH, which can be integrated with Decentralized Finance (DeFi) protocols such as Aave. The composability between Lido and Aave enables a novel strategy called \"leverage staking\", where users stake ETH on Lido to acquire stETH, utilize stETH as collateral on Aave to borrow ETH, and then restake the borrowed ETH on Lido. Users can iteratively execute this process to optimize potential returns based on their risk profile.\n  This paper systematically studies the opportunities and risks associated with leverage staking. We are the first to formalize the leverage staking strategy within the Lido-Aave ecosystem. Our empirical study identifies 262 leverage staking positions on Ethereum, with an aggregated staking amount of 295,243 ETH (482M USD). We discover that 90.13% of leverage staking positions have achieved higher returns than conventional staking. Furthermore, we perform stress tests to evaluate the risk introduced by leverage staking under extreme conditions. We find that leverage staking significantly amplifies the risk of cascading liquidations. We hope this paper can inform and encourage the development of robust risk management approaches to protect the Lido-Aave LSD ecosystem.",
        "comments": " ",
        "date": "28 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08610"
    },
    {
        "doc_id": 337,
        "title": "Forking paths in financial economics",
        "authors": [
            "Guillaume Coqueret"
        ],
        "subjects": [
            "General Finance",
            "Methodology"
        ],
        "abstract": "We argue that spanning large numbers of degrees of freedom in empirical analysis allows better characterizations of effects and thus improves the trustworthiness of conclusions. Our ideas are illustrated in three studies: equity premium prediction, asset pricing anomalies and risk premia estimation. In the first, we find that each additional degree of freedom in the protocol expands the average range of $t$-statistics by at least 30%. In the second, we show that resorting to forking paths instead of bootstrapping in multiple testing raises the bar of significance for anomalies: at the 5% confidence level, the threshold for bootstrapped statistics is 4.5, whereas with paths, it is at least 8.2, a bar much higher than those currently used in the literature. In our third application, we reveal the importance of particular steps in the estimation of premia. In addition, we use paths to corroborate prior findings in the three topics. We document heterogeneity in our ability to replicate prior studies: some conclusions seem robust, others do not align with the paths we were able to generate.",
        "comments": " ",
        "date": "25 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08606"
    },
    {
        "doc_id": 338,
        "title": "Reinforcement Learning and Deep Stochastic Optimal Control for Final Quadratic Hedging",
        "authors": [
            "Bernhard Hientzsch"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "We consider two data driven approaches, Reinforcement Learning (RL) and Deep Trajectory-based Stochastic Optimal Control (DTSOC) for hedging a European call option without and with transaction cost according to a quadratic hedging P&L objective at maturity (\"variance-optimal hedging\" or \"final quadratic hedging\"). We study the performance of the two approaches under various market environments (modeled via the Black-Scholes and/or the log-normal SABR model) to understand their advantages and limitations. Without transaction costs and in the Black-Scholes model, both approaches match the performance of the variance-optimal Delta hedge. In the log-normal SABR model without transaction costs, they match the performance of the variance-optimal Barlett's Delta hedge. Agents trained on Black-Scholes trajectories with matching initial volatility but used on SABR trajectories match the performance of Bartlett's Delta hedge in average cost, but show substantially wider variance. To apply RL approaches to these problems, P&L at maturity is written as sum of step-wise contributions and variants of RL algorithms are implemented and used that minimize expectation of second moments of such sums.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2302.07996",
        "date": "20 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.08600"
    },
    {
        "doc_id": 339,
        "title": "Fitting random cash management models to data",
        "authors": [
            "Francisco Salas-Molina"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "Organizations use cash management models to control balances to both avoid overdrafts and obtain a profit from short-term investments. Most management models are based on control bounds which are derived from the assumption of a particular cash flow probability distribution. In this paper, we relax this strong assumption to fit cash management models to data by means of stochastic and linear programming. We also introduce ensembles of random cash management models which are built by randomly selecting a subsequence of the original cash flow data set. We illustrate our approach by means of a real case study showing that a small random sample of data is enough to fit sufficiently good bound-based models.",
        "comments": "19 pages,6 figures, 1 table",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08548"
    },
    {
        "doc_id": 340,
        "title": "Dynamic portfolio selection under generalized disappointment aversion",
        "authors": [
            "Zongxia Liang",
            "Sheng Wang",
            "Jianming Xia",
            "Fengyi Yuan"
        ],
        "subjects": [
            "Mathematical Finance",
            "Portfolio Management"
        ],
        "abstract": "This paper addresses the continuous-time portfolio selection problem under generalized disappointment aversion (GDA). The implicit definition of the certainty equivalent within GDA preferences introduces time inconsistency to this problem. We provide the sufficient and necessary conditions for a strategy to be an equilibrium by a fully nonlinear ordinary differential equation (ODE). Through an exploration of the existence and uniqueness of solution to the ODE, we establish the existence and uniqueness of the equilibrium. Our findings indicate that under disappointment aversion (DA) preferences, non-participation in the stock market is the unique equilibrium. The numerical analysis reveals that, under GDA preferences, the investment proportion in the stock market consistently remains smaller than the investment proportion under the classical Expected Utility (EU) theory.",
        "comments": "27 pages, 4 figures",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08323"
    },
    {
        "doc_id": 341,
        "title": "Do backrun auctions protect traders?",
        "authors": [
            "Andrew W. Macpherson"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Distributed, Parallel, and Cluster Computing",
            "Computer Science and Game Theory"
        ],
        "abstract": "We study a new \"laminated\" queueing model for orders on batched trading venues such as decentralised exchanges. The model aims to capture and generalise transaction queueing infrastructure that has arisen to organise MEV activity on public blockchains such as Ethereum, providing convenient channels for sophisticated agents to extract value by acting on end-user order flow by performing arbitrage and related HFT activities. In our model, market orders are interspersed with orders created by arbitrageurs that under idealised conditions reset the marginal price to a global equilibrium between each trade, improving predictability of execution for liquidity traders.\n  If an arbitrageur has a chance to land multiple opportunities in a row, he may attempt to manipulate the execution price of the intervening market order by a probabilistic blind sandwiching strategy. To study how bad this manipulation can get, we introduce and bound a price manipulation coefficient that measures the deviation from global equilibrium of local pricing quoted by a rational arbitrageur. We exhibit cases in which this coefficient is well approximated by a \"zeta value' with interpretable and empirically measurable parameters.",
        "comments": "Keywords: MEV, queue discipline, sandwich, CFMM, arbitrage, blockchain, Ethereum",
        "date": "16 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08302"
    },
    {
        "doc_id": 342,
        "title": "Optimal Insurance to Maximize Exponential Utility when Premium is Computed by a Convex Functional",
        "authors": [
            "Jingyi Cao",
            "Dongchen Li",
            "Virginia R. Young",
            "Bin Zou"
        ],
        "subjects": [
            "Mathematical Finance",
            "Optimization and Control",
            "Risk Management"
        ],
        "abstract": "We find the optimal indemnity to maximize the expected utility of terminal wealth of a buyer of insurance whose preferences are modeled by an exponential utility. The insurance premium is computed by a convex functional. We obtain a necessary condition for the optimal indemnity; then, because the candidate optimal indemnity is given implicitly, we use that necessary condition to develop a numerical algorithm to compute it. We prove that the numerical algorithm converges to a unique indemnity that, indeed, equals the optimal policy. We also illustrate our results with numerical examples.",
        "comments": "12 pages, 3 figures",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08094"
    },
    {
        "doc_id": 343,
        "title": "A Two-Step Longstaff Schwartz Monte Carlo Approach to Game Option Pricing",
        "authors": [
            "Ce Wang"
        ],
        "subjects": [
            "Computational Finance",
            "Pricing of Securities"
        ],
        "abstract": "We proposed a two-step Longstaff Schwartz Monte Carlo (LSMC) method with two regression models fitted at each time step to price game options. Although the original LSMC can be used to price game options with an enlarged range of path in regression and a modified cashflow updating rule, we identified a drawback of such approach, which motivated us to propose our approach. We implemented numerical examples with benchmarks using binomial tree and numerical PDE, and it showed that our method produces more reliable results comparing to the original LSMC.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08093"
    },
    {
        "doc_id": 344,
        "title": "Transformer-based approach for Ethereum Price Prediction Using Crosscurrency correlation and Sentiment Analysis",
        "authors": [
            "Shubham Singh",
            "Mayur Bhat"
        ],
        "subjects": [
            "Machine Learning",
            "Artificial Intelligence",
            "Pricing of Securities"
        ],
        "abstract": "The research delves into the capabilities of a transformer-based neural network for Ethereum cryptocurrency price forecasting. The experiment runs around the hypothesis that cryptocurrency prices are strongly correlated with other cryptocurrencies and the sentiments around the cryptocurrency. The model employs a transformer architecture for several setups from single-feature scenarios to complex configurations incorporating volume, sentiment, and correlated cryptocurrency prices. Despite a smaller dataset and less complex architecture, the transformer model surpasses ANN and MLP counterparts on some parameters. The conclusion presents a hypothesis on the illusion of causality in cryptocurrency price movements driven by sentiments.",
        "comments": "12 pages",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.08077"
    },
    {
        "doc_id": 345,
        "title": "Provisions and Economic Capital for Credit Losses",
        "authors": [
            "Dorinel Bastide",
            "St\u00e9phane Cr\u00e9pey"
        ],
        "subjects": [
            "Risk Management",
            "Probability",
            "General Finance"
        ],
        "abstract": "Based on supermodularity ordering properties, we show that convex risk measures of credit losses are nondecreasing  w.r.t. credit-credit and, in a wrong-way risk setup, credit-market, covariances of elliptically distributed latent factors. These results support the use of such setups for computing credit provisions and economic capital or for conducting stress test exercises and risk management analysis.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07728"
    },
    {
        "doc_id": 346,
        "title": "Cash and Card Acceptance in Retail Payments: Motivations and Factors",
        "authors": [
            "Samuel Vandak",
            "Geoffrey Goodell"
        ],
        "subjects": [
            "Computers and Society",
            "Computational Engineering, Finance, and Science",
            "General Finance"
        ],
        "abstract": "The landscape of payment methods in retail is a complex and evolving area. Vendors are motivated to conduct an appropriate analysis to decide what payment methods to accept out of a vast range of options. Many factors are included in this decision process, some qualitative and some quantitative. The following research project investigates vendors' acceptance of cards and cash from various viewpoints, all chosen to represent a novel perspective, including the barriers and preferences for each and correlations with external demographic factors. We observe that lower interchange fees, limited in this instance by the regulatory framework, play a crucial role in facilitating merchants' acceptance of card payments. The regulatory constraints on interchange fees create a favorable cost structure for merchants, making card payment adoption financially feasible. However, additional factors like technological readiness and consumer preferences might also play a significant role in their decision-making process. We also note that aggregate Merchant Service Providers (MSPs) have positively impacted the payment landscape by offering more competitive fee rates, particularly beneficial for small merchants and entrepreneurs. However, associated risks, such as account freezes or abrupt terminations, pose challenges and often lack transparency. Last, the quantitative analysis of the relationship between demographic variables and acceptance of payment types is presented. This analysis combines the current landscape of payment acceptance in the UK with data from the most recent census from 2021. We show that the unemployment rates shape card and cash acceptance, age affects contactless preference, and work-from-home impacts credit card preference.",
        "comments": "34 pages, 19 figures, 5 tables",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07682"
    },
    {
        "doc_id": 347,
        "title": "Empirical Evidence for the Fragment level Understanding on Drug Molecular Structure of LLMs",
        "authors": [
            "Xiuyuan Hu",
            "Guoqing Liu",
            "Yang Zhao",
            "Hao Zhang"
        ],
        "subjects": [
            "Machine Learning",
            "Computational Engineering, Finance, and Science",
            "Biomolecules"
        ],
        "abstract": "AI for drug discovery has been a research hotspot in recent years, and SMILES-based language models has been increasingly applied in drug molecular design. However, no work has explored whether and how language models understand the chemical spatial structure from 1D sequences. In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments. The results indicate that language models can understand chemical structures from the perspective of molecular fragments, and the structural knowledge learned through fine-tuning is reflected in the high-frequency SMILES substrings generated by the model.",
        "comments": "Accepted by AAAI 2024 workshop: Large Language Models for Biological Discoveries (LLMs4Bio)",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07657"
    },
    {
        "doc_id": 348,
        "title": "Graph database while computationally efficient filters out quickly the ESG integrated equities in investment management",
        "authors": [
            "Partha Sen",
            "Sumana Sen"
        ],
        "subjects": [
            "Computational Finance"
        ],
        "abstract": "Design/methodology/approach This research evaluated the databases of SQL, No-SQL and graph databases to compare and contrast efficiency and performance. To perform this experiment the data were collected from multiple sources including stock price and financial news. Python is used as an interface to connect and query databases (to create database structures according to the feed file structure, to load data into tables, objects, to read data , to connect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM (Large language model) including RAG (Retrieval Augmented Generation) with Machine Learning, deep learning, NLP (natural language processing) or Decision Analytics are computationally expensive. Finding a better option to consume less resources and time to get the result. Findings The Graph database of ESG (Environmental, Social and Governance) is comparatively better and can be considered for extended analytics to integrate ESG in business and investment. Practical implications A graph ML with a RAG architecture model can be introduced as a new framework with less computationally expensive LLM application in the equity filtering process for portfolio management. Originality/value Filtering out selective stocks out of two thousand or more listed companies in any stock exchange for active investment, consuming less resource consumption especially memory and energy to integrate artificial intelligence and ESG in business and investment.",
        "comments": "10 pages, 17 figures",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07483"
    },
    {
        "doc_id": 349,
        "title": "Herd Behavior in Optimal Investment: A Dual-Agent Approach with Investment Opinion and Rational Decision Decomposition",
        "authors": [
            "Huisheng Wang",
            "H. Vicky Zhao"
        ],
        "subjects": [
            "Systems and Control",
            "Optimization and Control",
            "Mathematical Finance",
            "Portfolio Management"
        ],
        "abstract": "In this paper, we study the optimal investment problem involving two agents, where the decision of one agent is influenced by the other. To measure the distance between two agents' decisions, we introduce the average deviation. We formulate the stochastic optimal control problem considering herd behavior and derive the analytical solution through the variational method. We theoretically analyze the impact of users' herd behavior on the optimal decision by decomposing it into their rational decisions, which is called the rational decision decomposition. Furthermore, to quantify the preference for their rational decision over that of the other agent, we introduce the agent's investment opinion. Our study is validated through simulations on real stock data.",
        "comments": " ",
        "date": "13 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.07183"
    },
    {
        "doc_id": 350,
        "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
        "authors": [
            "Varshini Reddy",
            "Rik Koncel-Kedziorski",
            "Viet Dac Lai",
            "Chris Tanner"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence"
        ],
        "abstract": "Research in quantitative reasoning within the financial domain indeed necessitates the use of realistic tasks and data, primarily because of the significant impact of decisions made in business and finance. Financial professionals often interact with documents hundreds of pages long, but most research datasets drastically reduce this context length. To address this, we introduce a long-document financial QA task. We augment 7,621 questions from the existing FinQA dataset with full-document context, extending the average context length for each question from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments of retrieval-based QA pipelines and long-context language models on the augmented data. Our results show that DocFinQA provides challenges for even the strongest, state-of-the-art systems.",
        "comments": "13 pages",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06915"
    },
    {
        "doc_id": 351,
        "title": "A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models",
        "authors": [
            "Emmanuil H. Georgoulis",
            "Antonis Papapantoleon",
            "Costas Smaragdakis"
        ],
        "subjects": [
            "Computational Finance",
            "Machine Learning",
            "Numerical Analysis",
            "Probability",
            "Machine Learning"
        ],
        "abstract": "We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assessed in a series of numerical experiments involving the Merton jump-diffusion model.",
        "comments": "16 pages, 11 figures",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06740"
    },
    {
        "doc_id": 352,
        "title": "Equity auction dynamics: latent liquidity models with activity acceleration",
        "authors": [
            "Mohammed Salek",
            "Damien Challet",
            "Ioane Muni Toke"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Statistical Finance"
        ],
        "abstract": "Equity auctions display several distinctive characteristics in contrast to continuous trading. As the auction time approaches, the rate of events accelerates causing a substantial liquidity buildup around the indicative price. This, in turn, results in a reduced price impact and decreased volatility of the indicative price. In this study, we adapt the latent/revealed order book framework to the specifics of equity auctions. We provide precise measurements of the model parameters, including order submissions, cancellations, and diffusion rates. Our setup allows us to describe the full dynamics of the average order book during closing auctions in Euronext Paris. These findings support the relevance of the latent liquidity framework in describing limit order book dynamics. Lastly, we analyze the factors contributing to a sub-diffusive indicative price and demonstrate the absence of indicative price predictability.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06724"
    },
    {
        "doc_id": 353,
        "title": "SpotV2Net: Multivariate Intraday Spot Volatility Forecasting via Vol-of-Vol-Informed Graph Attention Networks",
        "authors": [
            "Alessio Brini",
            "Giacomo Toscano"
        ],
        "subjects": [
            "Statistical Finance",
            "Computational Finance"
        ],
        "abstract": "This paper introduces SpotV2Net, a multivariate intraday spot volatility forecasting model based on a Graph Attention Network architecture. SpotV2Net represents financial assets as nodes within a graph and includes non-parametric high-frequency Fourier estimates of the spot volatility and co-volatility as node features. Further, it incorporates Fourier estimates of the spot volatility of volatility and co-volatility of volatility as features for node edges. We test the forecasting accuracy of SpotV2Net in an extensive empirical exercise, conducted with high-frequency prices of the components of the Dow Jones Industrial Average index. The results we obtain suggest that SpotV2Net shows improved accuracy, compared to alternative econometric and machine-learning-based models. Further, our results show that SpotV2Net maintains accuracy when performing intraday multi-step forecasts. To interpret the forecasts produced by SpotV2Net, we employ GNNExplainer, a model-agnostic interpretability tool and thereby uncover subgraphs that are critical to a node's predictions.",
        "comments": "34 pages, 9 figures",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06249"
    },
    {
        "doc_id": 354,
        "title": "CNN-DRL for Scalable Actions in Finance",
        "authors": [
            "Sina Montazeri",
            "Akram Mirzaeinia",
            "Haseebullah Jumakhan",
            "Amir Mirzaeinia"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "The published MLP-based DRL in finance has difficulties in learning the dynamics of the environment when the action scale increases. If the buying and selling increase to one thousand shares, the MLP agent will not be able to effectively adapt to the environment. To address this, we designed a CNN agent that concatenates the data from the last ninety days of the daily feature vector to create the CNN input matrix. Our extensive experiments demonstrate that the MLP-based agent experiences a loss corresponding to the initial environment setup, while our designed CNN remains stable, effectively learns the environment, and leads to an increase in rewards.",
        "comments": "10th Annual Conf. on Computational Science & Computational Intelligence",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06179"
    },
    {
        "doc_id": 355,
        "title": "CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine Learning Methods",
        "authors": [
            "Yue Chen",
            "Xingyi Andrew",
            "Salintip Supasanya"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "Historically, the economic recession often came abruptly and disastrously. For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from October 2007 to March 2009. If we could detect the signals of the crisis earlier, we could have taken preventive measures. Therefore, driven by such motivation, we use advanced machine learning techniques, including Random Forest and Extreme Gradient Boosting, to predict any potential market crashes mainly in the US market. Also, we would like to compare the performance of these methods and examine which model is better for forecasting US stock market crashes. We apply our models on the daily financial market data, which tend to be more responsive with higher reporting frequencies. We consider 75 explanatory variables, including general US stock market indexes, SP 500 sector indexes, as well as market indicators that can be used for the purpose of crisis prediction. Finally, we conclude, with selected classification metrics, that the Extreme Gradient Boosting method performs the best in predicting US stock market crisis events.",
        "comments": "14 pages, 9 figures",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.06172"
    },
    {
        "doc_id": 356,
        "title": "Multimodal Gen-AI for Fundamental Investment Research",
        "authors": [
            "Lezhi Li",
            "Ting-Yu Chang",
            "Hai Wang"
        ],
        "subjects": [
            "General Finance",
            "Machine Learning"
        ],
        "abstract": "This report outlines a transformative initiative in the financial investment industry, where the conventional decision-making process, laden with labor-intensive tasks such as sifting through voluminous documents, is being reimagined. Leveraging language models, our experiments aim to automate information summarization and investment idea generation. We seek to evaluate the effectiveness of fine-tuning methods on a base model (Llama2) to achieve specific application-level goals, including providing insights into the impact of events on companies and sectors, understanding market condition relationships, generating investor-aligned investment ideas, and formatting results with stock recommendations and detailed explanations. Through state-of-the-art generative modeling techniques, the ultimate objective is to develop an AI agent prototype, liberating human investors from repetitive tasks and allowing a focus on high-level strategic thinking. The project encompasses a diverse corpus dataset, including research reports, investment memos, market news, and extensive time-series market data. We conducted three experiments applying unsupervised and supervised LoRA fine-tuning on the llama2_7b_hf_chat as the base model, as well as instruction fine-tuning on the GPT3.5 model. Statistical and human evaluations both show that the fine-tuned versions perform better in solving text modeling, summarization, reasoning, and finance domain questions, demonstrating a pivotal step towards enhancing decision-making processes in the financial domain. Code implementation for the project can be found on GitHub: https://github.com/Firenze11/finance_lm.",
        "comments": " ",
        "date": "23 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06164"
    },
    {
        "doc_id": 357,
        "title": "De novo Drug Design using Reinforcement Learning with Multiple GPT Agents",
        "authors": [
            "Xiuyuan Hu",
            "Guoqing Liu",
            "Yang Zhao",
            "Hao Zhang"
        ],
        "subjects": [
            "Biomolecules",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT.",
        "comments": "Accepted by NeurIPS 2023",
        "date": "21 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06155"
    },
    {
        "doc_id": 358,
        "title": "A Statistical Field Perspective on Capital Allocation and Accumulation: Individual dynamics",
        "authors": [
            "Pierre Gosselin",
            "A\u00efleen Lotz"
        ],
        "subjects": [
            "General Finance",
            "High Energy Physics - Theory"
        ],
        "abstract": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups. The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends to their investors. Financial capital is thus a function of dividends and stock valuations, whereas physical capital is a function of the total capital allocated by the financial sector. Whereas our previous work focused on the background fields that describe potential long-term equilibria, here we compute the transition functions of individual agents and study their probabilistic dynamics in the background field, as a function of their initial state. We show that capital accumulation depends on various factors. The probability associated with each firm's trajectories is the result of several contradictory effects: the firm tends to shift towards sectors with the greatest long-term return, but must take into account the impact of its shift on its attractiveness for investors throughout its trajectory. Since this trajectory depends largely on the average capital of transition sectors, a firm's attractiveness during its relocation depends on the relative level of capital in those sectors. Thus, an under-capitalized firm reaching a high-capital sector will experience a loss of attractiveness, and subsequently, in investors. Moreover, the firm must also consider the effects of competition in the intermediate sectors. An under-capitalized firm will tend to be ousted out towards sectors with lower average capital, while an over-capitalized firm will tend to shift towards higher averagecapital sectors. For investors, capital allocation depends on their short and long-term returns. These returns are not independent: in the short-term, returns are composed of both the firm's dividends and the increase in its stock prices. In the long-term, returns are based on the firm's growth expectations, but also, indirectly, on expectations of higher stock prices. Investors' capital allocation directly depends on the volatility of stock prices and {\\ldots}rms'dividends. Investors will tend to reallocate their capital to maximize their short and long-term returns. The higher their level of capital, the stronger the reallocation will be.",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2312.16173, arXiv:2205.03087",
        "date": "30 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06142"
    },
    {
        "doc_id": 359,
        "title": "StockFormer: A Swing Trading Strategy Based on STL Decomposition and Self-Attention Networks",
        "authors": [
            "Bohan Ma",
            "Yiheng Wang",
            "Yuchao Lu",
            "Tianzixuan Hu",
            "Jinling Xu",
            "Patrick Houlihan"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Machine Learning"
        ],
        "abstract": "Amidst ongoing market recalibration and increasing investor optimism, the U.S. stock market is experiencing a resurgence, prompting the need for sophisticated tools to protect and grow portfolios. Addressing this, we introduce \"Stockformer,\" a cutting-edge deep learning framework optimized for swing trading, featuring the TopKDropout method for enhanced stock selection. By integrating STL decomposition and self-attention networks, Stockformer utilizes the S&P 500's complex data to refine stock return predictions. Our methodology entailed segmenting data for training and validation (January 2021 to January 2023) and testing (February to June 2023). During testing, Stockformer's predictions outperformed ten industry models, achieving superior precision in key predictive accuracy indicators (MAE, RMSE, MAPE), with a remarkable accuracy rate of 62.39% in detecting market trends. In our backtests, Stockformer's swing trading strategy yielded a cumulative return of 13.19% and an annualized return of 30.80%, significantly surpassing current state-of-the-art models. Stockformer has emerged as a beacon of innovation in these volatile times, offering investors a potent tool for market forecasting. To advance the field and foster community collaboration, we have open-sourced Stockformer, available at https://github.com/Eric991005/Stockformer.",
        "comments": "Currently under consideration for publication in the International Journal of Forecasting",
        "date": "22 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.06139"
    },
    {
        "doc_id": 360,
        "title": "Quantum Probability Theoretic Asset Return Modeling: A Novel Schr\u00f6dinger-Like Trading Equation and Multimodal Distribution",
        "authors": [
            "Li Lin"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "Quantum theory provides a comprehensive framework for quantifying uncertainty, often applied in quantum finance to explore the stochastic nature of asset returns. This perspective likens returns to microscopic particle motion, governed by quantum probabilities akin to physical laws. However, such approaches presuppose specific microscopic quantum effects in return changes, a premise criticized for lack of guarantee. This paper diverges by asserting that quantum probability is a mathematical extension of classical probability to complex numbers. It isn't exclusively tied to microscopic quantum phenomena, bypassing the need for quantum effects in returns.By directly linking quantum probability's mathematical structure to traders' decisions and market behaviors, it avoids assuming quantum effects for returns and invoking the wave function. The complex phase of quantum probability, capturing transitions between long and short decisions while considering information interaction among traders, offers an inherent advantage over classical probability in characterizing the multimodal distribution of asset returns.Utilizing Fourier decomposition, we derive a Schr\u00f6dinger-like trading equation, where each term explicitly corresponds to implications of market trading. The equation indicates discrete energy levels in financial trading, with returns following a normal distribution at the lowest level. As the market transitions to higher trading levels, a phase shift occurs in the return distribution, leading to multimodality and fat tails. Empirical research on the Chinese stock market supports the existence of energy levels and multimodal distributions derived from this quantum probability asset returns model.",
        "comments": " ",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05823"
    },
    {
        "doc_id": 361,
        "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis",
        "authors": [
            "Frank Xing"
        ],
        "subjects": [
            "Computation and Language",
            "Artificial Intelligence",
            "Multiagent Systems",
            "General Finance"
        ],
        "abstract": "Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed.",
        "comments": "15 pages",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05799"
    },
    {
        "doc_id": 362,
        "title": "Super-hedging-pricing formulas and Immediate-Profit arbitrage for market models under random horizon",
        "authors": [
            "Tahir Choulli",
            "Emmanuel Lepinette"
        ],
        "subjects": [
            "Mathematical Finance",
            "Optimization and Control",
            "Probability",
            "Pricing of Securities"
        ],
        "abstract": "In this paper, we consider the discrete-time setting, and the market model described by (S,F,T)$. Herein F is the ``public\" flow of information which is available to all agents overtime, S is the discounted price process of d-tradable assets, and T is an arbitrary random time whose occurrence might not be observable via F. Thus, we consider the larger flow G which incorporates F and makes T an observable random time. This framework covers the credit risk theory setting, the life insurance setting and the setting of employee stock option valuation. For the stopped model (S^T,G) and for various vulnerable claims, based on this model, we address the super-hedging pricing valuation problem and its intrinsic Immediate-Profit arbitrage (IP hereafter for short). Our first main contribution lies in singling out the impact of change of prior and/or information on conditional essential supremum, which is a vital tool in super-hedging pricing. The second main contribution consists of describing as explicit as possible how the set of super-hedging prices expands under the stochasticity of T and its risks, and we address the IP arbitrage for (S^T,G) as well. The third main contribution resides in elaborating as explicit as possible pricing formulas for vulnerable claims, and singling out the various informational risks in the prices' dynamics.",
        "comments": " ",
        "date": "11 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05713"
    },
    {
        "doc_id": 363,
        "title": "Boundary conditions at infinity for Black-Scholes equations",
        "authors": [
            "Yukihiro Tsuzuki"
        ],
        "subjects": [
            "Mathematical Finance",
            "Computational Finance"
        ],
        "abstract": "We propose numerical procedures for computing the prices of forward contracts where the underlying asset price is a Markovian local martingale. If the underlying process is a strict local martingale, multiple solutions exist for the corresponding Black-Scholes equations, and the derivative prices are characterized as the minimal solutions. Our prices are upper and lower bounds obtained using numerical methods on a finite grid under the respective boundary conditions. These bounds and the boundary values converge to the exact value as the underlying price approaches infinity. The proposed procedures are demonstrated through numerical tests.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05549"
    },
    {
        "doc_id": 364,
        "title": "Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market Wraps?",
        "authors": [
            "Baptiste Lefort",
            "Eric Benhamou",
            "Jean-Jacques Ohana",
            "David Saltiel",
            "Beatrice Guez",
            "Damien Challet"
        ],
        "subjects": [
            "Statistical Finance",
            "Artificial Intelligence"
        ],
        "abstract": "We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to 2023, reposted on large financial media, to determine how global news headlines may affect stock market movements using ChatGPT and a two-stage prompt approach. We document a statistically significant positive correlation between the sentiment score and future equity market returns over short to medium term, which reverts to a negative correlation over longer horizons. Validation of this correlation pattern across multiple equity markets indicates its robustness across equity regions and resilience to non-linearity, evidenced by comparison of Pearson and Spearman correlations. Finally, we provide an estimate of the optimal horizon that strikes a balance between reactivity to new information and correlation.",
        "comments": " ",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05447"
    },
    {
        "doc_id": 365,
        "title": "An adaptive network-based approach for advanced forecasting of cryptocurrency values",
        "authors": [
            "Ali Mehrban",
            "Pegah Ahadian"
        ],
        "subjects": [
            "Statistical Finance",
            "Computational Engineering, Finance, and Science",
            "Cryptography and Security",
            "Machine Learning"
        ],
        "abstract": "This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time.",
        "comments": "11 pages",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05441"
    },
    {
        "doc_id": 366,
        "title": "Multi-relational Graph Diffusion Neural Network with Parallel Retention for Stock Trends Classification",
        "authors": [
            "Zinuo You",
            "Pengju Zhang",
            "Jin Zheng",
            "John Cartlidge"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning",
            "Neural and Evolutionary Computing"
        ],
        "abstract": "Stock trend classification remains a fundamental yet challenging task, owing to the intricate time-evolving dynamics between and within stocks. To tackle these two challenges, we propose a graph-based representation learning approach aimed at predicting the future movements of multiple stocks. Initially, we model the complex time-varying relationships between stocks by generating dynamic multi-relational stock graphs. This is achieved through a novel edge generation algorithm that leverages information entropy and signal energy to quantify the intensity and directionality of inter-stock relations on each trading day. Then, we further refine these initial graphs through a stochastic multi-relational diffusion process, adaptively learning task-optimal edges. Subsequently, we implement a decoupled representation learning scheme with parallel retention to obtain the final graph representation. This strategy better captures the unique temporal features within individual stocks while also capturing the overall structure of the stock graph. Comprehensive experiments conducted on real-world datasets from two US markets (NASDAQ and NYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the effectiveness of our method. Our approach consistently outperforms state-of-the-art baselines in forecasting next trading day stock trends across three test periods spanning seven years. Datasets and code have been released (https://github.com/pixelhero98/MGDPR).",
        "comments": "5 pages, 2 figures. Author manuscript accepted for ICASSP 2024 (IEEE International Conference on Acoustics, Speech and Signal Processing)",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05430"
    },
    {
        "doc_id": 367,
        "title": "Introduction of L0 norm and application of L1 and C1 norm in the study of time-series",
        "authors": [
            "Victor Ujaldon Garcia"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "Four markets are considered: Cryptocurrencies / South American exchange rate / Spanish Banking indices and European Indices and studied using TDA (Topological Data Analysis) tools. These tools are used to predict and showcase both strengths and weakness of the current TDA tools. In this paper a new tool $L0$ norm is defined and complemented with the already existing $C1$ norm.",
        "comments": "14 pages 8 figures",
        "date": "30 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05423"
    },
    {
        "doc_id": 368,
        "title": "Multiple-bubble testing in the cryptocurrency market: a case study of bitcoin",
        "authors": [
            "Sanaz Behzadi",
            "Mahmonir Bayanati",
            "Hamed Nozari"
        ],
        "subjects": [
            "Statistical Finance"
        ],
        "abstract": "Economic periods and financial crises have highlighted the importance of evaluating financial markets to investors and researchers in recent decades.",
        "comments": " ",
        "date": "29 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05417"
    },
    {
        "doc_id": 369,
        "title": "On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors",
        "authors": [
            "Xinshuai Dong",
            "Haoyue Dai",
            "Yewen Fan",
            "Songyao Jin",
            "Sathyamoorthy Rajendran",
            "Kun Zhang"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning",
            "Methodology"
        ],
        "abstract": "Financial data is generally time series in essence and thus suffers from three fundamental issues: the mismatch in time resolution, the time-varying property of the distribution - nonstationarity, and causal factors that are important but unknown/unobserved. In this paper, we follow a causal perspective to systematically look into these three demons in finance. Specifically, we reexamine these issues in the context of causality, which gives rise to a novel and inspiring understanding of how the issues can be addressed. Following this perspective, we provide systematic solutions to these problems, which hopefully would serve as a foundation for future research in the area.",
        "comments": " ",
        "date": "12 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05414"
    },
    {
        "doc_id": 370,
        "title": "RIVCoin: an alternative, integrated, CeFi/DeFi-Vaulted Cryptocurrency",
        "authors": [
            "Roberto Rivera",
            "Guido Rocco",
            "Massimiliano Marzo",
            "Enrico Talin"
        ],
        "subjects": [
            "General Finance",
            "General Economics"
        ],
        "abstract": "This whitepaper introduces RIVCoin, a cryptocurrency built on Cosmos, fully stabilized by a diversified portfolio of both CeFi and DeFi assets, available in a digital, non-custodial wallet called RIV Wallet, that aims to provide Users an easy way to access the cryptocurrency markets, compliant to the strictest AML laws and regulations up to date. The token is a cryptocurrency at any time stabilized by a basket of assets: reserves are invested in a portfolio composed long term by 50% of CeFi assets, comprised of Fixed Income, Equity, Mutual and Hedge Funds and 50% of diversified strategies focused on digital assets, mainly staking and LP farming on the major, battle tested DeFi protocols. The cryptocurrency, as well as the dollar before Bretton Woods, is always fully stabilized by vaulted proof of assets: it is born and managed as a decentralized token, minted by a Decentralized Autonomous Organization, and entirely stabilized by assets evaluated by professional independent third parties. Users will trade, pool, and exchange the token without any intermediary, being able to merge them into a Liquidity Pool whose rewards will be composed by both the trading fees and the liquidity rewards derived from the reserve's seigniorage.\n  Users who wish and decide to pool RIVCoin in the Liquidity Pool will receive additional RIVCoin for themselves, and new RIVCoin are minted when the reserves increase in value or in case of purchase of new RIVCoin. The proposed model allows for alignment of incentives: decreasing the risk exposure by wealthier Users, but implicitly increasing that of smaller ones to a level perceived by them as still sustainable. Users indirectly benefit from the access to the rewards of sophisticated cryptocurrency portfolios hitherto precluded to them, without this turning into a disadvantage for the wealthy User.",
        "comments": " ",
        "date": "19 December, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05393"
    },
    {
        "doc_id": 371,
        "title": "Optimal Linear Signal: An Unsupervised Machine Learning Framework to Optimize PnL with Linear Signals",
        "authors": [
            "Pierre Renucci"
        ],
        "subjects": [
            "Statistical Finance",
            "Machine Learning"
        ],
        "abstract": "This study presents an unsupervised machine learning approach for optimizing Profit and Loss (PnL) in quantitative finance. Our algorithm, akin to an unsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL generated from signals constructed linearly from exogenous variables. The methodology employs a linear relationship between exogenous variables and the trading signal, with the objective of maximizing the Sharpe Ratio through parameter optimization. Empirical application on an ETF representing U.S. Treasury bonds demonstrates the model's effectiveness, supported by regularization techniques to mitigate overfitting. The study concludes with potential avenues for further development, including generalized time steps and enhanced corrective terms.",
        "comments": "The code of the model and the empiric strategy are available on my GitHub: Cnernc/OptimalLinearSignal",
        "date": "22 November, 2023",
        "pdf_url": "https://arxiv.org/pdf/2401.05337"
    },
    {
        "doc_id": 372,
        "title": "Comparison of Markowitz Model and Single-Index Model on Portfolio Selection of Malaysian Stocks",
        "authors": [
            "Zhang Chern Lee",
            "Wei Yun Tan",
            "Hoong Khen Koo",
            "Wilson Pang"
        ],
        "subjects": [
            "Portfolio Management"
        ],
        "abstract": "Our article is focused on the application of Markowitz Portfolio Theory and the Single Index Model on 10-year historical monthly return data for 10 stocks included in FTSE Bursa Malaysia KLCI, which is also our market index, as well as a risk-free asset which is the monthly fixed deposit rate. We will calculate the minimum variance portfolio and maximum Sharpe portfolio for both the Markowitz model and Single Index model subject to five different constraints, with the results presented in the form of tables and graphs such that comparisons between the different models and constraints can be made. We hope this article will help provide useful information for future investors who are interested in the Malaysian stock market and would like to construct an efficient investment portfolio. Keywords: Markowitz Portfolio Theory, Single Index Model, FTSE Bursa Malaysia KLCI, Efficient Portfolio",
        "comments": "19 pages, 5 figures",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05264"
    },
    {
        "doc_id": 373,
        "title": "A Mean Field Game between Informed Traders and a Broker",
        "authors": [
            "Philippe Bergault",
            "Leandro S\u00e1nchez-Betancourt"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Optimization and Control"
        ],
        "abstract": "We find closed-form solutions to the stochastic game between a broker and a mean-field of informed traders. In the finite player game, the informed traders observe a common signal and a private signal. The broker, on the other hand, observes the trading speed of each of his clients and provides liquidity to the informed traders. Each player in the game optimises wealth adjusted by inventory penalties. In the mean field version of the game, using a G\u00e2teaux derivative approach, we characterise the solution to the game with a system of forward-backward stochastic differential equations that we solve explicitly. We find that the optimal trading strategy of the broker is linear on his own inventory, on the average inventory among informed traders, and on the common signal or the average trading speed of the informed traders. The Nash equilibrium we find helps informed traders decide how to use private information, and helps brokers decide how much of the order flow they should externalise or internalise when facing a large number of clients.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05257"
    },
    {
        "doc_id": 374,
        "title": "On the Martingale Schr\u00f6dinger Bridge between Two Distributions",
        "authors": [
            "Marcel Nutz",
            "Johannes Wiesel"
        ],
        "subjects": [
            "Probability",
            "Mathematical Finance"
        ],
        "abstract": "We study a martingale Schr\u00f6dinger bridge problem: given two probability distributions, find their martingale coupling with minimal relative entropy. Our main result provides Schr\u00f6dinger potentials for this coupling. Namely, under certain conditions, the log-density of the optimal coupling is given by a triplet of real functions representing the marginal and martingale constraints. The potentials are also described as the solution of a dual problem.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05209"
    },
    {
        "doc_id": 375,
        "title": "Markowitz Portfolio Construction at Seventy",
        "authors": [
            "Stephen Boyd",
            "Kasper Johansson",
            "Ronald Kahn",
            "Philipp Schiele",
            "Thomas Schmelzer"
        ],
        "subjects": [
            "Portfolio Management",
            "Optimization and Control"
        ],
        "abstract": "More than seventy years ago Harry Markowitz formulated portfolio construction as an optimization problem that trades off expected return and risk, defined as the standard deviation of the portfolio returns. Since then the method has been extended to include many practical constraints and objective terms, such as transaction cost or leverage limits. Despite several criticisms of Markowitz's method, for example its sensitivity to poor forecasts of the return statistics, it has become the dominant quantitative method for portfolio construction in practice. In this article we describe an extension of Markowitz's method that addresses many practical effects and gracefully handles the uncertainty inherent in return statistics forecasting. Like Markowitz's original formulation, the extension is also a convex optimization problem, which can be solved with high reliability and speed.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.05080"
    },
    {
        "doc_id": 376,
        "title": "Scaling Laws And Statistical Properties of The Transaction Flows And Holding Times of Bitcoin",
        "authors": [
            "Didier Sornette",
            "Yu Zhang"
        ],
        "subjects": [
            "Trading and Market Microstructure"
        ],
        "abstract": "We study the temporal evolution of the holding-time distribution of bitcoins and find that the average distribution of holding-time is a heavy-tailed power law extending from one day to over at least $200$ weeks with an exponent approximately equal to $0.9$, indicating very long memory effects. We also report significant sample-to-sample variations of the distribution of holding times, which can be best characterized as multiscaling, with power-law exponents varying between $0.3$ and $2.5$ depending on bitcoin price regimes. We document significant differences between the distributions of book-to-market and of realized returns, showing that traders obtain far from optimal performance. We also report strong direct qualitative and quantitative evidence of the disposition effect in the Bitcoin Blockchain data. Defining age-dependent transaction flows as the fraction of bitcoins that are traded at a given time and that were born (last traded) at some specific earlier time, we document that the time-averaged transaction flow fraction has a power law dependence as a function of age, with an exponent close to $-1.5$, a value compatible with priority queuing theory. We document the existence of multifractality on the measure defined as the normalized number of bitcoins exchanged at a given time.",
        "comments": " ",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04702"
    },
    {
        "doc_id": 377,
        "title": "Proof of Efficient Liquidity: A Staking Mechanism for Capital Efficient Liquidity",
        "authors": [
            "Arman Abgaryan",
            "Utkarsh Sharma",
            "Joshua Tobkin"
        ],
        "subjects": [
            "General Finance"
        ],
        "abstract": "The Proof of Efficient Liquidity (PoEL) protocol, designed for specialised Proof of Stake (PoS) consensus-based blockchain infrastructures that incorporate intrinsic DeFi applications, aims to support sustainable liquidity bootstrapping and network security. This innovative mechanism efficiently utilises budgeted staking rewards to attract and sustain liquidity through a risk structuring engine and incentive allocation strategy, both of which are designed to maximise capital efficiency. The proposed protocol seeks to serve the dual objective of - (i) capital creation, by efficiently attracting risk capital, and maximising its operational utility for intrinsic DeFi applications, thereby asserting sustainability; and (ii) enhancing the adopting blockchain network's economic security, by augmenting their staking (PoS) mechanism with a harmonious layer seeking to attract a diversity of digital assets. Finally, in the appendix, we seek to generalise the financial incentivisation protocol to the notion of service fee credits, such that it utilises the network's auxiliary services as a means to propagate incentives to attract liquidity and facilitate the network to achieve the critical mass of usage necessary for sustained operations and growth.",
        "comments": " ",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04521"
    },
    {
        "doc_id": 378,
        "title": "Computing the Gerber-Shiu function with interest and a constant dividend barrier by physics-informed neural networks",
        "authors": [
            "Zan Yu",
            "Lianzeng Zhang"
        ],
        "subjects": [
            "Numerical Analysis",
            "Probability",
            "Risk Management"
        ],
        "abstract": "In this paper, we propose a new efficient method for calculating the Gerber-Shiu discounted penalty function. Generally, the Gerber-Shiu function usually satisfies a class of integro-differential equation. We introduce the physics-informed neural networks (PINN) which embed a differential equation into the loss of the neural network using automatic differentiation. In addition, PINN is more free to set boundary conditions and does not rely on the determination of the initial value. This gives us an idea to calculate more general Gerber-Shiu functions. Numerical examples are provided to illustrate the very good performance of our approximation.",
        "comments": "23 pages; 5 figures",
        "date": "9 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04378"
    },
    {
        "doc_id": 379,
        "title": "Expiring Assets in Automated Market Makers",
        "authors": [
            "Kenan Wood",
            "Maurice Herlihy",
            "Hammurabi Mendes",
            "Jonad Pulaj"
        ],
        "subjects": [
            "Computer Science and Game Theory",
            "Distributed, Parallel, and Cluster Computing",
            "Mathematical Finance",
            "Trading and Market Microstructure"
        ],
        "abstract": "An automated market maker (AMM) is a state machine that manages pools of assets, allowing parties to buy and sell those assets according to a fixed mathematical formula. AMMs are typically implemented as smart contracts on blockchains, and its prices are kept in line with the overall market price by arbitrage: if the AMM undervalues an asset with respect to the market, an \"arbitrageur\" can make a risk-free profit by buying just enough of that asset to bring the AMM's price back in line with the market.\n  AMMs, however, are not designed for assets that expire: that is, assets that cannot be produced or resold after a specified date. As assets approach expiration, arbitrage may not be able to reconcile supply and demand, and the liquidity providers that funded the AMM may have excessive exposure to risk due to rapid price variations.\n  This paper formally describes the design of a decentralized exchange (DEX) for assets that expire, combining aspects of AMMs and limit-order books. We ensure liveness and market clearance, providing mechanisms for liquidity providers to control their exposure to risk and adjust prices dynamically in response to situations where arbitrage may fail.",
        "comments": "33 pages",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04289"
    },
    {
        "doc_id": 380,
        "title": "Economic Forces in Stock Returns",
        "authors": [
            "Yue Chen",
            "Mohan Li"
        ],
        "subjects": [
            "General Economics",
            "Statistical Finance"
        ],
        "abstract": "When analyzing the components influencing the stock prices, it is commonly believed that economic activities play an important role. More specifically, asset prices are more sensitive to the systematic economic news that impose a pervasive effect on the whole market. Moreover, the investors will not be rewarded for bearing idiosyncratic risks as such risks are diversifiable. In the paper Economic Forces and the Stock Market 1986, the authors introduced an attribution model to identify the specific systematic economic forces influencing the market. They first defined and examined five classic factors from previous research papers: Industrial Production, Unanticipated Inflation, Change in Expected Inflation, Risk Premia, and The Term Structure. By adding in new factors, the Market Indices, Consumptions and Oil Prices, one by one, they examined the significant contribution of each factor to the stock return. The paper concluded that the stock returns are exposed to the systematic economic news, and they are priced with respect to their risk exposure. Also, the significant factors can be identified by simply adopting their model. Driven by such motivation, we conduct an attribution analysis based on the general framework of their model to further prove the importance of the economic factors and identify the specific identity of significant factors.",
        "comments": "11 pages, 10 figures",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.04132"
    },
    {
        "doc_id": 381,
        "title": "Decomposing Smiles: A Time Change Approach",
        "authors": [
            "Liexin Cheng",
            "Xue Cheng"
        ],
        "subjects": [
            "Pricing of Securities",
            "Mathematical Finance"
        ],
        "abstract": "We develop a novel time-change approach to study the shape of implied volatility smiles. The method is applicable to common semimartingale models, including jump-diffusion, rough volatility and infinite activity models. We approximate the at-the-money skew and curvature with an improved moment-based formula. The moments are further explicitly computed under a time change framework. The limiting skew and curvature for several models are considered. We also test the accuracy of the short-term approximation results on models via numerical methods and on empirical data. Finally, we apply the method to the calibration problem.",
        "comments": " ",
        "date": "24 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03776"
    },
    {
        "doc_id": 382,
        "title": "Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection",
        "authors": [
            "Georgios Fatouros",
            "Konstantinos Metaxas",
            "John Soldatos",
            "Dimosthenis Kyriazis"
        ],
        "subjects": [
            "Computational Finance",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Computation and Language",
            "Machine Learning"
        ],
        "abstract": "In the dynamic and data-driven landscape of financial markets, this paper introduces MarketSenseAI, a novel AI-driven framework leveraging the advanced reasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI incorporates Chain of Thought and In-Context Learning methodologies to analyze a wide array of data sources, including market price dynamics, financial news, company fundamentals, and macroeconomic reports emulating the decision making process of prominent financial investment teams. The development, implementation, and empirical validation of MarketSenseAI are detailed, with a focus on its ability to provide actionable investment signals (buy, hold, sell) backed by cogent explanations. A notable aspect of this study is the use of GPT-4 not only as a predictive tool but also as an evaluator, revealing the significant impact of the AI-generated explanations on the reliability and acceptance of the suggested investment signals. In an extensive empirical evaluation with S&P 100 stocks, MarketSenseAI outperformed the benchmark index by 13%, achieving returns up to 40%, while maintaining a risk profile comparable to the market. These results demonstrate the efficacy of Large Language Models in complex financial decision-making and mark a significant advancement in the integration of AI into financial analysis and investment strategies. This research contributes to the financial AI field, presenting an innovative approach and underscoring the transformative potential of AI in revolutionizing traditional financial analysis investment methodologies.",
        "comments": "15 pages, 12 figures, 12 tables",
        "date": "8 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03737"
    },
    {
        "doc_id": 383,
        "title": "Structured factor copulas for modeling the systemic risk of European and United States banks",
        "authors": [
            "Hoang Nguyen",
            "Audron\u0117 Virbickait\u0117",
            "M. Concepci\u00f3n Aus\u00edn",
            "Pedro Galeano"
        ],
        "subjects": [
            "Statistical Finance",
            "Applications"
        ],
        "abstract": "In this paper, we employ Credit Default Swaps (CDS) to model the joint and conditional distress probabilities of banks in Europe and the U.S. using factor copulas. We propose multi-factor, structured factor, and factor-vine models where the banks in the sample are clustered according to their geographic location. We find that within each region, the co-dependence between banks is best described using both, systematic and idiosyncratic, financial contagion channels. However, if we consider the banking system as a whole, then the systematic contagion channel prevails, meaning that the distress probabilities are driven by a latent global factor and region-specific factors. In all cases, the co-dependence structure of bank CDS spreads is highly correlated in the tail. The out-of-sample forecasts of several measures of systematic risk allow us to identify the periods of distress in the banking sector over the recent years including the COVID-19 pandemic, the interest rate hikes in 2022, and the banking crisis in 2023.",
        "comments": " ",
        "date": "7 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03443"
    },
    {
        "doc_id": 384,
        "title": "Modelling and Predicting the Conditional Variance of Bitcoin Daily Returns: Comparsion of Markov Switching GARCH and SV Models",
        "authors": [
            "Dennis Koch",
            "Vahidin Jeleskovic",
            "Zahid I. Younas"
        ],
        "subjects": [
            "Statistical Finance",
            "Risk Management"
        ],
        "abstract": "This paper introduces a unique and valuable research design aimed at analyzing Bitcoin price volatility. To achieve this, a range of models from the Markov Switching-GARCH and Stochastic Autoregressive Volatility (SARV) model classes are considered and their out-of-sample forecasting performance is thoroughly examined. The paper provides insights into the rationale behind the recommendation for a two-stage estimation approach, emphasizing the separate estimation of coefficients in the mean and variance equations. The results presented in this paper indicate that Stochastic Volatility models, particularly SARV models, outperform MS-GARCH models in forecasting Bitcoin price volatility. Moreover, the study suggests that in certain situations, persistent simple GARCH models may even outperform Markov-Switching GARCH models in predicting the variance of Bitcoin log returns. These findings offer valuable guidance for risk management experts, highlighting the potential advantages of SARV models in managing and forecasting Bitcoin price volatility.",
        "comments": " ",
        "date": "10 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03393"
    },
    {
        "doc_id": 385,
        "title": "Volatility models in practice: Rough, Path-dependent or Markovian?",
        "authors": [
            "Eduardo Abi Jaber",
            "Shaun",
            "Li"
        ],
        "subjects": [
            "Mathematical Finance",
            "Computational Finance",
            "Pricing of Securities"
        ],
        "abstract": "An extensive empirical study of the class of Volterra Bergomi models using SPX options data between 2011 and 2022 reveals the following fact-check on two fundamental claims echoed in the rough volatility literature:\n  Do rough volatility models with Hurst index $H \\in (0,1/2)$ really capture well SPX implied volatility surface with very few parameters? No, rough volatility models are inconsistent with the global shape of SPX smiles. They suffer from severe structural limitations imposed by the roughness component, with the Hurst parameter $H \\in (0,1/2)$ controlling the smile in a poor way. In particular, the SPX at-the-money skew is incompatible with the power-law shape generated by rough volatility models. The skew of rough volatility models increases too fast on the short end, and decays too slow on the longer end where \"negative\" $H$ is sometimes needed.\n  Do rough volatility models really outperform consistently their classical Markovian counterparts? No, for short maturities they underperform their one-factor Markovian counterpart with the same number of parameters. For longer maturities, they do not systematically outperform the one-factor model and significantly underperform when compared to an under-parametrized two-factor Markovian model with only one additional calibratable parameter.\n  On the positive side: our study identifies a (non-rough) path-dependent Bergomi model and an under-parametrized two-factor Markovian Bergomi model that consistently outperform their rough counterpart in capturing SPX smiles between one week and three years with only 3 to 4 calibratable parameters. \\end{abstract}",
        "comments": " ",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03345"
    },
    {
        "doc_id": 386,
        "title": "Negatively dependent optimal risk sharing",
        "authors": [
            "Jean-Gabriel Lauzier",
            "Liyuan Lin",
            "Ruodu Wang"
        ],
        "subjects": [
            "Theoretical Economics",
            "Risk Management"
        ],
        "abstract": "We analyze the problem of optimally sharing risk using allocations that exhibit counter-monotonicity, the most extreme form of negative dependence. Counter-monotonic allocations take the form of either \"winner-takes-all\" lotteries or \"loser-loses-all\" lotteries, and we respectively refer to these (normalized) cases as jackpot or scapegoat allocations. Our main theorem, the counter-monotonic improvement theorem, states that for a given set of random variables that are either all bounded from below or all bounded from above, one can always find a set of counter-monotonic random variables such that each component is greater or equal than its counterpart in the convex order. We show that Pareto optimal allocations, if they exist, must be jackpot allocations when all agents are risk seeking. We essentially obtain the opposite when all agents have discontinuous Bernoulli utility functions, as scapegoat allocations maximize the probability of being above the discontinuity threshold. We also consider the case of rank-dependent expected utility (RDU) agents and find conditions which guarantee that RDU agents prefer jackpot allocations. We provide an application for the mining of cryptocurrencies and show that in contrast to risk-averse miners, RDU miners with small computing power never join a mining pool. Finally, we characterize the competitive equilibria with risk-seeking agents, providing a first and second fundamental theorem of welfare economics where all equilibrium allocations are jackpot allocations.",
        "comments": "35 pages, 1 figure, Keywords: Pareto optimality, Risk sharing, Counter-monotonicity, Risk seeking, Rank-dependent expected utility, Cryptocurrency mining pools",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03328"
    },
    {
        "doc_id": 387,
        "title": "Optimal Order Execution subject to Reservation Strategies under Execution Risk",
        "authors": [
            "Xue Cheng",
            "Peng Guo",
            "Tai-ho Wang"
        ],
        "subjects": [
            "Trading and Market Microstructure"
        ],
        "abstract": "The paper addresses the problem of meta order execution from a broker-dealer's point of view in Almgren-Chriss model under order fill uncertainty. A broker-dealer agency is authorized to execute an order of trading on client's behalf. The strategies that the agent is allowed to deploy is subject to a benchmark, referred to as the reservation strategy, regulated by the client. We formulate the broker's problem as a utility maximization problem in which the broker seeks to maximize his utility of excess profit-and-loss at the execution horizon. Optimal strategy in feedback form is obtained in closed form. In the absence of execution risk, the optimal strategies subject to reservation strategies are deterministic. We establish an affine structure among the trading trajectories under optimal strategies subject to general reservation strategies using implementation shortfall and target close orders as basis. We conclude the paper with numerical experiments illustrating the trading trajectories as well as histograms of terminal wealth and utility at investment horizon under optimal strategies versus those under TWAP strategies.",
        "comments": " ",
        "date": "6 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.03305"
    },
    {
        "doc_id": 388,
        "title": "Synergistic Formulaic Alpha Generation for Quantitative Trading based on Reinforcement Learning",
        "authors": [
            "Hong-Gi Shin",
            "Sukhyun Jeong",
            "Eui-Yeon Kim",
            "Sungho Hong",
            "Young-Jin Cho",
            "Yong-Hoon Choi"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "Artificial Intelligence"
        ],
        "abstract": "Mining of formulaic alpha factors refers to the process of discovering and developing specific factors or indicators (referred to as alpha factors) for quantitative trading in stock market. To efficiently discover alpha factors in vast search space, reinforcement learning (RL) is commonly employed. This paper proposes a method to enhance existing alpha factor mining approaches by expanding a search space and utilizing pretrained formulaic alpha set as initial seed values to generate synergistic formulaic alpha. We employ information coefficient (IC) and rank information coefficient (Rank IC) as performance evaluation metrics for the model. Using CSI300 market data, we conducted real investment simulations and observed significant performance improvement compared to existing techniques.",
        "comments": "Accepted by ICOIN 2024",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02710"
    },
    {
        "doc_id": 389,
        "title": "Displaying risk in mergers: a diagrammatic approach for exchange ratio determination",
        "authors": [
            "Alessandra Mainini",
            "Enrico Moretto",
            "Daniela Visetti"
        ],
        "subjects": [
            "General Finance"
        ],
        "abstract": "This article extends, in a stochastic setting, previous results in the determination of feasible exchange ratios for merging companies. A first outcome is that shareholders of the companies involved in the merging process face both an upper and a lower bounds for acceptable exchange ratios. Secondly, in order for the improved `bargaining region' to be intelligibly displayed, the diagrammatic approach developed by Kulpa is exploited.",
        "comments": " ",
        "date": "5 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02681"
    },
    {
        "doc_id": 390,
        "title": "Constrained Max Drawdown: a Fast and Robust Portfolio Optimization Approach",
        "authors": [
            "Albert Dorador"
        ],
        "subjects": [
            "Portfolio Management",
            "Optimization and Control"
        ],
        "abstract": "We propose an alternative linearization to the classical Markowitz quadratic portfolio optimization model, based on maximum drawdown. This model, which minimizes maximum portfolio drawdown, is particularly appealing during times of financial distress, like during the COVID-19 pandemic. In addition, we will present a Mixed-Integer Linear Programming variation of our new model that, based on our out-of-sample results and sensitivity analysis, delivers a more profitable and robust solution with a 200 times faster solving time compared to the standard Markowitz quadratic formulation.",
        "comments": " ",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02601"
    },
    {
        "doc_id": 391,
        "title": "Opinion formation in the world trade network",
        "authors": [
            "C\u00e9lestin Coquid\u00e9",
            "Jos\u00e9 Lages",
            "Dima L. Shepelyansky"
        ],
        "subjects": [
            "Trading and Market Microstructure",
            "Statistical Mechanics",
            "Social and Information Networks",
            "Physics and Society"
        ],
        "abstract": "We extend the opinion formation approach to probe the world influence of economical organizations. Our opinion formation model mimics a battle between currencies within the international trade network. Based on the United Nations Comtrade database, we construct the world trade network for the years of the last decade from 2010 to 2020. We consider different core groups constituted by countries preferring to trade in a specific currency. We will consider principally two core groups, namely, 5 Anglo-Saxon countries which prefer to trade in US dollar and the 11 BRICS+ which prefer to trade in a hypothetical currency, hereafter called BRI, pegged to their economies. We determine the trade currency preference of the other countries via a Monte Carlo process depending on the direct transactions between the countries. The results obtained in the frame of this mathematical model show that starting from year 2014 the majority of the world countries would have preferred to trade in BRI than USD. The Monte Carlo process reaches a steady state with 3 distinct groups: two groups of countries preferring, whatever is the initial distribution of the trade currency preferences, to trade, one in BRI and the other in USD, and a third group of countries swinging as a whole between USD and BRI depending on the initial distribution of the trade currency preferences. We also analyze the battle between USD, EUR and BRI, and present the reduced Google matrix description of the trade relations between the Anglo-Saxon countries and the BRICS+.",
        "comments": "16 pages, 19 figures (including 9 figures present in Appendix section) and 1 table",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02378"
    },
    {
        "doc_id": 392,
        "title": "ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach",
        "authors": [
            "Zeynep Hilal Kilimci",
            "Mustafa Yalcin"
        ],
        "subjects": [
            "Biomolecules",
            "Artificial Intelligence",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "Anticancer peptides (ACPs) are a class of molecules that have gained significant attention in the field of cancer research and therapy. ACPs are short chains of amino acids, the building blocks of proteins, and they possess the ability to selectively target and kill cancer cells. One of the key advantages of ACPs is their ability to selectively target cancer cells while sparing healthy cells to a greater extent. This selectivity is often attributed to differences in the surface properties of cancer cells compared to normal cells. That is why ACPs are being investigated as potential candidates for cancer therapy. ACPs may be used alone or in combination with other treatment modalities like chemotherapy and radiation therapy. While ACPs hold promise as a novel approach to cancer treatment, there are challenges to overcome, including optimizing their stability, improving selectivity, and enhancing their delivery to cancer cells, continuous increasing in number of peptide sequences, developing a reliable and precise prediction model. In this work, we propose an efficient transformer-based framework to identify anticancer peptides for by performing accurate a reliable and precise prediction model. For this purpose, four different transformer models, namely ESM, ProtBert, BioBERT, and SciBERT are employed to detect anticancer peptides from amino acid sequences. To demonstrate the contribution of the proposed framework, extensive experiments are carried on widely-used datasets in the literature, two versions of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of proposed model enhances classification accuracy when compared to the state-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of accuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and 88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.",
        "comments": " ",
        "date": "4 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02124"
    },
    {
        "doc_id": 393,
        "title": "Forecasting Bitcoin Volatility: A Comparative Analysis of Volatility Approaches",
        "authors": [
            "Cristina Chinazzo",
            "Vahidin Jeleskovic"
        ],
        "subjects": [
            "Trading and Market Microstructure"
        ],
        "abstract": "This paper conducts an extensive analysis of Bitcoin return series, with a primary focus on three volatility metrics: historical volatility (calculated as the sample standard deviation), forecasted volatility (derived from GARCH-type models), and implied volatility (computed from the emerging Bitcoin options market). These measures of volatility serve as indicators of market expectations for conditional volatility and are compared to elucidate their differences and similarities. The central finding of this study underscores a notably high expected level of volatility, both on a daily and annual basis, across all the methodologies employed. However, it's crucial to emphasize the potential challenges stemming from suboptimal liquidity in the Bitcoin options market. These liquidity constraints may lead to discrepancies in the computed values of implied volatility, particularly in scenarios involving extreme moneyness or maturity. This analysis provides valuable insights into Bitcoin's volatility landscape, shedding light on the unique characteristics and dynamics of this cryptocurrency within the context of financial markets.",
        "comments": " ",
        "date": "3 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.02049"
    },
    {
        "doc_id": 394,
        "title": "Notes on the SWIFT method based on Shannon Wavelets for Option Pricing -- Revisited",
        "authors": [
            "Fabien Le Floc'h"
        ],
        "subjects": [
            "Computational Finance",
            "Numerical Analysis"
        ],
        "abstract": "This note revisits the SWIFT method based on Shannon wavelets to price European options under models with a known characteristic function in 2023. In particular, it discusses some possible improvements and exposes some concrete drawbacks of the method.",
        "comments": " ",
        "date": "7 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01758"
    },
    {
        "doc_id": 395,
        "title": "Text mining arXiv: a look through quantitative finance papers",
        "authors": [
            "Michele Leonardo Bianchi"
        ],
        "subjects": [
            "Digital Libraries",
            "Information Retrieval",
            "General Finance"
        ],
        "abstract": "This paper explores articles hosted on the arXiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research. Employing text mining techniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arXiv from 1997 to 2022. We extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain. Additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches.",
        "comments": " ",
        "date": "3 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01751"
    },
    {
        "doc_id": 396,
        "title": "Non-Atomic Arbitrage in Decentralized Finance",
        "authors": [
            "Lioba Heimbach",
            "Vabuk Pahari",
            "Eric Schertenleib"
        ],
        "subjects": [
            "Computational Engineering, Finance, and Science",
            "General Finance"
        ],
        "abstract": "The prevalence of maximal extractable value (MEV) in the Ethereum ecosystem has led to a characterization of the latter as a dark forest. Studies of MEV have thus far largely been restricted to purely on-chain MEV, i.e., sandwich attacks, cyclic arbitrage, and liquidations. In this work, we shed light on the prevalence of non-atomic arbitrage on decentralized exchanges (DEXes) on the Ethereum blockchain. Importantly, non-atomic arbitrage exploits price differences between DEXes on the Ethereum blockchain as well as exchanges outside the Ethereum blockchain (i.e., centralized exchanges or DEXes on other blockchains). Thus, non-atomic arbitrage is a type of MEV that involves actions on and off the Ethereum blockchain.\n  In our study of non-atomic arbitrage, we uncover that more than a fourth of the volume on Ethereum's biggest five DEXes from the merge until 31 October 2023 can likely be attributed to this type of MEV. We further highlight that only eleven searchers are responsible for more than 80% of the identified non-atomic arbitrage volume sitting at a staggering 137 billion US$ and draw a connection between the centralization of the block construction market and non-atomic arbitrage. Finally, we discuss the security implications of these high-value transactions that account for more than 10% of Ethereum's total block value and outline possible mitigations.",
        "comments": " ",
        "date": "15 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01622"
    },
    {
        "doc_id": 397,
        "title": "An arbitrage driven price dynamics of Automated Market Makers in the presence of fees",
        "authors": [
            "Joseph Najnudel",
            "Shen-Ning Tung",
            "Kazutoshi Yamazaki",
            "Ju-Yi Yen"
        ],
        "subjects": [
            "Mathematical Finance"
        ],
        "abstract": "We present a model for price dynamics in the Automated Market Makers (AMM) setting. Within this framework, we propose a reference market price following a geometric Brownian motion. The AMM price is constrained by upper and lower bounds, determined by constant multiplications of the reference price. Through the utilization of local times and excursion-theoretic approaches, we derive several analytical results, including its time-changed representation and limiting behavior.",
        "comments": " ",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01526"
    },
    {
        "doc_id": 398,
        "title": "Nash Equilibria in Greenhouse Gas Offset Credit Markets",
        "authors": [
            "Liam Welsh",
            "Sebastian Jaimungal"
        ],
        "subjects": [
            "General Finance",
            "Computational Finance",
            "Risk Management"
        ],
        "abstract": "In response to the global climate crisis, governments worldwide are introducing legislation to reduce greenhouse gas (GHG) emissions to help mitigate environmental catastrophes. One method to encourage emission reductions is to incentivize carbon capturing and carbon reducing projects while simultaneously penalising excess GHG output. Firms that invest in carbon capturing projects or reduce their emissions can receive offset credits (OCs) in return. These OCs can be used for regulatory purposes to offset their excess emissions in a compliance period. OCs may also be traded between firms. Thus, firms have the choice between investing in projects to generate OCs or to trade OCs. In this work, we present a novel market framework and characterise the optimal behaviour of GHG OC market participants in both single-player and two-player settings. We analyse both a single-period and multi-period setting. As the market model does not elicit a closed form solution, we develop a numerical methodology to estimate players' optimal behaviours in accordance to the Nash equilibria. Our findings indicate the actions players take are dependent on the scale of their project opportunities as well as their fellow market participants. We demonstrate the importance of behaving optimally via simulations in order to offset emission penalties and the importance of investing in GHG reducing or capturing projects from a financial perspective.",
        "comments": "MSC Class:          91G99; 35Q91; 91-08; 91A80; 91B74",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01427"
    },
    {
        "doc_id": 399,
        "title": "Accelerating Black-Box Molecular Property Optimization by Adaptively Learning Sparse Subspaces",
        "authors": [
            "Farshud Sorourifar",
            "Thomas Banker",
            "Joel A. Paulson"
        ],
        "subjects": [
            "Biomolecules",
            "Computational Engineering, Finance, and Science",
            "Machine Learning"
        ],
        "abstract": "Molecular property optimization (MPO) problems are inherently challenging since they are formulated over discrete, unstructured spaces and the labeling process involves expensive simulations or experiments, which fundamentally limits the amount of available data. Bayesian optimization (BO) is a powerful and popular framework for efficient optimization of noisy, black-box objective functions (e.g., measured property values), thus is a potentially attractive framework for MPO. To apply BO to MPO problems, one must select a structured molecular representation that enables construction of a probabilistic surrogate model. Many molecular representations have been developed, however, they are all high-dimensional, which introduces important challenges in the BO process -- mainly because the curse of dimensionality makes it difficult to define and perform inference over a suitable class of surrogate models. This challenge has been recently addressed by learning a lower-dimensional encoding of a SMILE or graph representation of a molecule in an unsupervised manner and then performing BO in the encoded space. In this work, we show that such methods have a tendency to \"get stuck,\" which we hypothesize occurs since the mapping from the encoded space to property values is not necessarily well-modeled by a Gaussian process. We argue for an alternative approach that combines numerical molecular descriptors with a sparse axis-aligned Gaussian process model, which is capable of rapidly identifying sparse subspaces that are most relevant to modeling the unknown property function. We demonstrate that our proposed method substantially outperforms existing MPO methods on a variety of benchmark and real-world problems. Specifically, we show that our method can routinely find near-optimal molecules out of a set of more than $>100$k alternatives within 100 or fewer expensive queries.",
        "comments": "9 pages, 2 figures consisting of 6 and 4 plots, accepted to NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World",
        "date": "2 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.01398"
    },
    {
        "doc_id": 400,
        "title": "Almost Perfect Shadow Prices",
        "authors": [
            "Eberhard Mayerhofer"
        ],
        "subjects": [
            "Portfolio Management",
            "Optimization and Control",
            "Probability"
        ],
        "abstract": "Shadow prices simplify the derivation of optimal trading strategies in markets with transaction costs by transferring optimization into a more tractable, frictionless market. This paper establishes that a na\u00efve shadow price Ansatz for maximizing long term returns given average volatility yields a strategy that is, for small bid-ask-spreads, asymptotically optimal at third order. Considering the second-order impact of transaction costs, such a strategy is essentially optimal. However, for risk aversion different from one, we devise alternative strategies that outperform the shadow market at fourth order. Finally, it is shown that the risk-neutral objective rules out the existence of shadow prices.",
        "comments": "15 pages",
        "date": "1 January, 2024",
        "pdf_url": "https://arxiv.org/pdf/2401.00970"
    }
]